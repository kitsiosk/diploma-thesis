 almost stable matchings in the hospitals residents problem with couples the hospitals   residents problem with couples  hrc  models the allocation of intending junior doctors to hospitals where couples are allowed to submit joint preference lists over pairs of  typically geographically close  hospitals  it is known that a stable matching need not exist  so we consider min bp hrc  the problem of finding a matching that admits the minimum number of blocking pairs  i e   is  as stable as possible    we show that this problem is np hard and difficult to approximate even in the highly restricted case that each couple finds only one hospital pair acceptable  however if we further assume that the preference list of each single resident and hospital is of length at most 2  we give a polynomial time algorithm for this case  we then present the first integer programming  ip  and constraint programming  cp  models for min bp hrc  finally  we discuss an empirical evaluation of these models applied to randomly generated instances of min bp hrc  we find that on average  the cp model is about 1 15 times faster than the ip model  and when presolving is applied to the cp model  it is on average 8 14 times faster  we further observe that the number of blocking pairs admitted by a solution is very small  i e   usually at most 1  and never more than 2  for the  28 000  instances considered  
 cognition and dynamical cognitive science several philosophers have expressed concerns with some recent uses of the term  cognition   underlying a number of these concerns are claims that cognition is only located in the brain and that no compelling case has been made to use  cognition  in any way other than as a cause of behavior that is representational in nature  these concerns center on two primary misapprehensions  first  that some adherents of dynamical cognitive science  dcs  think dcs implies the thesis of extended cognition and the rejection of representation  and second  that cognition is mistakenly equated with behavior  we make three points in response to these claims  first  there is no thoroughly entrenched conception of cognition as distinct from behavior that is being illegitimately disregarded  second  we present shapiro s  minds mach 23  353 375  2013  exposition of dynamical systems theory as revealing a misunderstanding of the way that dynamical models are used in explanations of cognition and related phenomena  accordingly  a proper conception of dcs s methods facilitates an appreciation of extended cognition as a legitimate phenomenon of scientific investigation  finally  we demonstrate that practicing cognitive scientists and psychologists are far more pluralistic in the phenomena they apply  cognition  to than is suggested by some  at the heart of our disagreement with these concerned folks is that although we think it likely that some cognitive phenomena are representational  non extended  and only in between the ears  we also think there are good conceptual and empirical reasons to believe that many cognitive phenomena are non representational  extended  and not confined to the brain  
 i spy  with my little sensor  fair data handling practices for robots between privacy  copyright and security the paper suggests an amendment to principle 4 of ethical robot design  and a demand for transparency by design  it argues that while misleading vulnerable users as to the nature of a robot is a serious ethical issue  other forms of intentionally deceptive or unintentionally misleading aspects of robotic design pose challenges that are on the one hand more universal and harmful in their application  on the other more difficult to address consistently through design choices  the focus will be on transparent design regarding the sensory capacities of robots  intuitive  low tech but highly efficient privacy preserving behaviour is regularly dependent on an accurate understanding of surveillance risks  design choices that hide  camouflage or misrepresent these capacities can undermine these strategies  however  formulating an ethical principle of sensor transparency is not straightforward  as openness can also lead to greater vulnerability and with that security risks  we argue that the discussion on sensor transparency needs to be embedded in a broader discussion of fair data handling principles for robots that involve issues of privacy  but also intellectual property rights such as copyright  
 2 tuple linguistic intuitionistic preference relation and its application in sustainable location planning voting system sustainability has increasingly become a major consideration in location planning  directing urban distribution  and regional development  this scenario can be regarded as a group decision making  gdm  voting system case  taking into account artificial thinking and the characteristics of the case  this paper introduces the 2 tuple linguistic intuitionistic preference relation  lipr  to resolve the gdm voting system circumstance  firstly  the archimedean t norm and t conorm based operational laws and the 2 tuple linguistic intuitionistic fuzzy weighted geometric  ats 2tlifwg  operator are developed to aggregate and rank linguistic intuitionistic fuzzy numbers  lifns   secondly  various concepts related to the 2 tuple lipr and geometric consistency are extended for further analysis  including the completely geometrically consistent 2 tuple lipr  the acceptably geometrically consistent 2 tuple lipr  and the consistency index  thirdly  motivated by the adverse impact of contradictory individual comparisons  we investigate a consistency modification algorithm to approximate the acceptable geometric consistency  fourthly  we consider a consensus reaching algorithm as a unanimous elimination technique in the gdm procedure  a gdm algorithm  involving a consistency modification process as well as a consensus reaching process  and overall value aggregation process  is embedded into the gdm voting system  finally  we conduct an illustrative example concerning smart city location planning  and employ the proposed algorithms in a comparison analysis to illustrate their feasibility and applicability  
 3d anthropometric algorithms for the estimation of measurements required for specialized garment design 3d anthropometry is key to the automation and facilitation of tedious  costly  and time consuming traditional anthropometric tasks such as the size designation of a specialized garment  this article provides a short survey of 3d anthropometry in garment design  the results show that due to a lack of detailed  complex analyses of the entire anthropometric procedure from a raw body scan to a set of anthropometric measurements  there was a need to prepare and describe a comprehensive system from scratch  the aim of this article is  therefore  to present the developed 3d anthropometric solution  along with fully documented algorithms for automatic measurement extraction from a 3d body scan  results of tests  and its validation  based on a list of desired measurements  a general processing path was prepared  its methods are described in detail  including algorithms for body segmentation  characteristic points localization  body landmarking   and final specific measurements of girth  arc  and linear lengths  widths and heights   furthermore  pseudo code for the algorithms and 3d description of the characteristic points is included in appendices a and b  the scanning procedure and posture recommendations are explained  finally  test results and analyses are presented  the validation dataset consisted of 40 subjects  21 male and 19 female volunteers  aged between 25 and 55 years  weighing from 55 to 105 kg and 150 to 205 cm tall  each of the subjects was measured once manually and three times automatically by both our system and the human solutions system to check the repeatability of the consecutive measurements  the performance of the 3d anthropometric system was evaluated by comparing its output with the manual measurements treated as the best possible ground truth  the output was also compared with the results from a leading commercial 3d anthropometric scanning solution provided by human solutions gmbh  the accuracy of both systems was measured using both the relative percent and millimeter difference error  consistency and absolute agreement between systems and manual measurements were evaluated using the intraclass correlation coefficient  icc  method  finally  comparability between the manual and automated measurements was assessed according to iso 20685  it appeared that for our system  only 5 out of 26 measurement types passed the highly demanding test from iso 20685 and for the human solutions system  3 out of 26 types passed the test  however  most of the accuracy  consistency  and absolute agreement results appeared to be on a satisfactory level according to garment design experts taking part in the project  girth measurements were characterized by the highest degree of consistency and absolute agreement  while arc length measurements were still acceptable  but least satisfactory  the degrees of general consistency and absolute agreement between our system and manual measurements fell into the excellent interval of the icc practical significance measure  these tests also showed high inter rater agreement between the proposed system and the commercial one  the human solutions product   it appeared that the current version of the system can provide results relatively close to those of the state of the art solution  however the system still needs further development and tests on a larger population  as it is not completely free of errors  particularly those resulting from improper body landmarking and inconsistent posture   c  2017 elsevier ltd  all rights reserved  
 3d parametric human face modeling for personalized product design  eyeglasses frame design case personalized design enhances the values added by a product or service by satisfying individual customer requirements  it has nowadays become a trend in consumer product development  this paper proposes a computational framework for personalized design of the eyeglasses frame based on parametric face modeling  a large amount of three dimensional facial models is collected by non contact scanning as training data  applying principal component analysis reduces the data complexity while preserving sufficient data variance  the reduced models are modified using cross parameterization so that they have the same mesh connectivity  kriging characterizes the correlation between the mesh point coordinates of a face model and a set of feature parameters  the kriging result synthesizes 3d facial geometry approximating to individual users with given parameter values  rendering the synthesized geometry with facial images of real persons generates realistic face models  these models not only allow adjusting the frame design in realtime  but also evaluating whether or how the design style fits individual face characteristics  this study enhances the practical values of 3d anthropometric data by realizing the concept of human centric design   c  2017 elsevier ltd  all rights reserved  
   joint gender  ethnicity and age estimation from 3d faces an experimental illustration of their correlations humans present clear demographic traits which allow their peers to recognize their gender and ethnic groups as well as estimate their age  abundant literature has investigated the problem of automated gender  ethnicity and age recognition from facial images  however  despite the co existence of these traits  most of the studies have addressed them separately  very little attention has been given to their correlations  in this work  we address the problem of joint demographic estimation and investigate the correlation through the morphological differences in 3d facial shapes  to this end  a set of facial features are extracted to capture the 3d shape differences among the demographic groups  then  a correlation based feature selection is applied to highlight salient features and remove redundancy  these features are later fed to random forest for gender and ethnicity classification  and age estimation  extensive experiments conducted on frgcv2 dataset  under expression dependent and expression independent settings  demonstrate the effectiveness of the proposed approaches for the three traits  and also show the accuracy improvement when considering their correlations  to the best of our knowledge  this is the first study exploring the correlations of these facial soft biometric traits using 3d faces  this is also the first work which studies the problem of age estimation from 3d faces  1   c  2017 elsevier b v  all rights reserved  
 a b2c e commerce intelligent system for re engineering the e order fulfilment process in today s world of digitization  the rise of the e commerce business around the globe has brought a tremendous change not only in our purchasing habits  but also to the entire retail and logistics industry  given the irregular e commerce order arrival patterns  limited time for order processing in e fulfilment centres  and the guaranteed delivery schedules offered by e retailers  such as same day or next day delivery upon placing an order  logistics service providers  lsps  must be extremely efficient in handling outsourced e commerce logistics orders  without re engineering the order fulfilment processes  the lsps are found to have difficulties in executing the order fulfilment process due to the tight handling requirements  this  in turn  delays the subsequent processes in the supply chain  such as last mile delivery operations  consequently affecting customer satisfaction towards both the retailer and the lsp  in view of the need to improve the efficiency in handling e commerce orders  this study aims at re engineering the fulfilment process of e commerce orders in distribution centres  the concept of warehouse postponement is embedded into a new cloud based e order fulfilment pre processing system  ceps   by incorporating the genetic algorithm  ga  approach for e commerce order grouping decision support and a rule based inference engine for generating operating guidelines and suggesting the use of appropriate handling equipment  through a case study conducted in a logistics company  the ceps provides order handling solutions for processing e commerce logistics orders very efficiently  with a significant reduction in order processing time and traveling distance  in turn  improved operating efficiency in e commerce order handling allows lsps to better align strategically with online retailers  who provide customers with aggressive  guaranteed delivery dates   c  2017 elsevier ltd  all rights reserved  
 a backtracking search hyper heuristic for the distributed assembly flow shop scheduling problem distributed assembly permutation flow shop scheduling problem  dapfsp  is recognized as an important class of problems in modern supply chains and manufacturing systems  in this paper  a backtracking search hyper heuristic  bs hh  algorithm is proposed to solve the dapfsp  in the bs hh scheme  ten simple and effective heuristic rules are designed to construct a set of low level heuristics  llhs   and the backtracking search algorithm is employed as the high level strategy to manipulate the llhs to operate on the solution space  additionally  an efficient solution encoding and decoding scheme is proposed to generate a feasible schedule  the effectiveness of the bs hh is evaluated on two typical benchmark sets and the computational results indicate the superiority of the proposed bs hh scheme over the state of the art algorithms  
 a banzhaf value for games with a proximity relation among the agents the banzhaf index is a function determining the power or influence in the decision of a set of agents  the extension of this index to the family of the cooperative games is named banzhaf value  the relationships of closeness among the agents should modify their power  games with a priori unions study situations where the closeness relations among the agents are taken into account  in this model the agents are organized in an a priori partition where each element of the partition represents a group of agents with close interests or ideas  the power is determined in two steps  first as a problem among the unions and later  inside each one  the power of each agent is determined  proximity relations extend this model considering leveled closeness among the agents  in this paper we analyze a version of the banzhaf value for games with a proximity relation and we show the interest of this value by applying it to the allocation of the power of the political groups in the european parliament   c  2017 elsevier inc  all rights reserved  
 a behavioral sequence analyzing framework for grouping students in an e learning system grouping of students benefits the formation of virtual learning communities  and contributes to collaborative learning space and recommendation  however  the existed grouping criteria are mainly limited in the learning portfolios  profiles  and social attributes etc  in this paper  we aim to build a unified framework for grouping students based on the behavioral sequences and further predicting which group a newcomer will be  the sequences are represented as a series of behavioral trajectories  we discuss a shape descriptor to approximately express the geometrical information of trajectories  and then capture the structural  micro  and hybrid similarities  a weighted undirected graph  using the sequence as a node  the relation as an edge  and the similarity as the weight  is constructed  on which we perform an extended spectral clustering algorithm to find fair groups  in the phase of prediction  an indexing and retrieval scheme is proposed to assign a newcomer to the corresponding group  we conduct some preliminary experiments on a real dataset to test the availability of the framework and to determine the parameterized conditions for an optimal grouping  additionally  we also experiment on the grouping prediction with a synthetic data generator  our proposed method outperforms the counterparts and makes grouping more meaningful   c  2016 elsevier b v  all rights reserved  
 a bi criteria evolutionary algorithm for a constrained multi depot vehicle routing problem most research about the vehicle routing problem  vrp  does not collectively address many of the constraints that real world transportation companies have regarding route assignments  consequently  our primary objective is to explore solutions for real world vrps with a heterogeneous fleet of vehicles  multi depot subcontractors  drivers   and pickup delivery time window and location constraints  we use a nested bi criteria genetic algorithm  ga  to minimize the total time to complete all jobs with the fewest number of route drivers  our model will explore the issue of weighting the objectives  total time vs  number of drivers  and provide pareto front solutions that can be used to make decisions on a case by case basis  three different real world data sets were used to compare the results of our ga vs  transportation field experts  job assignments  for the three data sets  all 21 pareto efficient solutions yielded improved overall job completion times  in 57    12 21  of the cases  the pareto efficient solutions also utilized fewer drivers than the field experts  job allocation strategies  
 a bibliometric based survey on ahp and topsis techniques in recent years  the employment of multiple criteria decision analysis  mcda  techniques in solving complex real world problems has increased exponentially  the willingness to build advanced decision models  with higher capabilities to support decision making in a wide range of applications  promotes the integration of mcda techniques with efficient systems such as intelligence and expert systems  geographic information systems  etc  amongst the most applied mcda techniques are analytic hierarchy process  ahp  and technique for order of preference by similarity to ideal solution  topsis   the development of a comprehensive perspective on research activities associated with the applications of these methods provides insights into the contributions of countries  institutes  authors and journals towards the advancements of these methods  furthermore  it helps in identifying the status and trends of research  this in turn will help researchers in shaping up and improving future research activities and investments  to meet these aims  a bibliometric analysis based on data harvested from scopus database was carried out to identify a set of bibliometric performance indicators  i e  quantitative indicators such as productivity  and qualitative indicators such as citations and hirsch index  h index    additionally  bibliometric visualization maps were employed to identify the hot spots of research  the total research output was 10 188 documents for ahp and 2412 documents for topsis  china took a leading position in ahp research  3513 documents  34 5    it was also the leading country in topsis research  846 documents  35 1    the most collaborated country in ahp research was the united states  while in case of topsis it was china  the united states had gained the highest h index  78  in ahp research  while in topsis it was taiwan with h index of 46  expert systems with applications journal was the most productive journal in ahp  204  2 0   and topsis research  125  5 2    simultaneously  university of tehran  iran and islamic azad university  iran were the most productive institutions in ahp  173  1 7   and topsis  115  4 8   research  simultaneously  the major hot topics that utilized ahp and will continue to be active include different applications of geographic information systems  risk modeling and supply chain management  while for topsis  they are supply chain management and sustainability research  overall  this analysis has shown increasing recognition of powerful of mcda techniques to support strategic decisions  the efficacy of these methods in the previous context promotes their progress and advancements   c  2017 elsevier ltd  all rights reserved  
 a bio statistical mining approach for classifying multivariate clinical time series data observed at irregular intervals in medical information system  the data that describe patient health records are often time stamped  these data are liable to complexities such as missing data  observations at irregular time intervals and large attribute set  due to these complexities  mining in clinical time series data  remains a challenging area of research  this paper proposes a bio statistical mining framework  named statistical tolerance rough set induced decision tree  strid   which handles these complexities and builds an effective classification model  the constructed model is used in developing a clinical decision support system  cdss  to assist the physician in clinical diagnosis  the strid framework provides the following functionalities namely temporal pre processing  attribute selection and classification  in temporal pre processing  an enhanced fuzzy inference based double exponential smoothing method is presented to impute the missing values and to derive the temporal patterns for each attribute  in attribute selection  relevant attributes are selected using the tolerance rough set  a classification model is constructed with the selected attributes using temporal pattern induced decision tree classifier  for experimentation  this work uses clinical time series datasets of hepatitis and thrombosis patients  the constructed classification model has proven the effectiveness of the proposed framework with a classification accuracy of 91 5  for hepatitis and 90 65  for thrombosis   c  2017 elsevier ltd  all rights reserved  
 a biologically inspired framework for visual information processing and an application on modeling bottom up visual attention an emerging trend in visual information processing is toward incorporating some interesting properties of the ventral stream in order to account for some limitations of machine learning algorithms  selective attention and cortical magnification are two such important phenomena that have been the subject of a large body of research in recent years  in this paper  we focus on designing a new model for visual acquisition that takes these important properties into account  we propose a new framework for visual information acquisition and representation that emulates the architecture of the primate visual system by integrating features such as retinal sampling and cortical magnification while avoiding spatial deformations and other side effects produced by models that tried to implement these two features  it also explicitly integrates the notion of visual angle  which is rarely taken into account by vision models  we argue that this framework can provide the infrastructure for implementing vision tasks such as object recognition and computational visual attention algorithms  to demonstrate the utility of the proposed vision framework  we propose an algorithm for bottom up saliency prediction implemented using the proposed architecture  we evaluate the performance of the proposed model on the mit saliency benchmark and show that it attains state of the art performance  while providing some advantages over other models  here is a summary of the main contributions of this paper   1  introducing a new bio inspired framework for visual information acquisition and representation that offers the following properties   a  providing a method for taking the distance between an image and the viewer into account  this is done by incorporating a visual angle parameter which is ignored by most visual acquisition models   b  reducing the amount of visual information acquired by introducing a new scheme for emulating retinal sampling and cortical magnification effects observed in the ventral stream   2  providing a concrete application of the proposed framework by using it as a substrate for building a new saliency based visual attention model  which is shown to attain state of the art performance on the mit saliency benchmark   3  providing an online git repository that implements the introduced framework that is meant to be developed as a scalable  collaborative project  
 a bionic incremental model of the fashion industry value chain based on a multicellular network with the development of the fashion industry  it is useful to analyze its value chain increment mechanism in different social environments and economic development situations  in this paper  we propose a bionic increment model of the fashion industry value chain  fivc  based on a multicellular network structure  first  we summarize the main factors influencing the increment in the fivc  and regard these as symbiotic biological tissue cells in a multicellular network structure  we then explore the relationships between the cells of different biological tissues in various simulated environments  furthermore  we analyze the characteristics of the fivc based on our multicellular network model  a simulated analysis based on a simplified six cell network is used to demonstrate the flexibility of the bionic increment model  in the six cell network  the profitability of an enterprise is determined by its four abilities   1  basic operational ability   2  human resource utilization  natural resource development and utilization ability   3  marketing ability  and  4  fashion brand influence  and scientific innovation ability  through the simulation results  we find that the multicellular tissue network model can effectively simulate the incremental process of the fivc  which conforms to the intuitive cognition  it can help an enterprise to predict and regulate its value chain increment  and thus has both practical and theoretical significance  
 a brain inspired method of facial expression generation using chaotic feature extracting bidirectional associative memory human cognitive system adapts many different environments by exhibiting a broad range of behaviors according to the context  these behaviors vary from general abstractions referred as prototypes to specific perceptual patterns referred as exemplars  a chaotic feature extracting associative memory is proposed to mimic human brain in generating prototype and exemplar facial expressions  this model automatically extracts features of each category of images related to a specific subject and expression  in the training phase  the features are extracted as fixed points  in recall phase  the output attractor of the network ranges from fixed point which results in a prototype facial image  to chaotic attractors which lead to generating exemplar faces  the generative model is applied to enrich a facial image dataset in terms of variability by generating various virtual patterns  in case that only one image per subject is provided  a face recognition task is implemented to compare the enriched and original dataset in training classifiers  our results show that recognition accuracy increases from 32 to 100  when exemplars generated by the proposed model are used to enrich the training dataset  
 a brief review of modeling approaches based on fuzzy time series recently  there seems to be increased interest in time series forecasting using soft computing  sc  techniques  such as fuzzy sets  artificial neural networks  anns   rough set  rs  and evolutionary computing  ec   among them  fuzzy set is widely used technique in this domain  which is referred to as  fuzzy time series  fts    in this survey  extensive information and knowledge are provided for the fts concepts and their applications in time series forecasting  this article reviews and summarizes previous research works in the fts modeling approach from the period 1993 2013  june   here  we also provide a brief introduction to sc techniques  because in many cases problems can be solved most effectively by integrating these techniques into different phases of the fts modeling approach  hence  several techniques that are hybridized with the fts modeling approach are discussed briefly  we also identified various domains specific problems and research trends  and try to categorize them  the article ends with the implication for future works  this review may serve as a stepping stone for the amateurs and advanced researchers in this domain  
 a case based reasoning system based on weighted heterogeneous value distance metric for breast cancer diagnosis objective  we present the implementation and application of a case based reasoning  cbr  system for breast cancer related diagnoses  by retrieving similar cases in a breast cancer decision support system  oncologists can obtain powerful information or knowledge  complementing their own experiential knowledge  in their medical decision making  methods  we observed two problems in applying standard cbr to this context  the abundance of different types of attributes and the difficulty in eliciting appropriate attribute weights from human experts  we therefore used a distance measure named weighted heterogeneous value distance metric  which can better deal with both continuous and discrete attributes simultaneously than the standard euclidean distance  and a genetic algorithm for learning the attribute weights involved in this distance measure automatically  we evaluated our cbr system in two case studies  related to benign malignant tumor prediction and secondary cancer prediction  respectively  result  weighted heterogeneous value distance metric with genetic algorithm for weight learning outperformed several alternative attribute matching methods and several classification methods by at least 3 4   reaching 0 938  0 883  0 933  and 0 984 in the first case study  and 0 927  0 842  0 939  and 0 989 in the second case study  in terms of accuracy  sensitivity x specificity  f measure  and area under the receiver operating characteristic curve  respectively  conclusion  the evaluation result indicates the potential of cbr in the breast cancer diagnosis domain   c  2017 elsevier b v  all rights reserved  
 a causal foundation for consciousness in biological and artificial agents traditional approaches model consciousness as the outcome either of internal computational processes or of cognitive structures  we advance an alternative hypothesis   consciousness is the hallmark of a fundamental way to organise causal interactions between an agent and its environment  thus consciousness is not a special property or an addition to the cognitive processes  but rather the way in which the causal structure of the body of the agent is causally entangled with a world of physical causes  the advantage of this hypothesis is that it suggests how to exploit causal coupling to envisage tentative guidelines for designing conscious artificial agents  in this paper  we outline the key characteristics of these causal building blocks and then a set of standard technologies that may take advantage of such an approach  consciousness is modelled as a kind of cognitive middle ground and experience is not an internal by product of cognitive processes but the external world that is carved out by means of causal interaction  thus  consciousness is not the penthouse on top of a 50 stores cognitive skyscraper  but the way in which the steel girders snap together from bottom to top   c  2016 elsevier b v  all rights reserved  
 a class of level 2 fuzzy decision making model with expected objectives and chance constraints  application to supply chain network design in this paper  we concentrate on dealing with a class of decision making problems with level 2 fuzzy coeffcients  we first discuss how to transform a level 2 fuzzy decision making model with expected objectives and chance constrained into crisp equivalent models  then an interactive fuzzy satisfying method is introduced to obtain the decision makers satisfying solution  in addition  the technique of level 2 simulations is applied to deal with general level 2 fuzzy models which are usually hard to be converted into their crisp equivalents  furthermore  based on the level 2 fuzzy programming  we focus on the supply chain network design problem where the total transport costs and the customer demands are assumed to be level 2 fuzzy numbers  a hybrid intelligent algorithm based on ga is used to solve the general supply chain design model  finally  numerical example and a case study are presented to illustrate the effectiveness of the model and the algorithm  
 a classification approach for detecting cross lingual biomedical term translations finding translations for technical terms is an important problem in machine translation  in particular  in highly specialized domains such as biology or medicine  it is difficult to find bilingual experts to annotate sufficient cross lingual texts in order to train machine translation systems  moreover  new terms are constantly being generated in the biomedical community  which makes it difficult to keep the translation dictionaries up to date for all language pairs of interest  given a biomedical term in one language  source language   we propose a method for detecting its translations in a different language  target language   specifically  we train a binary classifier to determine whether two biomedical terms written in two languages are translations  training such a classifier is often complicated due to the lack of common features between the source and target languages  we propose several feature space concatenation methods to successfully overcome this problem  moreover  we study the effectiveness of contextual and character n gram features for detecting term translations  experiments conducted using a standard dataset for biomedical term translation show that the proposed method outperforms several competitive baseline methods in terms of mean average precision and top k translation accuracy  
 a collaborative system for capturing and reusing in context design knowledge with an integrated representation model current research on design knowledge capture and reuse has predominantly focused on either the codification view of knowledge or the personalisation view of knowledge  resulting in a failure to address designers  knowledge needs caused by a lack of context of information and insufficient computational support  precisely motivated by this gap  this work aims to address the integration of these two views into a complete  contextual and trustworthy knowledge management scheme enabled by the emerging collaborative technologies  specifically  a knowledge model is developed to represent an integrated knowledge space  which can combine geometric model  knowledge based analysis codes and problem solving strategies and processes  on this basis  a smart collaborative system is also designed and developed to streamline the design process as well as to facilitate knowledge capture  retrieval and reuse as users with different roles are working on various tasks within this process  an engineering case study is undertaken to demonstrate the idea of collaborative knowledge creation and sharing and evaluate the effectiveness of the knowledge representation model and the collaborative technologies employed  as evidenced in the development and evaluation  the methods proposed are effective for capturing an integrated knowledge space and the collaborative knowledge management system not only facilitates problem solving using knowledge based analysis but also supplies in context tacit knowledge captured from the communications between users throughout the design process   c  2016 elsevier ltd  all rights reserved  
 a collaborative web based platform for the prescription of custom made insoles many foot pathologies are prevented or treated with custom made insoles  cmis   although a strong computerization has characterized the shoe development process during the last decade  the cmi sector still lacks a software platform integrating the design and diagnosis tools used by the stakeholders of this area  moreover  the prescription of cmis is only based on the experience of skilled podiatrists rather than on a common and shared knowledge  e g  guidelines  best practices  rules  etc    this paper presents a multi users and knowledge based platform  called smart prescription platform  spp   covering the whole cmi development phases  from foot diagnosis to the production  involving clinicians  patients  manufacturers and controllers  the web based platform is fully integrated with the technologies available in the orthopaedic sector  which are 3d 4d scanners  baropodometric platforms  footwear virtual catalogues  plantar pressure simulators  augmented reality devices and 3d cad systems  the use of standard file formats  e g  stl  bmp  xml  allows an electronic dataflow among the tools  the main module of the platform  called prescription system  ps   is used for prescribing custom made insoles for patients with different health conditions  satisfying the needs of all actors and optimizing the data exchange  ps is a knowledge based prescription system integrating the best practices related to the prescription of cmis  the ps output is a xml file representing the electronic order  used to exchange data with the other tools of the spp  the proposed platform has been tested with a twofold aim  to validate the usability of the prescription system and the inter operability of the platform tools  the positive results gathered during the validation  led the experts to start using the web platform for their daily work   c  2016 elsevier ltd  all rights reserved  
 a combination of active learning and self learning for named entity recognition on twitter using conditional random fields in recent years  many applications in natural language processing  nlp  have been developed using the machine learning approach  annotating data is an important task in applying machine learning to nlp applications  a common approach to improve the system performance is to train on a large and high quality set of training data that is annotated by experts  besides  active learning  al  and self learning can be utilized to reduce the annotation costs  the self learning method discovers highly reliable instances based on a trained classifier  while al queries the most informative instances based on active query algorithms  this paper proposes a method that combines al and self learning to reduce the labeling effort for the named entity recognition task from tweet streams by using both machine labeled and manually labeled data  we employ al queries based on the diversity of the context and content of instances to select the most informative instances  the conditional random fields are also chosen as an underlying model to train a classifier for selecting highly reliable instances  the experiments using twitter data show that the proposed method achieves good results in reducing the human labeling effort  and it can significantly improve the performance of the systems   c  2017 elsevier b v  all rights reserved  
 a comparative analysis of data preparation algorithms for customer churn prediction  a case study in the telecommunication industry data preparation is a process that aims to convert independent  categorical and continuous  variables into a form appropriate for further analysis  we examine data preparation alternatives to enhance the prediction performance for the commonly used logit model  this study  conducted in a churn prediction modeling context  benchmarks an optimized logit model against eight state of the art data mining techniques that use standard input data  including real world cross sectional data from a large european telecommunication provider  the results lead to following conclusions   i  analysts better acknowledge that the data preparation technique they choose actually affects churn prediction performance  we find improvements of up to 14 5 in the area under the receiving operating characteristics curve and 34  in the top decile lift   ii  the enhanced logistic regression also is competitive with more advanced single and ensemble data mining algorithms  this article concludes with some managerial implications and suggestions for further research  including evidence of the generalizability of the results for other business settings  c  2016 elsevier b v  all rights reserved  
 a comparative analysis of operational performance of cellular mobile telephone service providers in the delhi working area using an approach of fuzzy electre the purpose of this research paper is to develop a framework to analyze the operational performance of cellular mobile telephone service providers  cmtsp  in the delhi working area  india  delhi area has the highest teledensity in india  the present study uses a fuzzy electre  elimination and choice expressing reality  approach to compare the performance of cellular mobile telephone service providers  the data for the analysis have been taken from the telecom regulatory authority of india  trai   apriljune 2015  most of the data vary from one month to another  therefore  these small ranges of variation in the data are incorporated in this study using the fuzzy number  total six major telecom service providers are considered in this analysis  the findings of the study suggest that the performances of airtel and reliance communications are in the first rank  vodafone and idea are in the second rank  aircel is in the third rank  and mahanagar telephone nigam limited  mtnl  is outranked by all the other service providers  the performance is analyzed on the basis of three major parameters consisting six criteria  i e  network availability  base transceiver station accumulated downtime   connection accessibility  call setup success rate  channel congestion  and traffic channel congestion   and connection retainability  call drop rate and connection with good voice quality   it has been observed that mtnl is outranked due to the highest traffic channel congestion and highest call drop rate  using this framework  a decision maker can develop a strategy to improve the performance by benchmarking operational parameters  the research analysis is limited to only gsm  global system for mobile communication  service providers in the delhi working area  including ghaziabad  noida  faridabad  and gurgaon   c  2017 elsevier b v  all rights reserved  
 a comparative study of location based recommendation systems recent advancements in location based recommendation system  lbrs  and the availability of online applications  such as twitter  instagram  foursquare  path  and facebook have introduced new research challenges in the area of lbrs  use of content  such as geo tagged media  point location based  and trajectory based information help in connecting the gap between the online social networking services and the physical world  in this article  we present a systematic review of the scientific literature of lbrs and summarize the efforts and contributions proposed in the literature  we have performed a qualitative comparison of the existing techniques used in the area of lbrs  we present the basic filtration techniques used in lbrs followed by a discussion on the services and the location features the lbrs utilizes to perform the recommendations  the classification of criteria for recommendations and evaluation metrics are also presented  we have critically investigated the techniques proposed in the literature for lbrs and extracted the challenges and promising research topics for future work  
 a comparative study of machine learning classifiers for modeling travel mode choice the analysis of travel mode choice is an important task in transportation planning and policy making in order to understand and predict travel demands  while advances in machine learning have led to numerous powerful classifiers  their usefulness for modeling travel mode choice remains largely unexplored  using extensive dutch travel diary data from the years 2010 to 2012  enriched with variables on the built and natural environment as well as on weather conditions  this study compares the predictive performance of seven selected machine learning classifiers for travel mode choice analysis and makes recommendations for model selection  in addition  it addresses the importance of different variables and how they relate to different travel modes  the results show that random forest performs significantly better than any other of the investigated classifiers  including the commonly used multinomial logit model  while trip distance is found to be the most important variable  the importance of the other variables varies with classifiers and travel modes  the importance of the meteorological variables is highest for support vector machine  while temperature is particularly important for predicting bicycle and public transport trips  the results suggest that the analysis of variable importance with respect to the different classifiers and travel modes is essential for a better understanding and effective modeling of people s travel behavior   c  2017 elsevier ltd  all rights reserved  
 a comparative study of robust efficiency analysis and data envelopment analysis with imprecise data data envelopment analysis gauges the performance of operating entities in the best scenario for input and output multipliers  robust efficiency analysis is a conservative approach that is concerned with an assured level of performance for an entity across all possible multiplier scenarios  in this study  we extend the robust efficiency analysis procedure to the situation where precise information on some input and output data is unavailable  perfect efficiency analysis and potential efficiency analysis methods are developed to determine  respectively  the lower and upper bounds of an entity s robust efficiency rating  the concepts of robust efficiency are expanded to classify entities in consideration into three groups  perfectly robust efficient  potentially robust efficient and robust inefficient  two approaches are presented to convert robust efficiency analysis models into linear programs  it is claimed that data envelopment analysis and robust efficiency analysis together provide a comprehensive picture of an entity s relative efficiency  a computational experiment is conducted to compare the traditional efficiency analysis method with robust efficiency analysis in the presence of imprecise data  the results illustrate that perfect efficiency analysis exhibits a superior power of discrimination than potential efficiency analysis and that an entity recommended by perfect efficiency analysis has a satisfactory average performance   c  2017 elsevier ltd  all rights reserved  
 a comparative study on base classifiers in ensemble methods for credit scoring in the last years  the application of artificial intelligence methods on credit risk assessment has meant an improvement over classic methods  small improvements in the systems about credit scoring and bankruptcy prediction can suppose great profits  then  any improvement represents a high interest to banks and financial institutions  recent works show that ensembles of classifiers achieve the better results for this kind of tasks  in this paper  it is extended a previous work about the selection of the best base classifier used in ensembles on credit data sets  it is shown that a very simple base classifier  based on imprecise probabilities and uncertainty measures  attains a better trade off among some aspects of interest for this type of studies such as accuracy and area under roc curve  auc   the auc measure can be considered as a more appropriate measure in this grounds  where the different type of errors have different costs or consequences  the results shown here present to this simple classifier as an interesting choice to be used as base classifier in ensembles for credit scoring and bankruptcy prediction  proving that not only the individual performance of a classifier is the key point to be selected for an ensemble scheme   c  2016 elsevier ltd  all rights reserved  
 a comparison of bidding strategies for online auctions using fuzzy reasoning and negotiation decision functions bidders often feel challenged when looking for the best bidding strategies to excel in the competitive environment of multiple and simultaneous online auctions for same or similar items  bidders face complicated issues for deciding which auction to participate in  whether to bid early or late  and how much to bid  in this paper  we present the design of bidding strategies  which aim to forecast the bid amounts for buyers at a particular moment in time based on their bidding behavior and their valuation of an auctioned item  the agent develops a comprehensive methodology for final price estimation  which designs bidding strategies to address buyers  different bidding behaviors using two approaches  mamdani method with regression analysis and negotiation decision functions  the experimental results show that the agents who follow fuzzy reasoning with a regression approach outperform other existing agents in most settings in terms of their success rate and expected utility  
 a comprehensive analysis of bilingual lexicon induction bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages  in this article we present the most comprehensive analysis of bilingual lexicon induction to date  we present experiments on a wide range of languages and data sizes  we examine translation into english from 25 foreign languages  albanian  azeri  bengali  bosnian  bulgarian  cebuano  gujarati  hindi  hungarian  indonesian  latvian  nepali  romanian  serbian  slovak  somali  spanish  swedish  tamil  telugu  turkish  ukrainian  uzbek  vietnamese  and welsh  we analyze the behavior of bilingual lexicon induction on low frequency words  rather than testing solely on high frequency words  as previous research has done  low frequency words are more relevant to statistical machine translation  where systems typically lack translations of rare words that fall outside of their training data  we systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction  we provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity  we analyze the effects of frequency and burstiness  and the sizes of the seed bilingual dictionaries and the monolingual training corpora  additionally  we introduce a novel discriminative approach to bilingual lexicon induction  our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence  when feature weights are discriminatively set  these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion  e g   using minimum reciprocal rank   we also directly compare our model s performance against a sophisticated generative approach  the matching canonical correlation analysis  mcca  algorithm used by haghighi et al   2008   our algorithm achieves an accuracy of 42  versus mcca s 15   
 a comprehensive cluster and classification mining procedure for daily stock market return forecasting data mining and big data analytic techniques are playing an important role in many application fields  including the financial markets  however  only few studies have focused on predicting daily stock market returns  and among these studies  the data mining procedures utilized are either incomplete or inefficient  this paper presents a comprehensive data mining process to forecast the daily direction of the s p 500 index etf  spy  return based on 60 financial and economical features  the fuzzy c means method  fcm  is initially used to cluster the preprocessed data  a principal component analysis  pca  is applied next to the entire data set and each of seven clusters  the dimension of the entire cleaned data set is then reduced according to the combining results from the entire data set and each cluster  corresponding to different levels of the dimensionality reduction  twelve new data sets are generated from the entire cleaned data  artificial neural networks  anns  and logistic regression models are then used with the twelve transformed data sets for classification in order to forecast the daily direction of future market returns and indicate the efficiency of dimensionality reduction with pca  a group of hypothesis tests are performed over the classification and simulation results to show that the anns give significantly higher classification accuracy than logistic regression  and that the trading strategies guided by the comprehensive cluster and classification mining procedure based on pca and anns gain higher risk adjusted profits than the comparison benchmarks  as well as those strategies guided by the forecasts based on pca and logistic regression models   c  2017 elsevier b v  all rights reserved  
 a comprehensive review of smart wheelchairs  past  present  and future a smart wheelchair  sw  is a power wheelchair  pw  to which computers  sensors  and assistive technology are attached  in the past decade  there has been little effort to provide a systematic review of sw research  this paper aims to provide a complete state of the art overview of sw research trends  we expect that the information gathered in this study will enhance awareness of the status of contemporary pw as well as sw technology and increase the functional mobility of people who use pws  we systematically present the international sw research effort  starting with an introduction to pws and the communities they serve  then  we discuss in detail the sw and associated technological innovations with an emphasis on the most researched areas  generating the most interest for future research and development  we conclude with our vision for the future of sw research and how to best serve people with all types of disabilities  
 a computational analysis of general intelligence tests for evaluating cognitive development the progression in several cognitive tests for the same subjects at different ages provides valuable information about their cognitive development  one question that has caught recent interest is whether the same approach can be used to assess the cognitive development of artificial systems  in particular  can we assess whether the  fluid  or  crystallised  intelligence of an artificial cognitive system is changing during its cognitive development as a result of acquiring more concepts  in this paper  we address several iq tests problems  odd one out problems  raven s progressive matrices and thurstone s letter series  with a general learning system that is not particularly designed on purpose to solve intelligence tests  the goal is to better understand the role of the basic cognitive operational constructs  such as identity  difference  order  counting  logic  etc   that are needed to solve these intelligence test problems and serve as a proof of concept for evaluation in other developmental problems  from here  we gain some insights into the characteristics and usefulness of these tests and how careful we need to be when applying human test problems to assess the abilities and cognitive development of robots and other artificial cognitive systems   c  2017 elsevier b v  all rights reserved  
 a computational model for mining consumer perceptions in social media the proliferation of big data   analytics in recent years has compelled marketing practitioners to search for new methods when faced with assessing brand performance during brand equity appraisal  one of the challenges of current practices is that these methods rely heavily on traditional data collection and analysis methods such as questionnaires  and face to face or telephone interviews  which have a significant time lag  in this paper we introduce a computational model that combines topic and sentiment classification to elicit influential subjects from consumer perceptions in social media  our model devises a novel genetic algorithm to improve clustering of tweets in semantically coherent groups  which act as an essential prerequisite when searching for prevailing topics and sentiment in big pools of data  to illustrate the validity of our model  we apply it to the uber transportation network  from data collected through twitter for the period between january and april 2015  the results obtained present consumer perceptions and produce insights for two fundamental brand equity dimensions  brand awareness and brand meaning  simultaneously  they improve clustering results  in comparison to the k means approach   c  2016 elsevier b v  all rights reserved  
 a computational model for spatial navigation based on reference frames in the hippocampus  retrosplenial cortex  and posterior parietal cortex behavioral studies for humans  monkeys  and rats have shown that  while traversing an environment  these mammals tend to use different frames of reference and frequently switch between them  these frames represent allocentric  egocentric  or route centric views of the environment  however  combinations of either of them are often deployed  neurophysiological studies on rats have indicated that the hippocampus  the retrosplenial cortex  and the posterior parietal cortex contribute to the formation of these frames and mediate the transformation between those  in this paper  we construct a computational model of the posterior parietal cortex and the retrosplenial cortex for spatial navigation  we demonstrate how the transformation of reference frames could be realized in the brain and suggest how different brain areas might use these reference frames to form navigational strategies and predict under what conditions an animal might use a specific type of reference frame  our simulated navigation experiments demonstrate that the models results closely resemble behavioral findings in humans and rats  these results suggest that navigation strategies may depend on the animals reliance in a particular reference frame and shows how low confidence in a reference frame can lead to fluid adaptation and deployment of alternative navigation strategies  because of its flexibility  our biologically inspired navigation system may be applied to autonomous robots  
 a computational model of conditioning inspired by drosophila olfactory system recent studies have demonstrated that drosophila melanogaster  briefly drosophila  can successfully perform higher cognitive processes including second order olfactory conditioning  understanding the neural mechanism of this behavior can help neuroscientists to unravel the principles of information processing in complex neural systems  e g  the human brain  and to create efficient and robust robotic systems  in this work  we have developed a biologically inspired spiking neural network which is able to execute both first and second order conditioning  experimental studies demonstrated that volume signaling  e g  by the gaseous transmitter nitric oxide  contributes to memory formation in vertebrates and invertebrates including insects  based on the existing knowledge of odor encoding in drosophila  the role of retrograde signaling in memory function  and the integration of synaptic and non synaptic neural signaling  a neural system is implemented as simulated fly  simulated fly navigates in a two dimensional environment in which it receives odors and electric shocks as sensory stimuli  the model suggests some experimental research on retrograde signaling to investigate neural mechanisms of conditioning in insects and other animals  moreover  it illustrates a simple strategy to implement higher cognitive capabilities in machines including robots   c  2016 elsevier ltd  all rights reserved  
 a computational model to simulate development and recovery of traumatised patients in this paper  a computational model is presented to simulate emotional response of people after traumatising events  including their development  recovery  and the effect of group support  the model is built upon mechanisms known from cognitive and social neuroscience  using the model  several scenarios were explored  considering both individual and groups of people  the simulation results were validated on a dataset of symptoms and recovery of patients with ptsd  the obtained model enables simulation and analysis of emotional response evolution of diverse personality types  and how extensible are effects of group therapy on patients with ptsd  simulation results show that group therapy is positive but in the long term its effect reduces  suggesting that changing people in groups every period of time helps to keep group s atmosphere healthy  contributing to recovering of patients  the gain of group therapy also depends on the type of people present in the group which in some cases can be prejudicial to some members when dependent and toxic relations are formed between them   c  2017 elsevier b v  all rights reserved  
 a computer model of context dependent perception in a very simple world we propose the foundations of a computer model of scientific discovery that takes into account certain psychological aspects of human observation of the world  to this end  we simulate two main components of such a system  the first is a dynamic microworld in which physical events take place  and the second is an observer that visually perceives entities and events in the microworld  for reason of space  this paper focuses only on the starting phase of discovery  which is the relatively simple visual inputs of objects and collisions  
 a concept reduction approach for fuzzy cognitive map models in decision making and management policy making  strategic planning and management in general are complex decision making tasks  where the formulation of a quantitative mathematical model may be difficult or impossible due to lack of numerical data and dependence on imprecise verbal expressions  for such systems  knowledge representation graphs and cognitive maps are most familiar and often used for modelling complexity and aiding decision making  fuzzy cognitive maps  fcm   as graph based cognitive models  have been successfully used for knowledge representation and reasoning  in modelling complex systems usually a large number of concepts need to be considered  however  it is often difficult in real applications to find the appropriate number of concepts  using only a few concepts is not enough to represent the modelled system with the required precision  and increasing the number of concepts increases the complexity of the model quadratically  it is burdensome to work with for the experts  the contribution of this paper is two fold   i  to propose a new concept reduction approach for fcm and  ii  to apply it on developing less complex fcm for management and decision making  the behaviour of reduced models is analysed through a number of scenarios with respect to the original complex system  the main idea of the reduction is a clustering based on fuzzy tolerance relations  the new approach is focused on reducing complexity in the modelling process  which provides a more transparent and easy to use model for policy makers  the applicability of the proposed method is demonstrated via literature examples and a solid waste management case study that initiated this research  the results clearly show the advantageous characteristics of the proposed concept reduction method for fcm and its aid in policy making  
 a consolidated view of context for intelligent systems this paper s main objective is to consolidate the knowledge on context in the realm of intelligent systems  systems that are aware of their context and can adapt their behavior accordingly  we provide an overview and analysis of 36 context models that are heterogeneous and scattered throughout multiple fields of research  in our analysis  we identify five shared context categories  social context  location  time  physical context  and user context  in addition  we compare the context models with the context elements considered in the discourse on intelligent systems and find that the models do not properly represent the identified set of 3 741 unique context elements  as a result  we propose a consolidation of the findings from the 36 context models and the 3 741 unique context elements  the analysis reveals that there is a long tail of context categories that are considered only sporadically in context models  however  particularly these context elements in the long tail may be necessary for improving intelligent systems  context awareness  
 a constructionist philosophy of logic this paper develops and refines the suggestion that logical systems are conceptual artefacts that are the outcome of a design process by exploring how a constructionist epistemology and meta philosophy can be integrated within the philosophy of logic  
 a context aware researcher recommendation system for university industry collaboration on r d projects university industry collaboration plays an important role in the success of r d projects  one of the main challenges of university industry collaboration is the identification of suitable partners  due to the information asymmetry problem  it is difficult for companies to identify researchers from universities for collaboration on their r d projects  various expert recommendation systems  e g   question responder recommenders and co author recommenders  have been proposed  but they fail to characterize companies  needs in identifying suitable researchers  this paper proposes a context aware researcher recommendation system to encourage university industry collaboration on industrial r d projects  the system has two modules  an offline preparation module and an online recommendation module  in the offline preparation module  candidate researchers are identified in advance to improve the efficiency of the context aware recommendation  in the online recommendation module  contextual information  i e   r d projects  is captured from a social network platform  and then  candidate researchers are recommended based on a contextual trust analysis model  which combines the expertise relevance  quality  and trust relations of researchers to profile and evaluate candidate researchers for the r d project collaboration  an offline experiment and a user study are conducted to evaluate the effectiveness of the proposed recommendation system  the results show that the proposed method achieves better performance than the baseline methods   c  2017 elsevier b v  all rights reserved  
 a cooperative game theoretic approach to the social ridesharing problem in this work  we adopt a cooperative game theoretic approach in order to tackle the social ridesharing  sr  problem  where a set of commuters  connected through a social network  form coalitions and arrange one time rides at short notice  in particular  we address two fundamental aspects of this problem  first  we focus on the optimisation problem of forming the travellers  coalitions that minimise the travel cost of the overall system  to this end  we model the formation problem as a graph constrained coalition formation  gccf  one  where the set of feasible coalitions is restricted by a graph  i e   the social network   our approach allows users to specify both spatial and temporal preferences for the trips  second  we tackle the payment allocation aspect of sr   by proposing the first approach that computes kernel stable payments for systems with thousands of agents  we conduct a systematic empirical evaluation that uses real world datasets  i e   geolife and twitter   we are able to compute optimal solutions for medium sized systems  i e   with 100 agents   and high quality solutions for very large systems  i e   up to 2000 agents   our results show that our approach improves the social welfare  i e   reduces travel costs  by up to 36 22  with respect to the scenario with no ridesharing  finally  our payment allocation method computes kernel stable payments for 2000 agents in less than an hour while the state of the art is able to compute payments only for up to 100 agents  and does so 84 times slower than our approach   c  2017 elsevier b v  all rights reserved  
 a cooperative negotiation embedded nsga ii for solving an integrated product family and supply chain design problem with remanufacturing consideration product family design is a popular approach adopted by manufacturers to increase their product varieties in order to satisfy the needs of various markets  in recent years  because of increasing environmental concerns in societies and strict regulations of environmental protection  quite a number of manufacturers adopted remanufacturing strategy in their product development in response to the challenges  remanufacturing of used products unavoidably involves a closed loop supply chain system  to achieve the best outcomes  the supply chain design should be considered in product family design process  in this research  a multi objective optimization model of integrated product family and closed loop supply chain design is formulated based on a cooperative game model for minimizing manufacturer s total cost and maximize suppliers  total payoffs  since the optimization problem could be a large  scale one and involves mixed continuous discrete variables  a new version of nondominated sorting genetic algorithm ii  nsga ii   namely cooperative negotiation embedded nsga ii  nsga co   is proposed to solve the optimization model  simulation tests are conducted to validate the effectiveness of the proposed nsga co  the test results indicate that the proposed nsga co outperforms nsga ii in solving various scale of multi objective optimization problems in terms of convergence  with the formulated optimization model and the proposed nsga co  a case study of integrated product family and supply chain design is conducted to investigate the effects of environmental penalty  quantity of demand and marginal cost of remanufacturing on used product return rate  manufacturers  and suppliers  profits and joint payoff   c  2017 elsevier b v  all rights reserved  
 a copula based clustering algorithm to analyse eu country diets the aim of the paper is to explore the evolution of food diets in 40 european countries according to the common european policies and guidelines on healthy diets  to this end  an innovative clustering method  called coclust  has been adopted  by means of the copula function  this algorithm is able to find clusters based on the complex multivariate dependence structure of the data generating process  overcoming the limits of classical approaches that cope with only linear bivariate relationships  the analysed database contains information on the average calories from 16 food aggregates in 40 european countries observed over 40 years by the food and agriculture organisation of the united nations  fao   our findings suggest that european country diets are changing  individually or as a group  but not in a unique direction  central and eastern european countries are becoming unhealthier  while the tendency followed by the majority of the remaining countries is to integrate the common european guidelines on healthy  balanced  and diversified diets in their national policies   c  2017 elsevier b v  all rights reserved  
 a credibilistic decision support system for portfolio optimization in this paper  a decision support system  dss  for generating a suitable portfolio for an investor in an uncertain multi criteria framework is proposed  we model uncertain parameters like return and illiquidity of various assets using l r fuzzy numbers belonging to a power reference function family  such usage of l r fuzzy numbers is more generic as compared to the conventional triangular or trapezoidal fuzzy numbers and is a closer representation of uncertain behavior of the asset parameters  the credibility measure which has an advantage of being self dual as compared to usual possibility measure marks the uncertain context of the entire setup and adds a new dimension to existing studies  we use an  entropy cross entropy  ece  algorithm   for finding the solution of the optimization problem meant for finding the best fit l r fuzzy number corresponding to uncertain asset parameters  this automates the entire subjective exercise where  otherwise  a human intervention is required for feeding the parameters required for fitting l r fuzzy number  once the l r fuzzy number are created around various assets available for the portfolio formation  the portfolio optimization problem is solved using hybrid intelligent algorithm  hia   hia is designed by embedding fuzzy simulation within the  mibex sm  genetic algorithm  to demonstrate the entire solution approach  four portfolio optimization models are solved using historical data from the national stock exchange  nse  of india  the performance of the models is compared using a modified sharpe ratio in the fuzzy context  namely the  credibilistic sharpe ratio  crsr    to carry out the empirical study  firstly we use the data from the period of 2008 2013 for training the optimization models  thereafter  to test the performance of the models  the data set from the two year period 2013 2015 is used  the process of validation of the results is then carried out using the data set of the one year period from 2015 to 2016   c  2017 elsevier b v  all rights reserved  
 a data mining application to deposit pricing  main determinants and prediction models this study provides unique empirical evidence regarding the determinants of deposit pricing by employing data mining methods and making use of proprietary data provided by a commercial bank  results highlight the importance of taking into account customer  and account specific characteristics in the determination of deposit rates  contrary to existing evidence obtained from macro level bank data  the customer  level data used in this study suggest that depositors with a multi faceted and long term relationship with the same bank seem to benefit from higher deposit rates as a reward for being a core depositor  the location of the customer is also shown to have a limited effect on the deposit rates   c  2017 elsevier b v  all rights reserved  
 a decision support methodology for locating bank branches  a case study in turkey this paper presents an integrated decision support methodology for locating bank branches  the methodology is composed of two stages  problem structuring  and modeling  in the first stage  initially  a number of criteria are selected with the help of a detailed literature review and expert opinions  subsequently  importance weights of these criteria for different types of bank branches are identified based on judgments of experts for pairwise comparison questions  at the modeling stage  considering the characteristics and importance of the criteria  a novel multiobjective mathematical programming model is proposed to find specific locations of bank branches  the proposed methodology is applied in locating branches of a turkish bank  in order to test the validity of the results and robustness  a sensitivity analysis is conducted and the solutions are found to be robust  
 a decision support system for detecting serial crimes serial crimes pose a great threat to public security  linking crimes committed by the same offender can assist the detecting of serial crimes and is of great importance in maintaining public security  currently  most crime analysts still link serial crimes empirically especially in china and desire quantitative tools to help them  this paper presents a decision support system for crime linkage based on various  including behavioral  features of criminal cases  its underlying technique is pairwise classification based on similarity  which is interpretable and easy to tune  we design feature similarity algorithms to calculate the pairwise similarities and build up a classifier to determine whether a case pair should belong to a series  a comprehensive case study of a real world robbery dataset demonstrates its promising performance even with the default setting  this system has been deployed in a public security bureau of china and running for more than one year with positive feedback from users  the use of this system would provide individual officers with strong support in crimes investigation then allow law enforcement agency to save resources  since the system not only can link serial crimes automatically based on a elassification model learned from historical crime data  but also has flexibility in training data update and domain experts interaction  including adjusting the key components like similarity matrices and decision thresholds to reach a good tradeoff between caseload and number of true linked pairs   c  2017 elsevier b v  all rights reserved  
 a design for a common sense knowledge enhanced decision support system  integration of high frequency market data and real time news according to efficient markets theory  information is an important factor that affects market performance and serves as a source of first hand evidence in decision making  in particular with the rapid rise of internet technologies in recent years  however  a lack of knowledge and inference ability prevents current decision support systems from processing the wide range of available information  in this paper  we propose a common sense knowledge supported news model  compared with previous work  our model is the first to incorporate broad common sense knowledge into a decision support system  thereby improving the news analysis process through the application of a graphic random walk framework  prototype and experiments based on hong kong stock market data have demonstrated that common sense knowledge is an important factor in building financial decision models that incorporate news information  
 a design theory for cognitive workflow systems this paper addresses the design problem of providing cognitive support for workflow systems in software development  software development is demanding knowledge work that requires creativity and adaptability to changing requirements and situations  this type of work involves cognitive actions that require substantial support in several forms in order to address needs such as collaboration  communication  knowledge management  awareness and transparency  and the coordination and structuring of the development processes  the literature and our empirical results show that there is a lack of cognitive support in current workflow models  hence  we identify the need for a design theory for cognitive workflow systems  cws   in this paper  such a theory is presented  the proposed design theory for cws is validated through an action research intervention  this design theory has important implications from both research and practical perspectives  the results will help developers in their daily work  enhance the efficiency of the development processes  and facilitate decision making activities  
 a developmental learning approach of mobile manipulator via playing inspired by infant development theories  a robotic developmental model combined with game elements is proposed in this paper  this model does not require the definition of specific developmental goals for the robot  but the developmental goals are implied in the goals of a series of game tasks  the games are characterized into a sequence of game modes based on the complexity of the game tasks from simple to complex  and the task complexity is determined by the applications of developmental constraints  given a current mode  the robot switches to play in a more complicated game mode when it cannot find any new salient stimuli in the current mode  by doing so  the robot gradually achieves it developmental goals by playing different modes of games  in the experiment  the game was instantiated into a mobile robot with the playing task of picking up toys  and the game is designed with a simple game mode and a complex game mode  a developmental algorithm   lift constraint  act and saturate  is employed to drive the mobile robot move from the simple mode to the complex one  the experimental results show that the mobile manipulator is able to successfully learn the mobile grasping ability after playing simple and complex games  which is promising in developing robotic abilities to solve complex tasks using games  
 a direct consensus framework based on extended mccm for multiperson decision making problem with different preference representation structures this paper studies on multiperson decision making  mpdm  problem with five kinds of preference representation structures  namely preference orderings  utility functions  fuzzy preference relations  multiplicative preference relations and linguistic preference relations  we propose a direct consensus framework based on a novel minimum cost consensus model  mccm  which we extend to solve the consensus problem with multiple alternatives  this framework has two processes  selection process and consensus process  in the selection process  we first use five transformation functions to make the preference representation structures uniform  then standardize the transformed preference information and obtained the collective preference vector  in the consensus process  we define a consensus measure and design a feedback adjustment based on the extended mccm to achieve the preset consensus level  this framework first embeds mccm into a direct consensus framework to solve the mpdm problem with five kinds of preference representation structures including linguistic preference relations  finally  we provide an example to illustrate the effectiveness of the method  and find out the convergence is fast in the consensus process  
 a discourse based approach for arabic question answering the treatment of complex questions with explanatory answers involves searching for arguments in texts  because of the prominent role that discourse relations play in reflecting text producers  intentions  capturing the underlying structure of text constitutes a good instructor in this issue  from our extensive review  a system for automatic discourse analysis that creates full rhetorical structures in large scale arabic texts is currently unavailable  this is due to the high computational complexity involved in processing a large number of hypothesized relations associated with large texts  therefore  more practical approaches should be investigated  this article presents a new arabic text parser oriented for question answering systems dealing with  why  and  how to  questions  the text parser presented here considers the sentence as the basic unit of text and incorporates a set of heuristics to avoid computational explosion  with this approach  the developed question answering system reached a significant improvement over the baseline with a recall of 68  and mrr of 0 62  
 a domain independent data adhd student model for computer based educational systems  data analysis in higher education in traditional learning  teachers can easily have an understanding of how their students work and learn  however  in e learning it is more difficult for teachers to monitor how their students behave and learn in the system  as well as to identify if they have any specific awkwardness in their educational process  in this paper  a student model to infer the presence of adhd symptoms in a computer based educational system  also known as learning management systems  lms   is presented  the student model takes into account two types of students  characteristics  generic and psychological  each one is measured through a set of variables  which are correlated to obtain a final profile that can be useful to assist the teaching learning process  in order to reach this purpose  three web application tools that collect information about these characteristics have been developed  integrated into a lms and validated in a case study composed of 30 students  5 suffering from adhd  5 that present similar characteristics to adhd and 20 that supposed do not suffer from adhd   this case study was carried out through a quantitative research approach and a descriptive scope  results show that the implemented tools are useful to identify attention problems symptoms in students enrolled in e learning courses  
 a double copula stochastic frontier model with dependent error components and correction for sample selection in the standard stochastic frontier model with sample selection  the two components of the error term are assumed to be independent  and the joint distribution of the unobservable in the selection equation and the symmetric error term in the stochastic frontier equation is assumed to be bivariate normal  in this paper  we relax these assumptions by using two copula functions to model the dependences between the symmetric and inefficiency terms on the one hand  and between the errors in the sample selection and stochastic frontier equation on the other hand  several families of copula functions are investigated  and the best model is selected using the malice information criterion  aic   the methodology was applied to a sample of 200 rice farmers from northern thailand  the main findings are that  1  the double copula stochastic frontier model outperforms the standard model in terms of aic  and  2  the standard model underestimates the technical efficiency scores  potentially resulting in wrong conclusions and recommendations   c  2016 elsevier inc  all rights reserved  
 a dual decomposition of some rank dependent social evaluation functions in the context of the dual decomposition of the rank dependent social evaluation functions we examine the k pts principle introduced by gajdos and introduce a new property with balanced sensitivity to both tails of the distribution  in particular we analyse its implications for the s gini family  
 a dynamic programming operator for tour location problems applied to the covering tour problem this paper presents an evaluation operator for single trip vehicle routing problems where it is not necessary to visit all the nodes  such problems are known as tour location problems  the operator  called selector  is a dynamic programming algorithm that converts a given sequence of nodes into a feasible tour  the operator returns a subsequence of this giant tour which is optimal in terms of length  the procedure is implemented in an adaptive large neighborhood search to solve a specific tour location problem  the covering tour problem  this problem consists in finding a lowest cost hamiltonian cycle over a subset of nodes such that nodes outside the tour are within a given distance from a visited node  the metaheuristic proposed is competitive as shown by the quality of results evaluated using the output of a state of the art exact algorithm  
 a dynamic trading rule based on filtered flag pattern recognition for stock market price forecasting in this paper we propose and validate a trading rule based on flag pattern recognition  incorporating important innovations with respect to the previous research  firstly  we propose a dynamic window scheme that allows the stop loss and take profit to be updated on a quarterly basis  in addition  since the flag pattern is a trend following pattern  we have added the ema indicator to filter trades  this technical analysis indicator is calculated both for 15 min and 1 day timeframes  which enables short and medium terms to be considered simultaneously  we also filter the flags according to the price range on which they are developed and have limited the maximum loss of each trade to 100 points  the proposed methodology was applied to 91 309 intraday observations of the djia index  considerably improving the results obtained in the previous proposals and those obtained by the buy   hold strategy  both for profitability and risk  and also after taking into account the transaction costs  these results seem to challenge market efficiency in line with other similar studies  in the specific analysis carried out on the djia index and is also limited to the setup considered   c  2017 elsevier ltd  all rights reserved  
 a formal machine learning approach to generating human machine interfaces from task models user centered design  ucd  is an approach for creating human machine interfaces that are usable and support the human operator s tasks  ucd can be challenging because designers can fail to account for human machine interactions that occur due to the concurrency between the human and the other system elements  formal methods are tools that enable analysts to consider all of the possible system interactions using a combination of formal modeling  specification  and proof based verification  however  creating formal interface design models can be extremely difficult  this work describes a method that supports ucd by automatically generating formal designs of human machine interface behavior from task analytic models  the resulting interface design will always support the behavior captured in the task model  this paper describes the method and demonstrates its capabilities with three case studies  a light switch  a vending machine  and a patient controlled analgesia pump  the produced designs are validated with formal verifications to prove that they support their associated tasks  results and future research are discussed  
 a framework for designing socially assistive robot interactions robots are increasingly tested in different socially assistive scenarios  future applications range from dieting  coaching  tutoring to autism therapy  in such applications the success of the system is commonly evaluated by the ability to encourage the user to keep up with a task  hence  one important requirement for supportive systems is to have an interactional motivational model that formalizes the way how users can be assisted  in this paper we describe our framework for coordinating motivational interaction scenarios with socially assistive robots  sar  in the context of sport assistance  we exemplify three different sport scenarios where we have used the same motivational interaction model  furthermore  we show how this model can be used to systematically test the different aspects of motivation in the context of sar in sport domains  therefore  we have conducted an experiment to evaluate the importance of acknowledgement from sar for human interaction partners  the results show that users exercise longer if acknowledgment is included into the motivational model   c  2016 elsevier b v  all rights reserved  
 a framework for increasing the value of predictive data driven models by enriching problem domain characterization with novel features the need to leverage knowledge through data mining has driven enterprises in a demand for more data  however  there is a gap between the availability of data and the application of extracted knowledge for improving decision support  in fact  more data do not necessarily imply better predictive data driven marketing models  since it is often the case that the problem domain requires a deeper characterization  aiming at such characterization  we propose a framework drawn on three feature selection strategies  where the goal is to unveil novel features that can effectively increase the value of data by providing a richer characterization of the problem domain  such strategies involve encompassing context  e g   social and economic variables   evaluating past history  and disaggregate the main problem into smaller but interesting subproblems  the framework is evaluated through an empirical analysis for a real bank telemarketing application  with the results proving the benefits of such approach  as the area under the receiver operating characteristic curve increased with each stage  improving previous model in terms of predictive performance  
 a framework for the multimodal joint work of turn construction in face to face interaction we propose a framework for understanding the multimodal joint work of turn construction in face to face interaction  using concepts from conversation analysis  nonverbal communication  and gesture studies  in a qualitative analysis of face to face interaction  we observe that  collaboratively and in a joint work  participants produce moves  within the current speaker s turn  that allow them to deal with possible moves that could compromise the projectable trajectory of the interaction in progress  working at the micro level of interaction  we propose a framework that will allows a better understanding of how a turn can be collaboratively produced and how other levels of sequence organization can be produced in order to achieve the desired social agreement outcome   c  2016 elsevier b v  all rights reserved  
 a framework to design a human centred adaptive manufacturing system for aging workers the so called smart manufacturing systems  sms  combine smart manufacturing technologies  cyberphysical infrastructures  and data control to realize predictive and adaptive behaviours  in this context  industrial research focused mainly on improving the manufacturing system performance  almost neglecting human factors  hf  and their relation to the production systems  however  in order to create an effective smart factory context  human performance should be included to drive smart system adaptation in efficient and effective way  also by exploiting the linkages between tangible and intangible entities offered by industry 4 0  furthermore  modern companies are facing another interesting trend  aging workers  the age of workers is generally growing up and  consequently  the percentage of working 4564 years old population with different needs  capabilities  and reactions  is increasing  this research focuses on the design of human centred adaptive manufacturing systems  ams  for the modern companies  where aging workers are more and more common  in particular  it defines a methodology to design ams able to adapt to the aging workers  needs considering their reduced workability  due to both physical and cognitive functional decrease  with the final aim to improve the human machine interaction and the workers  wellbeing  the paper finally presents an industrial case study focusing on the woodworking sector  where an existing machine has been re designed to define a new human centred ams  the new machine has been engineered and prototyped by adopting cyber physical systems  cps  and pervasive technologies to smartly adapt the machine behaviour to the working conditions and the specific workers  skills  tasks  and cognitive physical abilities  with the final aim to support aging workers  the achieved benefits were expressed in terms of system usability  focusing on human interaction quality   c  2017 elsevier ltd  all rights reserved  
 a full ranking methodology in data envelopment analysis based on a set of dummy decision making units in this paper  we propose a new methodology for ranking decision making units in data envelopment analysis  dea   our approach is a benchmarking method  seeks a common set of weights using a proposed linear programming model and is based on the topsis approach in multiple attribute decision making  madm   to this end  five artificial or dummy decision making units  dmus  are defined  the ideal dmu  idmu   the anti ideal dmu  admu   the right ideal dmu  ridmu   the left anti ideal dmu  ladmu  and the average dmu  avdmu   we form two comprehensive indexes for the avdmu called the left relative closeness  lrc  and the right relative closeness  rrc  with respect to the ridmu and ladmu  the lrc and rrc indexes will be used in the new proposed linear programming model to estimate the common set of weights  the new efficiency of dmus and finally an overall ranking for all the dmus  the change of the ratio between lrc and rrc indexes is capable to be provoked alternative rankings  one of the best advantages of this model is that we can make a rationale ranking which is demonstrated by the realized correlation analysis  also  the new proposed efficiency score of the dmus is close to the efficiency score of the dea  ccr  methodology  three numerical examples are provided to illustrate the applicability of the new approach and the effectiveness of the new approach in dea ranking in comparison with other conventional ranking methods  also  an  error  analysis proves the robustness of the proposed methodology   c  2017 elsevier ltd  all rights reserved  
 a fuzzy decision making methodology based on fuzzy ahp and fuzzy topsis with a case study for information systems outsourcing decisions many companies outsource their information systems  iss  to share risk  reduce cost and achieve high level of performance quality  one of the most important steps in is outsourcing is selection of the best appropriate candidate information systems for outsourcing  to address such problem  the multiple criteria decision making  mcdm  methods could be applied  therefore two mcdm approaches consisting of the integrated fuzzy analytical hierarchy process  fahp  and fuzzy technique for order performance by similarity to ideal solution  ftopsis  for evaluating and selecting the appropriate information system project  isp  can be employed  the proposed method has been applied in an actual case  an online book store in iran  the fahp is used to analyze the structure of the outsourcing problem and determine weights of the criteria  and ftopsis method is used for final ranking of isps  finally to validate the obtained results  a sensitivity analysis is carried out  
 a fuzzy hybrid integrated framework for portfolio optimization in private banking decision making processes in private banking must comply with standards for risk management and transparency enforced by banking regulations  therefore  investors must be supported throughout a risk informed decision process  this paper contributes to the literature by presenting a hybrid integrated framework that considers personal features of the investor and additional characteristics imposed by regulations  for which linguistic evaluations are used with regard to risk exposure  the proposed approach for personal investment portfolios considers legal aspects and investor s preferences as an input to the novel fuzzy multiple attribute decision making approach for sorting problems proposed in this paper  called ftopsis class  then  the next step of the proposed framework uses the sorting results for a fuzzy multi objective optimization model that considers the risk and return associated with the investor s profile over three objectives  the contributions of this paper are illustrated and validated by using a numerical application in line with a new trend for modern portfolio theory which enables a real world investor s characteristics to be considered throughout the decision making process   c  2017 elsevier ltd  all rights reserved  
 a fuzzy indiscernibility based measure of distance between semantic spaces towards automatic evaluation of free text answers quantitative models built as tools for evaluating human language performance  can prove useful for both theoretical and applied areas of discourse comprehension  assessment  and education  the current paper offers a test case focused on mathematical articulation of a model and a test of that model with existing corpora  the model presented has been developed to evaluate free text answers of students based on the fuzzy indiscernibility between the semantic spaces created by the model answer and the learners  response  the model semantic space represented as a knowledge matrix can be constructed out of one or more model answers prepared by human experts and closely resemble the knowledge of the human evaluator  the proposed model finds out the indiscernibility between the types of word usage and scores the answer based on the fuzzy indiscernibility measures stored in a graded thesaurus prepared specifically for this purpose  the results returned on experimental data correlates well with human evaluators  this simulated learner friendly atmosphere  inspired by a teacher s intelligent benevolence  ensures effective attainment of learning objective in e learning environment  
 a fuzzy linguistic extended libqual plus model to assess service quality in academic libraries libqual  model is the best known method for the quality evaluation of library services  but it has two major drawbacks  first  to measure the quality  it is devised on a cardinal scale  the service levels range from 1 to 9  however  the standard representation of the concepts used by humans for communication is the natural language and  hence  users should express their judgments by using words instead of numbers  second  it considers that all users  opinions are equally important  nevertheless  users do not play an equal role in assessing the service quality  i e   the opinion given by some users should be more relevant than the opinion provided by others  to solve these drawbacks  we present an extended libqual  model representing the users  perceptions by using a fuzzy linguistic modeling and taking into account that users  opinions on the library services are not equally important  
 a fuzzy optimization method to select marketing strategies for new products based on similar cases successful new product launches are significant for company survival in intense competition environments  companies generally divide customers into different marketing segments to increase profits due to the growing focus on customer relationship management  crm   for new product launches  developing marketing strategies towards customers in different segments is a critical issue faced by companies  to address this issue  this paper proposes a fuzzy optimization method to select marketing strategies for new products based on similar cases  in the proposed method  a case database and a target case are constructed that consist of relevant data about historical products and a new product  respectively  then  historical similar cases are retrieved based on similarities and adapted according to the current situation  next  triangular fuzzy numbers are used to describe customers  responses to the marketing strategies  finally  a fuzzy integer linear model is constructed  by solving this model  marketing strategies for the new product towards customers in different segments can be obtained  a case study is provided to illustrate the potential for the practical application of the proposed methodology and several managerial implications are also noted  
 a fuzzy owen function on games with coalition structure and fuzzy coalitions in this paper  a game with coalition structure and fuzzy coalitions is introduced  which can be regarded as an extension of the game with coalition structure  firstly  an owen function is defined  then  the unique explicit form of the owen function is given by extending the owen value  secondly  as an extension of owen function  a fuzzy owen function is proposed  in addition  an explicit form of the fuzzy owen function is given by considering the fuzzy intermediate game with owen extension form  finally  an illustrate example is provided to illustrate the allocation method of gains  
 a fuzzy reasoning approach for assessing morningness of individuals using reduced version of morningness eveningness questionnaire in this article assessment of morningness of individuals has been performed using fuzzy reasoning approach  the responses are quantified using fuzzy numbers  based on experts  opinion a fuzzy rule base is prepared  the model is validated by considering responses of some students  selected randomly  and assessing their degree of morningness  the achieved results are compared with that of existing classical method  results show that proposed approach outperforms the existing classical approaches by capturing the inherent ambiguity and vagueness of morningness study  
 a fuzzy weighted average approach for selecting portfolio of new product development projects new product portfolio selection is a multi criteria decision making problem including both qualitative and quantitative criteria  determining the exact values for these criteria is often difficult or even impossible taking into account uncertainty and complexity associated with new prdduct development prdjects  to assist managers in making portfolio selection decisions  this study proposes a new project portfolio selection model that uses a fuzzy weighted average approach for ranking new product projects and artificial neural networks for estimating project performance  new product development projects are evaluated according to criteria related to marketing  project team  project performance  risk  and strategy  the use of neural networks enables more precise evaluation of project performance criteria and provides additional information in portfolio selection  a case study of the evaluation of new product projects illustrates the usefulness of the proposed approach  
 a game theoretic analysis of multichannel retail in the context of  showrooming  showrooming as a market phenomenon in multichannel retailing has grown in importance over the last few years  consumers nowadays use the brick and mortar store to research about a product before purchasing it online  this leads to the offline stores being converted into showrooms for the online retailers  therefore  popular notion suggests that showrooming should benefit the online retailer  in this paper  our objective is to analyze multichannel retailing under showrooming and determine the veracity of the popularly held belief  we develop a series of game theoretic models that involve a traditional retailer and an online retailer under showrooming  we determine optimal pricing strategies for each player and also the sales effort expended by the traditional retailer based on the interplay of  power  dynamics  market potential and the impact of showrooming  our results indicate that profit for the traditional as well as the online retailer decreases with rising levels of showrooming  hence  high levels of showrooming are not beneficial from the perspective of the online retailer  thus  contrary to popular intuition  lessening of showrooming benefits not only the traditional retailer but also the online retailer  nevertheless  from the consumer s point of view showrooming is beneficial as it leads to overall reduction in retail prices  we also analyze the viability of a click and mortar model as a strategy of the traditional retailer to counter the threat of showrooming   c  2017 elsevier b v  all rights reserved  
 a game theoretic approach to word sense disambiguation this article presents a new model for word sense disambiguation formulated in terms of evolutionary game theory  where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes  the words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose  we use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices  with this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory  maintaining the textual coherence  the model is based on two ideas  similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them  the article provides an in depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory  which is illustrated by an example  the conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state of the art systems  the results show that our model outperforms state of the art algorithms and can be applied to different tasks and in different scenarios  
 a general architecture for robotics systems  a perception based approach to artificial life departing from the conventional view of the reasons for the behavior of living systems  this research presents a radical and unique view of that behavior  as the observed side effects of a hierarchical set of simple  continuous  and dynamic negative feedback control systems  by way of an experimental model implemented on a real world autonomous robotic rover  rather than generating specific output from input  the systems control their perceptual inputs by varying output  the variables controlled do not exist in the environment  but are entirely internal perceptions constructed as a result of the layout and connections of the neural architecture  as the underlying processes are independent of the domain  the architecture is universal and thus has significant implications not only for understanding natural living systems  but also for the development of robotics systems  the central process of perceptual control has the potential to unify the behavioral sciences and is proposed as the missing behavioral principle of artificial life  
 a generalized fuzzy data envelopment analysis with restricted fuzzy sets and determined constraint condition the existing fuzzy data envelopment analysis  dea  models for evaluating the relative efficiencies of a set of entities called decision making units  dmus  converting multiple fuzzy inputs into multiple fuzzy outputs are limited to convex and normal fuzzy data  moreover  the constraint condition consists of the evaluated dmus  and efficiency is evaluated through transforming the fuzzy dea model into the crisp dea model by applying an alternative alpha cut technique  however  in many practical evaluation problems  the fuzzy data of inputs and outputs may be non convex or abnormal  and the evaluated dmu may not be contrasted with the dmus under evaluation  but with sample standards determined by production strategy  in such a case  the notion of restricted fuzzy set is proposed  the constraint condition is extended to a more general form  i e   the constraint condition  in which the sample dmus have similar behaviors  consists of one of sample standards determined by production strategy  the basic idea is to transform the fuzzy dea model to the conventional dea model by applying the formula of center of mass  and the relationship between the production frontier and fuzzy dea efficiency is given  to reduce the computational load  an algorithm based on the golden section method is employed to locate the standard that the evaluated dmu best fits  finally  a numerical example is provided to illustrate the proposed model  
 a genetic algorithm for permutation flowshop scheduling under practical make to order production system the aim of this work is to bridge the gap between the theory and actual practice of production scheduling by studying a problem from a real life production environment  this paper considers a practical sanitaryware production system as a number of make to order permutation flowshop problems  due to the wide range of variation in its products  real time arrival of customer orders  dynamic batch adjustments  and time for machine setup  sanitaryware production system is complex and also time sensitive  in practice  many such companies run with suboptimal solutions  to tackle this problem  in this paper  a memetic algorithm based real time approach has been proposed  numerical experiments based on real data are also been presented in this paper  
 a geometric principle of indifference that one s degrees of belief at any one time obey the axioms of probability theory is widely regarded as a necessary condition for static rationality  many theorists hold that it is also a sufficient condition  but according to critics this yields too subjective an account of static rationality  however  there are currently no good proposals as to how to obtain a tenable stronger probabilistic theory of static rationality  in particular  the idea that one might achieve the desired strengthening by adding some symmetry principle to the probability axioms has appeared hard to maintain  starting from an idea of carnap and drawing on relatively recent work in cognitive science  this paper argues that conceptual spaces provide the tools to devise an objective probabilistic account of static rationality  specifically  we propose a principle that derives prior degrees of belief from the geometrical structure of concepts   c  2016 elsevier b v  all rights reserved  
 a graph based semantic relatedness assessment method combining wikipedia features semantic relatedness assessment between concepts is a critical issue in many domains such as artificial intelligence  information retrieval  psychology  biology  linguistics and cognitive science  therefore  several methods assess relatedness by exploiting knowledge bases to express the semantics of concepts  however  there are some limitations such as high dimensional space  high computational complexity  fitting non dynamic domains  considering that wikipedia  a domain independent encyclopedic repository  which provides very large coverage  has been exploited by many methods as a huge semantic resource  in this paper  we propose a novel graph based relatedness assessment method using wikipedia features to avoid some of the limitations and drawbacks mentioned above  firstly  for each term in a word pair  the top k most relevant wikipedia concepts are returned by the naive esa algorithm to reduce the dimensional space of explicit semantic analysis  esa  method  secondly  for each different candidate concept in two relevant concept sets  we collect its categories set from the wikipedia category graph  wcg   based on the categories in wcg network  the relatedness between concepts at the correspondence position of the two sorted concept sets is computed as the association coefficient  thirdly  based on this parameter  a novel relatedness assessment metric is presented  the evaluation is performed on some datasets well recognized as benchmarks  using several widely used metrics and a new metric designed by ourselves  the result demonstrates that our method has a better correlation with the intuitions of human judgments than other related works   c  2017 elsevier ltd  all rights reserved  
 a group decision making support system in logistics and supply chain management purpose  the paper proposes a decision support system for selecting logistics providers based on the quality function deployment  qfd  and the technique for order preference by the similarity to ideal solution  topsis  for agricultural supply chain in france  the research provides a platform for group decision making to facilitate decision process and check the consistency of the outcomes  methodology  the proposed model looks at the decision problem from two points of view considering both technical and customer perspectives  the main customer criteria are confidence in a safe and durable product  emission of pollutants and hazardous materials  social responsibility  etc  the main technical factors are financial stability  quality  delivery condition  services  etc  based on the literature review  the second stage in the adopted methodology is the combination of quality function deployment and the technique for order preference by similarity to ideal solution to effectively analyze the decision problem  in final section we structure a group decision system called group system  grus  which has been developed by institut de recherche en informatique de toulouse  irit  in the toulouse university  results  this paper designs a group decision making system to interface decision makers and customer values in order to aid agricultural partners and investors in the selection of third party logistic providers  moreover  we have figured out a decision support system under fuzzy linguistic variables is able to assist agricultural parties in uncertain situations  this integrated and efficient decision support system enhances quality and reliability of the decision making  novelty originality  the novelty of this paper is reflected by several items  the integration of group multi criteria decision tools enables decision makers to obtain a comprehensive understanding of customer needs and technical requirements of the logistic process  in addition  this investigation is carried out under a european commission project called risk and uncertain conditions for agriculture production systems  ruc aps  which models risk reduction and elimination from the agricultural supply chain  ultimately  we have implemented the decision support tool to select the best logistic provider among france logistics and transportation companies   c  2017 elsevier ltd  all rights reserved  
 a heuristic initialized stochastic memetic algorithm for mdpvrp with interdependent depot operations the vehicle routing problem  vrp  is a widely studied combinatorial optimization problem  we introduce a variant of the multidepot and periodic vrp  mdpvrp  and propose a heuristic initialized stochastic memetic algorithm to solve it  the main challenge in designing such an algorithm for a large combinatorial optimization problem is to avoid premature convergence by maintaining a balance between exploration and exploitation of the search space  we employ intelligent initialization and stochastic learning to address this challenge  the intelligent initialization technique constructs a population by a mix of random and heuristic generated solutions  the stochastic learning enhances the solutions  quality selectively using simulated annealing with a set of random and heuristic operators  the hybridization of randomness and greediness in the initialization and learning process helps to maintain the balance between exploration and exploitation  our proposed algorithm has been tested extensively on the existing benchmark problems and outperformed the baseline algorithms by a large margin  we further compared our results with that of the state of the art algorithms working under mdpvrp formulation and found a significant improvement over their results  
 a hidden markov model approach to the problem of heuristic selection in hyper heuristics with a case study in high school timetabling problems operations research is a well established field that uses computational systems to support decisions in business and public life  good solutions to operations research problems can make a large difference to the efficient running of businesses and organisations and so the field often searches for new methods to improve these solutions  the high school timetabling problem is an example of an operations research problem and is a challenging task which requires assigning events and resources to time slots subject to a set of constraints  in this article  a new sequence based selection hyper heuristic is presented that produces excellent results on a suite of high school timetabling problems  in this study  we present an easy to implement  easy to maintain  and effective sequence based selection hyper heuristic to solve high school timetabling problems using a benchmark of unified real world instances collected from different countries  we show that with sequence based methods  it is possible to discover new best known solutions for a number of the problems in the timetabling domain  through this investigation  the usefulness of sequence based selection hyper heuristics has been demonstrated and the capability of these methods has been shown to exceed the state of the art  
 a hierarchical predictive coding model of object recognition in natural images predictive coding has been proposed as a model of the hierarchical perceptual inference process performed in the cortex  however  results demonstrating that predictive coding is capable of performing the complex inference required to recognise objects in natural images have not previously been presented  this article proposes a hierarchical neural network based on predictive coding for performing visual object recognition  this network is applied to the tasks of categorising hand written digits  identifying faces  and locating cars in images of street scenes  it is shown that image recognition can be performed with tolerance to position  illumination  size  partial occlusion  and within category variation  the current results  therefore  provide the first practical demonstration that predictive coding  at least the particular implementation of predictive coding used here  the pc bc dim algorithm  is capable of performing accurate visual object recognition  
 a historical survey of algorithms and hardware architectures for neural inspired and neuromorphic computing applications biological neural networks continue to inspire new developments in algorithms and microelectronic hardware to solve challenging data processing and classification problems  here  we survey the history of neural inspired and neuromorphic computing in order to examine the complex and intertwined trajectories of the mathematical theory and hardware developed in this field  early research focused on adapting existing hardware to emulate the pattern recognition capabilities of living organisms  contributions from psychologists  mathematicians  engineers  neuroscientists  and other professions were crucial to maturing the field from narrowly tailored demonstrations to more generalizable systems capable of addressing difficult problem classes such as object detection and speech recognition  algorithms that leverage fundamental principles found in neuroscience such as hierarchical structure  temporal integration  and robustness to error have been developed  and some of these approaches are achieving world leading performance on particular data classification tasks  in addition  novel microelectronic hardware is being developed to perform logic and to serve as memory in neuromorphic computing systems with optimized system integration and improved energy efficiency  key to such advancements was the incorporation of new discoveries in neuroscience research  the transition away from strict structural replication and towards the functional replication of neural systems  and the use of mathematical theory frameworks to guide algorithm and hardware developments   c  2016 elsevier b v  all rights reserved  
 a holistic approach for performance evaluation using quantitative and qualitative data  a food industry case study sustainability of business is often reliant on the efficiency of its internal and external operations as well as on overall customer satisfaction  evaluating overall operational performance via utilizing both qualitative and quantitative information is an essential first step towards a sustainable and reliable business environment  with this motivation  this study proposes a hybrid approach combining fuzzy ahp  dea and topsis methodologies for retail performance evaluation  a food industry case study is considered to illustrate its implementation   c  2017 elsevier ltd  all rights reserved  
 a human machine centered design method for  powered  lower limb prosthetics this paper proposes a human machine centered approach to lower limb prosthetic design  the approach is based on a profound analysis and modeling of human factors from user and expert survey data  with this knowledge  user demands are considered in the prioritization of technical requirements  to evaluate the design framework  it is applied to the example of the design of a powered prosthetic knee  key result of this application are a distinct changes in technical requirement priorities that might yield completely different prosthetic designs  thereby  the potential of the proposed method is substantiated while a practical evaluation is aspect to future studies  beyond this  the method is easily transferable to other robotic devices operating close to their users  e g   exoskeletons or teleoperators   c  2017 elsevier b v  all rights reserved  
 a hybrid approach to decision making and information fusion  combining humans and artificial agents this paper argues that hybrid human agent systems can support powerful solutions to relevant problems such as environmental crisis management  however  it shows that such solutions require comprehensive approaches covering different aspects of data processing  model construction and the usage  in particular  the solutions  i  must be able to cope with complex correlations  as different data sources are used  and processing of large amounts of data   ii  must be robust against modeling imperfections and  iii  human machine interaction  hmi  approaches must facilitate human use of crisis management tools and reduce the likelihood of miscommunication  in this paper the relevant problem is an environmental protection application involving the detection and tracking of gases in case of chemical spills in an urban area  we show that a combination of bayesian networks  agent paradigm and systematic approaches to implementing hmi  support effective and robust solutions  to better integrate human information and demonstrate the usefulness of user generated crisis response information we developed a social media harvesting interface based on data from twitter tweets and a visual interface to facilitate human smell classification   c  2016 elsevier b v  all rights reserved  
 a hybrid artificial bee colony algorithm for the cooperative maximum covering location problem this paper proposes a hybrid artificial bee colony algorithm for the cooperative maximum covering location problem  cmclp  on a network  in location covering problems  it is assumed that each facility generates a signal whose strength decreases with the increase in distance and a demand point is considered to be covered if the total signal strength received by it from various facilities exceeds a certain threshold  the objective of the cmclp is to locate the facilities in such a way that maximizes the total demands covered  the proposed hybrid approach obtained better quality solutions in comparison to the methods available in the literature  
 a hybrid artificial bee colony for optimizing a reverse logistics network system this paper proposes a hybrid discrete artificial bee colony  hdabc  algorithm for solving the location allocation problem in reverse logistics network system  in the proposed algorithm  each solution is represented by two vectors  i e   a collection point vector and a repair center vector  eight well designed neighborhood structures are proposed to utilize the problem structure and can thus enhance the exploitation capability of the algorithm  a simple but efficient selection and update approach is applied to the onlooker bee to enhance the exploitation process  a scout bee applies different local search methods to the abandoned solution and the best solution found so far  which can increase the convergence and the exploration capabilities of the proposed algorithm  in addition  an enhanced local search procedure is developed to further improve the search capability  finally  the proposed algorithm is tested on sets of large scale randomly generated benchmark instances  through the analysis of experimental results  the highly effective performance of the proposed hdbac algorithm is shown against several efficient algorithms from the literature  
 a hybrid automated trading system based on multi objective grammatical evolution this paper describes a hybrid automated trading system  ats  based on grammatical evolution and microeconomic analysis  the proposed system takes advantage from the flexibility of grammars for introducing and testing novel characteristics  the ats introduces the self generation of new technical indicators and multi strategies for stopping unforeseen losses  additionally  this work copes with a novel optimization method combining multi objective optimization with a grammatical evolution methodology  we implemented the ats testing three different fitness functions under three mono objective approaches and also two multi objective atss  experimental results test and compare them to the buy and hold strategy and a previous approach  beating both in returns and in number of positive operations  in particular  the multi objective approach demonstrated returns up to 20  in very volatile periods  proving that the combination of fitness functions is beneficial for the ats  
 a hybrid ets ann model for time series forecasting over the past few decades  a large literature has evolved to forecast time series using various linear  nonlinear and hybrid linear nonlinear models  recently  hybrid models by suitably combining linear models like autoregressive integrated moving average  arima  with nonlinear models like artificial neural network  ann  have become popular due to superior performance than individual models  these models assume the time series to be a sum of a linear and a nonlinear component  however  a real world time series may be purely linear or purely nonlinear or often contains a combination of linear and nonlinear patterns  motivated by this need  a new hybrid methodology is developed by combining linear and nonlinear exponential smoothing models from innovation state space  ets  with ann  the proposed hybrid ets ann model glorifies the chances of capturing different combination of linear and or nonlinear patterns in time series  this is because both ets and ann models have linear as well as nonlinear modeling capability  however  ann cannot handle linear patterns equally well as nonlinear patterns  therefore  in the proposed method  first ets is applied to the given time series and predictions are obtained  this enhances the chances of capturing existing linear patterns  if any  well using linear ets models  then residual error sequence is calculated by subtracting the ets predictions from the original series  the residual error sequence obtained is modeled by ann  then final prediction is obtained by combining the ets predictions with ann predictions  sixteen time series datasets are used for comparative performance analysis of the proposed methodology with arima  ets  multilayer perceptron mlp  and some existing hybrid arima ann models  experimental results show that the proposed hybrid model shows statistically promising result for the datasets used   c  2017 elsevier ltd  all rights reserved  
 a hybrid genetic algorithm for a home health care routing problem with time window and fuzzy demand home health care  hhc  companies are widespread in european countries  and aim to serve patients at home to help them recover from illness and injury in a personal environment  since transportation costs are among the biggest sources of expenditure in company activities  it is of great significance to optimize this in the home health care industry  from the perspective of optimizing the cost of transportation  this paper studies the vehicle routing scheduling problem as it applies to hhc companies  according to a survey of the hhc companies  during the process of delivering medication drugs  the quantity of drugs required for each patient is non deterministic when the company makes planned routes  this paper considers uncertain demand as a fuzzy variable  which is closer to a potential real life scenario  a home health care scheduling problem with fuzzy demand is considered and a fuzzy chance constraint model is designed  we propose a hybrid genetic algorithm integrated with stochastic simulation methods to solve the proposed model  firstly  the problem is reduced to the classical vehicle routing problem within a time window  experimental results for solomon s and homberger s benchmark instances show that the proposed algorithm performs efficiently  then other experiments on the fuzzy version model are undertaken with the variable value of the dispatcher preference index  dpi  parameter between  0  1   finally  the influence of dpi on the final objective and the indicators of the problem are discussed using stochastic simulation  and the best value of dpi is obtained  this research will help hhc companies to make appropriate decisions when arranging their vehicle scheduling routes   c  2016 elsevier ltd  all rights reserved  
 a hybrid goal programming and dynamic data envelopment analysis framework for sustainable supplier evaluation the evaluation of sustainable suppliers is one of the most complex tasks in sustainable supply chain management  sscm   classical data envelopment analysis  dea  and dynamic dea  ddea  models are heavily dependent on historical data and do not forecast future efficiencies of decision making units  dmus   the primary objective of this paper is to present a new predictive paradigm for ranking sustainable suppliers in sscm  the proposed model combines goal programming and ddea in an integrated and seamless paradigm to determine the future efficiencies of dmus  suppliers   it also shifts the decision maker s role from monitoring the past to planning the future  a case study is presented to demonstrate the applicability of the proposed model and exhibit the efficacy of the procedures and algorithms  
 a hybrid grid ga based lssvr learning paradigm for crude oil price forecasting in order to effectively model crude oil spot price with inherently high complexity  a hybrid learning paradigm integrating least squares support vector regression  lssvr  with a hybrid optimization searching approach for the parameters selection in the lssvr  consisting of grid method and genetic algorithm  ga    i e   a hybrid grid ga based lssvr model  is proposed in this study  in the proposed hybrid learning paradigm  the grid method  a simple but efficient searching method  is first applied to roughly but rapidly determine the proper boundaries of the parameters in the lssvr  then  the ga  an effective and powerful intelligent searching algorithm  is further implemented to select the most suitable parameters  for illustration and verification  the proposed learning paradigm is used to predict the crude oil spot prices of the west texas intermediate and the brent markets  the empirical results demonstrate that the proposed hybrid grid ga based lssvr learning paradigm can outperform its benchmarking models  including some popular forecasting techniques and similar lssvrs with other parameter searching algorithms  in terms of both prediction accuracy and time savings  indicating that it can be utilized as one effective forecasting tool for crude oil price with high volatility and irregularity  
 a hybrid harmony search algorithm with efficient job sequence scheme and variable neighborhood search for the permutation flow shop scheduling problems the permutation flow shop scheduling problem  pfssp   one of the most widely studied production scheduling problems  is a typical np hard combinatorial optimization problem  in this paper  a hybrid harmony search algorithm with efficient job sequence mapping scheme and variable neighborhood search  vns   named hhs  is proposed to solve the pffsp with the objective to minimize the makespan  first of all  to extend the hhs algorithm to solve the pfssp effectively  an efficient smallest order value  sov  rule based on random key is introduced to convert continuous harmony vector into a discrete job permutation after fully investigating the effect of different job sequence mapping schemes  secondly  an effective initialization scheme  which is based on neh heuristic mechanism combining with chaotic sequence  is employed with the aim of improving the solution s quality of the initial harmony memory  hm   thirdly  an opposition based learning technique in the selection process and the best harmony  best individual  in the pitch adjustment process are made full use of to accelerate convergence performances and improve solution accuracy  meanwhile  the parameter sensitivity is studied to investigate the properties of heis  and the recommended values of parameters adopted in hhs are presented  finally  by making use of a novel variable neighborhood search  the efficient insert and swap structures are incorporated into the hhs to adequately emphasize local exploitation ability  experimental simulations and comparisons on both continuous and combinatorial benchmark problems demonstrate that the he is algorithm outperforms the standard hs algorithm and other recently proposed efficient algorithms in terms of solution quality and stability   c  2017 elsevier ltd  all rights reserved  
 a hybrid of genetic algorithm and evidential reasoning for optimal design of project scheduling  a systematic negotiation framework for multiple decision makers traditional project scheduling methods inherently assume that the decision makers  dms  are a unique entity whose acts are based on group rationality  however  in practice  dms  reliance on individual rationality and the wish to optimize their own objectives skew negotiations towards their preferred solutions  this makes conventional project scheduling solutions unrealistic  here  a new two step method is proposed that seeks to increase the overall efficiency of project schedules without violating individual rationality criteria  to find scheduling solutions that are acceptable to all dms  first  a genetic algorithm is combined with evidential reasoning   er  to obtain near optimal project schedule alternatives with respect to the priorities of each dm  separately  second  the fallback bargaining method is used to help the dms reach a consensus on an alternative with the highest group satisfaction  the proposed model is tested on a benchmark project scheduling problem with over 3 6 billion possible project scheduling alternatives  the results show that the model helps dms when appointing their preferences using a well organized procedure to provide a transparent view of each project schedule performance solution  furthermore  the model is able to absorb the maximum support from the dms  not necessarily a unique entity  by collecting all the self optimizing dms  preferences and fairly allocating the benefits  
 a hybrid pomdp bdi agent architecture with online stochastic planning and plan caching this article presents an agent architecture for controlling an autonomous agent in stochastic  noisy environments  the architecture combines the partially observable markov decision process  pomdp  model with the belief desire intention  bdi  framework  the hybrid pomdp bdi agent architecture takes the best features from the two approaches  that is  the online generation of reward maximizing courses of action from pomdp theory  and sophisticated multiple goal management from bdi theory  we introduce the advances made since the introduction of the basic architecture  including  i  the ability to pursue and manage multiple goals simultaneously and  ii  a plan library for storing pre written plans and for storing recently generated plans for future reuse  a version of the architecture is implemented and is evaluated in a simulated environment  the results of the experiments show that the improved hybrid architecture outperforms the standard pomdp architecture and the previous basic hybrid architecture for both processing speed and effectiveness of the agent in reaching its goals   c  2016 elsevier b v  all rights reserved  
 a hybrid recommender system using artificial neural networks in the context of recommendation systems  metadata information from reviews written for businesses has rarely been considered in traditional systems developed using content based and collaborative filtering approaches  collaborative filtering and content based filtering are popular memory based methods for recommending new products to the users but suffer from some limitations and fail to provide effective recommendations in many situations  in this paper  we present a deep learning neural network framework that utilizes reviews in addition to content based features to generate model based predictions for the business user combinations  we show that a set of content and collaborative features allows for the development of a neural network model with the goal of minimizing logloss and rating misclassification error using stochastic gradient descent optimization algorithm  we empirically show that the hybrid approach is a very promising solution when compared to standalone memory based collaborative filtering method   c  2017 elsevier ltd  all rights reserved  
 a hybridisation of adaptive variable neighbourhood search and large neighbourhood search  application to the vehicle routing problem in this paper  an adaptive variable neighbourhood search  avns  algorithm that incorporates large neighbourhood search  lns  as a diversification strategy is proposed and applied to the capacitated vehicle routing problem  the avns consists of two stages  a learning phase and a multi level vns with guided local search  the adaptive aspect is integrated in the local search where a set of highly successful local searches is selected based on the intelligent selection mechanism  in addition  the hybridisation of lns with the avns enables the solution to escape from the local minimum effectively  to make the algorithm more competitive in terms of the computing time  a simple and flexible data structure and a neighbourhood reduction scheme are embedded  finally  we adapt a new local search move and an effective removal strategy for the lns  the proposed avns was tested on the benchmark data sets from the literature and produced very competitive results   c  2016 elsevier ltd  all rights reserved  
 a hyperheuristic methodology to generate adaptive strategies for games hyperheuristics have been successfully applied in solving a variety of computational search problems  in this paper  we investigate a hyperheuristic methodology to generate adaptive strategies for games  based on a set of low level heuristics  or strategies   a hyperheuristic game player can generate strategies which adapt to both the behavior of the co players and the game dynamics  by using a simple heuristic selection mechanism  a number of existing heuristics for specialized games can be integrated into an automated game player  as examples  we develop hyperheuristic game players for three games  iterated prisoner s dilemma  repeated goofspiel and the competitive traveling salesmen problem  the results demonstrate that a hyperheuristic game player outperforms the low level heuristics  when used individually in game playing and it can generate adaptive strategies even if the low level heuristics are deterministic  this methodology provides an efficient way to develop new strategies for games based on existing strategies  
 a kernel independence test for geographical language variation quantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation  however  existing approaches have important drawbacks  first  they are based on parametric models of dependence  which limits their power in cases where the underlying parametric assumptions are violated  second  they are not applicable to all types of linguistic data  some approaches apply only to frequencies  others to boolean indicators of whether a linguistic variable is present  we present a new method for measuring geographical language variation  which solves both of these problems  our approach builds on reproducing kernel hilbert space  rkhs  representations for nonparametric statistics  and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins  we compare this test with prior work using synthetic data as well as a diverse set of real data sets  a corpus of dutch tweets  a dutch syntactic atlas  and a data set of letters to the editor in north american newspapers  our proposed test is shown to support robust inferences across a broad range of scenarios and types of data  
 a kinect based wearable face recognition system to aid visually impaired users in this paper  we introduce a real time face recognition  and announcement  system targeted at aiding the blind and low vision people  the system uses a microsoft kinect sensor as a wearable device  performs face detection  and uses temporal coherence along with a simple biometric procedure to generate a sound associated with the identified person  virtualized at his her estimated 3 d location  our approach uses a variation of the k nearest neighbors algorithm over histogram of oriented gradient descriptors dimensionally reduced by principal component analysis  the results show that our approach  on average  outperforms traditional face recognition methods while requiring much less computational resources  memory  processing power  and battery life  when compared with existing techniques in the literature  deeming it suitable for the wearable hardware constraints  we also show the performance of the system in the dark  using depth only information acquired with kinect s infrared camera  the validation uses a new dataset available for download  with 600 videos of 30 people  containing variation of illumination  background  and movement patterns  experiments with existing datasets in the literature are also considered  finally  we conducted user experience evaluations on both blindfolded and visually impaired users  showing encouraging results  
 a lamstar network based human judgment analysis judgment analysis  ja  is a technique for modeling and interpreting human judgments that is usually based on multiple linear regression  however  the linear assumptions inherent to this approach can be limiting for modeling both the human judgments and the environmental criterion  this paper addresses this by introducing a formulation of ja based on large memory storage and retrieval  lamstar  artificial neural networks  we describe our lamstar network ja process and use it to analyze data from an air traffic control conflict prediction task  these results are compared with those of a traditional regression based lens model analysis  we found that the lamstar based ja did a better job of capturing human judgment  while the regression based model was more appropriate for the criterion  this suggest that the lamstar based ja approach has utility when human judgments are not well represented by a linear model  we discuss our results with respect to both the specific application we evaluated as well as meta analyses of the ja literature  we also explore avenues for future research  
 a learning system for automatic berg balance scale score estimation the objective of this work is the development of a learning system for the automatic assessment of balance abilities in elderly people  the system is based on estimating the berg balance scale  bbs  score from the stream of sensor data gathered by a wii balance board  the scientific challenge tackled by our investigation is to assess the feasibility of exploiting the richness of the temporal signals gathered by the balance board for inferring the complete bbs score based on data from a single bbs exercise  the relation between the data collected by the balance board and the bbs score is inferred by neural networks for temporal data  modeled in particular as echo state networks within the reservoir computing  rc  paradigm  as a result of a comprehensive comparison among different learning models  the proposed system results to be able to estimate the complete bbs score directly from temporal data on exercise  10 of the bbs test  with p 1 0 s of duration  experimental results on real world data show an absolute error below 4 bbs score points  i e  below the 7  of the whole bbs range   resulting in a favorable trade off between predictive performance and user s required time with respect to previous works in literature  results achieved by rc models compare well also with respect to different related learning models  overall  the proposed system puts forward as an effective tool for an accurate automated assessment of balance abilities in the elderly and it is characterized by being unobtrusive  easy to use and suitable for autonomous usage   c  2017 elsevier ltd  all rights reserved  
 a linguistic treatment for automatic external plagiarism detection plagiarism is the unauthorized use of the ideas  presentation of someone else s words or work as your own  this paper presents an external plagiarism detection system  epds   which employs a combination of the semantic role labeling  srl  technique  the semantic and syntactic information  most of the available methods fail to capture the meaning in the comparison between a source document sentence and a suspicious document sentence when two sentences have same surface text  therefore  it leads to incorrect or even unnecessary matching results  however  the proposed method is able to avoid selecting the source text sentence whose similarity with suspicious text sentence is high but its meaning is different  on the other hand  an author may change the sentence from  active to passive and vice versa  hence  the method also employed the srl technique to tackle the aforementioned challenge  furthermore  the method used the content word expansion approach to bridge the lexical gaps and identify the similar ideas that are expressed using different wording  the proposed method is able to detect different types of plagiarism such as the exact verbatim copying  paraphrasing  transformation of sentences  changing of word structure  as a result  the experimental results have displayed that the proposed method is able to improve the performance compared with the participating systems in pan pc 11 and other existing techniques   c  2017 elsevier b v  all rights reserved  
 a low cost socially assistive robot and robot assisted intervention for children with autism spectrum disorder  field trials and lessons learned recent research has employed socially assistive robots as catalysts for social interaction and improved communication in young children with autism spectrum disorder  asd   studies describe observed therapeutic outcomes such as increased speech  social interaction  joint and directed attention  but few detail a robot inclusive protocol which evaluates a set of robot tasks using widely accepted  clinical assessments to evaluate the efficacy of the approach  in this study  we employed a low cost  toy like robot prototype with safety features such as a snap off head and two snap off arms  a camera for face  hand detection and session recording  two autonomous games and a teleoperated mode  we then developed and tested a new  robot assisted intervention  eight study participants and three controls diagnosed with asd and a speech deficiency were recruited  the study group received pre   post intervention measures with the vineland adaptive behavioral scale ii  vabs ii   mean length spontaneous utterance determination  mlsud   motor imitation scale  mis   unstructured imitation assessment  uia  and expressive vocabulary test 2  evt 2  and participated in twelve 30 min interventions  to explore the efficacy of the robot and new robot assisted intervention we  1  measured improvements in spontaneous speech  communication and social skills using standard measures of performance   2  compared improvements observed with a study group receiving the robot assisted intervention with a control group receiving speech therapy but no robot assisted intervention and   3  validated a set of robot behaviors that may inform an integrated  cross platform  approach for incorporating an autonomous  robot assisted asd intervention within a clinical methodology  paired samples t test results indicate significantly improved adaptive functioning in the vabs ii socialization and communication domains  mlsud  uia social interaction  uia requesting  and uia joint attention domains  between group analyses also suggest significant improvement in vabs ii play and leisure  receptive language subdomains and trends in vabs ii coping skills and interpersonal scale subdomains  
 a machine learning approach to measure and monitor physical activity in children the growing trend of obesity and overweight worldwide has reached epidemic proportions with one third of the global population now considered obese  this is having a significant medical impact on children and adults who are at risk of developing osteoarthritis  coronary heart disease and stroke  type 2 diabetes  cancers  respiratory problems  and non alcoholic fatty liver disease  in an attempt to redress the issue  physical activity is being promoted as a fundamental component for maintaining a healthy lifestyle  recommendations for physical activity levels are issued by most governments as part of their public health measures  however  current techniques and protocols  including those used in laboratory settings  have been criticised  the main concern is that it is not feasible to use multiple pieces of measurement hardware  such as vo2 masks and heart rate monitors  to monitor children in free living environments due to weight and encumbrance constraints  this has prompted research in the use of wearable sensing and machine learning technology to produce classifications for specific physical activity events  this paper builds on this approach and presents a supervised machine learning method that utilises data obtained from accelerometer sensors worn by children in free living environments  our results show that when using an artificial neural network algorithm it is possible to obtain an overall accuracy of 96  using four features from the initial dataset  with sensitivity and specificity values equal to 95  and 99  respectively  expanding the dataset with interpolated cases  it was possible to improve on these results with 98 8  for accuracy  and 99  for sensitivity and specificity when four features were used  
 a machine learning based system for berth scheduling at bulk terminals the increasing volume of maritime freight is presented as a challenge to those skilled terminal managers seeking to maintain or increase their market share  in this context  an efficient management of scarce resources as berths arises as a reasonable option for reducing costs while enhancing the productivity of the overall terminal  in this work  we tackle the berth scheduling operations by considering the bulk berth allocation problem  bulk bap   this problem  for a given yard layout and location of the cargo facilities  aims to coordinate the berthing and yard activities for giving service to those vessels arriving at the terminal  considering the multitude of scenarios arising in this environment and the no free lunch theorem  the drawback concerning the selection of the best algorithm for solving the bulk bap in each particular case is addressed by a machine learning based system  it provides  based on the scenario at hand  a ranking of algorithms sorted by appropriateness  the computational study shows an increase in the quality of the provided solutions when the algorithm to be used is selected according to the features of the instance instead of selecting the best algorithm on average   c  2017 elsevier ltd  all rights reserved  
 a main directional maximal difference analysis for spotting facial movements from long term videos there is an increasing interests in micro expression researches  spotting micro expressions in long term videos is very important  not only for providing clues for lie detection  but also for reducing the labor required to collect micro expression data  however  little progress has been made in spotting micro expressions  in this paper  we propose a main directional maximal difference  mdmd  analysis for micro expression spotting  mdmd uses the magnitude maximal difference in the main direction of optical flow features to spot facial movements  including micro expressions  using block structured facial regions  mdmd obtains more accurate features of movement of expressions for automatically spotting micro expressions and macro expressions from videos  this method involves both the temporal and spatial locations of face movements  evaluations using the cas me  2  database containing micro expressions and macro expressions show that mdmd is more robust than some state of the art algorithms  
 a mathematical theory of evidence turns 40 the book that launched the dempster shafer theory of belief functions appeared 40 years ago  this intellectual autobiography looks back on how i came to write the book and how its ideas played out in my later work   c  2016 elsevier inc  all rights reserved  
 a me based rough approximation approach for multi period and multi product fashion assortment planning problem with substitution assortment planning is the process conducted by fashion retailers to determine the variety and quantity of products to sell at each sales period  as it has a great impact on the financial performance for retail stores  retailers take it as the crucial activity and the nucleus for an intelligent inventory control system  in this paper  we consider the assortment planning with substitution  a substitute product when the original choice is unavailable  under a fuzzy environment and then a fuzzy optimization model is built to obtain maximum benefits for retailers  for the fuzzy variables in the objective functions  we propose a fuzzy measure which can represent any attitudes between extremely optimistic and pessimistic to handle objective functions and get their expected value  for the constraints  a similarity relationship based on the fuzzy measure is defined  and the feasible region is addressed by the rough approximation based on this similarity relationship  then  the lower approximation model  lam  and the upper approximation model  uam   which can avoid losing much information in the modeling process  are generated  to solve the model  a hybrid genetic algorithm with rough simulation is proposed  finally  an application is used to demonstrate the practicality and effectiveness of the model and solution algorithm   c  2017 elsevier ltd  all rights reserved  
 a metaheuristic optimization based indirect elicitation of preference parameters for solving many objective problems a priori incorporation of the decision maker s preferences is a crucial issue in many objective evolutionary optimization  some approaches characterize the best compromise solution of this problem through fuzzy outranking relations  however  they require the elicitation of a large number of parameters  weights and different thresholds   this paper proposes a novel metaheuristic based optimization method to infer the model s parameters of a fuzzy relational system of preferences  based on a small number of judgments given by the decision maker  the results show a satisfactory rate of error when predicting new outcomes with the parameter values obtained by using small size reference sets  
 a method based on rules and machine learning for logic form identification in spanish logic forms  lf  are simple  first order logic knowledge representations of natural language sentences  each noun  verb  adjective  adverb  pronoun  preposition and conjunction generates a predicate  lf systems usually identify the syntactic function by means of syntactic rules but this approach is difficult to apply to languages with a high syntax flexibility and ambiguity  for example  spanish  in this study  we present a mixed method for the derivation of the lf of sentences in spanish that allows the combination of hard coded rules and a classifier inspired on semantic role labeling  thus  the main novelty of our proposal is the way the classifier is applied to generate the predicates of the verbs  while rules are used to translate the rest of the predicates  which are more straightforward and unambiguous than the verbal ones  the proposed mixed system uses a supervised classifier to integrate syntactic and semantic information in order to help overcome the inherent ambiguity of spanish syntax  this task is accomplished in a similar way to the semantic role labeling task  we use properties extracted from the ancora es corpus in order to train a classifier  a rule based system is used in order to obtain the lf from the rest of the phrase  the rules are obtained by exploring the syntactic tree of the phrase and encoding the syntactic production rules  the lf algorithm has been evaluated by using shallow parsing with some straightforward spanish phrases  the verb argument labeling task achieves 84  precision and the proposed mixed lfi method surpasses 11  a system based only on rules  
 a methodology based on deep learning for advert value calculation in cpm  cpc and cpa networks in this research  we propose a methodology for advert value calculation in cpm  cpc and cpa networks  accurately estimating this value increases the three previous networks  incomes by selecting the most profitable advert  by increasing income  publishers are better paid and improved services are afforded to advertisers  to develop this methodology  we propose a system based on traditional machine learning methods and deep learning methods  the system has two inputs and one output  the inputs are the user visit and the data about the advertiser  the output is the advert value expressed in dollars  deep learning predicts model behavior more precisely for many supervised problems  the three experiments carried out allow us to conclude that dl is a supervised method that is very efficient in the classification of spam adverts and in the estimation of the ctr  in the prediction of online sales  dlnn have shown  on average  worse performance than cubist and random forest methods  although better performance than model tree  model rules and linear regression methods  
 a methodology for assessing the effect of portfolio management on npd performance based on bayesian network scenarios firm growth and profitability come primarily from new product development  portfolio management has been emphasized in improving new product development  npd  performance under multiple project environments  however  few researchers have demonstrated the consequence of different combinations of portfolio management practices on npd performance  in this study  a decision support methodology based on bayesian network scenarios is used to simulate the effect of portfolio management on npd performance in uncertain environments  firstly  portfolio management factors are identified and performance criteria determined  and then  the causal relationships among the factors are modelled within similar time frames  and a bayesian network model is developed by parameter learning from data  a case study is carried out for project portfolio managers in chinese firms  the most informative factors affecting npd performance are identified by sensitive analysis  and the best and worst scenarios with different combinations of portfolio management practices are analysed  the study extends the application of bayesian networks to assess the performance under changing conditions and highlights some managerial suggestions to improve npd performance  
 a model based on 2 tuple fuzzy linguistic representation and analytic hierarchy process for supplier segmentation using qualitative and quantitative criteria the literature on supply base segmentation has increasingly adopted multi criteria decision making  mcdm  techniques into recently proposed models  however  most proposals segment the supply base from the standpoint of the purchased item  which prevents them from providing guidelines that are specific to each supplier  some authors have attempted to overcome these limitations by putting forward portfolio models based on the relationship with suppliers  these approaches use fuzzy variables and mcdm methods that take qualitative judgements by experts as the only input for decision making  however  many companies have databases with historical data about the performance of past transactions with suppliers that should be considered by expert systems that aim to comprehensively evaluate suppliers  performance  this paper seeks to address this gap by proposing a segmentation model based on the relationship with suppliers capable of aggregating quantitative and qualitative criteria  analytic hierarchy process  ahp  was used to determine the relative importance of each criteria  fuzzy 2 tuple  a prominent computing with word  cww  approach  was used to evaluate suppliers with a mixture of historical quantitative data and qualitative judgements by purchasing experts  an illustrative application of the proposed model was carried out in the pharmaceutical supply center  psc  of a teaching hospital  the proposed model can be viewed as a decision support system capable of aggregating the qualitative judgements of experts and quantitative historical performance measures  thus providing guidelines to improve the relationship between suppliers and the buyer firm   c  2017 elsevier ltd  all rights reserved  
 a model of language learning with semantics and meaning preserving corrections we present a computational model that takes into account semantics for language learning and allows us to model meaning preserving corrections  the model is constructed with a learner and a teacher who interact in a sequence of shared situations by producing utterances intended to denote a unique object in each situation  we test our model with limited sublanguages of 10 natural languages exhibiting a variety of linguistic phenomena  the results show that learning to a high level of performance occurs after a reasonable number of interactions  comparing the effect of a teacher who does no correction to that of a teacher who corrects whenever possible  we show that under certain conditions corrections can accelerate the rate of learning  we also define and analyze a simplified model of a probabilistic process of collecting corrections to help understand the possibilities and limitations of corrections in our setting   c  2016 elsevier b v  all rights reserved  
 a model of symbolic processing in raven s progressive matrices raven s progressive matrices probe the capacity to discover a common rule in a 3 by 3 matrix of stimuli by requiring the correct selection of a probe stimulus for one stimulus location intentionally left blank  this task is purely visual and excludes verbal cues  we propose a model of the performance of this task that classifies a variety of visual stimuli  either as rotated  scaled  or with elements added or subtracted or otherwise altered within each of the 3 by 3 matrix s fields  in our model  q learning uses a state space based on the transitions between fields of the 3 by 3 matrix of stimuli  actions are modeled as alterations of visual stimuli between these matrix s fields  using q learning our model successfully selects the correct probe stimulus from eight choices offered in the task   c  2017 elsevier b v  all rights reserved  
 a modified particle swarm optimization algorithm to mixed model two sided assembly line balancing in this paper  a new modified particle swarm optimization algorithm with negative knowledge is proposed to solve the mixed model two sided assembly line balancing problem  the proposed approach includes new procedures such as generation procedure which is based on combined selection mechanism and decoding procedure  these new procedures enhance the solution capability of the algorithm while enabling it to search at different points of the solution space  efficiently  performance of the proposed approach is tested on a set of test problem  the experimental results show that the proposed approach can be acquired distinguished results than the existing solution approaches  
 a modular architecture for transparent computation in recurrent neural networks computation is classically studied in terms of automata  formal languages and algorithms  yet  the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism  therefore  we suggest a unique perspective on this central issue  to which we would like to refer as transparent connectionism  by proposing accounts of how symbolic computation can be implemented in neural substrates  in this study we first introduce a new model of dynamics on a symbolic space  the versatile shift  showing that it supports the real time simulation of a range of automata  we then show that the godelization of versatile shifts defines nonlinear dynamical automata  dynamical systems evolving on a vectorial space  finally  we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks  the mapping defines an architecture characterized by its granular modularity  where data  symbolic operations and their control are not only distinguishable in activation space  but also spatially localizable in the network itself  while maintaining a distributed encoding of symbolic representations  the resulting networks simulate automata in real time and are programmed directly  in the absence of network training  to discuss the unique characteristics of the architecture and their consequences  we present two examples   i  the design of a central pattern generator from a finite state locomotive controller  and  ii  the creation of a network simulating a system of interactive automata that supports the parsing of garden path sentences as investigated in psycholinguistics experiments   c  2016 elsevier ltd  all rights reserved  
 a multi criteria approach to rank the municipalities of the states of mexico by its marginalization level  the case of jalisco marginalization studies of a population are tools that enable the mexican government to understand and compare the socio demographic situation of different regions of the country  the goal is to implement effectively various programs of social or economic development whose aims are to ght against the population s lag  which has affected the quality of life of mexican citizens  in this paper  a multi criteria approach for ranking the municipalities of the states of mexico by their levels of marginalization is proposed  and the case of jalisco  mexico  is presented  the approach uses the electre iii method to construct a mediumsized valued outranking relation and then employs a new multi objective evolutionary algorithm  moea  based on the nondominated sorting genetic algorithm  nsga  ii to exploit the relation to obtain a recommendation  the results of this application can be useful for policymakers  planners  academics  investors  and business leaders  this study also contributes to an important  yet relatively new  body of application based literature that investigates multi criteria approaches to decision making that use fuzzy theory and evolutionary multi objective optimization methods  a comparison of the ranking obtained with the proposed methodology and the stratification created by the national population council of mexico shows that the methodology presented is consistent and yields reliable results for this problem  
 a multi level classification framework for multi site medical data  application to the adhd 200 collection recently  the culture of sharing medical data has emerged impressively  reducing significantly the barrier to the development of medical research accordingly  as open access large datasets result from this significant initiative  data mining techniques can be considered for the development of interpretable expert systems to help in diagnosis  however  the collaborative effort of information gathering yields heterogeneous databases because of technical and geographical factors  indeed  on the one hand  the harmonization of protocols for data collection is still missing  on the other hand  cultural and social factors impact locally both the epidemiology and etiology of a given disease  ignoring these factors could weaken the credibility of studies based on multi site data  thereby  our work tackles the development of computer aided diagnosis systems relying on heterogeneous data  for such a purpose  we propose a multi level approach  inspired by multi level statistical modeling  based on decision trees  in the sense of machine learning   this framework is applied on the public adhd 200 collection for the study of attention deficit hyperactivity disorder  adhd    c  2017 elsevier ltd  all rights reserved  
 a multi modal architecture for non intrusive analysis of performance in the workplace human performance  in all its different dimensions  is a very complex and interesting topic  in this paper we focus on performance in the workplace which  asides from complex is often controversial  while organizations and generally competitive working conditions push workers into increasing performance demands  this does not necessarily correlates positively to productivity  moreover  existing performance monitoring approaches  electronic or not  are often dreaded by workers since they either threat their privacy or are based on productivity measures  with specific side effects  we present a new approach for the problem of performance monitoring that is not based on productivity measures but on the workers  movements while sitting and on the performance of their interaction with the machine  we show that these features correlate with mental fatigue and provide a distributed architecture for the non  intrusive and transparent collection of this data  the easiness in deploying this architecture  its non  intrusive nature  the potential advantages for better human resources management and the fact that it is not based on productivity measures will  in our belief  increase the willingness of both organizations and workers to implement this kind of performance management initiatives  
 a multi phase oscillated variable neighbourhood search algorithm fora real world open vehicle routing problem the aim of this study is to solve the newspaper delivery optimization problem for a media delivery company in turkey by reducing the total cost of carriers  the problem is modelled as an open vehicle routing problem  ovrp   which is a variant of the vehicle routing problem  a variable neighbourhood search based algorithm is proposed to solve a real world ovrp  the proposed algorithm is tested with varieties of small and large scale benchmark suites and a very large scale real world problem instance  the results of the proposed algorithm provide either the best known solution or a competitive solution for each of the benchmark instances  the algorithm also improves the real world companys solutions by more than 10    c  2017 elsevier b v  all rights reserved  
 a multi scale seriation algorithm for clustering sparse imbalanced data  application to spike sorting seriation is a useful statistical method to visualize clusters in a dataset  however  as the data are noisy or unbalanced  visualizing the data structure becomes challenging  to alleviate this limitation  we introduce a novel metric based on common neighborhood to evaluate the degree of sparsity in a dataset  a pile of matrices are derived for different levels of sparsity  and the matrices are permuted by a branch and bound algorithm  the matrix with the best block diagonal form is then selected by a compactness criterion  the selected matrix reveals the intrinsic structure of the data by excluding noisy data or outliers  this seriation algorithm is applicable even if the number of clusters is unknown or if the clusters are imbalanced  however  if the metric introduces too much sparsity in the data  the sub sampled groups of data could be ousted  to resolve this problem  a multi scale approach combining different levels of sparsity is proposed  the capability of the proposed seriation method is examined both by toy problems and in the context of spike sorting  
 a multi stage conflict style large group emergency decision making method unconventional emergencies usually have the characteristics of complexity  dynamic  and unpredictability  which greatly enhances the difficulty of emergency decision making  aiming at the multi stage large group emergency decision making problem featuring unknown stage weight and preference information expressed as interval numbers  we propose a new decision making method  first  we present a similarity measurement formula for interval numbers  each stage s preference information is clustered using this similarity  to minimize the conflict of preferences  we derived two relative entropy optimization models to calculate the aggregation and stage weights  next  we rank the alternatives based on the comprehensive group preference information  finally  we present an illustrative example to verify the validity and practicability of this approach  and discuss several advantages of this method for managing emergency decision making problems  
 a multi unit combinatorial auction based approach for decentralized multi project scheduling in industry  many problems are considered as the decentralized resource constrained multi project scheduling problem  drcmpsp   existing approaches encounter difficulties in dealing with large drcmpsp cases while respecting the information privacy requirements of the project agents  in this paper  we tackle drcmpsp by formulating it as a multi unit combinatorial auction  wellman et al  in games econ behav 35 1  271 303  2001   which does not require sensitive private project information  to handle the hardness of bidder valuation  we introduce the capacity query which uses different item capacity profiles to efficiently elicit valuation information from bidders  based on the capacity query  we adopt two existing strategies  gonen and lehmann in proceedings of the 2nd acm conference on electronic commerce  pp 13 20  2000  for solving multi unit winner determination problems to find good allocations of the drcmpsp auctions  the first strategy employs a greedy allocation process  which can rapidly find good allocations by allocating the bidder with the best answer after each query  the second strategy is based on a branch and bound process to improve the results of the first strategy  by searching for a better sequence of granting the bids from the bidders  empirical results indicate that the two strategies can find good solutions with higher quality than state of the art decentralized approaches  and scale well to large scale problems with thousands of activities from tens of projects  
 a multimodel context aware tourism recommendation service  approach and architecture recommender systems and services are now widely used to support decision making in the fields characterized by the selection from a large number of alternatives with a significant influence of subjective preferences  a comprehensive multimodel approach to the development of context aware recommender systems in the field of tourism information support is proposed  in particular  it is proposed to construct a recommender system based on loosely coupled modules  in which both personalized and nonpersonalized recommendation methods are implemented  and the synthesis module  which adapts the module system to the specific conditions of different kinds of initial information  
 a multiobjective fuzzy chance constrained programming model for land allocation in agricultural sector  a case study in this article a fuzzy multiobjective chance constrained programming model is used for modeling and solving land allocation problems efficiently with the help of fuzzy goal programming  optimal production of seasonal crops and related expenditures are considered from the viewpoint of proper utilization of total cultivating land and different farming resources  some resource parameters associated with the probabilistic constraints are taken as normally distributed fuzzy random variables  the potential use of this methodology is illustrated by a case example  
 a multiple attribute interval type 2 fuzzy group decision making and its application to supplier selection with extended linmap method supplier selection is a key issue in supply chain management  which directly impacts the manufacturer s performance  the problem can be viewed as a multiple attribute group decision making  magdm  that concerns many conflicting evaluation attributes  both being of qualitative and quantitative nature  due to the increasing complexity and uncertainty of socio economic environment  some evaluations of attributes are not adequately represented by numerical assessments and type 1 fuzzy sets  in this paper  we develop some linear programming models with the aid of multidimensional analysis of preference  linmap  method to solve interval type 2 fuzzy magdm problems  in which the information about attribute weights is incompletely known  and all pairwise comparison judgments over alternatives are represented by it2fss  first  we introduce a new distance measure based on the centroid interval between the it2fss  then  we construct the linear programming model to determine the interval type 2 fuzzy positive ideal solution  it2pis  and corresponding attributes weight vector  based on it  an extended linmap method to solve magdm problem under it2fss environment is developed  finally  a supplier selection example is provided to demonstrate the usefulness of the proposed method  
 a multiple criteria supplier segmentation using outranking and value function methods suppliers play a key role in supply chain management which involves evaluation for supplier selection problem  as well as other complex issues that companies should take into account  the purpose of this research is to develop and test an integrated system  which allows qualifying providers and also supplier segmentation by monitoring their performance based on a multiple criteria tool for systematic decision making  this proposal consists in a general procedure to assess suppliers based mainly on exploiting all reliable databases of the company  firstly  for each group of products  their evaluation criteria are defined collaboratively in order to determine their critical and strategic performance  which are then integrated with other criteria that are specific of the suppliers and represent relevant aspects for the company  also classified by critical and strategic dimensions  two multiple criteria methods  compensatory and non  compensatory  are used and compared so as to point out their strengths  weaknesses and flexibility for the supplier evaluation in different contexts  which are usually relevant in the supply chain management  a value function approach is the appropriate method to qualify providers to be included in the panel of approved suppliers of the company as this process depends only on own features of the supplier  on the other hand  outranking methods such as promethee have shown greater potential and robustness to develop portfolios with suppliers that should be partners of the company  as well as to identify other types of relationships  such as long term contracts  market policies or to highlight those to be removed from their portfolio  these results and conclusions are based on an empirical research in a multinational company for food  pharmaceuticals and chemicals  this system has shown a great impact as it represents the first supplier segmentation proposal applied to industry  in which decision making not only takes into account opinions and judgements  but also integrates historical data and expert knowledge  this approach provides a robust support system to inform operative  tactical and strategic decisions  which is very relevant when applying an advanced management in practice   c  2016 elsevier ltd  all rights reserved  
 a multiple support vector machine approach to stock index forecasting with mixed frequency sampling the independent variables commonly used to predict the stock price index usually contain data sampled at different frequencies  and simultaneously  there exist multiple outputs  however  most current researches ignore different frequencies among independent variables and multi output issues  this paper proposes a multiple output support vector machine unrestricted mixed data sampling  msvm umidas  approach which can achieve multiple results for sequential points simultaneously by applying mixed frequency independent variables  we test the in sample and out of sample performances of msvm umidas for stock forecasting in terms of  t 1    t 2  and  t 3  and then compare the performances of the proposed model with those of other models  the results indicate that our model performs better when assessed by four different measurements  thus  our proposed model is more realistic in practice and an appropriate tool for multi output and mixed frequency issues for stock price forecasting   c  2017 elsevier b v  all rights reserved  
 a multistrain bacterial diffusion model for link prediction topological link prediction is the task of assessing the likelihood of new future links based on topological properties of entities in a network at a given time  in this paper  we introduce a multistrain bacterial diffusion model for link prediction  where the ranking of candidate links is based on the mutual transfer of bacteria strains via physical social contact  the model incorporates parameters like efficiency of the receiver surface  reproduction rate and number of social contacts  the basic idea is that entities continuously infect their neighborhood with their own bacteria strains  and such infections are iteratively propagated on the social network over time  the probability of transmission can be evaluated in terms of strains  reproduction  previous transfer  surface transfer efficiency  number of direct social contacts i e  neighbors  multiple paths between entities  the value of the mutual strains of infection between a pair of entities is used to rank the potential arcs joining the entity nodes  the proposed multistrain diffusion model and mutual strain infection ranking technique have been implemented and tested on widely accepted social network data sets  experiments show that the msdm lp and mutual strain diffusion ranking technique outperforms state of the art algorithms for neighbor based ranking  
 a narrative in three acts  using combinations of image schemas to model events image schemas have been proposed as conceptual building blocks corresponding to the hypothesised most fundamental embodied experiences  we formally investigate how combinations of image schemas  or  image schematic profiles   can model essential aspects of events  and discuss benefits for artificial intelligence and cognitive systems research  in particular concerning the role of such basic events in concept formation  more specifically  as exemplary illustrations and proof of concept the image schemas object  contact  and path are combined to form the events blockage  bouncing  and caused movement  additionally  an outline of a proposed conceptual hierarchy of levels of modelling for image schemas and similar cognitive theories is given   c  2016 elsevier b v  all rights reserved  
 a natural language generation approach to support understanding and traceability of multi dimensional preferential sensitivity analysis in multi criteria decision making multi criteria decision analysis  mcda  enables decision makers  dm  and decision analysts  da  to analyse and understand decision situations in a structured and formalised way  with the increasing complexity of decision support systems  dsss   it becomes challenging for both expert and novice users to understand and interpret the model results  natural language generation  nlg  techniques are used in various dsss to cope with this challenge as they reduce the cognitive effort to achieve understanding of decision situations  however  nlg techniques in mcda have so far mainly been developed for deterministic decision situations or one dimensional sensitivity analyses  in this paper  a concept for the generation of textual explanations for a multi dimensional preferential sensitivity analysis in mcda is developed  the key contribution is a nlg approach that provides detailed explanations of the implications of preferential uncertainties in multi attribute value theory  mavt   it generates a report that assesses the influences of simultaneous or separate variations of inter criteria and intra criteria preferential parameters determined within the decision analysis  we explore the added value of the natural language report in an online survey  our results show that the nlg approach is particularly beneficial for difficult interpretational tasks   c  2017 elsevier ltd  all rights reserved  
 a natural language interface to a graph based bibliographic information retrieval system with the ever increasing volume of scientific literature  there is a need for a natural language interface to bibliographic information retrieval systems to retrieve relevant information effectively  in this paper  we propose one such interface  nli gibir  which allows users to search for a variety of bibliographic data through natural language  nli gibir makes use of a novel framework applicable to graph based bibliographic information retrieval systems in general  this framework incorporates algorithms heuristics for interpreting and analyzing natural language bibliographic queries via a series of text  and linguistic based techniques  including tokenization  named entity recognition  and syntactic analysis  we find that our framework  as implemented in nli gibir  can effectively represent and address complex bibliographic information needs  thus  the contributions of this paper are as follows  first  to our knowledge  it is the first attempt to propose a natural language interface for graph based bibliographic information retrieval  second  we propose a novel customized natural language processing framework that integrates a few original algorithms heuristics for interpreting and analyzing bibliographic queries  third  we show that the proposed framework and natural language interface provide a practical solution for building real world bibliographic information retrieval systems  our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities  
 a neural approach for estimation of per capita electricity consumption due to age and income electricity consumption is influenced by number of adults and children and their relationship at household level  household income also plays a critical role on expenditure on electricity  accordingly  this article presents a joint probability model of electricity demand based on occupants  age grades and household income levels  a bottom up strategy is developed using a micro level database of 70 australian households  a neural regression generalization technique is devised to estimate electricity demand using back propagation and cognitive mapping  the aggregated result is then validated against 2012 australian national census  accordingly  the model is improved based on a top down review  the results show per capita electricity demand by adult and child at 0 408 kw  69 kwh week  and 0 226 kw  38 kwh week   respectively  the equivalent dollar values are  13 6 week and  7 6 week in 2012  at macro level  the model reveals per capita demand by all individuals at 0 324 kw  54 35 kwh week  equivalent to dollar value of  10 87 week  across australia  the results also show higher percentage of per capita demand for adults in high and medium income classes  and the otherwise for low income class  ratio of child s demand over adult s demand is highest among the low income households  and lowset among the middle income households  while best balance between adult and child per capita demand belongs to the high income  
 a neural implementation of bayesian inference based on predictive coding predictive coding  pc  is a leading theory of cortical function that has previously been shown to explain a great deal of neurophysiological and psychophysical data  here it is shown that pc can perform almost exact bayesian inference when applied to computing with population codes  it is demonstrated that the proposed algorithm  based on pc  can  decode probability distributions encoded as noisy population codes  combine priors with likelihoods to calculate posteriors  perform cue integration and cue segregation  perform function approximation  be extended to perform hierarchical inference  simultaneously represent and reason about multiple stimuli  and perform inference with multi modal and non gaussian probability distributions  pc thus provides a neural network based method for performing probabilistic computation and provides a simple  yet comprehensive  theory of how the cerebral cortex performs bayesian inference  
 a neuro computational model of sequence learning in macaques  the simultaneous chaining paradigm we present a novel model  scp1  of monkey sequence learning that takes the processes of stimulus recognition and motor planning seriously in addressing a robust dataset on list learning obtained through the simultaneous chaining paradigm  scp   strikingly  scp violates stimulus response  s r  mappings in that after several different lists are learned  monkeys are able to conserve this learning on a new list which is conserved in the sense that the jth element is specified as the jth element of any one of the previously learned lists  we demonstrate list acquisition as a result of multiple concurrent learning processes that together contribute to competent performance  in addition to reproducing behavioral results  we offer observations linking the work to macaque neurophysiology  
 a new adaptive pca based thresholding scheme for fault detection in complex systems for large scale and complex processes  data driven analysis methods are receiving increasing attention for fault detection and diagnosis to improve process operation by detecting when abnormal process operations exist and diagnosing the sources of the abnormalities  common methods based on multivariate statistical analysis are widely used and particularly principal component analysis  pca   fault detection indices used along with pca including the hotelling t 2 statistic and the sum of squared prediction error  spe  known as the q statistic can be used to identify faults  this paper develops a new adaptive thresholding scheme based on a modified exponentially weighted moving average  ewma  control chart statistic  which is effective in detecting small changes and abrupt shifts in the process operation  the aim is to enhance the performance of pca methods for process monitoring  while maintaining a low false alarm rate with good sensitivity of anomalies  the performance of the developed scheme is compared to a conventional fixed thresholding technique by evaluating the detection performance across various types of faults that occurred in the tennessee eastman process  the results demonstrate the promising capabilities of our proposed scheme  
 a new approach of multi criteria analysis for the evaluation and selection of sustainable transport investment projects under uncertainty  a case study selecting transport project to invest is an important task  this paper offers a new sustainable transport investment selection approach that applies interval valued fuzzy sets  ivfss  to address uncertainty  relative preference relation is employed to address importance of criteria  judgments of experts are given a weight based on all the gathered judgments and the expertise of experts  furthermore  the concept of prospect theory is used to rank the alternatives  the approach is applied to a case study and the results are discussed  
 a new class of modwt svm de hybrid model emphasizing on simplification structure in data pre processing  a case study of annual electricity consumptions in recent years  electricity crisis still becomes noticeable in some countries due to a widening gap between demand and supply  consequently  the future demand plays a significant role in efficient management and utilization of electricity  pertaining to efficient supply handling to increase the power system reliability  an electricity demand forecasting is one of the most crucial tools  the forecasting technique is used by decision makers all over the world to predict the future demand as key information for a proper policy  in this research  the hybrid model consists of maximal overlap discrete wavelet transform  modwt   support vector machine  svm   and differential evolution  de  optimization emphasizing on simplifying the complex structure in data pre processing is proposed to forecast the thirty two annual electricity consumptions and is compared with traditional forecasting models  hybrid model of modwt and svm  and combined model of svm and de optimization based on mean absolute error  mae   mean absolute percentage error  mape  and symmetric mean absolute percentage error  smape  measures as well as friedman test and post hoc test  the empirical results indicate that the proposed model outperforms other forecasting models and provides more accurate forecasts than other candidate models at 0 05 significance levels and the nearly highest precision  consequently  the proposed model is able to reduce the limitations of individual models regarding annual electricity consumptions and can be used as a promising tool in order to forecast annual electricity consumptions as well   c  2017 elsevier b v  all rights reserved  
 a new enhanced support vector model based on general variable neighborhood search algorithm for supplier performance evaluation  a case study in sustainable supply chain networks  companies are obligated to have a systematic decision support system in place to help it adopt right decisions at right times  among strategic decisions  supplier selection and evaluation outranks other decisions in terms of importance due to its long term impacts  besides  the adoption of such strategic decision entails exploring several factors that contribute to the complexity of decision making in the supply chain  for the purpose of solving non linear regression problems  a novel neural network technique known as least squaresupport vector machine  ls svm  with maximum generalization ability has successfully been implemented  however  the performance quality of the ls svm is recognized to notoriously vary depending on the rigorous selection of its parameters  therefore  in this paper  a continuous general variable neighborhood search  cgvns  which is an effective meta heuristic algorithm to solve the real world engineering continuous optimization problems is proposed to be integrated with ls svm  the cgvns is hybridized in our novel integrated ls svm and cgvns model  to tune the parameters of the ls svm to better estimate performance rating of supplier selection and evaluation problem  to demonstrate the improved performance of our proposed integrated model  a real data set from a case study of a supplier selection and evaluation problem is presented in a cosmetics industry  additionally  comparative evaluations between our proposed model and the conventional techniques  namely nonlinear regression  multi layer perceptron  mlp  neural network and ls svm is provided  the experimental results simply manifest the outperformance of our proposed model in terms of estimation accuracy and effective prediction  
 a new extension to promethee under intuitionistic fuzzy environment for solving supplier selection problem with linguistic preferences this paper presents a new two tier decision making framework with linguistic preferences for scientific decision making  the major reason for adopting linguistic preference is to ease the process of rating of alternatives by allowing decision makers  dms  to strongly emphasize their opinion on each alternative  in the first tier  aggregation is done using a newly proposed operator called linguistic based aggregation  lba   which aggregates linguistic terms directly without making any conversion  the main motivation for this proposal is driven by the previous studies on aggregation theory which reveals that conversion leads to loss of information and formation of virtual sets which are no longer sensible and rational for decision making process  secondly  in the next tier  a new ranking method called ifsp  intuitionistic fuzzy set based promethee  is proposed which is an extension to promethee  preference ranking organization method for enrichment evaluation  under intuitionistic fuzzy set  ifs  context  unlike previous ranking methods  this ranking method follows a new formulation by considering personal choice of the dms over each alternative  the main motivation for such formulation is derived from the notion of not just obtaining a suitable alternative but also coherently satisfying the dms  viewpoint during decision process  finally  the practicality of the framework is tested by using supplier selection  ss  problem for an automobile factory  the strength and weakness of the proposed lba ifsp framework are verified by comparing with other methods under the realm of theoretical and numerical analysis  the results from the analysis infer that proposed lba ifsp framework is rationally coherent to dms  viewpoint  moderately consistent with other methods and highly stable and robust against rank reversal issue   c  2017 elsevier b v  all rights reserved  
 a new framework for cognitive mobility of visually impaired users in using tactile device this paper proposes a novel framework for a better understanding of human cognitive locomotion and its interaction with the new tactile technologies that assist mobility and the acquisition of spatial knowledge in the absence of sight  case of visually impaired people  vip   seniors  etc    unlike the existing mobility models  the proposed framework encompasses four elements  1  walking with obstacle avoidance  2  orientation  3  spatial awareness  and 4  actual physical displacement  and is based on the concept of the tactile gist  a refreshable  dynamic  egocentric tactile representation of the perceived scene  the relevance of the tactile gist concept for achieving the assistance of human mobility has been evaluated with three basic original spatial experiments  homing with obstacle avoidance  localization of a passage and its size estimation  and navigation in an apartment  using a purposely designed perception movement platform and a touch stimulating tangible wearable refreshable device  the collected results show that the tactile gist might be considered as a scene representation and should be integrated into an assistive device for sightless people  the proposed approachmay impact the design of mobility aids for vip as it reinforces their natural mobility skills and may lead to new mobility strategies of vip in  un   known environment  
 a new fuzzy logic based query expansion model for efficient information retrieval using relevance feedback approach efficient query expansion  qe  terms selection methods are really very important for improving the accuracy and efficiency of the system by removing the irrelevant and redundant terms from the top retrieved feedback documents corpus with respect to a user query  each individual qe term selection method has its weaknesses and strengths  to overcome the weaknesses and to utilize the strengths of the individual method  we used multiple terms selection methods together  in this paper  we present a new method for qe based on fuzzy logic considering the top retrieved document as relevance feedback documents for mining additional qe terms  different qe terms selection methods calculate the degrees of importance of all unique terms of top retrieved documents collection for mining additional expansion terms  these methods give different relevance scores for each term  the proposed method combines different weights of each term by using fuzzy rules to infer the weights of the additional query terms  then  the weights of the additional query terms and the weights of the original query terms are used to form the new query vector  and we use this new query vector to retrieve documents  all the experiments are performed on trec and fire benchmark datasets  the proposed qe method increases the precision rates and the recall rates of information retrieval systems for dealing with document retrieval  it gets a significant higher average recall rate  average precision rate and f measure on both datasets  
 a new hybrid ensemble credit scoring model based on classifiers consensus system approach during the last few years there has been marked attention towards hybrid and ensemble systems development  having proved their ability to be more accurate than single classifier models  however  among the hybrid and ensemble models developed in the literature there has been little consideration given to  1  combining data filtering and feature selection methods 2  combining classifiers of different algorithms  and 3  exploring different classifier output combination techniques other than the traditional ones found in the literature  in this paper  the aim is to improve predictive performance by presenting a new hybrid ensemble credit scoring model through the combination of two data pre processing methods based on gabriel neighbourhood graph editing  gng  and multivariate adaptive regression splines  mars  in the hybrid modelling phase  in addition  a new classifier combination rule based on the consensus approach  consa  of different classification algorithms during the ensemble modelling phase is proposed  several comparisons will be carried out in this paper  as follows  1  comparison of individual base classifiers with the gng and mars methods applied separately and combined in order to choose the best results for the ensemble modelling phase  2  comparison of the proposed approach with all the base classifiers and ensemble classifiers with the traditional combination methods  and 3  comparison of the proposed approach with recent related studies in the literature  five of the well known base classifiers are used  namely  neural networks  nn   support vector machines  svm   random forests  rf   decision trees  dt   and naive bayes  nb   the experimental results  analysis and statistical tests prove the ability of the proposed approach to improve prediction performance against all the base classifiers  hybrid and the traditional combination methods in terms of average accuracy  the area under the curve  auc  h measure and the brier score  the model was validated over seven real world credit datasets   c  2016 elsevier ltd  all rights reserved  
 a new hybrid multi start tabu search for finding hidden purchase decision strategies in www based on eye movements it is known that the decision strategy performed by a subject is implicit in his her external behaviors  eye movement is one of the observable external behaviors when humans are performing decision activities  due to the dramatic increase of e commerce volume on www  it is beneficial for the companies to know where the customers focus their attention on the webpage in deciding to make a purchase  this study proposes a new hybrid multi start tabu search  hmts  algorithm for finding the hidden decision strategies by clustering the eye movement data obtained during the decision activities  the hmts uses adaptive memory and employs both multi start and local search strategies  an empirical dataset containing 294 eye fixation sequences and a synthetic dataset consisting of 360 sequences were experimented with  we conduct the sign test and the result shows that the proposed hmts method significantly outperforms its variants which implement just one strategy  and the hmts algorithm shows an improvement over genetic algorithm  particle swarm optimization  and k means  with a level of significance alpha   0 01  the scalability and robustness of the hmts is validated through a series of statistical tests   c  2016 elsevier b v  all rights reserved  
 a new hybrid parametric and machine learning model with homogeneity hint for european style index option pricing here  we propose and investigate a hybrid model that combines parametric option pricing models such as black scholes  bs  option pricing model  monte carlo option pricing model  and finite difference method with nonparametric machine learning techniques such as support vector regression  svr  and extreme learning machine based regression models  the purpose of this model is to support better investment decisions by forecasting the option price with high predictive accuracy  to further reduce the forecasting error  we incorporate a homogeneity hint  i e   training the model by categorizing the options data based on moneyness and time to maturity of the option contract  into the model  we examine the feasibility and effectiveness of this model using a case study to predict the one day ahead price of index options traded in the national stock exchange of india limited  our experimental results show that the proposed new hybrid model is viable and effective and provides better predictive performance as compared with our benchmark models  standard bs model  standard monte carlo  standard finite difference model  and standard svr model   for example  the proposed hybrid model using svr improved  respectively  the root mean square error and mean absolute error by 83 66 and 85 46    d1 dataset   78 02 and 76 0    d2 dataset   91 86 and 90 62    d3 dataset   and 87 7 and 90 29    d4 dataset   when compared with the benchmarked bs model  we observe similar improvements over the other benchmarked models  therefore  the proposed new hybrid model is a suitable alternative model for option pricing when higher predictive accuracy is desired  
 a new intuitionistic fuzzy linguistic hybrid aggregation operator and its application for linguistic group decision making the linguistic aggregation operator is an important decision making model that is proving effective for dealing with the input data that takes the form of uncertain information  in this paper  considering the principal component of the intuitionistic fuzzy linguistic variables  we develop a new intuitionistic fuzzy linguistic hybrid aggregation  niflha  operator to solve group decision making problems under the situation with intuitionistic fuzzy linguistic information  then  we study some of its main properties by utilizing some operational laws of intuitionistic fuzzy linguistic variables and the different families of the niflha operator  moreover  the multiperson niflha  mp niflha  operator is introduced to evaluate the opinions of experts  finally  an illustrative example about a multiperson decision making problem is developed to reveal the applicability and the availability of the raised operator  
 a new lot sizing and scheduling heuristic for multi site biopharmaceutical production biopharmaceutical manufacturing requires high investments and long term production planning  for large biopharmaceutical companies  planning typically involves multiple products and several production facilities  production is usually done in batches with a substantial set up cost and time for switching between products  the goal is to satisfy demand while minimising manufacturing  set up and inventory costs  the resulting production planning problem is thus a variant of the capacitated lot sizing and scheduling problem  and a complex combinatorial optimisation problem  inspired by genetic algorithm approaches to job shop scheduling  this paper proposes a tailored construction heuristic that schedules demands of multiple products sequentially across several facilities to build a multi year production plan  solution   the sequence in which the construction heuristic schedules the different demands is optimised by a genetic algorithm  we demonstrate the effectiveness of the approach on a biopharmaceutical lot sizing problem and compare it with a mathematical programming model from the literature  we show that the genetic algorithm can outperform the mathematical programming model for certain scenarios because the discretisation of time in mathematical programming artificially restricts the solution space  
 a new test for the significance of neural network inputs this paper introduces a new formal test of the significance of neural network inputs  it is simple  accurate  and powerful and is based on a linear relationship between the output of a neural network when all of the input variables are fixed at their mean values other than the input variable  which is subject to significance testing and the target values of the network  simulation results show that as the number of observations increases  the power of the test tends to 1 in all cases  and that the empirical size approaches the nominal size in some cases  the results  based on the ordinary least squares  ols  estimation of parameters  are very encouraging  but using a heteroscedasticity and autocorrelation consistent covariance matrix and fast double bootstrap improves the speed of the convergence to the nominal size  the test can also be used for nonlinear models with nuisance parameters   c  2017 elsevier b v  all rights reserved  
 a new web personalization decision support artifact for utility sensitive customer review analysis in recent years there has been increased consumer use of the vast array of online reviews  given the increasingly high volume of such reviews  automatic analyses of their quality have become imperative  not surprisingly  this situation has attracted the interest of researchers  however  prior approaches are insufficient to address the consumers  need for non burdensome sense making of online reviews  this research attempts to close this gap by proposing novel design science artifacts  i e  construct  architecture  algorithms and prototype  to address the consumers  need  we evaluate these artifacts using a set of experiments and hypothesis tests  the results validate the effectiveness and efficiency of the proposed artifacts  we demonstrate their practical utility and relevance using real world pilot experiments  this paper contributes theoretical knowledge to the review quality literature and  what we believe is the first exemplifier for adequately validating the solutions of review quality research   c  2016 elsevier b v  all rights reserved  
 a non radial directional distance method on classifying inputs and outputs in dea  application to banking industry the original data envelopment analysis  dea  models have required an assumption that the status of all inputs and outputs be known exactly  whilst we may face a case with some flexible performance measures whose status is unknown  some classifier approaches have been proposed in order to deal with flexible measures  this contribution develops a new classifier non radial directional distance method with the aim of taking into account input contraction and output expansion  simultaneously  in the presence of flexible measures  to make the most appropriate decision for flexible measures  we suggest two pessimistic and optimistic approaches from both individual and summative points of view  finally  a numerical real example in the banking system in the countries of the visegrad four  i e  czech republic  hungary  poland  and slovakia  is presented to elaborate applicability of the proposed method   c  2017 elsevier ltd  all rights reserved  
 a novel approach for combination of individual and group decisions based on fuzzy best worst method the process of decision making in an enterprise may either keep the business on track or derail it  thus  a senior decision maker often use a group of experts as the supportive team to ensure appropriate decisions  the experts often have different expertise level regarding their knowledge  talent  proficiency  and experience  in this study  we first extend the best worst method based on the linguistic preferences of decision makers about importance of attributes  these preferences are converted into triangular fuzzy numbers to be utilized in the linear programming model  that is  in contrast with the original best worst method in which the preferences towards the attributes are crisp  fuzzy preferences are considered in the proposed method to reflect the imprecise comments of experts  second  we propose a novel group decision making approach based on the fuzzy best worst method to combine the opinion of senior decision maker and the opinions of the experts  indeed  our model helps the senior decision maker to make a significant trade off between democratic and autocratic decision making styles  from sensitivity analyses on two numerical examples  we show that  when there is conflict between senior decision maker and group of decision makers  the consistency of group decision making  democracy  will increase as it tends to individual decision making  autocracy    c  2017 elsevier b v  all rights reserved  
 a novel automatic satire and irony detection using ensembled feature selection and data mining figurative language detection has always been a difficult task for human beings while being a more difficult proposition  even if automated using text and data mining  the available computational approaches are also quite limited in their capabilities and scope  in this regard  we propose an ensembled text feature selection method followed by a new framework in the paradigm of text and data mining to automatically detect satire  sarcasm  and irony found in news and customer reviews  the effectiveness of the proposed approach was demonstrated on three datasets including two satiric and one ironic dataset  the proposed methodology performed well on one satiric dataset and yielded promising results on the remaining two datasets  moreover  we found out some interesting common characteristics of satire and irony like affective process  negative emotion   personal concern  leisure   biological process  body and sexual   perception  see   informal language  swear   social process  male   cognitive process  certain   and psycholinguistic  concreteness and imageability   which were extracted from three corpora  of particular significance is the comparison of our approach with human annotators  evaluations  which served as a baseline in these tasks   c  2016 elsevier b v  all rights reserved  
 a novel consensus based prediction strategy for data sensing financial contagion problems have been extensively studied in area of financial research  most of the works focus on studying the contagion effect on the financial system  contagion prediction is considered as one of the most important strategies to prevent contagion  but the prediction issue is seldom researched in the financial area  traditional financial management uses a centralized method to predict contagion risk  but the central management cannot instantly acquire complete information of entire network  a decentralized method is needed to achieve a prediction in real time  this paper introduces a distributed risk contagion prediction strategy of the financial network  firstly  consensus algorithm is used to distributively acquire contagion risk information of the entire financial network  this distributed strategy enables the system to instantly predict the risk of contagion  secondly  the impact of the financial crisis could enormously influence the convergency of consensus algorithm  so a consensus based kalman filter  caf  is proposed to maintain the convergency of consensus and ensure the accuracy of the prediction  finally  the strategy is tested in different kinds of financial systems which are impacted by different levels of the financial crisis  the simulation result shows that the strategy is robust  flexible and feasible for practical use  it also proves that the proposed strategy can provide an accurate prediction in any condition   c  2016 elsevier b v  all rights reserved  
 a novel correspondence based face hallucination method this paper addresses the problem of estimating high resolution  hr  facial images from a single low resolution  lr  input  we assume that the input lr and estimated hr images are under the same view point and illumination condition  i e  the setting of image super resolution  at the core of our techniques is that the facial images can be decomposed as a texture vector  characterized in terms of the appearance  and a shape vector  characterized in terms of the geometry variations  this enables a two stage successive estimation framework that is geometry aware and obviates the needs in sophisticated optimizations  in particular  the proposed technique first solves for appearance of the hr faces form the correspondence derived between an interpolated lr face and its corresponding hr face  given the texture of the hr faces  we incorporate optical flow to solve the local structure at sub pixel level for the hr faces  here  we use additional geometry inspired priors to further regularize the solution  experimental results show that our method outperforms other state of the art methods in terms of retaining the facial feature shape and the estimation of novel features   c  2017 elsevier b v  all rights reserved  
 a novel decision support framework for computing expected utilities from linguistic evaluations the increase in the amount and variety of evaluations provided by the users of different websites regarding the products displayed is becoming an increasingly familiar scenario  that is  decision makers  dms  constantly receive linguistic evaluations  les  from unknown evaluators when considering different choice alternatives  the imprecision of the les and the fact that the evaluators may have biased interests when describing a product must be considered by the dms when computing their expected utilities  we define a bayesian updated probability  bup  function that accounts for the fuzziness inherent in the les and the reputation of the evaluator to represent the beliefs of dms  the proposed bup process allows the dms to subjectively adjust the probability mass that is shifted across evaluation intervals when updating their beliefs and computing their corresponding expected utilities  we illustrate the behavior of the bup function numerically and describe potential decision support applications  
 a novel fuzzy pso term weighting automatic query expansion approach using combined semantic filtering information retrieval system retrieves relevant documents from large datasets  automatic query expansion  aqe  is one of the approaches to enhance ir performance by adding additional terms to original query  the selection of suitable additional terms for aqe is a crucial task  term weighting method is one of the ways to deal with such a problem  this paper presents a new term weighting based aqe approach to retrieve more relevant documents from data corpus  the proposed approach comprises of three major steps  first step determines the optimal weights of different ir evidences for different terms using particle swarm optimization  pso   fuzzy logic technique is used to improve performance of pso by controlling inertia and acceleration coefficients during the optimization  co occurrence score is introduced as new ir evidence in the proposed approach  second step is focused on removal of noisy terms by using new combined semantic filtering method  third step reweights the terms using rocchio method  the proposed approach is compared with recently developed automatic query expansion approaches in terms of performance measures such as precision  recall  f measure and map  mean average precision   three benchmark datasets cacm  cisi and trec 3 are used to verify the results  the proposed approach is found better than other approaches according to results obtained for these benchmark datasets   c  2017 elsevier b v  all rights reserved  
 a novel hybrid algorithm for solving continuous single objective defensive location problem the continuous defensive location problem  cdlp  is an np hard problem well investigated in the fields of competitive facility location  cdlp is a bi level programming problem  where a decision maker locates defensive facilities with different capacities in the vertices of the network in order to avoid her his aggressors from reaching core which is an important vertex in the network  in the present research  a hybrid method combining the imperialist competitive algorithm  ica  and bfgs algorithm is presented to solve the cdlp  the proposed hybrid method integrates the ica and the bfgs algorithm  providing a highly near optimal solution  the upper level problem solves the optimal location of defense facilities  and hybrid algorithm is applied  the lower level problem is the shortest path problem which is solved by the dijkstra method  the feasibility of the proposed hybrid method is demonstrated for a number of small  medium and large instances of the problem  the test results are compared with those obtained by genetic algorithm  particle swarm optimization and ica in terms of solution accuracy and required cpu time  simulation results reveal that the proposed hybrid method is feasible  robust and more effective in solving the cdlp than conventional metaheuristic methods  
 a novel hybrid learning achievement prediction model  a case study in gamification education applications  apps  adaptive neuro fuzzy inference system  anfis  used to be applied to finance  engineering  material design  and decision making management in past research  but seldom to predict educational learning performance  in recent research  gamification learning material design is often applied to reinforce learning performance  while the prediction of gamification learning performance is seldom discussed  this study therefore applies rough set theory to extract core set and generating rule  anfis for learning achievement predication  in order to evaluate the performance of proposed model  the vccsegls dataset are collected as experimental dataset and compared with other models  the results show that the proposed method outperforms the listing models in accuracy  the three key factors are extract   g7  time spent on game based learning   l1  examination  normal drugs and treatment  and  l2  integration ability  time scoring  stability scoring  strain capacity  completeness scoring   the proposed model also can offer accurate predictions and provide some simple decision rules  which can be accurately used by decision makers and game designers  
 a novel intelligent approach for state space evolving forecasting of seasonal time series this paper proposes a new methodology for modelling based on an evolving neuro fuzzy takagi sugeno  nf ts  network used for seasonal time series forecasting  the nf ts considers the unobservable components extracted from the time series data to evolve  that is  to adapt and to adjust its structure  where the fuzzy rules number of this network can increase or decrease according to components behaviour  the method used to extract these components is a recursive version  proposed in this paper  based on the spectral singular analysis  ssa  technique  the nf ts network adopts the principle divide to conquer  where it divides a complex problem into subproblems easier to deal  forecasting separately each unobservable component  because they present dynamic behaviours that are simpler to forecast  the consequent propositions of fuzzy rules are linear state space models  where the states are the unobservable components data  when there are available observations from the time series  the training stage of nf ts is performed  i e   the nf ts evolves its structure and adapts its parameters to carry out the mapping between the components data and the available sample of original time series  on the other hand  if this observation is not available  the network considers the forecasting stage  keeping its structure fixed and using the states of consequent fuzzy rules to feedback the unobservable components data to nf ts  the nf ts was evaluated and compared with other recent and traditional techniques for seasonal time series forecasting  obtaining competitive and advantageous results in relation to other papers  this paper also presents a case study about real time detection of anomalies based on a patient electrocardiogram data   c  2017 elsevier ltd  all rights reserved  
 a novel machine learning method based on generalized behavioral learning theory learning is an important talent for understanding the nature and accordingly controlling behavioral characteristics  behavioral learning theories are one of the popular learning theories which are built on experimental findings  these theories are widely applied in psychotherapy  psychology  neurology as well as in advertisements and robotics  there is an abundant literature associated with understanding learning mechanism  and various models have been proposed for the realization of learning theories  nevertheless  none of those models are able to satisfactorily simulate the concept of classical conditioning  in this study  popular behavioral learning theories were firstly simplified and the contentious issues with them were clarified by conducting intuitive experiments  the experimental results and information available in the literature were evaluated  and behavioral learning theories were jointly generalized accordingly  the proposed model  to our knowledge  is the first one that possesses not only modeling all features of classical conditioning but also including all features with behavioral theories such as pavlov  watson  guthrie  thorndike and skinner  also  a microcontroller card  arduino mega 2560  was used to validate the applicability of the proposed model in robotics  obtained results showed that this generalized model has a high capacity for modeling human learning  then  the proposed learning model was further improved to be utilized as a machine learning method that can continuously learn similar to human being  the result obtained from the use of this method  in terms of computational cost and accuracy  showed that the proposed method can be successfully employed in machine learning  especially for time ordered datasets  
 a novel methodology for stock investment using high utility episode mining and genetic algorithm in this paper  we present a novel methodology for stock investment using the technique of high utility episode mining and genetic algorithms  our objective is to devise a profitable episode based investment model to reveal hidden events that are associated with high utility in the stock market  the time series data of stock price and the derived technical indicators  including moving average  moving average convergence and divergence  random index and bias index  are used for the construction of episode events  we then employ the genetic algorithm for the simultaneous optimization on parameters and selection of subsets of models  the empirical results show that our proposed method significantly outperforms the state of the art methods in terms of annualized returns of investment and precision  we also provide a set of z tests to statistically validate the effectiveness of our proposed method  based upon the promising results obtained  we expect this novel methodology can advance the research in data mining for computational finance and provide an alternative to stock investment in practice   c  2017 elsevier b v  all rights reserved  
 a novel model  dynamic choice artificial neural network  dcann  for an electricity price forecasting system big data mining  analysis  and forecasting always play a vital role in modern economic and industrial fields  thus  how to select an optimization model to improve the forecasting accuracy of electricity price is not only an extremely challenging problem but also a concerned problem for different participants in an electricity market due to our society becoming heavily reliant on electricity  many researchers developed hybrid models through the use of optimization methods  classical statistical models  artificial intelligence approaches and de noising methods  however  few researchers aim to select reasonable samples and determine appropriate features when forecasting electricity price  based on the index of bad samples matrix  ibsm   a novel method to dynamically confirm bad training samples  and the optimization algorithm  oa   dcann and updated dcann are proposed in this paper for forecasting the day ahead electricity price  this model is a hybrid system of supervised and unsupervised learning and creatively applies the idea of deleting bad samples and searching quality inputs to develop and learn  which is unlike bpann  rbfn  svm and lssvm  numerical results show that the proposed model is not only able to approximate the actual electricity price  normal or high volatility  but also an effective tool for h step ahead forecasting  his less than 10  compared to benchmarks   c  2016 elsevier b v  all rights reserved  
 a novel semi  quantitative fuzzy cognitive map model for complexsystems for addressing challenging participatory real life problems fuzzy cognitive maps  fcm  are a promising approach for socio ecological systems modelling  fcms represent problem knowledge extracted from different stakeholders in the form of connected factors variables with imprecise cause effect relationships and many feedback loops  these typically large maps are condensed and aggregated to obtain a summary view of the system  however  representation  condensation and aggregation of previous fcm models are qualitative due to lack of appropriate quantitative methods  this study tackles these drawbacks by developing a semi quantitative fcm model consisting of robust methods for adequately and accurately representing and manipulating imprecise data describing a complex problem involving stakeholders for pragmatic decision making  the model starts with collecting qualitative imprecise data from relevant stakeholders  these data are then transformed into stakeholder perceptions fcms with different causal relationship formats  linguistic or numeric  which the proposed model then represents in a unified format using a 2 tuple fuzzy linguistic representation model which allows combining imprecise linguistic and numeric values with different granularity and or semantic without loss of information  the proposed model then condenses large fcms using a semi quantitative method that allows multi level condensation  in each level of condensation  groups of similar variables are subjectively condensed and the corresponding imprecise connections are computationally condensed using robust calculations involving credibility weights assigned to variables  variables  importance   the model then uses a quantitative fuzzy method to aggregate perceptions fcms into a stakeholder group or social perception fcm based on the 2 tuple model and credibility weights assigned to fcms  stakeholders  importance   thereafter  the structure of produced fcms is analysed using graph theory indices to examine differences in perceptions between stakeholders or groups  finally  the model applies various what if policy scenario simulations on group fcms using a dynamical systems approach with neural networks and analyses scenario outcomes to provide appropriate recommendations to decision makers  an example application illustrates method s effectiveness and usefulness   c  2016 elsevier b v  all rights reserved  
 a partial contour similarity based approach to visual affordances in habile agents in a typical tool use task  we can view both the relationship between the agent and the tool and the relationship between the tool and the target in terms of affordances  one set of affordances relates to the ability of the agent to manipulate the tool  while a second set of affordances relates to the ability of the agent to manipulate the target by means of the tool  in both cases  effective tool use is facilitated by the coupling of one object to another  agent to tool to target  in this paper  we focus on the visual identification of such affordances via contour similarity  objects with complementary contour segments can fit together  which suggests possible opportunities for effective interactions  we present a system for the identification and evaluation of partial contour based matches and analyze the system s behavior  we propose a set of sample tool use scenarios as part of our analysis  we demonstrate the use of the system in providing guidance to an autonomous robotic agent performing tool selection tasks  
 a partially integrated production inventory model with interval valued inventory costs  variable demand and flexible reliability this paper formulates a production inventory model to investigate the effects of partially integrated production and marketing policy of a manufacturing firm  demand is assumed to be variable and dependent on the selling price and marketing cost  also  different inventory costs are considered as interval valued  shortages are permitted and partially backlogged with a rate dependent on the waiting time  considering that manufacturing process generates defective units four possible cases have been identified and studied  basically  the optimization problems  maximization problem for marketing department and minimization problems for production and research   development departments  have been formulated and solved  for solving these optimization problems  an efficient soft computing algorithm based on particle swarm optimization constriction factor  pso co  is proposed  in order to illustrate and validate the production inventory model a numerical example is solved  finally  a sensitivity analysis is done to study the effect of changes of different system parameters on optimal policies   c  2017 elsevier b v  all rights reserved  
 a patent quality classification model based on an artificial immune system patents are business and financial assets which can enhance a company s competitive position  thus  patent analysis is important for defining business strategies and supporting decision making in organizations  however  patent analysis can involve vast data sets and are difficult to analyze  the purpose of this study is to apply artificial immune system hybrid collaborative filtering to build a patent quality classification model  we apply the model to predicting the quality of radio frequency identification patents  using a simple definition of quality  we define each patent s data as an antigen and then compute the affinities of the target patent to all immune networks  if the affinity is larger than a given threshold  the antibody is cloned to the related immune network  after the immune networks are constructed  they exhibit high affinity to the target patent  finally  a series of experiments show that the proposed model can accurately predict the quality of new patents  the resulting automatic patent quality classification model provides manufacturers with improved insights into their company s intellectual property strategy  product direction and long term vision  
 a pdf document re finding system with a q a wizard interface re finding electronic documents from personal computers is a frequent demand  for a simple re finding task  people can use many methods to retrieve the target document  such as navigating directly to its folder  searching with desktop search engines  or checking the recent files list  when encountering difficult re finding tasks  people usually cannot rely on attributes exploited by conventional re finding methods  the re finding would fail  we propose a new method to support difficult re finding tasks  by collecting extra new attributes of a document  such as number of pages  number of images  reading frequency  and coverage percentage  if the document is quested later  the collected attributes and experiences are used to filter out it  a question and answer wizard interface is utilized to alleviate cognitive burden when recollecting  finally  we develop a pdf document re finding system to evaluate the effectiveness of the method   c  2017 elsevier b v  all rights reserved  
 a performance comparison of neural networks in forecasting stock price trend the stock price shows the character of complex non linear system  along with changes of internal and external environmental factors in stock market  as a form of artificial intelligence  neural network can fully reveal the complex relationship between investors and price fluctuations  after comparing network structures of different neural networks  the conclusions show elman neural network has an obvious advantage over bp neural network in predicting price trend of chinese stock market both in theory and practice  
 a personalized point of interest recommendation model via fusion of geo social information recently  as location based social networks  lbsns  rapidly grow  general users utilize point of interest recommender systems to discover attractive locations  most existing poi recommendation algorithms always employ the check in data and rich contextual information  e  g   geographical information and users  social network information  of users to learn their preference on pois  unfortunately  these studies generally suffer from two major limitations   1  when modeling geographical influence  users  personalized behavior differences are ignored   2  when modeling the users  social influence  the implicit social influence is seldom exploited  in this paper  we propose a novel poi recommendation approach called geoeiso  geoeiso achieves three key goals in this work   1  we develop a kernel estimation method with a selfadaptive kernel bandwidth to model the geographical influence between pois   2  we use the gaussian radial basis kernel function based support vector regression  svr  model to predict explicit trust values between users  and then devise a novel trust based recommendation model to simultaneously incorporate both the explicit and implicit social trust information into the process of poi recommendation   3  we develop a unified geo social framework which combines users  preference on a poi with the geographical influence as well as social correlations  experimental results on two real world datasets collected from foursquare show that geoeiso provides significantly superior performances compared to other state of the art poi recommendation models   c  2017 elsevier b v  all rights reserved  
 a portfolio optimization model for minimizing soft margin based generalization bound roy s safety first  rsf  criterion aims to minimize the shortfall probability in portfolio selection  smoothed safety first portfolio optimization model is a useful tool to realize rsf criterion by minimizing an approximation of the empirical shortfall probability  however  the generalization performance of the smoothed safety first portfolio optimization model may be poor when the number of the samples is finite  in this paper  a soft margin based generalization bound on the shortfall probability is obtained firstly  then  a portfolio optimization model is built by minimizing the soft margin based generalization bound  finally  the good generalization performance of the portfolio optimization model is verified by experiments  
 a predictive model for use of an assistive robotic manipulator  human factors versus performance in pick and place retrieval tasks the goal of this study was to model the important individual differences to predict a user s performance when operating an assistive robotic manipulator for a general population  prior research done led to the identification of ten potential human factors to be observed including dexterity  gross and fine   spatial abilities  orientation and visualization   visual acuity in each eye  visual perception  depth perception  reaction time  and working memory  eighty nine individuals completed a test battery of potential human factors and  then  completed several tasks using a robotic manipulator designed to simulate find and fetch pick and place tasks  during interaction with the robot  time on task  number of moves  and number of moves per minute were recorded  we successfully developed statistical models predicting performance that revealed several important human factors  speed of information processing  spatial ability  dexterity  and working memory were all seen to be significant predictors of task performance  for time on task  linear and polynomial models showed roughly similar predictive performance on unseen test data achieving root mean square percentage error of about 7 3   for number of moves per minute  a polynomial model was best with 9 1  error  and for number of moves  a linear model was best with 12 8  error  
 a probabilistic formalization of the appraisal for the occ event based emotions article presents a logical formalization of the emotional appraisal theory  i e   it formalizes the cognitive process of evaluation that elicits an emotion  this formalization is psychologically grounded on the occ cognitive model of emotions  more specifically  we are interested in event based emotions  i e   emotions that are elicited by the evaluation of the consequences of an event that either happened or will happen  the formal modelling presented here is based on the afpl probabilistic logic  a bdi like probabilistic modal logic  which allows our model to verify whether the variables that determine the elicitation of emotions achieved the necessary threshold or not  the proposed logical formalization aims at addressing how the emotions are elicited by the agent cognitive mental states  desires  beliefs and intentions   and how to represent the intensity of the emotions  these are important initial points in the investigation of the dynamic interaction among emotions and other mental states  
 a production inventory model with price discounted fuzzy demand using an interval compared hybrid algorithm an economic production quantity  epq  model in an imprecise environment is proposed  where the production rate  planning horizon and demand coefficients are fuzzy in nature  at the beginning of each cycle a price discount is offered for a period to boost the demand  during this period  demand increases with time depending upon the amount of discount  here  demand also depends on the unit selling price  after withdrawal of the price discount  demand depends only on the unit selling price  the governing differential equation for the model is obviously fuzzy in nature as the production rate and demand are fuzzy  for this reason  the model is formulated using a fuzzy differential equation and the a cut of the total profit from the planning horizon is obtained using fuzzy riemann integration  to optimize the interval objective function  using a fuzzy preference relation on intervals and a fuzzy possibility necessity measure  a hybrid algorithm with varying population size is developed by combining the features of particle swarm optimization  pso  and a genetic algorithm  ga   this algorithm is named interval compared hybrid particle swarm genetic algorithm  ichpsga  and is used to find an optimal decision for the decision maker  dm  in different cases of the model  to test the efficiency of the algorithm  it is compared with two other established algorithms namely pso and psga  numerical experiments are performed to illustrate the model and some interesting observations are made  
 a proof of concept study for criminal network analysis with interactive strategies the communication data are becoming increasingly important for criminal network analysis nowadays  and these data provide a digital trace which can be regarded as a hidden clue to support the crack of criminal cases  additionally  performing a timely and effective analysis on it can predict criminal intents and take efficient actions to restrain and prevent crimes  the primary work of our research is to suggest an analytical process with interactive strategies as a solution to the problem of characterizing criminal groups constructed from the communication data  it is expected to assist law enforcement agencies in the task of discovering the potential suspects and exploring the underlying structures of criminal network hidden behind the communication data  this process allows for network analysis with commonly used metrics to identify the core members  it permits exploration and visualization of the network in the goal of improving the comprehension of interesting microstructures  most importantly  it also allows to extract community structures in an appropriate level with the label supervision strategy  our work concludes illustrating the application of our interactive strategies to a real world criminal investigation with mobile call logs  
 a prospect theory based approach to multiple attribute decision making considering the decision maker s attitudinal character the decision maker s attitudinal character towards risk and the psychological aspects are critical factors of the multiple attribute decision making  madm  problems  in this paper  we consider the attitudinal character during the whole decision making procedure  the reference points associated with prospect theory are in the form of interval valued intuitionistic fuzzy number  ivifn  and we assume the membership degree and non membership degree of the ivifn obey normal distribution according to the central limit theorem  we combine the properties of normal distribution and the attitudinal character associated with the ordered weighted averaging  owa  operator to convert all the reference points into crisp numbers  we construct one feasible multiple objective optimization programming to obtain the optimal weight vector of the attributes by minimizing the deviation between the alternative and the reference point corresponding to each alternative  the hybrid weighted averaging  hwa  operator is applied to aggregate the prospect values for each alternative  especially  we directly reorder the weighted prospect values considering their plus or minus  which reflects the decision maker s attitudinal character towards risk as well  finally  an illustrative example about group buying is provided to show the feasibility of the proposed method  
 a prospect theory based madm method for solar water heater selection problems recently  the solar water heaters have been widely used in china  for common consumers  the solar water heater selection problems have theoretical and practical significance  in this paper  a multiple attribute decision making method for solar water heater selection problems is proposed  firstly  the evaluations to alternatives take the form of linguistic terms and then are transformed into triangular fuzzy numbers  secondly  the reference points of the attributes are obtained from the decision maker  then  based on prospect theory and the operation rules of fuzzy numbers  the linguistic rating information is converted and aggregated  finally  the alternatives are ranked from best to worst  the first one is the optimal choice  the availability of the proposed method is illustrated through an application in solar water heater selection problems  
 a recommendation approach for programming online judges supported by data preprocessing techniques the use of programming online judges  poj  to support students acquiring programming skills is common nowadays because this type of software contains a large collection of programming exercises to be solved by students  a poj not only provides exercises but also automates the code compilation and its evaluation process  a common problem that students face when using poj is information overload  as choosing the right problem to solve can be quite frustrating due to the large number of problems offered  the integration of current pojs into e learning systems such as intelligent tutoring systems  itss  is hard because of the lack of necessary information in itss  hence  the aim of this paper is to support students with the information overload problem by using a collaborative filtering recommendation approach that filters out programming problems suitable for students  programming skills  it uses an enriched user problem matrix that implies a better student role representation  facilitating the computation of closer neighborhoods and hence a more accurate recommendation  additionally a novel data preprocessing step that manages anomalous users  behaviors that could affect the recommendation generation is also integrated in the recommendation process  a case study is carried out on a poj real dataset showing that the proposal outperforms other previous approaches  
 a recommender system based on collaborative filtering using ontology and dimensionality reduction techniques improving the efficiency of methods has been a big challenge in recommender systems  it has been also important to consider the trade off between the accuracy and the computation time in recommending the items by the recommender systems as they need to produce the recommendations accurately and meanwhile in real time  in this regard  this research develops a new hybrid recommendation method based on collaborative filtering  cf  approaches  accordingly  in this research we solve two main drawbacks of recommender systems  sparsity and scalability  using dimensionality reduction and ontology techniques  then  we use ontology to improve the accuracy of recommendations in cf part  in the cf part  we also use a dimensionality reduction technique  singular value decomposition  svd   to find the most similar items and users in each cluster of items and users which can significantly improve the scalability of the recommendation method  we evaluate the method on two real world datasets to show its effectiveness and compare the results with the results of methods in the literature  the results showed that our method is effective in improving the sparsity and scalability problems in cf   c  2017 elsevier ltd  all rights reserved  
 a relative value trading system based on a correlation and rough set analysis for the foreign exchange futures market this paper describes the conceptual framework of a relative value  rv  based trading system focused on the data characteristics of the foreign exchange futures market using a correlation and rough set analysis  rv trading is an investment strategy that can generate potential profits based on the rv of two securities  regardless of market direction  we select pairs with a positive correlation  negative correlation  or no correlation based on the correlation coefficients between foreign exchange futures contracts  to implement and experiment with the proposed system  trading rules are generated using a rough set analysis that employs technical indicators derived from the rvs of the pairs  the performance of the proposed trading system is analyzed using the momentum and buy and hold trading strategies as benchmarks  the experimental results and analyses demonstrate that the level of the correlation of the pairs must be considered when developing stable and profitable rv trading systems in a foreign exchange futures market  
 a review and future perspectives of arabic question answering systems question answering systems  qass  have emerged as a good alternative for information seekers to retrieve precise information over the internet  a good amount of research has been done to improve the performance of qass across several languages  including european and asian languages  however  arabic  a morphologically rich semitic language spoken by over 422 million people  has not seen similar development in the field of question answering  this article reviews the developments taking place in arabic qass as well as the challenges faced by researchers in developing arabic qass  after conducting an extensive literature survey of a number of english and arabic qass  this article classifies them according to several criteria  the most commonly used architecture for the development of an arabic qas  known as pipeline architecture  has been presented  in order to encourage and support the new researchers and scholars in conducting research in arabic qass  a list of techniques  tools  and computational linguistic resources  required to implement the components of the presented pipelined architecture  are described in this article in a simple and persuasive manner  finally  the gap analysis between the research in arabic and english qass has been performed and accordingly  some future directions for research in arabic qass have been proposed  
 a review of affective computing  from unimodal analysis to multimodal fusion affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields  ranging from artificial intelligence  natural language processing  to cognitive and social sciences  with the proliferation of videos posted online  e g   on youtube  facebook  twitter  for product reviews  movie reviews  political views  and more  affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis  this is the primary motivation behind our first of its kind  comprehensive literature review of the diverse field of affective computing  furthermore  existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks  which this review aims to address  multimodality is defined by the presence of more than one modality or channel  e g   visual  audio  text  gestures  and eye gage  in this paper  we focus mainly on the use of audio  visual and text information for multimodal affect analysis  since around 90  of the relevant literature appears to cover these three modalities  following an overview of different techniques for unimodal affect analysis  we outline existing methods for fusing information from different modalities  as part of this review  we carry out an extensive study of different categories of state of the art fusion techniques  followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis  a comprehensive overview of these two complementary fields aims to form the building blocks for readers  to better understand this challenging and exciting research field   c  2017 elsevier b v  all rights reserved  
 a review of tacit knowledge  current situation and the direction to go currently  tacit knowledge has attracted increasing research attention  however  the theoretical foundation of tacit knowledge is still not well formulated  because the researches are very disperse  this work provides a review of the current researches  first  the definition of tacit knowledge is discussed by answering several questions  next  tacit knowledge sharing  tacit knowledge quantization are identified as two research topics in the current research community  following that  the technical progress of each topic is summarized and analyzed  finally  we provide a thumbnail of the researches and identify three research consensuses to answer where we are  while  seven research directions are identified to answer where we shall go  
 a review of the nature and effects of guidance design features guidance design features in information systems are used to help people in decision making  problem solving  and task execution  various information systems instantiate guidance design features  which have specifically been researched in the field of decision support systems for decades  however  due to the lack of a common conceptualization  it is difficult to compare the research findings on guidance design features from different literature streams  this article reviews and analyzes the work of the research streams of decisional guidance  explanations  and decision aids conducted in the last 25 years  building on and grounded by the analyzed literature  we theorize an integrated taxonomy on guidance design features  applying the taxonomy  we discuss existing empirical results  identify effects of different guidance design features  and propose opportunities for future research  overall  this article contributes to research and practice  the taxonomy allows researchers to describe their work by using a set of dimensions and characteristics and to systematically compare existing research on guidance design features  from a practice oriented perspective  we provide an overview on design features to support implementing guidance in various types of information systems   c  2017 elsevier b v  all rights reserved  
 a review of uncertain portfolio selection this paper reviews the theory of uncertain portfolio selection which uses human estimates as inputs and applies uncertainty theory to select portfolios  the difference of the uncertain portfolio selection theory from the stochastic portfolio theory is given and the necessity and conditions for using the theory are presented  some basic works are introduced  including different types of mathematical risk measurements  their features  and some basic uncertain portfolio selection models and their theorems  finally  future work for uncertain portfolio selection is discussed  
 a review on feature binding theory and its functions observed in perceptual process binding problem  which is also called feature binding  is primarily about integrating distributed information scattered on different cortical areas in a reasonable way  as a key problem in cognitive science and neuroscience  this concept is increasingly becoming a focus of consciousness study  this paper first introduced the concept  characteristics  and biological basis of feature binding  then  this paper illustrated three feature binding theories namely feature integration theory  synchronous neural activation theory  and neural network model of feature binding  and then reviewed the advantages and disadvantages of these three feature binding theories  to demonstrate why feature binding indeed exists  we reviewed works on the functions of feature binding observed in perceptual learning  conclusions were reached that feature binding exists in many processes of perception  this paper also suggested future research in this area should focus on systematic study of bundled brain mechanisms  
 a revision and analysis of the comprehensiveness of the main longitudinal studies of human aging for data mining research human aging is a global problem that will have a large socioeconomic impact  a better understanding of aging can direct public policies that minimize its negative effects in the future  over many years  several longitudinal studies of human aging have been conducted aiming to comprehend the phenomenon  and various factors influencing human aging are under analysis  in this review  we categorize the main aspects affecting human aging into a taxonomy for assisting data mining  dm  research on this topic  we also present tables summarizing the main characteristics of 64 research articles using data from aging related longitudinal studies  in terms of the aging related aspects analyzed  the main data analysis techniques used  and the specific longitudinal database mined in each article  finally  we analyze the comprehensiveness of the main databases of longitudinal studies of human aging worldwide  regarding which proportion of the proposed taxonomy s aspects are covered by each longitudinal database  we observed that most articles analyzing such data use classical  parametric  linear  statistical techniques  with little use of more modern  nonparametric  nonlinear  dm methods for analyzing longitudinal databases of human aging  we hope that this article will contribute to dm research in two ways  first  by drawing attention to the important problem of global aging and the free availability of several longitudinal databases of human aging  second  by providing useful information to make research design choices about mining such data  e g   which longitudinal study and which types of aging related aspects should be analyzed  depending on the research s goals   c  2017 john wiley   sons  ltd 
 a robust approach to multiple vehicle location routing problems with time windows for optimization of cross docking under uncertainty the concern about significant changes in the logistics environment  such as the diversification of demands and supply quantities in pickup and delivery processes  has spurred an interest in designing scalable and robust cross docking planning  in this study  a robust optimization model is introduced to deal with the inherent uncertainty of input data in the location and vehicle routing scheduling problems in cross docking distribution networks  for this purpose  a new two phase deterministic mixed integer linear programming  milp  model is proposed for locating cross docks and scheduling vehicle routing with multiple cross docks  then  the robust counterpart of the proposed two phase milp model is proposed by employing the recent developments in robust optimization theory  finally  to evaluate the robustness of obtained solutions by the new robust optimization model  a comparison is made with the obtained solutions by the deterministic milp model in a number of realizations based on different test problems  moreover  a meta heuristic algorithm  namely self adaptive imperialist competitive algorithm  saica   is presented for the multiple vehicle location routing problems  finally  this study provides various computational test problems to demonstrate the applicability and capability of the proposed robust two phase milp model and meta heuristic solution approach  
 a robust cognitive architecture for learning from surprises learning from surprises is a cornerstone for building bio inspired cognitive architectures that can autonomously learn from interactions with their environments  however  distinguishing true surprises from which useful information can be extracted to improve an agent s world model from environmental noise is a fundamental challenge  this paper proposes a new and robust approach for actively learning a predictive model of discrete  stochastic  partially observable environments based on a concept called the stochastic distinguishing experiment  sde   sdes are conditional probability distributions over the next observation given a variable length sequence of ordered actions and expected observations up to the present that partition the space of possible agent histories  thus forming an approximate predictive representation of state  we derive this sde based learning algorithm and present theoretical proofs of its convergence and computational complexity  theoretical and experimental results in small environments with important theoretical properties demonstrate the algorithm s ability to build an accurate predictive model from one continuous interaction with its environment without requiring any prior knowledge of the underlying state space  the number of sdes to use  or even a bound on sde length   c  2017 elsevier b v  all rights reserved  
 a rotational motion perception neural network based on asymmetric spatiotemporal visual information processing all complex motion patterns can be decomposed into several elements  including translation  expansion contraction  and rotational motion  in biological vision systems  scientists have found that specific types of visual neurons have specific preferences to each of the three motion elements  there are computational models on translation and expansion contraction perceptions  however  little has been done in the past to create computational models for rotational motion perception  to fill this gap  we proposed a neural network that utilizes a specific spatiotemporal arrangement of asymmetric lateral inhibited direction selective neural networks  dsnns  for rotational motion perception  the proposed neural network consists of two parts presynaptic and postsynaptic parts  in the presynaptic part  there are a number of lateral inhibited dsnns to extract directional visual cues  in the postsynaptic part  similar to the arrangement of the directional columns in the cerebral cortex  these direction selective neurons are arranged in a cyclic order to perceive rotational motion cues  in the postsynaptic network  the delayed excitation from each direction selective neuron is multiplied by the gathered excitation from this neuron and its unilateral counterparts depending on which rotation  clockwise  cw  or counter cw  ccw   to perceive  systematic experiments under various conditions and settings have been carried out and validated the robustness and reliability of the proposed neural network in detecting cw or ccw rotational motion  this research is a critical step further toward dynamic visual information processing  
 a rough set based association rule approach implemented on a brand trust evaluation model in commerce  businesses use branding to differentiate their product and service offerings from those of their competitors  the brand incorporates a set of product or service features that are associated with that particular brand name and identifies the product service segmentation in the market  this study proposes a new data mining approach  a rough set based association rule induction  implemented on a brand trust evaluation model  in addition  it presents as one way to deal with data uncertainty to analyse ratio scale data  while creating predictive if then rules that generalise data values to the retail region  as such  this study uses the analysis of algorithms to find alcoholic beverages brand trust recall  finally  discussions and conclusion are presented for further managerial implications  
 a scaffolding approach to coreference resolution integrating statistical and rule based models we describe a scaffolding approach to the task of coreference resolution that incrementally combines statistical classifiers  each designed for a particular mention type  with rule based models  for sub tasks well matched to determinism   we motivate our design by an oracle based analysis of errors in a rule based coreference resolution system  showing that rule based approaches are poorly suited to tasks that require a large lexical feature space  such as resolving pronominal and common noun mentions  our approach combines many advantages  it incrementally builds clusters integrating joint information about entities  uses rules for deterministic phenomena  and integrates rich lexical  syntactic  and semantic features with random forest classifiers well suited to modeling the complex feature interactions that are known to characterize the coreference task  we demonstrate that all these decisions are important  the resulting system achieves 63 2 f1 on the conll 2012 shared task dataset  outperforming the rule based starting point by over seven f1 points  similarly  our system outperforms an equivalent sieve based approach that relies on logistic regression classifiers instead of random forests by over four f1 points  lastly  we show that by changing the coreference resolution system from relying on constituent based syntax to using dependency syntax  which can be generated in linear time  we achieve a runtime speedup of 550 per cent without considerable loss of accuracy   
 a scalable architecture for data intensive natural language processing computational power needs have greatly increased during the last years  and this is also the case in the natural language processing  nlp  area  where thousands of documents must be processed  i e   linguistically analyzed  in a reasonable time frame  these computing needs have implied a radical change in the computing architectures and big scale text processing techniques used in nlp  in this paper  we present a scalable architecture for distributed language processing  the architecture uses storm to combine diverse nlp modules into a processing chain  which carries out the linguistic analysis of documents  scalability requires designing solutions that are able to run distributed programs in parallel and across large machine clusters  using the architecture presented here  it is possible to integrate a set of third party nlp modules into a unique processing chain which can be deployed onto a distributed environment  i e   a cluster of machines  so allowing the language processing modules run in parallel  no restrictions are placed a priori on the nlp modules apart of being able to consume and produce linguistic annotations following a given format  we show the feasibility of our approach by integrating two linguistic processing chains for english and spanish  moreover  we provide several scripts that allow building from scratch a whole distributed architecture that can be then easily installed and deployed onto a cluster of machines  the scripts and the nlp modules used in the paper are publicly available and distributed under free licenses  in the paper  we also describe a series of experiments carried out in the context of the newsreader project with the goal of testing how the system behaves in different scenarios  
 a seasonal feedforward neural network to forecast electricity prices in power industry and management  given the peculiarity and high complexity of the time series  it is highly requested to make models more flexible and well adapted to the data  in order to give them the ability to capture and recognize the most complex patterns  and consequently gain more accuracy in forecasting  to fulfill this aim  we define the seasonal autoregressive neural network  sar nn  as a dynamic feedforward artificial neural network  ann   essentially conceived to forecast electricity prices  the sar nn is an ann based autoregressive model that considers that autoregressors are only those that are lagged by a multiple of the period p of the dominating seasonality  moving forward this neural network  step by step  allows to generate multiple steps ahead reliable forecasts  this model is specifically designed to robustly overcome the strong seasonal effects and the other nonlinear patterns that often harm anns forecasting performance  the strategy is tested and compared to a number of homologous models throughout empirical experiments  as a case study  we focus on the nord pool scandinavian power market  which is one of the most mature energy markets worldwide  
 a segmental framework for fully unsupervised large vocabulary speech recognition zero resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions  lexicons  or language modelling text  early term discovery systems focused on identifying isolated recurring patterns in a corpus  while more recent full coverage systems attempt to completely segment and cluster the audio into word like units effectively performing unsupervised speech recognition  this article presents the first attempt we are aware of to apply such a system to large vocabulary multi speaker data  our system uses a bayesian modelling framework with segmental word representations  each word segment is represented as a fixed dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector  we compare our system on english and xitsonga datasets to state of the art baselines  using a variety of measures including word error rate  obtained by mapping the unsupervised output to ground truth transcriptions   very high word error rates are reported in the order of 70 80  for speaker dependent and 80 95  for speaker independent systems highlighting the difficulty of this task  nevertheless  in terms of cluster quality and word segmentation metrics  we show that by imposing a consistent top down segmentation while also using bottom up knowledge from detected syllable boundaries  both single speaker and multi speaker versions of our system outperform a purely bottom up single speaker syllable based approach  we also show that the discovered clusters can be made less speaker  and gender specific by using an unsupervised autoencoder like feature extractor to learn better frame level features  prior to embedding   our system s discovered clusters are still less pure than those of unsupervised term discovery systems  but provide far greater coverage   c  2017 elsevier ltd  all rights reserved  
 a semantic and feature aggregated information retrieval technique for efficient geospatial text document retrieval processing the normal text is quite easier and the information can be efficiently retrieved  there are various algorithms have been already proposed for normal text retrieval  whereas retrieving the geospatial information are very complex due to nature of the data  it needs supplementary processes to be performed  since geospatial data contains complex information like location and direction  to effectively handle thegeospatial queries  a novel semantic and feature aggregated information retrieval is proposed in this paper  preliminary  there are four steps need to be perform  they are clustering  indexing  retrieval and ranking  also  context based query weighting  cqw  approach is proposed to cluster the documents present in the corpus and indexing is based on multilevel hashing  feature probability and density  fpd  technique is utilized to retrieve the document which matches the user query information  the semantic density  sd  technique is used to rank the retrieved documents  the experimental results shows that the proposed sfair technique provides better results than the existing technique  
 a semantic grained perspective of latent knowledge modeling in the era of web 2 0  the knowledge is the de facto social currency in the global network environment  knowledge is not an accumulation of data  but a relation based representation of the information content  which needs to be distilled and arranged in a semantic infrastructure to guarantee interoperability and sharable understanding  in the light of this scenario  the paper introduces a semantically enhanced document retrieval system that describes each retrieved document with an ontological multi grained network of the extracted conceptualization  the system is based on two well known latent models  latent semantic analysis  lsa  and latent dirichlet allocation  lda   lsa provides a spatial distribution of the input documents  facilitating their retrieval  thanks to an ontological representation of their relationship network  lda works instead at deeper level  it drives the ontological structuring of the knowledge inside the individual retrieved documents in terms of words  concepts and topics  the novelty of this approach is a multi level granulation of the knowledge  from a document matching the query  coarse granularity   to the topics that join documents  until to the words describing a concept into a topic  fine granularity   the final result is a skos based ontology  ad hoc created for a document corpus  graphically supported for the navigation  it enables the exploration of the concepts at different granularity levels   c  2016 elsevier b v  all rights reserved  
 a service selection model for digital music service platforms using a hybrid mcdm approach digital music services have provided more and more digital contents and service styles  the number of people paying to download music is on the rise  digital music files  mainly in mp3 format  have become widespread on the internet  downloading digital products for free may harm creators and music publishers  because it is very easy to obtain free music through peer to peer sharing technologies over the internet  at the same time  portable entertainment devices and mobile phones are now able to carry music files  enabling people to access music much more easily  on the other hand  with the coming of the 5g in the telecom infrastructure  the rise in downloading music using mobile devices becomes possible  people can access online music platform services through cable adsl with their digital devices  e g   personal computer  notebook and smart phone  or through the telecom services accompanying their mobile devices  therefore  a critical issue for record publishers  or digital music service providers  is how to provide services to create value and fulfill customer needs  by determining customer music service needs and intentions  this study identifies the selection criteria necessary for customers to evaluate and select digital music service platforms  a novel mcdm  multiple criteria decision making  technique is developed that integrates the decision making trial and evaluation laboratory  dematel   principal component analysis  pca   analytical network procedure  anp   and vlsekriterijumska optimizacija i kompromisno resenje  vikor   this technique ranks and improves the digital music service platforms to obtain the best win win service selection  this paper will propose the key driving aspects of the digital music service platforms and rank them by using the model proposed  the conclusions are composed of suggestions for service providers to improve their existing functions and plan further utilities for the digital music service platforms   c  2016 elsevier b v  all rights reserved  
 a shareholder voting method for proxy advisory firm selection based on 2 tuple linguistic picture preference relation in a modern economic and regulatory system  proxy advisory firms as the third party noticeably engage in promoting the competitiveness of enterprises by instructing corporate governance  shareholders tend to have the authority to vote for an optimal proxy advisory firm  consider the characteristics of voting selection problem  a group decision making  gdm  voting method based on 2 tuple linguistic picture preference relation  2tlppr  is newly generalized into this paper to resolve it  first  for a better description of the voting context  2 tuple linguistic model is introduced into picture fuzzy sets  pfss  as the representation of voting information  and 2tlppr is employed to the said issue owing to its capacity to model synthetic voting information  second  a novel ranking method that emphasizes the risk preference of decision makers  dms  is investigated for further analysis  third  motivated by predefined theoretical basics  we explore an additive consistency based conversion method to approximate consistency  this method adequately considers the influential factors such as risk preference and contradictory judgment of dms  fourth  2 tuple linguistic picture power weighted averaging  2tlppwa  operator is utilized as the aggregation method to weaken the impact of non cooperative dms  a 2tlppr based gdm framework  which consists of consistency conversion as well as aggregation and ranking methods  is constructed to match the voting selection context  finally  we conduct the gdm framework on an illustrative example concerning selecting a proxy advisory firm  and further demonstrate its feasibility and reasonability by applying it to a sensitive analysis and comparison analyses   c  2017 elsevier b v  all rights reserved  
 a ship navigator s mental workload using salivary no3  concentration for simulator based experiment we challenge an evaluation of a ship bridge teammate s mental workload  if we are able to evaluate using a quantitative index for a veteran navigator  it means we get more essential skill for practical education  for example  the educators show students the detail suggestion of skill of ship maneuvering for simulator based and real ship handling  we think that the both of performance and mental workload are important to evaluate the skill of ship maneuvering  however  the mental workload doesn t use yet  regarding the index of mental workload  the heart rate variability  hrv  is popular for a lot of research fields  we usual use hrv  nasal temperature  and try saliva  the saliva is possible to evaluate the mental workload on the spot for ship maneuvering  we hope simple measurement and clear result for the index  and it is practical method in a classroom  the spot measurement is one of research target in this study  we aim salivary no3  concentration for simulator based experiment  we show the results of the response of salivary no3  for narrow passage and entering a port in which the navigator needs the judgement for safe navigation  
 a simulated annealing heuristic for the hybrid vehicle routing problem this study proposes the hybrid vehicle routing problem  hvrp   which is an extension of the green vehicle routing problem  g vrp   we focus on vehicles that use a hybrid power source  known as the plug in hybrid electric vehicle  phev  and generate a mathematical model to minimize the total cost of travel by driving phev  moreover  the model considers the utilization of electric and fuel power depending on the availability of either electric charging or fuel stations  we develop simulated annealing with a restart strategy  sa rs  to solve this problem  and it consists of two versions  the first version determines the acceptance probability of a worse solution using the boltzmann function  denoted as sa rsbf  the second version employs the cauchy function to determine the acceptance probability of a worse solution  denoted as sa rscf  the proposed sa algorithm is first verified with benchmark data of the capacitated vehicle routing problem  cvrp   with the result showing that it performs well and confirms its efficiency in solving cvrp  further analysis show that sa rscf is preferable compared to sa rsrf and that sa with a restart strategy performs better than without a restart strategy  we next utilize the sa rscf method to solve hvrp  the numerical experiment presents that vehicle type and the number of electric charging stations have an impact on the total travel cost   c  2016 elsevier b v  all rights reserved  
 a simulation based multi attribute group decision making technique with decision constraints the technique for order preference by similarity to ideal solution  topsis  is a useful technique for solving multi attribute group decision making  magdm  problems  in magdm  the performance scores of the alternatives and the weights of assessment attributes are mostly vague  therefore  using of deterministic data throughout decision making process may lead to inaccurate results  in order to overcome inherent vagueness and uncertainty  various fuzzy magdm techniques were presented in the literature  however  these fuzzy magdm techniques are focused on expected and extreme values  which are sometimes insufficient for the precise determination of alternatives preference structure  in this paper  in order to eliminate the limitations of deterministic and fuzzy magdm methods  we present a probabilistic methodology  which is based on topsis and monte carlo simulation of triangular data  in addition to its straightforward application and thanks to its versatility  simulation enables decision makers to incorporate some decision constraints into decision making process  two illustrative examples are also given to show the effectiveness of the proposed methodology  the method is also compared with a fuzzy topsis technique from the literature   c  2016 published by elsevier b v  
 a snail shell process model for knowledge discovery via data analytics the rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process  kddm  models are not adequately suited to address  we propose a snail shell process model for knowledge discovery via data analytics  kdda  to addrets these challenges  we evaluate the utility of the kdda process model using real world analytic case studies at a global multi media company  by comparing against traditional kddm models  we demonstrate the need and relevance of the snail shell model  particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment  c  2016 elsevier b v  all rights reserved  
 a social route recommender mechanism for store shopping support to survive in a fiercely competitive business environment  it has become increasingly important for physical retailers to provide customers with services offering a better shopping experience  many renovate and enlarge their shopping spaces to make their stores more enjoyable places to visit  the growth in social media and the use of mobile devices provide retailers with an opportunity to offer a context aware guidance service to enhance customers  in store shopping experience  in this research  by extracting and analysing shopping information  shopping context  visiting trajectory  and social information  user s interest  friends  influence   a contextual store shopping recommendation system is proposed to provide an appropriate route for first time customers or those who are unfamiliar with a retailer s shopping space  our experimental results show that the proposed model is effective in providing an appropriate shopping route and enhancing users  shopping experience  which could significantly improve the profitability and competitive advantage of the retailers   c  2016 elsevier b v  all rights reserved  
 a social ties based approach for group decision making problems with incomplete additive preference relations with the rapid growth of web 2 0 technology  a new paradigm has been developed that allows many users to participate in decision making processes within online social networks  the social information  i e   social ties and social influence  of the members that is stored in online social networks provides a new perspective for investigating group decision making  gdm  problems  in this paper  a new interactive gdm approach  based on online social networks  is proposed to address a ranking problem with incomplete additive preference relations  iaprs   this approach incorporates the strength of social ties and social influence calculated by social network analysis methods regarding the decision making process  after decision makers  dms  provide iaprs  a searching algorithm is developed to identify the optimal preference information transfer path from dms to the decision supporters who can provide the corresponding preference information  next  a linear programming model is constructed to complete the missing preference values of the iaprs  the main features of the linear programming model include its ability to account for other dms  preference information and to maintain consistency  to help the group reach an agreement on the ranking of alternatives  a consensus reaching process is proposed  the strength of social ties and social influence are used to calculate the acceptable adjustment coefficients for dms in the feedback mechanism  finally  an illustrative example and further discussion demonstrate the validity of the proposed approach   c  2016 elsevier b v  all rights reserved  
 a socially based distributed self organizing algorithm for holonic multi agent systems  case study in a task environment holonic multi agent systems  hmass  have recently attracted many researches in multi agent systems community  inspired from the multi level and self similar structures of social and biological system  holonic multi agent systems have been widely used to model and solve complex real world problems  the main concern in deploying hmass is the problem of building the hierarchical holonic structure  called holarchy  and dynamically managing it during its lifetime  the way an hmas is organized has a great impact on its applicability and performance  this paper proposes a self organizing algorithm to build and manage the holoic structures in multi agent systems  this algorithm is based on the local information of the agents about other agents they can communicate with  using common social concepts  like skills  diversity  social exchange theory  and norms in definition of the algorithm  the outcomes of this research can be used in wide ranges of distributed applications  the proposed model is extensively tested in a task allocation problem  and its performance based on various design parameters is studied  empirical results show that the proposed model properly increases the performance of the system in terms of effectiveness and efficiency   c  2016 elsevier b v  all rights reserved  
 a soft supernumerary robotic finger and mobile arm support for grasping compensation and hemiparetic upper limb rehabilitation in this paper  we present the combination of our soft supernumerary robotic finger i e  soft sixthfinger with a commercially available zero gravity arm support  the saebomas  the overall proposed system can provide the needed assistance during paretic upper limb rehabilitation involving both grasping and arm mobility to solve task oriented activities  the soft sixthfinger is a wearable robotic supernumerary finger designed to be used as an active assistive device by post stroke patients to compensate the paretic hand grasp  the device works jointly with the paretic hand arm to grasp an object similarly to the two parts of a robotic gripper  the saebomas is a commercially available mobile arm support to neutralize gravity effects on the paretic arm specifically designed to facilitate and challenge the weakened shoulder muscles during functional tasks  the proposed system has been designed to be used during the rehabilitation phase when the arm is potentially able to recover its functionality  but the hand is still not able to perform a grasp due to the lack of an efficient thumb opposition  the overall system also act as a motivation tool for the patients to perform task oriented rehabilitation activities  with the aid of proposed system  the patient can closely simulate the desired motion with the nonfunctional arm for rehabilitation purposes  while performing a grasp with the help of the soft sixthfinger  as a pilot study we tested the proposed system with a chronic stroke patient to evaluate how the mobile arm support in conjunction with a robotic supernumerary finger can help in performing the tasks requiring the manipulation of grasped object through the paretic arm  in particular  we performed the frenchay arm test  fat  and box and block test  bbt   the proposed system successfully enabled the patient to complete tasks which were previously impossible to perform   c  2017 elsevier b v  all rights reserved  
 a solid transportation model with product blending and parameters as rough variables in this paper  we formulate a practical solid transportation problem with product blending which is a common issue in many operational and planning models in the chemical  petroleum  gasoline and process industries  in the problem formulation  we consider that raw materials from different sources with different quality  or purity  levels are to be transported to some destinations so that the materials received at each destination can be blended together into the final product to meet minimum quality requirement of that destination  the parameters such as transportation costs  availabilities  demands are considered as rough variables in designing the model  we construct a rough chance constrained programming  rccp  model for the problem with rough parameters based on trust measure  this rccp model is then transformed into deterministic form to solve the problem  numerical example is presented to illustrate the problem model and solution strategy  the results are obtained using the standard optimization solver lingo  
 a stacked generalization system for automated forex portfolio trading multiple forex time series forecasting is a hot research topic in the literature of portfolio trading  to this end  a large variety of machine learning algorithms have been examined  however  it is now widely understood that  in real world trading settings  no single machine learning model can consistently outperform the alternatives  in this work  we examine the efficacy and the feasibility of developing a stacked generalization system  intelligently combining the predictions of diverse machine learning models  our approach establishes a novel inferential framework that comprises the following levels of data processing   i  we model the dependence patterns between major currency pairs via a diverse set of commonly used machine learning algorithms  namely support vector machines  svms   random forests  rfs   bayesian autoregressive trees  bart   dense layer neural networks  nns   and naive bayes  nb  classifiers   ii  we generate implied signals of exchange rate fluctuation  based on the output of these models  as well as appropriate side information obtained by analyzing the correlations across currency pairs in our training datasets   iii  we finally combine these implied signals into an aggregate predictive waveforth  by leveraging majority voting  genetic algorithm optimization  and regression weighting techniques  we thoroughly test our framework in real world trading scenarios  we show that our system leads to significantly better trading performance than the considered benchmarks  thus  it represents an attractive solution for financial firms and corporations that perform foreign exchange portfolio management and daily trading  our system can be used as an integrated part in international commercial trade activities or in a quantitative investing framework for algorithmic trading and carry trade speculation   c  2017 elsevier ltd  all rights reserved  
 a state of the art survey   testbed of fuzzy ahp  fahp  applications as a practical popular methodology for dealing with fuzziness and uncertainty in multiple criteria decision making  mcdm   fuzzy ahp  fahp  has been applied to a wide range of applications  as of the time of writing there is no state of the art survey of fahp  we carry out a literature review of 190 application papers  i e   applied research papers   published between 2004 and 2016  by classifying them on the basis of the area of application  the identified theme  the year of publication  and so forth  the identified themes and application areas have been chosen based upon the latest state of the art survey of ahp conducted by  vaidya  o     kumar  s   2006   analytic hierarchy process  an overview of applications  european journal of operational research  169 1   1 29    to help readers extract quick and meaningful information  the reviewed papers are summarized in various tabular formats and charts  unlike previous literature surveys  results and findings are made available through an online  and free  testbed  which can serve as a ready reference for those who wish to apply  modify or extend fahp in various applications areas  this online testbed makes also available one or more fuzzy pairwise comparison matrices  fpcms  from all the reviewed papers  255 matrices in total   in terms of results and findings  this survey shows that   i  fahp is used primarily in the manufacturing  industry and government sectors   ii  asia is the torchbearer in this field  where fahp is mostly applied in the theme areas of selection and evaluation   iii  a significant amount of research papers  43  of the reviewed literature  combine fahp with other tools  particularly with topsis  qfd and anp  ahp s variant    iv  chang s extent analysis method  which is used for fpcms  weight derivation in fahp  is still the most popular method in spite of a number of criticisms in recent years  considered in 57  of the reviewed literature    c  2016 elsevier ltd  all rights reserved  
 a statistical  grammar based approach to microplanning although there has been much work in recent years on data driven natural language generation  little attention has been paid to the fine grained interactions that arise during microplanning between aggregation  surface realization  and sentence segmentation  in this article  we propose a hybrid symbolic statistical approach to jointly model the constraints regulating these interactions  our approach integrates a small handwritten grammar  a statistical hypertagger  and a surface realization algorithm  it is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence  we evaluate our approach in several ways  a quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage  results from a human study indicate that users find the output of this hybrid statistic symbolic system more fluent than both a template based and a purely symbolic grammar based approach  finally  we illustrate by means of examples that our approach can account for various factors impacting aggregation  sentence segmentation  and surface realization  
 a strength biased prediction model for forecasting exchange rates using support vector machines and genetic algorithms this paper addresses problem of predicting direction and magnitude of movement of currency pairs in the foreign exchange market  the study uses support vector machine with a novel approach for input data and trading strategy  the input data contain technical indicators generated from currency price data  i e   open  high  low and close prices  and representation of these technical indicators as trend deterministic signals  the input data are also dynamically adapted to each trading day with genetic algorithm  the study incorporates a currency strength biased trading strategy which selects the best pair to trade from the available set of currencies and is an improvement over the previous work  the accuracy of the prediction models are tested across several different sets of technical indicators and currency pair sets  spanning 5 years of historical data from 2010 to 2015  the experimental results suggest that using trend deterministic technical indicator signals mixed with raw data improves overall performance and dynamically adapting the input data to each trading period results in increased profits  results also show that using a strength biased trading strategy among a set of currency pair increases the overall prediction accuracy and profits of the models  
 a study in facial features saliency in face recognition  an analytic hierarchy process approach in this study  we develop a process of estimation of importance of features considered in face recognition by making use of the analytic hierarchy process  ahp   the ahp method of pairwise comparisons realized at three levels of hierarchy becomes crucial to realize a comprehensive weighting of cues so that sound estimates of weights associated with the individual features of faces can be formed  we demonstrate how to carry out an efficient process of face description by using a collection of linguistic descriptors of the features and their groups  numerical dependencies between the features are quantified with the help of experienced criminology and psychology experts  finally  we present an entropy based method of evaluation of the relevance of the estimation process completed by the individuals  the intuitively appealing results of experiments are presented and analyzed in detail  
 a study of contagion in the financial system from the perspective of network analytics the increase in the frequency and scope of financial crises has made the stability and robustness of the financial system a major concern in the field of finance worldwide  due to the interconnectedness between institutions  the negative effects of financial crises spread through the financial system in a process referred to as financial contagion  in this study  we focus on a financial system in which large numbers of financial institutions are connected by direct balance sheet linkages through their lending borrowing relationships  we mainly focus on modeling and analyzing financial contagion from a network analytics perspective  first  we model the financial system and the mechanism of contagion by introducing the concepts of exposure matrix  book value  market value and liquidation cost  second  we propose a simple contagion algorithm based on this modeling process  third  we study the effects of the financial system s heterogeneity on the magnitude of financial contagion by applying the proposed algorithm  the level of heterogeneity is measured by the diversification of exposure ratio and the extent of network connectivity  according to the results of our comprehensive numerical simulation  we conclude that an increase in heterogeneity has a significant influence on the stability of the financial system  our study has significant implications for the practice of financial regulation and surveillance   c  2017 elsevier b v  all rights reserved  
 a study of metrics of distance and correlation between ranked lists for compositionality detection compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined  based on the premise that substitution by synonyms is meaning preserving  compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms  different ways of representing such phrases exist  e g   vectors  kiela and clark  2013  or language models  lioma  simonsen  larsen  and hansen  2015    and the choice of representation affects the measurement of semantic similarity  we propose a new compositionality detection method that represents phrases as ranked lists of term weights  our method approximates the semantic similarity between two ranked list representations using a range of well known distance and correlation metrics  in contrast to most state of the art approaches in compositionality detection  our method is completely unsupervised  experiments with a publicly available dataset of 1048 human annotated phrases shows that  compared to strong supervised baselines  our approach provides superior measurement of compositionality using any of the distance and correlation metrics considered   c  2017 elsevier b v  all rights reserved  
 a study on text score disagreement in online reviews in this paper  we focus on online reviews and employ artificial intelligence tools  taken from the cognitive computing field  to help understand the relationships between the textual part of the review and the assigned numerical score  we move from the intuitions that  1  a set of textual reviews expressing different sentiments may feature the same score  and vice versa   and  2  detecting and analyzing the mismatches between the review content and the actual score may benefit both service providers and consumers  by highlighting specific factors of satisfaction  and dissatisfaction  in texts  to prove the intuitions  we adopt sentiment analysis techniques and we concentrate on hotel reviews  to find polarity mismatches therein  in particular  we first train a text classifier with a set of annotated hotel reviews  taken from the booking website  then  we analyze a large dataset  with around 160k hotel reviews collected from tripadvisor  with the aim of detecting a polarity mismatch  indicating if the textual content of the review is in line  or not  with the associated score  using well established artificial intelligence techniques and analyzing in depth the reviews featuring a mismatch between the text polarity and the score  we find that on a scale of five stars those reviews ranked with middle scores include a mixture of positive and negative aspects  the approach proposed here  beside acting as a polarity detector  provides an effective selection of reviews on an initial very large dataset that may allow both consumers and providers to focus directly on the review subset featuring a text score disagreement which conveniently convey to the user a summary of positive and negative features of the review target  
 a survey of learning classifier systems in games ames are becoming increasingly indispensable  not only for fun but also to support tasks that are more serious  such as education  strategic planning  and understanding of complex phenomena  computational intelligence  based methods are contributing significantly to this development  learning classifier systems  lcs  is a pioneering computational intelligence approach that combines machine learning methods with evolutionary computation  to learn problem solutions in the form of interpretable rules  these systems offer several advantages for game applications  including a powerful and flexible agent architecture built on a knowledgebased symbolic modeling engine  modeling flexibility that allows integrating domain knowledge and different machine learning mechanisms under a single computational framework  an ability to adapt to diverse game requirements  and an ability to learn and generate creative agent behaviors in real time dynamic environments  we present a comprehensive and dedicated survey of lcs in computer games  the survey highlights the versatility and advantages of these systems by reviewing their application in a variety of games  the survey is organized according to a general game classification and provides an opportunity to bring this important research direction into the public eye  we discuss the strengths and weaknesses of the existing approaches and provide insights into important future research directions  
 a survey of multimodal sentiment analysis sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity  the aggregation of these sentiments over a population represents opinion polling and has numerous applications  current text based sentiment analysis relies on the construction of dictionaries and machine learning models that learn sentiment from large text corpora  sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis  among others  with the proliferation of social media  multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text based sentiment analysis  since sentiment can be detected through affective traces it leaves  such as facial and vocal displays  multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content  these approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual s sentiment  in this survey  we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains  including spoken reviews  images  video blogs  human machine and human human interactions  challenges and opportunities of this emerging field are also discussed  leading to our thesis that multimodal sentiment analysis holds a significant untapped potential   c  2017 elsevier b v  all rights reserved  
 a survey of the applications of text mining in financial domain text mining has found a variety of applications in diverse domains  of late  prolific work is reported in using text mining techniques to solve problems in financial domain  the objective of this paper is to provide a state of the art survey of various applications of text mining to finance  these applications are categorized broadly into forex rate prediction  stock market prediction  customer relationship management  crm  and cyber security  since finance is a service industry  these problems are paramount in operational and customer growth aspects  we reviewed 89 research papers that appeared during the period 2000 2016  highlighted some of the issues  gaps  key challenges in this area and proposed some future research directions  finally  this review can be extremely useful to budding researchers in this area  as many open problems are highlighted   c  2016 elsevier b v  all rights reserved  
 a survey on heterogeneous face recognition  sketch  infra red  3d and low resolution heterogeneous face recognition  hfr  refers to matching face imagery across different domains  it has received much interest from the research community as a result of its profound implications in law enforcement  a wide variety of new invariant features  cross modality matching models and heterogeneous datasets are being established in recent years  this survey provides a comprehensive review of established techniques and recent developments in hfr  moreover  we offer a detailed account of datasets and benchmarks commonly used for evaluation  we finish by assessing the state of the field and discussing promising directions for future research   c  2016 elsevier b v  all rights reserved  
 a survey on the inventory routing problem with stochastic lead times and demands the integration of the different processes and players that compose the supply chain  sc  is essential to obtain a better coordination level  inventory control and distribution management are the two processes that researchers have identified as the key to gain or lose in efficiency and effectiveness in the field of logistics  with a direct effect on the synchronization and overall performance of scs  in practical situations demand is often not deterministic  and lead times are also variable  yielding a complex stochastic problem  in order to analyze the recent developments in the integration of these processes  this paper analyzes the state of the art of the information management in the sc  the relationship between inventory policies and available demand information  and the use of optimization methods to provide good solutions for the problem in single and multi depot versions   c  2016 elsevier b v  all rights reserved  
 a systematic literature review about technologies for self reporting emotional information emotional information is complex to manage by humans and computers alike  so it is difficult for users to express emotional information through technology  two main approaches are used to gather this type of information  objective  e g  through sensors or facial recognition  and subjective  reports by users themselves   subjective methods are less intrusive and may be more accurate  although users may fail to report their emotions or not be entirely truthful about them  the goal of this study is to identify trends in the area of interfaces for the self report of human emotions  under served populations of users  and avenues of future research  a systematic literature review was conducted on six search engines  resulting in a set of 863 papers  which were filtered in a systematic way until we established a corpus of 40 papers  we studied the technologies used for emotional self report as well as the issues regarding these technologies  such as privacy  interaction mechanisms  and how they are evaluated  
 a systematic literature review and critical assessment of model driven decision support for it outsourcing information technology outsourcing  ito  is a widely adopted strategy for it governance  the decisions involved in it outsourcing are complicated  empirical research confirms that a rational and formalized decision making process results in better decision outcomes  however  formal and systematic approaches for making ito decisions appear to be scarce in practice  to support organizational decision makers involved in it outsourcing  including cloud sourcing   researchers have suggested several decision support methods  to date there is no comprehensive review and assessment of the research in this domain  in this study 133 model driven decision support research articles for it outsourcing and cloud sourcing were identified through a systematic literature review and assessed based on a highly regarded research framework an analysis of these 133 research articles suggested a range of multiple criteria decision making  mcdm   optimization and simulation methods to support different it outsourcing decisions  our findings raise concerns about the limited use of reference design theories  and the lack of validation and naturalistic evaluation of the decision support artifacts reported in ito decision support literature  based on the review  we provide future research directions  as well as a number of recommendations to enhance the rigor and relevance of ito decision support systems research   c  2017 elsevier b v  all rights reserved  
 a systematic review of text stemming techniques stemming is a program that matches the morphological variants of the word to its root word  stemming is extensively used as a pre processing tool in the field of natural language processing  information retrieval  and language modeling  though a lot of advancements have been made in the field  yet organized arrangement of the previous work and efforts are lacking in this field  in this paper  we present a review of the text stemming theory  algorithms  and applications  it first describes the existing literature relevant to text stemming by classifying it according to certain key parameters  then it describes the deep analysis of some well known stemming algorithms on standard data sets  in the end  the current state of the art and certain open issues related to unsupervised stemming are presented  the main aim of this paper is to provide an extensive and useful understanding of the important aspects of text stemming  the open issues and analysis of the current stemming techniques will help the researchers to think of new lines to conduct research in future  
 a taxonomy for task allocation problems with temporal and ordering constraints previous work on assigning tasks to robots has proposed extensive categorizations of allocation of tasks with and without constraints  the main contribution of this paper is a specific categorization of problems that have temporal and ordering constraints  we propose a novel taxonomy that emphasizes the differences between temporal and ordering constraints  and organizes the current literature according to the nature of those constraints  we summarize widely used models and methods from the task allocation literature and related areas  such as vehicle routing and scheduling problems  showing similarities and differences   c  2016 elsevier b v  all rights reserved  
 a temporal model in electronic health record search electronic health records  ehrs  refer to a collection of patient data  including diagnosis  medical history  medication  allergies  etc   mostly contained in the form of unstructured text  ehrs are designed to capture the state of a patient over time  thus the temporal information is crucial  most previous works processing time in ehrs narrative focused on temporal expression extraction  using textual dimension to embody the temporal dimension  in this paper  we propose to model the textual and the temporal dimension of ehrs narrative jointly  to meet the challenge  we propose to model the ehrs narrative as temporal sequential data  a novel representation framework is designed to model the clinical narrative text as document sequence  where the textual and temporal dimension are modeled simultaneously  in the framework  a dynamic time warping based measure is proposed to quantify the similarity between ehrs of different patients  to verify the effectiveness of the model  the proposed model is applied in ehrs search via clustering algorithm  experiments on real world ehrs data set demonstrate that the proposed model sufficiently expresses the temporal feature of the ehrs and provides an effective solution for measuring the temporal similarity between ehrs of different patients   c  2017 elsevier b v  all rights reserved  
 a texture based pixel labeling approach for historical books over the last few years  there has been tremendous growth in the automatic processing of digitized historical documents  in fact  finding reliable systems for the interpretation of ancient documents has been a topic of major interest for many libraries and the prime issue of research in the document analysis community  one important challenge is to refine well known approaches based on strong a priori knowledge  e g   the document image content  layout  typography  font size and type  scanning resolution  image size  etc    nevertheless  a texture analysis approach has consistently been chosen to segment a page layout when information is lacking on document structure and content  thus  in this article  a framework is proposed to investigate the use of texture as a tool for automatically determining homogeneous regions in a digitized historical book and segmenting its contents by extracting and analyzing texture features independently of the layout of the pages  the proposed framework is parameter free and applicable to a large variety of ancient of books  it does not assume a priori information regarding document image content and structure  it consists of two phases  a texture based feature extraction step and unsupervised clustering and labeling task based on the consensus clustering  hierarchical ascendant classification  and nearest neighbor search algorithms  the novelty of this work lies in the clustering of extracted texture descriptors to find automatically homogeneous regions  i e   graphic and textual regions  using the clustering approach on an entire book instead of processing each page individually  our framework has been evaluated on a large variety of historical books and achieved promising results  
 a thermodynamical approach towards group multi criteria decision making  gmcdm  and its application to human resource selection in group multi criteria decision making  gmcdm  problems  ratings are assigned to the alternatives on different criteria by an expert group  in this paper  we propose a thermodynamically consistent model for gmcdm using the analogies for thermodynamical indicators   energy  exergy and entropy  the most commonly used method for analysing gmcdm problem is technique for order of preference by similarity to ideal solution  topsis   the conventional topsis method uses a measure similar to energy for the ranking of alternatives  we demonstrate that the ranking of the alternatives is more meaningful if we use exergy in place of energy  the use of exergy is superior due to the inclusion of a factor accounting for the quality of the ratings by the expert group  the unevenness in the ratings by the experts is measured by entropy  the procedure for the calculation of the thermodynamical indicators is explained in both crisp and fuzzy environments  finally  the effectiveness of the proposed method is demonstrated by applying to human resource selection problem   c  2016 elsevier b v  all rights reserved  
 a three layer planning architecture for the autonomous control of rehabilitation therapies based on social robots this manuscript focuses on the description of a novel cognitive architecture called naotherapist  which provides a social robot with enough autonomy to carry out a non contact upper limb rehabilitation therapy for patients with physical impairments  such as cerebral palsy and obstetric brachial plexus palsy  naotherapist comprises three levels of automated planning  in the high level planning  the physician establishes the parameters of the therapy such as the scheduling of the sessions  the therapeutic objectives to be achieved and certain constraints based on the medical records of the patient  this information is used to establish a customized therapy plan  the objective of the medium level planning is to execute and monitor every previous planned session with the humanoid robot  finally  the low level planning involves the execution of path planning actions by the robot to carry out different low level instructions such as performing poses  the technical evaluation shows an accurate definition and monitoring of the therapies and sessions and a fluent interaction with the robot  this automated process is expected to save time for the professionals while guaranteeing the medical criteria   c  2016 elsevier b v  all rights reserved  
 a transformation driven approach for recognizing textual entailment textual entailment is a directional relation between two text fragments  the relation holds whenever the truth of one text fragment  called hypothesis  h   follows from another text fragment  called text  t   up until now  using machine learning approaches for recognizing textual entailment has been hampered by the limited availability of data  we present an approach based on syntactic transformations and machine learning techniques which is designed to fit well with a new type of available data sets that are larger but less complex than data sets used in the past  the transformations are not predefined  but calculated from the data sets  and then used as features in a supervised learning classifier  the method has been evaluated using two data sets  the sick data set and the excitement english data set  while both data sets are of a larger order of magnitude than data sets such as rte 3  they are also of lower levels of complexity  each in its own way  sick consists of pairs created by applying a predefined set of syntactic and lexical rules to its t and h pairs  which can be accurately captured by our transformations  the excitement english data contains short pieces of text that do not require a high degree of text understanding to be annotated  the resulting adarte system is simple to understand and implement  but also effective when compared with other existing systems  adarte has been made freely available with the excitement open platform  an open source platform for textual inference  
 a trust induced recommendation mechanism for reaching consensus in group decision making this article addresses the inconsistency problem in group decision making caused by dispatate opinions of multiple experts  to do so  a trust induced recommendation mechanism is investigated to generate personalised advices for the inconsistent experts to reach higher consensus level  the concept of trust degree  td  is defined to identify the trusted opinion from group experts  and then the visual trust relationship is built to help experts  see  their own trust preferences within the group  consequently  trust based personalised advices are generated for the inconsistent experts to revisit their opinions  to model the uncertainty of experts  an interval valued trust decision making space is defined  it includes the novel concepts of interval valued trust functions  interval valued trust score  ivts  and interval  valued knowledge degree  ivkd   the concepts of consensus degree  cd  between an expert and the rest of experts in the group as well as the harmony degree  hd  between the original opinion and the revised opinion are developed for interval  valued trust functions  combining hd and cd  a more reasonable policy for group consensus is proposed as it should arrive at the threshold value with the maximum value of harmony and consensus degrees simultaneously  furthermore  because the trust induced recommendation mechanism focuses on changing inconsistent opinions using only opinions from the trusted experts and not from the distrusted ones  the hd based changes cost to reach the threshold value of consensus is lower than previous mechanisms based on the average of the opinion of all experts  finally  once consensus has been achieved  a ranking order relation for interval valued trust functions is constructed to select the most appropriate alternative   c  2016 elsevier b v  all rights reserved  
 a two machine flowshop scheduling problem with precedence constraint on two jobs job precedence can be found in some real life situations  for the application in the scheduling of patients from multiple waiting lines or different physicians  patients in the same waiting line for scarce resources such as organs  or with the same physician often need to be treated on the first come  first served basis to avoid ethical or legal issues  and precedence constraints can restrict their treatment sequence  in view of this observation  this paper considers a two machine flowshop scheduling problem with precedence constraint on two jobs with the goal to find a sequence that minimizes the total tardiness criterion  in searching solutions to this problem  we build a branch and bound method incorporating several dominances and a lower bound to find an optimal solution  in addition  we also develop a genetic and larger order value method to find a near optimal solution  finally  we conduct the computational experiments to evaluate the performances of all the proposed algorithms  
 a two phase multiobjective evolutionary algorithm for enhancing the robustness of scale free networks against multiple malicious attacks designing robust networks has attracted increasing attentions in recent years  most existing work focuses on improving the robustness of networks against a specific type of attacks  however  networks which are robust against one type of attacks may not be robust against another type of attacks  in the real world situations  different types of attacks may happen simultaneously  therefore  we use the pearson s correlation coefficient to analyze the correlation between different types of attacks  model the robustness measures against different types of attacks which are negatively correlated as objectives  and model the problem of optimizing the robustness of networks against multiple malicious attacks as a multiobjective optimization problem  furthermore  to effectively solve this problem  we propose a two phase multiobjective evolutionary algorithm  labeled as moea rsfmma  in moea rsfmma  a single objective sampling phase is first used to generate a good initial population for the later two objective optimization phase  such a two phase optimizing pattern well balances the computational cost of the two objectives and improves the search efficiency  in the experiments  both synthetic scale free networks and real world networks are used to validate the performance of moea rsfmma  moreover  both local and global characteristics of networks in different parts of the obtained pareto fronts are studied  the results show that the networks in different parts of pareto fronts reflect different properties  and provide various choices for decision makers  
 a universal multilingual weightless neural network tagger via quantitative linguistics in the last decade  given the availability of corpora in several distinct languages  research on multilingual part of speech tagging started to grow  amongst the novelties there is mwann tagger  multilingual weightless artificial neural network tagger   a weightless neural part of speech tagger capable of being used for mostly suffix oriented languages  the tagger was subjected to corpora in eight languages of quite distinct natures and had a remarkable accuracy with very low sample deviation in every one of them  indicating the robustness of weightless neural systems for part of speech tagging tasks  however  mwann tagger needed to be tuned for every new corpus  since each one required a different parameter configuration  for mwann tagger to be truly multilingual  it should be usable for any new language with no need of parameter tuning  this article proposes a study that aims to find a relation between the lexical diversity of a language and the parameter configuration that would produce the best performing mwann tagger instance  preliminary analyses suggested that a single parameter configuration may be applied to the eight aforementioned languages  the mwann tagger instance produced by this configuration was as accurate as the language dependent ones obtained through tuning  afterwards  the weightless neural tagger was further subjected to new corpora in languages that range from very isolating to polysynthetic ones  the best performing instances of mwann tagger are again the ones produced by the universal parameter configuration  hence  mwann tagger can be applied to new corpora with no need of parameter tuning  making it a universal multilingual part of speech tagger  further experiments with universal dependencies treebanks reveal that mwann tagger may be extended and that it has potential to outperform most state of the art part of speech taggers if better word representations are provided   c  2017 elsevier ltd  all rights reserved  
 a user experience evaluation framework for mobile usability the worldwide mobile software market has grown dramatically since feature phones became popular in the early 1990s  in practice  mobile usability   which can be defined for a resource constrained device in two ways  namely  user experience  ux  and user interface  ui    has been regarded as the key to gaining superiority in terms of both market share and customer loyalty  unfortunately  de facto standards for software design and the development process  such as unified modeling language  uml  and rational unified process  rup   do not seem to promote mobile usability in a systematic manner in practice  this paper proposes a systematic and generalizable approach to modeling and evaluating the properties of mobile usability  herein treating it as a first class software quality from the perspective of software engineering  we devise a ux evaluation framework for mobile usability  which we call ux evaluation framework  uef  throughout this paper  a ux is specified by inter scene interactions between users and terminals of software products using extended menu navigation viewpoints  emnvs   then  a model checker  nusmv  is adopted to observe whether the emnv model meets a set of given ux properties  importantly  the analysis and design of rup is extended to support the co design of ux and ui so that major roles  activities and artifacts in the ux and ui can be explicitly monitored and controlled by stakeholders  through case studies  we demonstrate that uef works properly to treat software products that prioritize mobile usability  consequently  uef plays a key role in filling the gap between two research disciplines to address usability  software engineering and human computer interactions  
 a variable neighborhood search for the network design problem with relays given a set of commodities to be routed over a network  the network design problem with relays involves selecting a route for each commodity and determining the location of relays where the commodities must be reprocessed at certain distance intervals  we propose a hybrid approach based on variable neighborhood search  the variable neighborhood algorithm searches for the route for each commodity and the optimal relay locations for a given set of routes are determined by an implicit enumeration algorithm  we show that dynamic programming can be used to determine the optimal relay locations for a single commodity  dynamic programming is embedded into the implicit enumeration algorithm to solve the relay location problem optimally for multiple commodities  the special structure of the problem is leveraged for computational efficiency  in the variable neighborhood search algorithm  the routes of the current solution are perturbed and reconstructed to generate neighbor solutions using random and greedy construction heuristics  computational experiments on three sets of problems  80 instances  show that the variable neighborhood search algorithm with optimal relay allocations outperforms all existing algorithms in the literature  
 a visual embedding for the unsupervised extraction of abstract semantics vector space word representations obtained from neural network models have been shown to enable semantic operations based on vector arithmetic  in this paper  we explore the existence of similar information on vector representations of images  for that purpose we define a methodology to obtain large  sparse vector representations of image classes  and generate vectors through the state of the art deep learning architecture googlenet for 20 k images obtained from imagenet  we first evaluate the resultant vector space semantics through its correlation with wordnet distances  and find vector distances to be strongly correlated with linguistic semantics  we then explore the location of images within the vector space  finding elements close in wordnet to be clustered together  regardless of significant visual variances  e g   118 dog types   more surprisingly  we find that the space unsupervisedly separates complex classes without prior knowledge  e g   living things   afterwards  we consider vector arithmetics  although we are unable to obtain meaningful results on this regard  we discuss the various problem we encountered  and how we consider to solve them  finally  we discuss the impact of our research for cognitive systems  focusing on the role of the architecture being used   c  2016 elsevier b v  all rights reserved  
 a wearable virtual usher for vision based cognitive indoor navigation inspired by progresses in cognitive science  artificial intelligence  computer vision  and mobile computing technologies  we propose and implement a wearable virtual usher for cognitive indoor navigation based on egocentric visual perception  a novel computational framework of cognitive wayfinding in an indoor environment is proposed  which contains a context model  a route model  and a process model  a hierarchical structure is proposed to represent the cognitive context knowledge of indoor scenes  given a start position and a destination  a bayesian network model is proposed to represent the navigation route derived from the context model  a novel dynamic bayesian network  dbn  model is proposed to accommodate the dynamic process of navigation based on real time first person view visual input  which involves multiple asynchronous temporal dependencies  to adapt to large variations in travel time through trip segments  we propose an online adaptation algorithm for the dbn model  leading to a self adaptive dbn  a prototype system is built and tested for technical performance and user experience  the quantitative evaluation shows that our method achieves over 13  improvement in accuracy as compared to baseline approaches based on hidden markov model  in the user study  our system guides the participants to their destinations  emulating a human usher in multiple aspects  
 a zslices based general type 2 fuzzy logic system for users centric adaptive learning in large scale e learning platforms sophisticated educational technologies are evolving rapidly  and online courses are becoming more easily available  generating interest in innovating lightweight data driven adaptive approaches that foster responsive teaching and improving the overall learning experience  however  in most existing adaptive educational systems  the black box modeling of learner and instructional models based on the views of a few designers or experts tended to drive the adaptation of learning content  however  different sources of uncertainty could affect these views  including how accurately the proposed adaptive educational methods actually assess student responses and the corresponding uncertainties associated with how students receive and comprehend the resulting instruction  e learning environments contain high levels of linguistic uncertainties  whereby students can interpret and act on the same terms  words  or methods  e g   course difficulty  length of study time  or preferred learning style  in various ways according to varying levels of motivation  pre knowledge  cognition  and future plans  thus  one adaptive instructional model does not fit the needs of all students  basing the instruction model on determining learners  interactions within the learning environment in interpretable and easily read white box models is crucial for adapting the model to students  needs and understanding how learning is realized  this paper presents a new zslices based type 2 fuzzy logic based system that can learn students  preferred knowledge delivery needs based on their characteristics and current levels of knowledge to generate an adaptive learning environment  we have evaluated the proposed system s efficiency through various large scale  real world experiments involving 1871 students from king abdulaziz university  these experiments demonstrate the proposed zslices type 2 fuzzy logic based system s capability for handling linguistic uncertainties to produce better performance  particularly in terms of enhanced student performance and improved success rates compared with interval type 2 fuzzy logic  type 1 fuzzy systems  adaptive  instructor led systems  and non adaptive systems  
 aarya   a kinesthetic companion for children with autism spectrum disorder autism spectrum disorder  asd  is defined as a condition or disorder that begins in childhood and that causes problems in establishing relationships and communicating with other people  aaryaworks as a personal well being companion to children with autism spectrum disorder while they interact with a virtual environment that is gesture based  by making an asd affected child face real world situations  we try to improve his her confidence in facing the world and being open to learning various skills  social interaction and communication are the major challenges faced by children with asd  in aarya  we use gesture based interface that is the microsoft kinect so that the child can find it easier to interact in the real world environment  through the interactions made with the children and the results obtained  we understand that this tool can be a companion while giving chance for growth and improving their interacting ability  with further refinement and expert inputs  this tool can be built better  
 absem  an agent based simulator of emotions in mindfulness  programs among other utilities  expert and intelligent systems can provide estimations similar to the ones from human experts  in this context  agent based simulators support the prediction of some features of groups of individuals based on the simulation of some autonomous entities called agents  however  the literature lacks the appropriate simulators to simulate the repercussions of mindfulness programs on the emotions of meditators  mindfulness has proven to be related to several indicators of health and quality of life in the later years  mindfulness has also proven to be strongly related with certain emotions  but there are different programs for training mindfulness  in this context  the current approach presents an agent based simulator of emotions in mindfulness programs  absem   the current approach allows instructors to define different mindfulness programs and simulate their repercussions on the emotions of a group of practitioners with certain features  in this way  these simulations support instructors in selecting an adequate program for a given group of practitioners  in addition  this simulator can also be useful for practitioners because it lets them know an estimation of the repercussions of each mindfulness program  in two different scenarios  the experiments show that the system provides simulated outcomes that are similar to the real ones  when establishing the same input circumstances  the current work distributes absem as an open source tool  this fact not only assures the reproducibility of the experiments  but also allows researchers and practitioners to use and extend the simulator  therefore  absem contributes on the application of expert and intelligent systems to the mindfulness field  allowing novel instructors and meditators benefit from the implicit knowledge integrated in the simulator  this system can promote both  a  the use of expert systems for exploring their performance in predicting emotional repercussions from any kind of health intervention  and  b  the practice of mindfulness for repairing negative emotions as a social benefit   c  2017 elsevier ltd  all rights reserved  
 accurate frequency based lexicon generation for opinion mining sentiment analysis deals with classifying the opinions in text  twitter is the most popular microblogging platform in social media  with hundreds of millions of tweets posted every day  a considerable number of tweets contain opinions  the goal of this paper is to classify the polarity of the tweets into positive and negative classes using dynamic sentiment lexicons based on frequencies of words in positive and negative classes  we extract five meta level features incorporating the generated sentiment lexicons and classify the text based on them  we also incorporate some previously known lexicon based and corpus based features  the proposed method is assessed on six datasets  and outperforms previous papers on accuracy on four datasets  and on f measure on three datasets  this method generates sentiment lexicons dynamically  the changes of meanings of words can be captured by the generated lexicons  our research produces very promising results in sentiment analysis in terms of accuracy and f measure  the accuracy of our method on four datasets and the f measure of our method on three datasets are higher than 85   
 accurate maximum margin training for parsing with context free grammars the task of natural language parsing can naturally be embedded in the maximum margin framework for structured output prediction using an appropriate joint feature map and a suitable structured loss function  while there are efficient learning algorithms based on the cutting plane method for optimizing the resulting quadratic objective with potentially exponential number of linear constraints  their efficiency crucially depends on the inference algorithms used to infer the most violated constraint in a current iteration  in this paper  we derive an extension of the well known cocke kasami younger  cky  algorithm used for parsing with probabilistic context free grammars for the case of loss augmented inference enabling an effective training in the cutting plane approach  the resulting algorithm is guaranteed to find an optimal solution in polynomial time exceeding the running time of the cky algorithm by a term  which only depends on the number of possible loss values  in order to demonstrate the feasibility of the presented algorithm  we perform a set of experiments for parsing english sentences  
 active and semi supervised learning for object detection with imperfect data in this paper  we address the combination of the active learning  al  and semi supervised  ssl  learnings  called assl  to leverage the strong points of the both learning paradigms for improving the performance of object detection  considering the pros and cons of the al and ssl learning methods  assl where ssl method provides the incremental improvement of semi supervised detection performance by combining the concept of diversity imported from al methods  the proposed method demonstrates outstanding performance compared with state of art methods on the challenging caltech pedestrian detection dataset  reducing the miss rate to 12 2   which is significantly smaller than current state of art  in addition  extensive experiments have been carried out using ilsvrc detection dataset and online evaluation for activity recognition   c  2017 elsevier b v  all rights reserved  
 active inference  a process theory this article describes a process theory based on active inference and belief propagation  starting from the premise that all neuronal processing  and action selection  can be explained by maximizing bayesian model evidenceor minimizing variational free energywe ask whether neuronal responses can be described as a gradient descent on variational free energy  using a standard  markov decision process  generative model  we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well characterized neuronal phenomena  these include repetition suppression  mismatch negativity  violation responses  place cell activity  phase precession  theta sequences  theta gamma coupling  evidence accumulation  race to bound dynamics  and transfer of dopamine responses  furthermore  the  approximately bayes  optimal  behavior prescribed by these dynamics has a degree of face validity  providing a formal explanation for reward seeking  context learning  and epistemic foraging  technically  the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a lyapunov function for neuronal dynamics  which therefore conform to hamilton s principle of least action  
 adapting sentiment lexicons to domain specific social media texts social media has become the largest data source of public opinion  the application of sentiment analysis to social media texts has great potential  but faces great challenges because of domain heterogeneity  sentiment orientation of words varies by content domain  but learning context specific sentiment in social media domains continues to be a major challenge  the language domain poses another challenge since the language used in social media today differs significantly from that used in traditional media  to address these challenges  we propose a method to adapt existing sentiment lexicons for domain specific sentiment classification using an unannotated corpus and a dictionary  we evaluate our method using two large developing corpora  containing 743 069 tweets related to the stock market and one million tweets related to political topics  respectively  and five existing sentiment lexicons as seeds and baselines  the results demonstrate the usefulness of our method  showing significant improvement in sentiment classification performance   c  2016 elsevier b v  all rights reserved  
 adaptive behaviors in multi agent source localization using passive sensing in this paper  the role of adaptive group cohesion in a cooperative multi agent source localization problem is investigated  a distributed source localization algorithm is presented for a homogeneous team of simple agents  an agent uses a single sensor to sense the gradient and two sensors to sense its neighbors  the algorithm is a set of individualistic and social behaviors where the individualistic behavior is as simple as an agent keeping its previous heading and is not self sufficient in localizing the source  source localization is achieved as an emergent property through agent s adaptive interactions with the neighbors and the environment  given a single agent is incapable of localizing the source  maintaining team connectivity at all times is crucial  two simple temporal sampling behaviors  intensity based adaptation and connectivity based adaptation  ensure an efficient localization strategy with minimal agent breakaways  the agent behaviors are simultaneously optimized using a two phase evolutionary optimization process  the optimized behaviors are estimated with analytical models and the resulting collective behavior is validated against the agent s sensor and actuator noise  strong multi path interference due to environment variability  initialization distance sensitivity and loss of source signal  
 adaptive budget portfolio investment optimization under risk tolerance ambiguity in this study  we consider a portfolio optimization incorporated budget investment problem under managers  risk tolerance ambiguity  in order to capture the decision dynamics driven by the risk tolerance ambiguity  a two stage adaptive optimization model is developed  the budget allocation is the first stage decision  which is made before knowing each manager s actual risk tolerance level  and the portfolio selection conducted by each manager is the second stage decision  which adapts to the manager s risk tolerance  we introduce the concept of risk neutral budget threshold  rnbt  that is modeled by a fuzzy set granule  and upon which the ambiguous risk tolerance curve is constructed  which can realistically capture the managers  risk averse and or risk seeking attitudes  due to the  realistic  nonconvex nonconcave structure of the risk tolerance curve  and the existence of the ambiguity  the resulting problem is essentially a nonconvex adaptive optimization problem under uncertainty  to achieve a robust modeling and an efficient solution  we first restructure and robustize the information of fuzzy rnbts and then transform the developed model into a mixed integer linear programming  milp   which can be handled efficiently by off the shelf mixed integer program solvers  leveraging the derived milp structure  we can use the benders decomposition to further enhance the scalability of the model  furthermore  some model extensions on robustizing the probability estimations are discussed  finally  computational studies are performed to demonstrate the effectiveness and insights of the model  
 adaptive consensus model with multiplicative linguistic preferences based on fuzzy information granulation an adaptive consensus model based on fuzzy information granulation  fuzzy ig  is presented for group consensus decision making problems with multiplicative linguistic preference relations  mlprs   firstly  a granular representation of linguistic terms is concerned with the triangular fuzzy formation of a family of information granules over given analytical hierarchy process  ahp  numerical scales  on this basis  the individual consistency and group consensus measure indices using fuzzy granulation technique are constructed  respectively  then  the optimal cut off points of fuzzy information granules are obtained by establishing a multi objective optimization model together with a multi objective particle swarm optimization  mopso  algorithm  a novel group consensus decision making approach where consensus reaching process  crp  is achieved by adaptively adjusting individual preferences through the optimization of the cut off points is proposed  after conflict elimination  the obtained group preference gives the ranking of the alternatives  finally  a real emergency decision making case for liquid ammonia leak is given to illustrate the application steps of the proposed method and comparative analysis with the existing gdm methods  comparative results demonstrate that the proposed method has some advantages in aspects of avoiding information loss or distortion and improving consensus performance   c  2017 elsevier b v  all rights reserved  
 adaptive control strategies for interlimb coordination in legged robots  a review walking animals produce adaptive interlimb coordination during locomotion in accordance with their situation  interlimb coordination is generated through the dynamic interactions of the neural system  the musculoskeletal system  and the environment  although the underlying mechanisms remain unclear  recently  investigations of the adaptationmechanisms of living beings have attracted attention  and bio inspired control systems based on neurophysiological findings regarding sensorimotor interactions are being developed for legged robots  in this review  we introduce adaptive interlimb coordination for legged robots induced by various factors  locomotion speed  environmental situation  body properties  and task   in addition  we show characteristic properties of adaptive interlimb coordination  such as gait hysteresis and different time scale adaptations  we also discuss the underlying mechanisms and control strategies to achieve adaptive interlimb coordination and the design principle for the control system of legged robots  
 adaptive feedback in computer based learning environments  a review adaptive support within a learning environment is useful because most learners have different personal characteristics such as prior knowledge  learning progress  and learning preferences  this study reviews various implementation of adaptive feedback  based on the four adaptation characteristics  means  target  goal  and strategy  this review focuses on 20 different implementations of feedback in a computer based learning environment  ranging from multimedia web based intelligent tutoring systems  dialog based intelligent tutoring systems  web based intelligent e learning systems  adaptive hypermedia systems  and adaptive learning environment  the main objective of the review is to compare computer based learning environments according to their implementation of feedback and to identify open research questions in adaptive feedback implementations  the review resulted in categorizing these feedback implementations based on the students  information used for providing feedback  the aspect of the domain or pedagogical knowledge that is adapted to provide feedback based on the students  characteristics  the pedagogical reason for providing feedback  and the steps taken to provide feedback with or without students  participation  other information such as the common adaptive feedback means  goals  and implementation techniques are identified  this review reveals a distinct relationship between the characteristics of feedback  features of adaptive feedback  and computer based learning models  other information such as the common adaptive feedback means  goals  implementation techniques  and open research questions are identified  
 adaptive guided ejection search for pickup and delivery with time windows the pickup and delivery problem with time windows is an np hard discrete optimization problem with two objectives to minimize the fleet serving transportation requests  and to minimize the distance traveled during this service  although there exist exact algorithms for tackling this problem  they are still difficult to apply in massively large practical scheduling scenarios due to their time complexities  hence  the approximate methods became the main stream of research in this field  in this paper  we propose an adaptive guided ejection search algorithm for solving the pickup and delivery with time windows  the pivotal part of this technique is the pre processing step  in which the instance characteristics concerning its underlying structure are extracted in the clustering and histogram based analyses  then  the k nearest neighbor algorithm is applied to classify the instance to an appropriate class  finally  the most suitable variant of our enhanced guided ejection search algorithm is adaptively chosen for solving this instance based on the classification outcome  an extensive experimental study performed on the full li and lim s benchmark  encompassing 354 problem instances belonging to 6 classes  revealed that our pre processing allows for achieving very high classification accuracy  thus for selecting the best variant of the enhanced guided ejection search  
 adding sensor free intention based affective support to an intelligent tutoring system emotional factors considerably influence learning and academic performance  in this paper  we validate the hypothesis that learning platforms can adjust their response to have an effect on the learner s pleasure  arousal and or dominance  without using a specific emotion detection system during operation  to this end  we have enriched an existing intelligent tutoring system  its  by designing a module that is able to regulate the level of help provided to maximize valence  arousal or autonomy as desired  the design of this module followed a two stage methodology  in the first stage  the its was adapted to collect data from several groups of students in primary education  by providing a random level of help and adding an emotional self report based on self assessment manikins  then  the collected data was used to learn a series of classifiers  in operation  self reporting was removed and the classifiers were used to choose the most convenient help level in order to positively affect the target variables  the effectiveness of the system has been extensively evaluated in a real educational setting  showing that the added module is successful at acting on the chosen target variable in a controlled way   c  2017 elsevier b v  all rights reserved  
 adolescent suicidal risk assessment in clinician patient interaction youth suicide is a major public health problem  it is the third leading cause of death in the united states for ages 13 through 18  many adolescents that face suicidal thoughts or make a suicide plan never seek professional care or help  within this work  we evaluate both verbal and nonverbal responses to a five item ubiquitous questionnaire to identify and assess suicidal risk of adolescents  we utilize a machine learning approach to identify suicidal from non suicidal speech as well as characterize adolescents that repeatedly attempted suicide in the past  our findings investigate both verbal and nonverbal behavior information of the face to face clinician patient interaction  we investigate 60 audio recorded dyadic clinician patient interviews of 30 suicidal  13 repeaters and 17 non repeaters  and 30 non suicidal adolescents  the interaction between clinician and adolescents is statistically analyzed to reveal differences between suicidal versus non suicidal adolescents and to investigate suicidal repeaters  behaviors in comparison to suicidal non repeaters  by using a hierarchical classifier we were able to show that the verbal responses to the ubiquitous questions sections of the interviews were useful to discriminate suicidal and non suicidal patients  however  to additionally classify suicidal repeaters and suicidal non repeaters more information especially nonverbal information is required  
 adversarial classification using signaling games with an application to phishing detection in adversarial classification  the interaction between classifiers and adversaries can be modeled as a game between two players  it is natural to model this interaction as a dynamic game of incomplete information  since the classifier does not know the exact intentions of the different types of adversaries  senders   for these games  equilibrium strategies can be approximated and used as input for classification models  in this paper we show how to model such interactions between players  as well as give directions on how to approximate their mixed strategies  we propose perceptron like machine learning approximations as well as novel adversary aware online support vector machines  results in a real world adversarial environment show that our approach is competitive with benchmark online learning algorithms  and provides important insights into the complex relations among players  
 affective evolutionary music composition with metacompose this paper describes the metacompose music generator  a compositional  extensible framework for affective music composition  in this context  affective  refers to the music generator s ability to express emotional information  the main purpose of metacompose is to create music in real time that can express different mood states  which we achieve through a unique combination of a graph traversal based chord sequence generator  a search based melody generator  a pattern based accompaniment generator  and a theory for mood expression  melody generation uses a novel evolutionary technique combining fi 2pop with multi objective optimization  this allows us to explore a pareto front of diverse solutions that are creatively equivalent under the terms of a multi criteria objective function  two quantitative user studies were performed to evaluate the system  one focusing on the music generation technique  and the other that explores valence expression  via the introduction of dissonances  the results of these studies demonstrate  i  that each part of the generation system improves the perceived quality of the music produced  and  ii  how valence expression via dissonance produces the perceived affective state  this system  which can reliably generate affect expressive music  can subsequently be integrated in any kind of interactive application  e g   games  to create an adaptive and dynamic soundtrack  
 affective parameter shaping in user experience prospect evaluation based on hierarchical bayesian estimation user experience  ux  design has become an important factor of product success  one of the important issues involved in ux design is how to evaluate ux  in this research  ux evaluation is quantitatively fulfilled by the cumulative prospect theory  in which ux is perceived from the perspective of the decision making procedure of two alternative design profiles  furthermore  we study the influence of affective states on ux prospect evaluation through shaping affective parameters involved in ux design  to account for multiple sources of uncertainties  we develop a hierarchical bayesian model via markov chain monte carlo technique for parameter estimation under three affective states  also  aircraft cabin interior design is studied as a case study to demonstrate the potential and feasibility of the proposed method   c  2017 elsevier ltd  all rights reserved  
 affective product form design using fuzzy kansei engineering and creativity how to design an affective product form meeting customer s requirements always draws much attention  in the past  a popular scheme  kansei engineering  was widely and successfully used in dealing with such kind of issue  however  among vast kansei studies  none of them concerned on the most important matter in product form design  creativity  in the first place  this study proposes a new kansei manipulation procedure in which it combines an associative creativity thinking process with the fuzzy kansei engineering to explore a new product form matching future customers  requirements  a horn speaker is selected as the demonstration target  in the fuzzy kansei process  totally four final kansei images were obtained via market survey and statistical analysis  seven design elements were obtained via product decomposition  then the 7 scale semantic differential scheme and fuzzification are used to quantify the qualitative properties of product image and design element respectively  a kansei evaluation was done to 30 subjects for 19 selected samples  based on the obtained evaluation data  the multi linear regression and back propagation neural network schemes are adopted to build the relationship between kansei words and design elements  in the creative form generation process  the obtained kansei outcomes were used as the design basis and an associative creativity thinking procedure merging with biological simulation forming  it includes six steps which is applied to develop a new product form  in the end  a verification test was performed and a satisfying result of 11 7   has been increased in customer satisfaction  
 affective associative two process theory  a neural network investigation of adaptive behaviour in differential outcomes training in this article we present a novel neural network implementation of associative two process  atp  theory based on an actor critic like architecture  our implementation emphasizes the affective components of differential reward magnitude and reward omission expectation and thus we model affective associative two process theory  aff atp   atp has been used to explain the findings of differential outcomes training  dot  procedures  which emphasize learning differentially valuated outcomes for cueing actions previously associated with those outcomes  atp hypothesizes the existence of a prospective  memory route through which outcome expectations can bring to bear on decision making and can even substitute for decision making based on the retrospective  inputs of standard working memory  while dot procedures are well recognized in the animal learning literature they have not previously been computationally modelled  the model presented in this article helps clarify the role of atp computationally through the capturing of empirical data based on dot  our aff atp model illuminates the different roles that prospective and retrospective memory can have in decision making  combining inputs to action selection functions   in specific cases  the model s prospective route allows for adaptive switching  correct action selection prior to learning  following changes in the stimulus response outcome contingencies  
 affordance research in developmental robotics  a survey affordances capture the relationships between a robot and the environment in terms of the actions that the robot is able to perform  the notable characteristic of affordance based perception is that an object is perceived by what it affords  e g   graspable and rollable   instead of identities  e g   name  color  and shape   affordances play an important role in basic robot capabilities such as recognition  planning  and prediction  the key challenges in affordance research are  1  how to automatically discover the distinctive features that specify an affordance in an online and incremental manner and 2  how to generalize these features to novel environments  this survey provides an entry point for interested researchers  including  1  a general overview  2  classification and critical analysis of existing work  3  discussion of how affordances are useful in developmental robotics  4  some open questions about how to use the affordance concept  and 5  a few promising research directions  
 agent based modelling as a decision support system for shadow accounting we propose the use of agent based modelling to create a shadow account  that is  a secondary account of a business which is used to audit or verify the primary account  such a model could be used to test the claims of industries and businesses  for example  the model could determine whether a business is generating enough funds to pay minimum wage  parameters in the model can be set by observation or a range of values can be tested to determine points at which enough revenue could be generated  we illustrate the potential of agent based modelling as a tool for shadow accounting with a case study of a car wash business   c  2017 elsevier b v  all rights reserved  
 aircraft re routing optimization and performance assessment under uncertainty the need for aircraft re routing arises when there is disruption in the system  such as when an airport is closed due to extreme weather  in this paper  we investigate a simulation based approach to optimize the aircraft re routing process  by considering multiple sources of uncertainty  the proposed approach has four main components  system simulation  uncertainty representation  aircraft re routing algorithm  and system performance assessment  several sources of uncertainty are accounted for in this approach  related to incoming aircraft  space availability in neighboring airports  radar performance  and communication delays  an aircraft re routing optimization model is formulated to make periodic re routing decisions with the objective of minimizing the overall distance travelled by all the aircraft  subject to the system resources  we analyze the performance of this aircraft re routing system using system failure time as the metric  since the simulation time is limited  right censored data arises with respect to system failure time  a novel methodology is developed to compute the lower bound of system failure time in the presence of right censored data  and to analyze the sensitivity of the system performance metric to the uncertain variables relating to the aircraft  radars  nearby airports  and communication system  since the simulation is time consuming  we build a support vector regression  svr  surrogate model to efficiently construct the system failure time distribution   c  2017 elsevier b v  all rights reserved  
 airline passenger profiling based on fuzzy deep machine learning passenger profiling plays a vital part of commercial aviation security  but classical methods become very inefficient in handling the rapidly increasing amounts of electronic records  this paper proposes a deep learning approach to passenger profiling  the center of our approach is a pythagorean fuzzy deep boltzmann machine  pfdbm   whose parameters are expressed by pythagorean fuzzy numbers such that each neuron can learn how a feature affects the production of the correct output from both the positive and negative sides  we propose a hybrid algorithm combining a gradient based method and an evolutionary algorithm for training the pfdbm  based on the novel learning model  we develop a deep neural network  dnn  for classifying normal passengers and potential attackers  and further develop an integrated dnn for identifying group attackers whose individual features are insufficient to reveal the abnormality  experiments on data sets from air china show that our approach provides much higher learning ability and classification accuracy than existing profilers  it is expected that the fuzzy deep learning approach can be adapted for a variety of complex pattern analysis tasks  
 algorithm for constructing the efficient frontier of an investment portfolio an algorithm for calculating the limiting  for large time values  efficient frontier of an investment portfolio with its assets given by stochastic differential equations is considered  the model also takes into account the influence of macroeconomic factors  what makes this algorithm special is that it uses only the simplest operations of linear algebra  
 algorithmic iteration for computational intelligence machine awareness is a disputed research topic  in some circles considered a crucial step in realising artificial general intelligence  understanding what that is  under which conditions such feature could arise and how it can be controlled is still a matter of speculation  a more concrete object of theoretical analysis is algorithmic iteration for computational intelligence  intended as the theoretical and practical ability of algorithms to design other algorithms for actions aimed at solving well specified tasks  we know this ability is already shown by current ais  and understanding its limits is an essential step in qualifying claims about machine awareness and super ai  we propose a formal translation of algorithmic iteration in a fragment of modal logic  formulate principles of transparency and faithfulness across human and machine intelligence  and consider the relevance to theoretical research on  super  ai as well as the practical import of our results  
 algorithms for hierarchical clustering  an overview  ii we survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in r and other software environments  we look at hierarchical self organizing maps and mixture models  we review grid based clustering  focusing on hierarchical density based approaches  finally  we describe a recently developed very efficient  linear time  hierarchical clustering algorithm  which can also be viewed as a hierarchical grid based algorithm  this review adds to the earlier version  murtagh f  contreras p  algorithms for hierarchical clustering  an overview  wiley interdiscip rev  data mining knowl discov 2012  2  86 97   c  2017 wiley periodicals  inc  
 algorithms for interval valued fuzzy soft sets in stochastic multi criteria decision making based on regret theory and prospect theory with combined weight this paper presents two novel interval valued fuzzy soft set approaches  first  we initiate a new axiomatic definition of interval valued fuzzy distance measure  which is expressed by interval valued fuzzy number  ivfn  that will reduce the information loss and remain more original information  then  the objective weights of various parameters are determined via normal distribution  combining objective weights with subjective weights  we present the combined weights  which can reflect both the subjective considerations of the decision maker and the objective information  later  we propose two algorithms to solve stochastic multi criteria decision making problem  which take regret aversion and prospect preference of decision makers into consideration in the decision process  finally  the effectiveness and feasibility of two approaches are demonstrated by two numerical examples   c  2016 elsevier b v  all rights reserved  
 aligning enterprise knowledge and knowledge management systems to improve efficiency and effectiveness performance  a three dimensional fuzzy based decision support system the purpose of this paper is to propose a three dimensional fuzzy logic approach to evaluate the level of alignment between the knowledge an enterprise possesses and the knowledge management systems  kmss  it adopts  the study also aims to propose the kmss best suited to reducing misalignment and improving operational performance in terms of efficiency and effectiveness  analysing the level of alignment between an enterprise s knowledge and its kmss from both the ontological and epistemological points of view  the authors have used the proposed methodology to develop a software based knowledge management decision support system  km dss   which was tested on a small and medium enterprise  sme  operating in the high tech industry  the results highlight that the proposed dss allows managers to evaluate knowledge management processes and identify which kmss to adopt to improve alignment with the nature of the knowledge their enterprise possesses as well as to increase their level of efficiency and effectiveness   c  2017 elsevier ltd  all rights reserved  
 aligning packed dependency trees  a theory of composition for distributional semantics we present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees  we show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization  
 an adaptive ant colony optimization algorithm for constructing cognitive diagnosis tests a critical issue in the applications of cognitive diagnosis models  cdms  is how to construct a feasible test that achieves the optimal statistical performance for a given purpose  as it is hard to mathematically formulate the statistical performance of a cdm test based on the items used  exact algorithms are inapplicable to the problem  existing test construction heuristics  however  suffer from either limited applicability or slow convergence  in order to efficiently approximate the optimal cdm test for different construction purposes  this paper proposes a novel test construction method based on ant colony optimization  aco tc   this method guides the test construction procedure with pheromone that represents previous construction experience and heuristic information that combines different item discrimination indices  each test constructed is evaluated through simulation to ensure convergence towards the actual optimum  to further improve the search efficiency  an adaptation strategy is developed  which adjusts the design of heuristic information automatically according to the problem instance and the search stage  the effectiveness and efficiency of the proposed method is validated through a series of experiments with different conditions  results show that compared with traditional test construction methods of cdms  the proposed aco tc method can find a test with better statistical performance at a faster speed   c  2016 elsevier b v  all rights reserved  
 an adaptive consensus method for multi attribute group decision making under uncertain linguistic environment for a multi attribute group decision making  magdm  problem  the so called consensus reaching process is used to achieve an agreement among experts and finally make a common decision  unfortunately  so far the consensus models for magdm havent been completely studied  especially for magdm under uncertain linguistic environment  the disadvantages of most existing consensus models could be summarized into 3 aspects   1  in most existing consensus models  all the experts opinions are weighted equally important  and or all the experts weights are treated statically   2  most of the interactive consensus methods are lack of effective feedback mechanism  while the automatic ones also have some defects  such as the lack of pertinence in adjustment process and the inability to reflect the subjective opinions of experts   3  also the comparison methods for uncertain linguistic variables therein are far from perfect  which require either complicated computing process or may cause non distinguishable cases  in order to solve the above problems and obtain final decision results more efficiently  an interactive method with adaptive experts weights and explicit guidance rules for magdm under uncertain linguistic environment is developed  our contributions can be summarized as follows   1  based on the definitions of closeness and consensus indices  a non linear programming model is constructed to dynamically adjust the experts weights by maximizing the group consensus   2  a targeted feedback mechanism including identification rules and recommendation rules is designed to guide the experts to modify their opinions more precisely and effectively   3  a more appropriate method for comparing uncertain linguistic variables named dominance index is proposed  which can simplify the calculation process significantly  finally  an illustrative example proves that the proposed consensus method is feasible and effective  and a detailed comparison and analysis highlights the advantages and characteristics of this method   c  2017 published by elsevier b v all rights reserved  
 an adaptive framework for mobile robot navigation collective behaviours observed in nature bring new methodologies in proposing control algorithms for robot groups to perform a variety of complex tasks  in this article  an adaptive algorithm  allowing the safe navigation of a group of robots in a collective manner  is proposed  the algorithm  inspired from the adaptive particle swarm optimization technique  proposes an efficient control approach to overcome both static and moving obstacles  accordingly  compared to the conventional particle swarm optimization algorithm  the proposed system allows a robot or group of robots  swarm  to complete the goal while avoiding static and moving obstacles as well as dynamic targets in a safe and collective manner  the simulation results verify the overall performance and reliability of the proposed system  
 an adaptive portfolio trading system  a risk return portfolio optimization using recurrent reinforcement learning with expected maximum drawdown dynamic control theory has long been used in solving optimal asset allocation problems  and a number of trading decision systems based on reinforcement learning methods have been applied in asset allocation and portfolio rebalancing  in this paper  we extend the existing work in recurrent reinforcement learning  rrl  and build an optimal variable weight portfolio allocation under a coherent downside risk measure  the expected maximum drawdown  e mdd   in particular  we propose a recurrent reinforcement learning method  with a coherent risk adjusted performance objective function  the calmar ratio  to obtain both buy and sell signals and asset allocation weights  using a portfolio consisting of the most frequently traded exchange traded funds  we show that the expected maximum drawdown risk based objective function yields superior return performance compared to previously proposed rrl objective functions  i e  the sharpe ratio and the sterling ratio   and that variable weight rrl long short portfolios outperform equal weight rrl long short portfolios under different transaction cost scenarios  we further propose an adaptive e mdd  risk based rrl portfolio rebalancing decision system with a transaction cost and market condition stop loss retraining mechanism  and we show that the proposed portfolio trading system responds to transaction cost effects better and outperforms hedge fund benchmarks consistently   c  2017 elsevier ltd  all rights reserved  
 an agent based model for understanding the influence of the 11 m terrorist attacks on the 2004 spanish elections government  politicians  and mass media generated a large quantity of information after the bombing attacks in madrid on the 11th of march 2004  this information had two competing dimensions on the terrorist group responsible for the attacks  eta and al qaeda  the framing theory could explain how this information influenced the spanish national elections on the 14th of march  three days after the attacks  we propose to analyze this political scenario using agent based modeling to recreate the environment and framing effect of the three days prior to the elections  using our model we define several experiments where we observe how media communications influence agent voters after calibrating the model with real data  these experiments are what if scenarios where we analyze alternatives for mass media communication messages and word of mouth behaviors  our results suggest that the framing effect affected the election results by influencing voters  these results also outline the aggregated impact of mass media channels and the different role of each party segment of voters during this period   c  2017 elsevier b v  all rights reserved  
 an aggregated fuzzy model for the selection of a managed security service provider in this study  by analyzing the related literature  the companies providing security services and  more importantly  the data provided by a group of experts  a novel set of 39 criteria is extracted which assists the managed security service provider   mssp  selection process  the set is further categorized into eight general classes  the validity and weights of these criteria are measured by a group of experts in iran  due to the large number and often confiicting criteria  and the qualitative nature of the evaluations of the service providers  fuzzy multi criteria decision making methods  fmcdm  are adopted  in order to demonstrate the application of the proposed model  a numerical example is included  in which eight service providers are evaluated by four decision makers applying fuzzy topsis  fuzzy vikor  fuzzy group electre  and fuzzy saw methods  owing to the variations of the outputs of the applied mcdm methods  they are further analyzed by an aggregation method to propose a unique service provider  a comparison between the output of the aggregation method and the four applied fuzzy mcdm methods is also made with the help of euclidean  hamming  manhattan and chebyshev distances  the comparison shows the minimum diversion between the outputs of the fuzzy topsis and the aggregation method  which indicates the appropriateness of the fuzzy topsis method in this particular problem  
 an algorithmic framework for frequent intraday pattern recognition and exploitation in forex market we present a knowledge discovery based framework that is capable of discovering  analyzing and exploiting new intraday price patterns in forex markets  beyond the well known chart formations of technical analysis  we present a novel pattern recognition algorithm for pattern matching  that we successfully used to construct more than 16 000 new intraday price patterns  after processing and analysis  we extracted 3518 chart formations that are capable of predicting the short term direction of prices  in our experiments  we used forex time series from 8 paired currencies in various time frames  the system computes the probabilities of events such as  within next 5 periods  price will increase more than 20 pips   results show that the system is capable of finding patterns whose output signals  tested on unseen data  have predictive accuracy which varies between 60 and 85  depending on the type of pattern  we test the usefulness of the discovered patterns  via implementation of an expert system using a straightforward strategy based on the direction and the accuracy of the pattern predictions  we compare our method against three standard trading techniques plus a  random trader   and we also test against the results presented in two recently published studies  our framework performs very well against all systems we directly compare   and also  against all other published results  
 an analytic approach to assessing organizational citizenship behavior this study examines the organizational citizenship behavior  ocb  of employees by designing and developing an analytic network process  anp  methodology  the viability of the proposed methodology is demonstrated via the sales representatives of beko  a brand name controlled by koc group  we first develop a conceptual framework based on qualitative research methods   in depth interviews and focus group sessions  we employ the principles of anp methodology to examine and discover the inter  relationships among the ocbs  this process results in a descriptive model that encapsulates the findings from both qualitative and analytics methods  necessity  altruism  departmental  compliance  and independence are the underlying dimensions of ocbs found to be the most influential important  the key novelty of this study resides in designing and developing a prescriptive analytics  i e  anp  methodology to evaluate the ocbs  which is rare in the area of organizational behavior  a managerial field of study that have been dominated by traditional statistical methods   and thus serves as a useful contribution augmentation to the business  managerial research methods  and also extends the reach coverage of analytics based decision support systems research and practice into a new direction   c  2017 elsevier b v  all rights reserved  
 an analytical approach for evaluation of atm deployment problem criteria automated teller machines  atms  are one of the most important key touch points to reach the customers  the investment in atms and the impact on the banking industry is growing steadily  therefore  in order to increase the profitability  banks seek to make their investments in the right places for atms  banks have to take into consideration many factors such as traffic flow  competition  cost and safety in order to determine the optimum locations of atms  this paper proposes a decision support system that determines the importance value of atm deployment decision criteria which can be used to decide the best atm locations afterwards  fuzzy analytic network process  fanp  methodology is preferred to evaluate the importance levels of deployment criteria and chang s extent analysis is employed for eliminating the vagueness of the decisions from the evaluation process  the decision makers are asked to express their opinions on the comparative importance of various factors in linguistic terms  these linguistic variable scales are then converted into fuzzy numbers  since it becomes more meaningful to quantify a subjective measurement into a range rather than an exact value  finally  the relative importance of the criteria is obtained  the decision process includes the identification of 17 criteria grouped into 5 clusters which are named financial  commercial  traffic  demographic and strategic  the application of the proposed method was performed for turkish banking industry  number of bank s atms  expected level of commission income and expected level of transaction income were found as the three most significant criteria that should be analyzed deeply by the decision makers in atm location selection decision  the results of the proposed method considered applicable and valid by the experts who included in the decision process and this analysis demonstrated by their responses to a set of questions  
 an ann based approach of interpreting user generated comments from social media  the it advancement facilitates growth of social media networks  which allow consumers to exchange information online  result  a vast amount of user  generated data is freely available via internet  these data  in the raw format  are qualitative  unstructured and highly subjective thus they do not generate any direct value for the business  given this potentially useful database it is beneficial to unlock knowledge it contains  this however is a challenge  which this study aims to address  this paper proposes an ann based approach to analyse user  generated comments from social media  the first mechanism of the approach is to map comments against predefined product attributes  the second mechanism is to generate input output models which are used to statistically address the significant relationship between attributes and comment length  the last mechanism employs artificial neural networks to formulate such a relationship  and determine the constitution of rich comments  the application of proposed approach is demonstrated with a case study  which reveals the effectiveness of the proposed approach for assessing product performance  recommendations are provided and direction for future studies in social media data mining is marked   c  2016 elsevier b v  all rights reserved  
 an ant colony system empowered variable neighborhood search algorithm for the vehicle routing problem with simultaneous pickup and delivery along with the progress in computer hardware architecture and computational power  in order to overcome technological bottlenecks  software applications that make use of expert and intelligent systems must race against time where nanoseconds matter in the long awaited future  this is possible with the integration of excellent solvers to software engineering methodologies that provide optimization based decision support for planning  since the logistics market is growing rapidly  the optimization of routing systems is of primary concern that motivates the use of vehicle routing problem  vrp  solvers as software components integrated as an optimization engine  a critical success factor of routing optimization is quality vs  response time performance  less time consuming and more efficient automated processes can be achieved by employing stronger solution algorithms  this study aims to solve the vehicle routing problem with simultaneous pickup and delivery  vrpspd  which is a popular extension of the basic vehicle routing problem arising in real world applications where pickup and delivery operations are simultaneously taken into account to satisfy the vehicle capacity constraint with the objective of total travelled distance minimization  since the problem is known to be np hard  a hybrid metaheuristic algorithm based on an ant colony system  acs  and a variable neighborhood search  vns  is developed for its solution  vns is a powerful optimization algorithm that provides intensive local search  however  it lacks a memory structure  this weakness can be minimized by utilizing long term memory structure of acs and hence the overall performance of the algorithm can be boosted  in the proposed algorithm  instead of ants  vns releases pheromones on the edges while ants provide a perturbation mechanism for the integrated algorithm using the pheromone information in order to explore search space further and jump from local optima  the performance of the proposed acs empowered vns algorithm is studied on well known benchmarks test problems taken from the open literature of vrpspd for comparison purposes  numerical results confirm that the developed approach is robust and very efficient in terms of both solution quality and cpu time since better results provided in a shorter time on benchmark data sets is a good performance indicator   c  2016 elsevier ltd  all rights reserved  
 an application of fuzzy prototypes to the diagnosis and treatment of fuzzy diseases decision support systems  embedded in modern telemedicine applications  are a tool to improve the skills of general practitioners and patients in decision making in medicine  nowadays  one of the more challenging problems in this context is how to diagnose those diseases  whose early clinical signs are often subtle  and many of their common signs and symptoms are similar to other  these   fuzzy diseases   even they can have distinctive features  are not diagnosable through a concrete clinical test or symptom  and  thus  they are difficult to recognize  especially in their initial phases when they might be mistaken for other similar ones  then  the diagnosis of a fuzzy disease set is based on the exclusion of symptoms and tests results  due to the similarity between them  in the present article  it is proposed the development of a clinical decision support system framework to diagnose a set of fuzzy diseases  concretely applied to fibromyalgia and associated syndromes  for this purpose  in this paper a reasoning method that uses theories about conceptual categorization from the psychology  pattern recognition  and zadeh s prototypes has been designed  through the use of this model  satisfactory results in the evaluation of patients were obtained   c  2016 wiley periodicals  inc  
 an application of owa operators in fuzzy business diagnosis the paper aims to develop an adjustment index based on owa operators to enrich the results of diagnostic fuzzy models of business failure  a proposal to verify the diseases prediction accuracy of the models is also added  this allows a reduction of the map of causes or diseases detected in strategic defined areas  at the same time  these key areas can be disaggregated when an alert indicator is identified  and shows which of the causes need special attention  this application of owa can encourage the development of suitable computer systems for monitoring companies  problems  warn of failures and facilitate decision making  in addition  taking vigier and terceho s 2008 model as a benchmark  causes aggregation operators are introduced to evaluate alternative groupings  and the adjustment measure using approximate solutions is proposed to test the model s prediction  the empirical estimation and the verification of the improvement proposals in a set of small and medium  sized enterprises  smes  in the construction industry are also presented  the functionality and the prediction capacity are thus measured and detected by monitoring key areas that warn about insolvency situations in the firm   c  2016 elsevier b v  all rights reserved  
 an approach for emotions and behavior modeling in a crowd in the presence of rare events a common phenomenon in everyday life is that  when a strange event occurs or is announced  a regular crowd can completely change  showing different intense emotions and sometimes uncontrollable and violent emerging behavior  these emotions and behaviors that disturb the organization of a crowd are of concern in our study  and we attempt to predict these suspicious circumstances and provide help in making the right decisions at the right time  furthermore  most of the models that address crowd disasters belong to the physical or the cognitive approaches  they study pedestrian flow and collision avoidance  etc   and they use walking speed and angle of vision  however  in this work  based on a behavioral rules approach  we aim to model emergent emotion  behavior and influence in a crowd  taking into account particularly the personality of members of the crowd  for this purpose  we have combined the ocean  openness  consciousness  extraversion  agreeableness  and neuroticism  personality model with the occ  ortony  clore  and collins  emotional model to indicate the susceptibility of each of the five personality factors to feeling every emotion  then we proposed an approach that uses first fuzzy logic for the emotional modeling of critical emotions of members of the crowd at the announcement or the presence of unusual events  in order to quantify emotions  then  we model the behavior and the tendency towards actions using probability theory  finally  the influence among the members of the crowd is modeled using the neighborhood principle and cellular automata  
 an approach to decision making based on dynamic argumentation systems in this paper we introduce a formalism for single agent decision making that is based on dynamic argumentation frameworks  the formalism can be used to justify a choice  which is based on the current situation the agent is involved  taking advantage of the inference mechanism of the argumentation formalism  it is possible to consider preference relations  and conflicts among the available alternatives for that reasoning  with this formalization  given a particular set of evidence  the justified conclusions supported by warranted arguments will be used by the agent s decision rules to determine which alternatives will be selected  we also present an algorithm that implements a choice function based on our formalization  finally  we complete our presentation by introducing formal results that relate the proposed framework with approaches of classical decision theory   c  2016 elsevier b v  all rights reserved  
 an approach to evaluating the knowledge innovation ability of new ventures based on knowledge management with fuzzy number intuitionistic fuzzy information currently  most new ventures are knowledge based enterprises  moreover  knowledge innovation constitutes the major activities of the enterprises  production and knowledge management  therefore  it is of great significance to be able to properly analyze and evaluate a new venture s knowledge innovation ability  in this paper  we investigate the multiple attribute decision making problems with fuzzy number intuitionistic fuzzy information  then  we develop the fuzzy number intuitionistic fuzzy hamacher power weighted geometric  fnifhpwg  operator  then  we apply the fnifhpwg operator to deal with multiple attribute decision making for evaluating the knowledge innovation ability of new ventures based on knowledge management under the fuzzy number intuitionistic fuzzy environments  finally  an illustrative example for evaluating the knowledge innovation ability of new ventures based on knowledge management is given to verify the developed approach  
 an approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity to efficiently learn from feedback  cortical networks need to update synaptic weights on multiple levels of cortical hierarchy  an effective and well known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm  however  in this algorithm  the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified  whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons  several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity  but these models require complex external control over the network or relatively complex plasticity rules  here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously  employing only simple local hebbian plasticity  furthermore  for certain parameters  the weight change in the predictive coding model converges to that of the backpropagation algorithm  this suggests that it is possible for cortical networks with simple hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output  
 an automated approach to estimate human interest can we model and estimate interest  in general  when an individual engages with an object  say facebook  instagram  a mobile game  or anything else  we know that there is some interest that the person has in the object  however  we do not have a procedure that can tell us by  how much  of a factor is the person interested  simply put  can we find a  number  for someone s interest  in this article  we propose the design of a framework that can handle this issue  we formulate the interest estimation problem as a state estimation problem and deduce interest indirectly from the activity  activity  stimulated by interest  is measured via a subjective objective weighted approach  further  we present a novel continuous time model for interest by drawing inspiration from physics and economics simultaneously  we model interest along the ornstein uhlenbeck process in physics and improve the performance by borrowing ideas from stochastic volatility models in economics  subsequently  we employ particle filter to solve the interest estimation problem  to validate the feasibility of the proposed theory in practice  we investigate the model by conducting numerical simulations on real world datasets  the results demonstrate good performance of the framework  and thus match the theoretical expectations from the method  lastly  we implement the framework in practice and deploy it as a restful service  thereby providing a uniform interface for accessing the procedure via any remote or local application  
 an autonomous robotic exercise tutor for elderly people ambient assisted living proposes to utilize technological solutions to sustain the well being of elderly people  in accordance with the vision of successful aging  we describe in this study an autonomous robotic exercise tutor for elderly people  the robot learns a set of physical exercises from a human demonstrator in an imitation framework  and performs these motions in an exercise scenario  while monitoring the elderly person to provide verbal feedback  we developed an exercise program in collaboration with a nursing home  and tested our system in a real world scenario with visitors of a day care center  over multiple sessions  we provide a detailed description of the system implementation  as well as our observations for the exercise program  for the study held in the day care center  video annotations and user self assessments are evaluated to measure the overall performance of the system and to validate our approach  the analysis revealed that elderly people can successfully exercise with the assistance of the robot  while staying engaged with the system over multiple sessions  
 an earned presence  studying the effect of multi task improvisation systems on cognitive and learning capacity in this article  we articulate preliminary insights from two pilot studies  these studies contribute to an ongoing process of developing empirical  cross disciplinary measures to understand the cognitive and learning effects of complex artistic practices   effects that we situate between theory of embodied concepts and conceptually calibrated physical attention and action  the stage of this process that we report on here was led by the cognitive performance studies scholar and dramaturge  pil hansen  and undertaken in collaboration with the experimental psychologist  vina goghari  and the behavioural economist  robert oxoby  assisted by four research assistants from drama  music  and psychology at the university of calgary  our team set out to test the following hypothesis  active participation in performance generating systems has a positive effect on advanced student performers  working memory capacity  executive functions  and learning  our results have implications  in particular  for understandings of embodied learning in the educational sector  however a perhaps more significant contribution is a better understanding of the measures and constructs needed to arrive at a more complex  yet operational concept of embodied learning and forward the experimental study of relationships between performing arts practices  cognition  and learning  
 an eemd based multi scale fuzzy entropy approach for complexity analysis in clean energy markets to measure the efficiency of clean energy markets  a multi scale complexity analysis approach is proposed  due to the coexisting characteristics of clean energy markets  the  divide and conquer  strategy is introduced to provide a more comprehensive complexity analysis framework for both overall dynamics and hidden features  in different time scales   and to identify the leading factors contributing to the complexity  in the proposed approach  ensemble empirical mode decomposition  eemd   a competitive multi scale analysis tool  is first implemented to capture meaningful features hidden in the original market system  second  fuzzy entropy  an effective complexity measurement  is employed to analyze both the whole system and inner features  in empirical analysis  the nuclear energy and hydropower markets in china and us are investigated  and some interesting results are obtained  for overall dynamics  the us clean energy markets appear a significantly higher complexity level than china s markets  implying market maturity and efficiency of us clean energy relative to china  for inner features  similar features  in terms of similar time scales  in different markets present similar complexity levels  for different inner features  there are some distinct differences in clean energy markets between us and china  china s markets are mainly driven by upward long term trends with a low level complexity  while short term fluctuations with high level complexity are the leading features for the us markets  all these results demonstrate that the proposed eemd based multi scale fuzzy entropy approach can provide a new analysis tool to understand the complexity of clean energy markets   c  2017 elsevier b v  all rights reserved  
 an effective ant colony optimization algorithm for multi objective job shop scheduling with equal size lot splitting this paper proposes several novel hybrid ant colony optimization  aco  based algorithms to resolve multi objective job shop scheduling problem with equal size lot splitting  the main issue discussed in this paper is lot splitting of jobs and tradeoff between lot splitting costs and makespan  one of the disadvantages of aco is its uncertainty on time of convergence  in order to enrich search patterns of aco and improve its performance  five enhancements are made in the proposed algorithms including  a new type of pheromone and greedy heuristic function  three new functions of state transition rules  a nimble local search algorithm for the improvements of solution quality  mutation mechanism for divisive searching  a particle swarm optimization  pso  based algorithm for adaptive tuning of parameters  the objectives that are used to measure the quality of the generated schedules are weighted sum of makespan  tardiness of jobs and lot splitting cost  the developed algorithms are analyzed extensively on real world data obtained from a printing company and simulated data  a mathematical programming model is developed and paired samples t tests are performed between obtained solutions of mathematical programming model and proposed algorithms in order to verify effectiveness of proposed algorithms   c  2017 elsevier b v  all rights reserved  
 an efficient corpus based stemmer word stemming is a linguistic process in which the various inflected word forms are matched to their base form  it is among the basic text pre processing approaches used in natural language processing and information retrieval  stemming is employed at the text pre processing stage to solve the issue of vocabulary mismatch or to reduce the size of the word vocabulary  and consequently also the dimensionality of training data for statistical models  in this article  we present a fully unsupervised corpus based text stemming method which clusters morphologically related words based on lexical knowledge  the proposed method performs cognitive inspired computing to discover morphologically related words from the corpus without any human intervention or language specific knowledge  the performance of the proposed method is evaluated in inflection removal  approximating lemmas  and information retrieval tasks  the retrieval experiments in four different languages using standard text retrieval conference  cross language evaluation forum  and forum for information retrieval evaluation collections show that the proposed stemming method performs significantly better than no stemming  in the case of highly inflectional languages  marathi and hungarian  the improvement in mean average precision is nearly 50  as compared to unstemmed words  moreover  the proposed unsupervised stemming method outperforms state of the art strong language independent and rule based stemming methods in all the languages  besides information retrieval  the proposed stemming method also performs significantly better in inflection removal experiments  the proposed unsupervised language independent stemming method can be used as a multipurpose tool for various tasks such as the approximation of lemmas  improving retrieval performance or other natural language processing applications  
 an effort feedback perspective on persuasive decision aids for multi attribute decision making decision strategies and the level of cognitive effort humans devote to decision making are highly sensitive  this study investigates the role of feedback interventions in decision aids  das  to direct the user s attention and consequently increase the level of effort spent on the thinking in multi attribute selection problems  guided by four research hypotheses  we conducted an experiment with two groups  one with feedback enabled  the other one with it disabled  and provide post hoc click data analysis  the self developed persuasive da used in the experiment featured a continuous feedback mechanism based on the users investment of time  this da led the users through a smartphone decision scenario with altering levels of complexity  results show that normative effort feedback increases the decision maker s willingness to spend more effort  we provide new evidence supporting the view that das should pay more attention to soft persuasion by guiding the decision maker towards working harder rather than only confronting the user with final recommendations  
 an electrophysiological model of working memory performance working memory  wm  enables us to keep a limited amount of information in active mode  it is believed that attention refreshes necessary information in wm and prevents their forgetting  despite a plethora of models offered  it is not fully understood that what factors may be involved in forgetfulness and in the required time for refreshing the information  in this study  an electrophysiological model of wm is proposed that consists of several resistor capacitor units  inspired of the  resource capacity theory   attention as a limited source of energy refreshes the voltage level of these units  according to the  time based resource sharing theory   only one of these units is allowed to use the limited source of attention at each moment  the source of attention is shared between active units  this model mimics the pattern of several well known observations of wm such as the recall interval  the word length  and the serial position effect  some suggestions have been provided about influencing factors in wm performance  model parameters give the ability of investigating the possible effect of some other factors on wm performance and also a probable prediction about how much information can we chunk   c  2017 elsevier b v  all rights reserved  
 an emerging hybrid mechanism for information disclosure forecasting corporate governance mechanisms ensure that investors get a fair return on their investment  a well established governance mechanism reduces the information asymmetry and agency cost between a firm s management and stakeholders  but decision makers find it difficult to assess the corporate governance status of publicly listed firms before the annual official announcement the following year  this study proposes a hybrid ensemble learning forecasting mechanism  helm   whose single component candidates from the extreme learning machine  elm  algorithm with dissimilar ensemble strategies  that is  data diversity  parameter diversity  kernel diversity  and preprocessing diversity  form one initial dataset  we implement locally linear embedding into the proposed mechanism to handle the dimensionality task and then utilize the weighted voting taken from the base components  cross validation performance on a training dataset as the integration mechanism  experimental results show that the proposed helm significantly outperforms the other classifiers  but its superior performance under many real life application domains comes with a critical drawback  it is incapable of providing an explanation for the underlying reasoning mechanisms  thus  this study advances the utilized rough set theory with its explanation capability to extract the inherent knowledge from the ensemble mechanism  helm   the informative rules can be used as a guideline for decision makers to make a reliable judgment under turbulent financial markets  
 an empirical assestment of fuzzy black and scholes pricing option model in spanish stock option market the main objective of this paper is assessing the empirical performance of fuzzy extension to black scholes option pricing formula  fbs   concretely we evaluate the goodness of the fbs predictions for traded prices of options on the spanish stock index ibex35 during march 2017  we firstly propose a procedure to fit  from real data  the fuzzy parameters to implement fbs in stock options  price of the subjacent asset  free discount rate and stock volatility  subsequently we evaluate the capability of fbs to include actual traded prices and whether this capability depends on option moneyness and expiration date  we find that fbs fits quite well actual traded prices  however  generally most representative market prices  closing and medium  are not better fitted than those more extreme  minimum and maximum   we have also check that the goodness of the fbs predictions often depends on the moneyness grade and the expiration date of options  
 an empirical investigation on the impact of xbrl adoption on information asymmetry  evidence from europe given the high cost of developing and implementing data standards such as extensible business reporting language  xbrl   it is critical to assess their influences before they are adopted on a large scale  the european parliament has voted for the new transparency directive that calls for the mandatory preparation of annual business performance reports in a single electronic reporting from january 1  2020 based on a cost benefit analysis by european securities and markets authority  esma   with due reference to current and future technological options such as xbrl  regulators in many other jurisdictions such as canadian securities administrators are also assessing the costs and benefits from xbrl adoption  this paper informs such analysis by examining whether the expected benefit of information asymmetry reduction is realized through xbrl adoption in a european context  xbrl adoption among european non financial firms is found to significantly increase market liquidity and thus reduce information asymmetry  the association is stronger for larger firms that have sufficient resources and expertise to properly implement the technology  the empirical findings also suggest that the association is stronger for non high technology firms whose financial statements affected by xbrl are more reliant upon by investors  based on these findings  xbrl evidences a viable option as an electronic reporting format with effective implementation for businesses   c  2016 elsevier b v  all rights reserved  
 an empirical study on the effect of data sparsity and data overlap on cross domain collaborative filtering performance in the present day  the oversaturation of data has complicated the process of finding information from a data source  recommender systems aim to alleviate this problem in various domains by actively suggesting selective information to potential users based on their personal preferences  amongst these approaches  collaborative filtering based recommenders  cf recommenders   which make use of users  implicit and explicit ratings for items  are widely regarded as the most successful type of recommender system  however  cf recommenders are sensitive to issues caused by data sparsity  where users rate very few items  or items receive very few ratings from users  meaning there is not enough data to give a recommendation  the majority of studies have attempted to solve these issues by focusing on developing new algorithms within a single domain  recently  cross domain recommenders that use multiple domain datasets have attracted increasing attention amongst the research community  cross domain recommenders assume that users who express their preferences in one domain  called the target domain  will also express their preferences in another domain  called the source domain   and that these additional preferences will improve precision and recall of recommendations to the user  the purpose of this study is to investigate the effects of various data sparsity and data overlap issues on the performance of cross domain cf recommenders  using various aggregation functions  in this study  several different cross domain recommenders were created by collecting three datasets from three separate domains of a large korean fashion company and combining them with different algorithms and different aggregation approaches  the cross recommenders that used high performance  high overlap domains showed significant improvement of precision and recall of recommendation when the recommendation scores of individual domains were combined using the summation aggregation function  however  the cross recommenders that used low performance  low overlap domains showed little or no performance improvement in all areas  this result implies that the use of cross domain recommenders do not guarantee performance improvement  rather that it is necessary to consider relevant factors carefully to achieve performance improvement when using cross domain recommenders   c  2017 elsevier ltd  all rights reserved  
 an encounter between 4e cognition and attachment theory this paper explores a constructive revision of the conceptual underpinnings of attachment theory through an encounter with the diverse elements of 4e cognition  attachment relationships involve the development of preference for one or a few carers and expectations about their availability and responsiveness as a haven of safety and a base from which to explore  in attachment theory  mental representations have been assigned a central organising role in explaining attachment phenomena  the 4e cognition approaches in cognitive science raise a number of questions about the development and interplay of attachment and cognition  these include   1  the nature of what bowlby called internal working models of attachment    2  the extent to which the infant carer dyad functions as an extension of the infant s mind  and  3  whether bowlby s attachment control system concept can be usefully re framed in enactive terms where traditional cognitivist representations are   3i  substituted for sensorimotor skill focused mediating representations   3ii  viewed as arising from autopoietic living organisms  and or  3iii  mostly composed from the non contentful mechanisms of basic minds  a theme that cross cuts these research questions is how representations for capturing meaning  and structures for adaptive control  are both required to explain the full range of behaviour of interest to attachment theory researchers  
 an enhanced fuzzy algorithm based on advanced signal processing for identification of stress nowadays  it is crucial to promote and develop the autonomy of people  and specifically of individuals with some disability  in order to improve their life quality and achieve a better inclusion into sociocultural life  therefore  the identification of stress situations can be a suitable assistive tool for improving their socio cultural inclusion  this work presents important enhancements and variations for an existing fuzzy logic stress detection system based on monitoring and processing different physiological signals  heart rate  galvanic skin response and breath   first  it proposes a method based on wavelet processing to improve the detection of r peaks of electrocardiograms  afterwards  it proposes to decompose the galvanic response signal into two components  the average value and the variations  in addition  it proposes to extract information out the breath signal by analyzing its frequential composition  finally  an improved response in detecting stress changes is shown in comparison with other previous works   c  2017 elsevier b v  all rights reserved  
 an evaluation of linear and non linear models of expressive dynamics in classical piano and symphonic music expressive interpretation forms an important but complex aspect of music  particularly in western classical music  modeling the relation between musical expression and structural aspects of the score being performed is an ongoing line of research  prior work has shown that some simple numerical descriptors of the score  capturing dynamics annotations and pitch  are effective for predicting expressive dynamics in classical piano performances  nevertheless  the features have only been tested in a very simple linear regression model  in this work  we explore the potential of non linear and temporal modeling of expressive dynamics  using a set of descriptors that capture different types of structure in the musical score  we compare linear and different non linear models in a large scale evaluation on three different corpora  involving both piano and orchestral music  to the best of our knowledge  this is the first study where models of musical expression are evaluated on both types of music  we show that  in addition to being more accurate  non linear models describe interactions between numerical descriptors that linear models do not  
 an evidential analysis of altman z score for financial predictions  case study on solar energy companies altman z score has been a well accepted model of predicting survivals and failures of manufactures since 1968  however  short of an underpinning theory causes a wide gap between asking and responding sides  which still has no effective solution  this research proposes a rough set approach to inducing granular evidence and solving evidential coefficients of financial ratios for the distressed companies  empirically  the proposed approach is applied to a financial database  taiwan economic journal  to analyze the solar energy industry during 2009 2014  the result shows the inferential evidence successfully serves as a basis for financial analysis and discloses that the profit efficiency of the distressed companies in taiwan s solar energy industry had been declining   c  2016 published by elsevier b v  
 an evolutionary agent based framework for modeling and analysis of labor market this paper presents an agent based model of labor market to investigate the relationship between company and worker  contrary to most of previous studies of labor market we apply a game theoretic approach to defining entities in labor market  companies and workers  a company can choose the level of wages  and workers can select the level of effort to increase the productivity in response to the wages  company and worker agents are designed to possess the basic attributes in order to reflect the real labor market and their activities are adaptively changed using evolutionary model  our approach is illustrated with four simulation results  the effect of workers resignation  sick leave  dismissal of companies  and productivity growth  various experiments were conducted to analyze the interactions between worker and company  indicating that performance based reward strategy and non greedy strategy in job changing are necessary for companies and workers  the experimental results confirm that the balanced power between worker and company is important in maintenance and extension of labor market  and nash equilibrium can be maintained in all the cases   c  2017 elsevier b v  all rights reserved  
 an evolutionary approach for dynamic single runway arrival sequencing and scheduling problem aircraft arrival sequencing and scheduling is a classic problem in the air traffic control to ensure safety and order of the operations at the terminal area  most of the related studies have formulated this problem as a static case and assume the information of all the flights is known in advance  however  the operation of the terminal area is actually a dynamic incremental process  various kinds of uncertainties may exist during this process  which will make the scheduling decision obtained in the static environment inappropriate  in this paper  aircraft arrival sequencing and scheduling problem is tackled in the form of a dynamic optimization problem  an evolutionary approach  namely dynamic sequence searching and evaluation  is proposed  the proposed approach employs an estimation of distribution algorithm and a heuristic search method to seek the optimal landing sequence of flights  compared with other related algorithms  the proposed method performs much better on several test instances including an instance obtained from the real data of the beijing capital international airport  
 an evolving possibilistic fuzzy modeling approach for value at risk estimation market risk exposure plays a key role in risk management  a way to measure risk exposure is to evaluate the losses likely to incur when the assets prices of a portfolio decline  most financial institutions rely on value at risk  var  estimates to measure downside market risk  this paper suggests an evolving possibilistic fuzzy modeling  epfm  approach to estimate var  the approach is an extension of the possibilistic fuzzy c means clustering and functional fuzzy rule based modeling within the framework of incremental learning  evolving possibilistic modeling employs memberships and typicalities to update the cluster structure and corresponding fuzzy rules using a statistical control distance based criterion  a utility measure evaluates the quality of the current cluster structure and associated model  data from the main global equity market indexes of united states  united kingdom  germany  spain  and brazil from january 2000 to december 2012 are used to estimate var using epfm  the performance of epfm is evaluated and compared with traditional var benchmarks such as historical simulation  garch  ewma  and extreme value theory based var  as well as with state of the art evolving approaches  the results suggest that epfm is a potential candidate for var modeling because it achieves better results than the alternative approaches   c  2017 elsevier b v  all rights reserved  
 an exact approach for the grocery delivery problem in urban areas in this paper  we face the problem of delivering a given amount of goods in urban areas in a business to consumer  b2c  electronic commerce  ec  environment  this problem can be considered as a particular case of vehicle routing problem  as a novel issue  here we have to determine the fleet of no homogeneous vehicles to be used for satisfying the demands of clients coming from grocery e channels  and their related itineraries  given the traveling limits imposed by the urban government  in fact  commercial vehicles are not allowed to go everywhere and can travel only in restricted daily time windows  according to their pollution emissions  we have to minimize the overall distribution costs  taking into account traveling components and setup ones  together with operative aspects and environmental issues  customer requirements  vehicle capacity and daily shift constraints have to be satisfied too  we outline the main characteristics of the problem in a b2c ec environment and propose a mixed integer linear programming model to solve this np hard problem  computational results of test bed cases related to different sized transportation networks and delivery demands are presented and analyzed with respect to the fleet of vehicles chosen for satisfying the customer demand and the street traffic limitations  then  a realistic case study derived from the e distribution channel of a grocery company of genoa  italy  is reported  considerations about cpu time and optimality gap are also given with the idea of making the proposed model effectively used and solved with any commercial software  
 an experimental psychological perspective on social robotics why should an experimental psychologist bother entering a relatively novel field like social robotics and how could the scientific community in social robotics potentially gain from this  this paper highlights the theoretical and practical gains and challenges associated with an interdisciplinary approach when addressing current research questions in human machine interaction  with a particular focus on social robotics  three core issues are discussed from an experimental social psychological perspective  first  this paper focuses on the importance of a scientific  theory driven approach to test causal relationships between key constructs of interest in the domain of human machine interaction  second  it addresses the need for advancement in the realm of measurement  e g   in terms of validity  to illustrate  this paper provides an overview of the author s own research in the domain of human robot interaction in which we have tried to live up to the aforementioned desiderata  finally  following common research development practice  we argue for bridging the gap between foundational and applied research by testing the validity of the theoretical assumptions and the robustness of our technical platforms in real world human machine interaction settings   c  2016 elsevier b v  all rights reserved  
 an expert system for extracting knowledge from customers  reviews  the case of amazon com  inc  e commerce has proliferated in the daily activities of end consumers and firms alike  for firms  consumer satisfaction is an important indicator of e commerce success  today  consumers  reviews and feedback are increasingly shaping consumer intentions regarding new purchases and repeated purchases  while helping to attract new customers  in our work  we use an expert system to predict the sentiment of a product considering a subset of available customers  reviews   c  2017 elsevier ltd  all rights reserved  
 an exploration of crime prediction using data mining on open data the increase in crime data recording coupled with data analytics resulted in the growth of research approaches aimed at extracting knowledge from crime records to better understand criminal behavior and ultimately prevent future crimes  while many of these approaches make use of clustering and association rule mining techniques  there are fewer approaches focusing on predictive models of crime  in this paper  we explore models for predicting the frequency of several types of crimes by lsoa code  lower layer super output areas   an administrative system of areas used by the uk police  and the frequency of anti social behavior crimes  three algorithms are used from direrent categories of approaches  instance based learning  regression and decision trees  the data are from the uk police and contain over 600 000 records before preprocessing  the results  looking at predictive performance as well as processing time  indicate that decision trees  m5p algorithm  can be used to reliably predict crime frequency in general as well as anti social behavior frequency  
 an extended fuzzy ahp approach to rank the influences of socialization externalization combination internalization modes on the development phase continuous innovation intended to deliver products with new attributes is an imperative driver for organizations to remain competitive in today s fast changing market  a successful innovation is often associated with adoption and execution of all seci  socialization externalization combination internalization  modes of knowledge creation within any product development  pd  phase  this article is an attempt to argue with the general notion and to distinguish different pd phases  affinity corresponding to distinct seci modes  in this regard  the paper proposes an extended fuzzy analytic hierarchy process  efahp  approach to determine the ranking in which any pd phase is influenced from seci modes  in the efahp approach  the complex problem of knowledge creation is first itemized into a simple hierarchical structure for pairwise comparisons  next  a triangular fuzzy number  tfn  concept is applied to capture the inherent vagueness in linguistic terms of a decision maker  this paper recommends mapping tfns with normal distributions about x axis  this allows us to develop a mathematical formulation to estimate the degree of possibility  importance value  when two tfns do not intersect with each other  current techniques in literature calculate this value as zero   in order to demonstrate the applicability and usefulness of the proposed efahp in ranking seci modes  an empirical study of development phase is considered  five criteria and their 19 sub criteria for measuring the phase s performance are identified based on both an extensive discussion with subject matter experts and rigorous literature survey  after stringent analysis  we found that the mode that highly influenced the development phase is  combination mode   the article discusses the application of lean tools that can be employed to improve the knowledge creation process   c  2016 elsevier b v  all rights reserved  
 an extended multiple criteria data envelopment analysis model several researchers have adapted the data envelopment analysis  dea  models to deal with two interrelated problems  weak discriminating power and unrealistic weight distribution  the former problem arises as an application of dea in the situations where decision makers seek to reach a complete ranking of units  and the latter problem refers to the situations in which basic dea model simply rates units 100  efficient on account of irrational input and or output weights and insufficient number of degrees of freedom  improving discrimination power and yielding more reasonable dispersion of input and output weights simultaneously remain a challenge for dea and multiple criteria dea  mcdea  models  this paper puts emphasis on weight restrictions to boost discriminating power as well as to generate true weight dispersion of mcdea when a priori information about the weights is not available  to this end  we modify a very recent mcdea models in the literature by determining an optimum lower bound for input and output weights  the contribution of this paper is sevenfold  first  we show that a larger amount for the lower bound on weights often leads to improving discriminating power and reaching realistic weights in mcdea models due to imposing more weight restrictions  second  the procedure for sensitivity analysis is designed to define stability for the weights of each evaluation criterion  third  we extend a weighted mcdea model to three evaluation criteria based on the maximum lower bound for input and output weights  fourth  we develop a super efficiency model for efficient units under the proposed mcdea model in this paper  fifth  we extend an epsilon based minsum bcc dea model to proceed our research objectives under variable returns to scale  vrs   sixth  we present a simulation study to statistically analyze weight dispersion and rankings between five different methods in terms of non parametric tests  and seventh  we demonstrate the applicability of the proposed models with an application to european union member countries   c  2016 elsevier ltd  all rights reserved  
 an extended outranking approach to rough stochastic multi criteria decision making problems randomness and roughness commonly exist simultaneously in one decision making problem in the real world  however  fewer systematic studies have been carried out for such problems  in this study  we employ interval valued rough random variables  irrvs  and interval valued rough numbers  irns  to process decision information  additionally  we propose a more reasonable comparison method of irns  we combine and extend the stochastic dominance of irrvs and the classic electre iii method  a useful outranking method   finally  we develop an extended electre iii approach for rough stochastic multi criteria decision making  mcdm  problems  we provide examples concerning site selection and investment appraisal in order to demonstrate the feasibility of our proposed approach  we verify the applicability and advantages of our approach through comparative analyses with other existing methods  illustrative and comparative analyses indicate that our proposed approach is feasible for many practical mcdm problems  and the final ranking results of the proposed approach are more accurate and consistent with those obtained in actual decision making processes  the irns and irrvs are useful for dealing with rough stochastic decision information  the proposed approach is feasible and effective for solving rough stochastic mcdm problems  and retains the merits of irrvs  sd relations  and outranking methods  the final outcomes from our approach are convincing and consist with actual decision making  thus  our proposed approach is more widely applicable  
 an i todim method for multi attribute decision making with interval numbers interval multi attribute decision making  i madm  is a topic of current interest  various methods have been proposed to address i madm problem  however  applying these methods is becoming increasingly difficult because the behavioural characteristics of the decision maker are often neglected  in this paper  a novel interval todim  i todim  method  in which two behavioural characteristics  i e   reference dependence and loss aversion  of the decision maker are considered  is proposed to address the i madm problem  the proposed i todim method is comprised of three modules  first  while considering the reference dependence behaviour of the decision maker  the relative gain and loss degrees of each alternative for each attribute are calculated  next  the relative perceived dominance degree of each alternative for each attribute is computed based on the loss aversion behaviour of the decision maker  after calculating the overall perceived dominance degree of each alternative  the ranking results of the alternatives can be obtained  finally  an example  a comparison and a simulation analysis are given to illustrate the feasibility and validity of the proposed method  
 an improved dempster shafer approach to construction safety risk perception this paper proposes a novel hybrid approach that merges fuzzy matter element  fme   monte carlo  mc  simulation technique  and dempster shafer  d s  evidence theory to perceive the risk magnitude of tunnel induced building damage at an early construction stage  the membership measurement in fme is used to construct basic probability assignments  bpas  of influential factors within different risk states  an improved evidence fusion rule that integrates the dempster  rule and the weighted average rule is developed to synthesize multi source conflicting evidence  a new defuzzification method  centre of distribution  cod   is proposed to achieve a crisp value that represents the final safety risk perception result  a confidence indicator  8  is put forward to measure the reliability of the safety risk perception result  a comprehensive information fusion framework that incorporates 14 influential factors is proposed to perceive the risk magnitude of tunnel induced building damage  six existing buildings adjacent to the excavation of wuhan yangtze metro tunnel  wymt   china  are utilized as a case study to verify the effectiveness and applicability of the proposed approach  results indicate that the proposed approach is capable of  i  achieving a more accurate result for safety risk perception  and  ii  identifying global sensitivities of input factors throughout a series of mc simulation enabled iterations  a discussion on how to define a reasonable membership function for configuration of bpas is further presented  the authors recommend that the constant coefficient lambda that affects the shape of the defined correlation function in bpa  basic probability assignment  constructs should have a value of three  and the risk perception result can thus reach up to the highest reliability level  this approach can enable a comprehensive preliminary safety risk perception during tunnel design phases  which can further substantially reduce the risk of building damage induced by tunneling excavation   c  2017 elsevier b v  all rights reserved  
 an improved fruit fly optimization algorithm for solving the multidimensional knapsack problem this paper presents an improved fruit fly optimization algorithm  iffoa  for solving the multidimensional knapsack problem  mkp   in iffoa  the parallel search is employed to balance exploitation and exploration  to make full use of swarm intelligence  a modified harmony search algorithm  mhs  is proposed and applied to add cooperation among swarms in iffoa  in mhs  novel pitch adjustment scheme and random selection rule are developed by considering specific characters of mkp and foa  moreover  a vertical crossover is designed to guide stagnant dimensions out of local optima and further improve the performance  extensive numerical simulations are conducted and comparisons with other state of the art algorithms verify that the proposed algorithm is an effective alternative for solving the mkp   c  2016 elsevier b v  all rights reserved  
 an in silico  biomarker based method for the evaluation of virtual neuropsychiatric drug effects the recent explosion in neuroscience research has markedly increased our understanding of the neurobiological correlates of many psychiatric illnesses  but this has unfortunately not translated into more effective pharmacologic treatments for these conditions  at the same time  researchers have increasingly sought out biological markers  or biomarkers  as away to categorize psychiatric illness  as these are felt to be closer to underlying genetic and neurobiological vulnerabilities  while biomarker based drug discovery approaches have tended to employ in vivo  e g   rodent  or in vitro test systems  relatively little attention has been paid to the potential of computational  or in silico  methodologies  here we describe such a methodology  using as an example a biophysically detailed computational model of hippocampus that is made to generate putative schizophrenia biomarkers by the inclusion of a number of neuropathological changes that have been associated with the illness  nmda system deficit  decreased neural connectivity  hyperdopaminergia   we use the specific inability to attune to gamma band  40 hz  auditory stimulus as our illness biomarker  we expose this system to a large number of virtual medications  defined by systematic variation of model parameters corresponding to five cellular level effects  the potential efficacy of virtual medications is determined by a wellness metric  wm  that we have developed  we identify a number of virtual agents that consist of combinations of mechanisms  which are not simply reversals of the causative lesions  the manner in which this methodology could be extended to other neuropsychiatric conditions  such as alzheimer s disease  autism  and fragile x syndrome  is discussed  
 an influence analysis of the number of members on the quality of knowledge in a collective collective knowledge is understood as the common knowledge state of a collective consisting of autonomous units  the knowledge states referred from these autonomous units to some degree reflect the real knowledge state of a subject in the real world  but it is not known to what degree because of incompleteness and uncertainty  although collective knowledge determination is an important task because these knowledge states can be different from each other  there exists another important issue with its quality  the quality of collective knowledge is based on the difference between the real knowledge state and the collective knowledge  in this study  we investigate the influence of the number of collective members on the quality of collective knowledge  through experimental analysis  the larger collective we use  the better the quality of collective knowledge will be  in other words  the large number of collective members positively affects the quality of collective knowledge  besides  some theorems about the relationship between the collective knowledge and the knowledge states in a collective  the influence of adding or removing members on the quality of collective knowledge are also proved  
 an insight into assistive technology for the visually impaired and blind people  state of the art and future trends assistive technology for the visually impaired and blind people is a research field that is gaining increasing prominence owing to an explosion of new interest in it from disparate disciplines  the field has a very relevant social impact on our ever increasing aging and blind populations  while many excellent state of the art accounts have been written till date  all of them are subjective in nature  we performed an objective statistical survey across the various sub disciplines in the field and applied information analysis and network theory techniques to answer several key questions relevant to the field  to analyze the field we compiled an extensive database of scientific research publications over the last two decades  we inferred interesting patterns and statistics concerning the main research areas and underlying themes  identified leading journals and conferences  captured growth patterns of the research field  identified active research communities and present our interpretation of trends in the field for the near future  our results reveal that there has been a sustained growth in this field  from less than 50 publications per year in the mid 1990s to close to 400 scientific publications per year in 2014  assistive technology for persons with visually impairments is expected to grow at a swift pace and impact the lives of individuals and the elderly in ways not previously possible  
 an integrated fuzzy dematel and fuzzy anp based balanced scorecard approach  application in turkish higher education institutions this study presents a fuzzy decision making based balanced scorecard  bsc  model for using in strategic management processes of universities  the model integrates bsc methodology with fuzzy multi criteria decision making methods which are fuzzy decision making trial and evaluation laboratory  dematel  and fuzzy analytic network process  anp   fuzzy dematel method is used for revealing the relations among the perspectives and the strategies of bsc quantitatively  the result of the fuzzy dematel is then used for constructing the network structure of the anp  finally  the weights of perspectives and strategies are obtained by applying fuzzy anp  the significant contribution of the proposed model is that it enables to prioritize the strategies  fuzzy anp  and also to determine the relationships among them  fuzzy dematel  which are two of the most important problems in strategic management process and also bsc applications  successful application of the proposed model to turkish universities strategic planning process within bsc framework is also presented  
 an intelligent hybrid trading system for discovering trading rules for the futures market using rough sets and genetic algorithms discovering intelligent technical trading rules from nonlinear and complex stock market data  and then developing decision support trading systems  is an important challenge  the objective of this study is to develop an intelligent hybrid trading system for discovering technical trading rules using rough set analysis and a genetic algorithm  ga   in order to obtain better trading decisions  a novel rule discovery mechanism using a ga approach is proposed for solving optimization problems  i e   data discretization and reducts  of rough set analysis when discovering technical trading rules for the futures market  experiments are designed to test the proposed model against comparable approaches  i e   random  correlation  and ga approaches   in addition  these comprehensive experiments cover most of the current trading system topics  including the use of a sliding window method  with or without validation dataset   the number of trading rules  and the size of training period  to evaluate an intelligent hybrid trading system  experiments were carried out on the historical data of the korea composite stock price index 200  kospi 200  futures market  in particular  trading performance is analyzed according to the number of sets of decision rules and the size of the training period for discovering trading rules for the testing period  the results show that the proposed model significantly outperforms the benchmark model in terms of the average return and as a risk adjusted measure   c  2017 elsevier b v  all rights reserved  
 an interactive possibilistic programming approach for a multi objective hub location problem  economic and environmental design this paper presents a new multi objective mathematical model for a multi modal hub location problem under a possibilistic stochastic uncertainty  the presented model aims to minimize the total transportation and traffic noise pollution costs  furthermore  it aims to minimize the maximum transportation time between origin destination nodes to ensure a high probability of meeting the service guarantee  in order to cope with the uncertainties and the multi objective model  we propose a two phase approach  including fuzzy interactive multi objective programming approach and an efficient method based on the me measure  due to the np hardness of the presented model  two meta heuristic algorithms  namely hybrid differential evolution and hybrid imperialist competitive algorithm  are developed  furthermore  a number of sensitivity analyses are provided to demonstrate the effectiveness of the presented model  finally  the foregoing meta heuristics are compared together through different comparison metrics   c  2016 elsevier b v  all rights reserved  
 an intuitionistic fuzzy programming method for group decision making with interval valued fuzzy preference relations the paper develops a new intuitionistic fuzzy  if  programming method to solve group decision making  gdm  problems with interval valued fuzzy preference relations  ivfprs   an if programming problem is formulated to derive the priority weights of alternatives in the context of additive consistent ivfpr  in this problem  the additive consistent conditions are viewed as the if constraints  considering decision makers   dms   risk attitudes  three approaches  including the optimistic  pessimistic and neutral approaches  are proposed to solve the constructed if programming problem  subsequently  a new consensus index is defined to measure the similarity between dms according to their individual ivfprs  thereby  dms  weights are objectively determined using the consensus index  combining dms  weights with the if program  a corresponding if programming method is proposed for gdm with ivfprs  an example of e commerce platform selection is analyzed to illustrate the feasibility and effectiveness of the proposed method  finally  the if programming method is further extended to the multiplicative consistent ivfpr  
 an iterated local search for the multi objective permutation flowshop scheduling problem with sequence dependent setup times due to its simplicity yet powerful search ability  iterated local search  ils  has been widely used to tackle a variety of single objective combinatorial optimization problems  however  applying ils to solve multi objective combinatorial optimization problems is scanty  in this paper we design a multi objective ils  moils  to solve the multi objective permutation flowshop scheduling problem with sequence dependent setup times to minimize the makespan and total weighted tardiness of all jobs  in the moils  we design a pareto based variable depth search in the multi objective local search phase  the search depth is dynamically adjusted during the search process of the moils to strike a balance between exploration and exploitation  we incorporate an external archive into the moils to store the non dominated solutions and provide initial search points for the moils to escape from local optima traps  we compare the moils with several multi objective evolutionary algorithms  moeas  shown to be effective for treating the multi objective permutation flowshop scheduling problem in the literature  the computational results show that the proposed moils outperforms the moeas   c  2016 elsevier b  v  all rights reserved  
 an ontology based model for competence management in the last years  the need for developing strategies  models and tools to manage competences clearly emerges in numerous scenarios  for instance  this emergence especially raises when it is required to realize effective recruiting platforms  decision support systems for human resource management  learning management systems and so on  this work proposes an ontology based model for the representation of competences able to support a wide range of scenarios where it is fundamental to model  organize and represent professional competences  enable interoperability and co operation among different and heterogeneous tools and  lastly  execute queries and inference operations over these competences  the proposed model starts from the outcomes of the specialized literature and the related r   d projects and produces a novel integrated model that represents both job offers and demands to support recruiting initiatives and to develop employability strategies aiming at a best matching as well as a careful skill gap analysis  the model has been evaluated by means of a three level approach also in the context of the siret project whose goal is defining a recruiting and training integrated system able to represent the professional competences of users and to understand the supplies and the demands in order to find optimal agreements in the job market  
 an open data approach for quantifying the potential of taxi ridesharing taxi ridesharing 1   trs  is an advanced form of urban transportation that matches separate ride requests with similar spatio temporal characteristics to a jointly used taxi  as collaborative consumption  trs saves customers money  enables taxi companies to economize use of their resources  and lowers greenhouse gas emissions  we develop a one to one trs approach that matches rides with similar start and end points  we evaluate our approach by analyzing an open dataset of  5 million taxi trajectories in new york city  our empirical analysis reveals that the proposed approach matches up to 48 34  of all taxi rides  saving 2 892 036 km of travel distance  231 362 891 of gas  and 532 134 64 kg of co2 emissions per week  compared to many to many trs approaches  our approach is competitive  simpler to implement and operate  and poses less rigid assumptions on data availability and customer acceptance   c  2017 elsevier b v  all rights reserved  
 an optimized real time crash prediction model on freeway with over sampling techniques based on support vector machine real time crash prediction is crucial in traffic management on freeway to improve traffic safety  this study presents an optimized crash prediction model on freeway with over sampling techniques based on support vector machine  svm   the model was constructed with traffic data collected by discrete loop detectors from a 48 7 km segment on the g60 freeway  shanghai  china  matched case control method and svm were applied to identify the high risk traffic flow status  two kinds of over sampling techniques have been conducted to optimize the raw samples  the adaptive synthetic over sampling technique presents better performance than the synthetic minority over sampling technique according to the nonparametric test  the results indicate that svm classifiers with the adaptive synthetic over sampling technique improve the accuracy and robustness when dealing with imbalanced data  mean impact value method was employed to rank the contributing factors leading to crash  this research contributes to more targeted strategies for real time safety management of freeway  
 an outranking method for multi criteria group decision making using hesitant intuitionistic fuzzy linguistic term sets this article proposes an outranking method for group decision making  gdm  using hesitant intuitionistic fuzzy linguistic term sets  hifltss   by means of hifltss  the flexibility in generating evaluation information under uncertainty can be achieved to a larger extent than intuitionistic fuzzy sets  ifss  or hesitant fuzzy linguistic term sets  hflss   based on intuitionistic fuzzy support function  ifsf   intuitionistic fuzzy risk function  ifrf  and intuitionistic fuzzy credibility function  ifcf   the net outranking flow index  nofi  of each alternative are calculated which represents the net outranking character of an alternative over the other  the linguistic scale functions  lsfs  are employed in this paper to conduct the transformation between qualitative information and quantitative data  finally  an outranking approach is constructed for ranking alternatives in multi criteria group decision making  mcgdm  problems  and the approach is demonstrated using a numerical example  
 an uncertain currency model with floating interest rates considering the uncertain fluctuations in the financial market from time to time  we propose a currency model with floating interest rates within the framework of uncertainty theory  different from the classical stochastic currency models  this paper is assumed that the domestic interest rate  the foreign interest rate and the exchange rate follow uncertain differential equations  after that  the pricing formulas of european and american currency options are derived  the simulation experiments presented in this paper illustrate the performance of the proposed model  and the relationship between the option pricing formulas and all relevant parameters is analyzed  
 an uncertain furniture production planning problem with cumulative service levels to investigate how the loss averse customer s psychological satisfaction affects the company s furniture production planning  we establish a furniture production planning model under uncertain environment  where customer demand and production costs are characterized by mutually independent uncertain variables  based on prospect theory  customer s psychological satisfaction about stockout performance is measured by cumulative service levels in our model  in the framework of uncertainty theory  the proposed uncertain model can be transformed into an equivalent deterministic form  however  the transformed model is a nonlinear mixed integer programming problem  which cannot be solved by conventional optimization algorithms  to cope with this difficulty  a chemical reaction optimization algorithm integrated with lingo software is designed to solve the proposed production planning problem  in order to verify the effectiveness of the designed hybrid chemical reaction optimization  cro  algorithm  we conduct several numerical experiments via an application example and compare with a spanning tree based genetic algorithm  hst ga   the computational results show that our proposed cro algorithm achieves better performance than hst ga  and the results also provide several interesting managerial insights in production planning problems  
 an uncertain multi objective programming model for machine scheduling problem this paper discusses a parallel machine scheduling problem in which the processing times of jobs and the release dates are independent uncertain variables with known uncertainty distributions  an uncertain programming model with multiple objectives is obtained  whose first objective is to minimize the maximum completion time or makespan  and second objective is to minimize the maximum tardiness time  a genetic algorithm is employed to solve the proposed uncertain machine scheduling model  and its efficiency is illustrated by some numerical experiments  
 an uncertain search model for recruitment problem with enterprise performance this paper studies a dynamic recruitment problem with enterprise performance in the uncertain environment  in which a firm first interviews finite job applicants sequentially and then makes an employment decision according to results of the interview  since the assessment of the firm about each interviewee s capability is subjective and the interviewees are heterogeneous  it is reasonable to characterize these assessments as independent but not identically distributed uncertain variables  what s more  an uncertain sequential search model is established to maximize the benefit of the recruitment firm  moreover  an optimal search strategy is presented by adopting the principle of optimality and the reservation value rule  the results demonstrate that the threshold of recruitment decreases with the search cost  and increases with the enterprise performance level  in addition  we find that the low employment risk applicant will be preferred  finally  some numerical examples are given to illustrate the effectiveness of the proposed model  
 an uncertain workforce planning problem with job satisfaction to investigate the effect of employees  job satisfaction on the firm s workforce planning  this paper builds a multi period uncertain workforce planning model with job satisfaction level  where the labor demands and operation costs are characterized as uncertain variables  the job satisfaction level is defined as the employees  psychological satisfaction about overtime through prospect theory  the proposed uncertain model can be transformed into an equivalent deterministic form  which contains complex nonlinear constraints and cannot be solved by conventional optimization methods  thus  a hybrid joint operations algorithm  joa  integrated with lingo software is designed to solve the proposed workforce planning problem  consequently  several numerical experiments are conducted to compare our proposed joa with a hybrid particle swarm optimization algorithm to verify the effectiveness of the joa algorithm  the results demonstrate that the firm s total operation cost increases with the employees  job satisfaction level  the loss averse degree and outside firms  overtime level  respectively  meanwhile  the firm would overpay in bounded rational cases with job satisfaction  and the overpayment can be seen as the value of bounded rationality  which ensures the firm s normal operation  
 an unobtrusive measurement method for assessing physiological response in physical human robot interaction the objective of this work was to develop and validate a novel unobtrusive method for measuring person s physiological response with a low cost integrated sensory system for use in a physical control task  two different sensory handles were designed  cylindrical and hemispherical shape  and used in a physical human robot control task  twenty three participants underwent a measurement session with both handles  performing four different tasks for each handle  two basic task conditions were permuted  physical load  high low  and task dynamics  high low   electrocardiogram  photoplethysmogram  electrodermal activity  and peripheral skin temperature signals were recorded by sensory handles and a reference high accuracy biosignal amplifier to determine the raw signal correlation between the measurement systems  additionally  several standardized physiological parameters were calculated and discussed for both systems  results of raw signal correlation showed a high correlation between the reference measurement system and the sensory handles  pearson s correlation coefficients were above 0 8 for most of the physiological signals in all task conditions  some effect of physical load and high task dynamics was registered  in terms of signal quality  the hemispherical design outperformed the cylindrical design  correlation results show that the proposed system correlates well with the reference system for all tasks  in terms of optimal design for signal quality and comfort  hemispherical handle shape is more appropriate  unobtrusive nature and short setup time of such a method deems it appropriate for home use  monitoring  and research  
 analysis and control of variability by using fuzzy individual control charts the detection of changes in a process within shortest time provides significant benefits in terms of cost and quality  when considering the cost which would show up because of delays in identifying variability  detecting the deviation in the process accurately and quickly has a great importance for investors  in this paper  return volatility in the borsa istanbul 30 index  bist 30  has been analyzed and a fuzzy control chart for individual measurements  fccim  has been proposed for use in determining and controlling in the variables of the bist 30 index  for this purpose  firstly exponential smoothing method is used to forecast the variability of stock price of bist 30 index by using minitab statistical software  and then a fuzzy control chart for individual measurements  fccim  which are fuzzy individual control chart  ficc  and fuzzy moving range control chart  fmrcc  with fuzzy control rules have been developed to be used in determining the variability of the process  for this aim  some fuzzy rules have been defined by using ms excel in fuzzy control chart for individual measurements  a real case application from istanbul stock exchange for bist 30 has been managed to check the effectiveness of suggested fuzzy control charts   c  2016 elsevier b v  all rights reserved  
 analysis of brand image effect on advertising awareness using a neuro fuzzy and a neural network prediction models almost all the worldwide and nationwide companies utilize advertising to increase their sales volume and profit  these companies pay millions of dollars to reach consumers and announce their products or services  this forces companies to evaluate advertising effects and check whether ads meet companys strategies  they need to evaluate the ads not only after announcement  but also before advertising  i e  they can be one step ahead by predicting the future advertising awareness through artificial intelligence tools such as fuzzy systems and neural networks  in this study  we propose to use adaptive neuro fuzzy inference system  anfis  and artificial neural network  ann  to analyze advertising decision making  anfis creates fuzzy rules and trains the neural network using given input data  this training ability of anfis and ann leads to predicting the advertising awareness outputs  here  we investigate three advertising awareness outputs  namely  top of mind  share of voice  and spontaneous awareness  in order to achieve the valid predictions  data are randomly divided into training data with 70 percent  validation data with 15 percent  and testing data with remained 15 percent of data  the correlation between actual data and predictions are calculated to check the accuracy of the predicted outputs  
 analysis of cognitive dissonance and overload through ability demand gap models maintaining alternative decisions in working memory  wm  can lead to accumulating high cognitive load  some aspects of cognitive load improve attentiveness  but adding a cognitively inconsistent  conflict  situation results in a failure in cognitive task performance  this research introduces the notion of the human ability demand gap  discrepancy between human cognitive ability and task performance  and its association with task evoked cognitive overload and cognitive dissonance  inconsistency   by using the ability demand gap as a 3 d response model  cognitive dissonance and overload was proposed to understand the confluence among working memory capacity  users  cognitive load  and task performance  the maximum gap was computed using kolmogorov smirnov  k s  statistics  the empirical studies show that the maximum ability demand gap can be considered as the threshold between cognitive dissonance and overload  it was also observed that there was a cyclical and nonlinear relationship between working memory capacity  cognitive dissonance lock up  and cognitive overload  
 analyzing and optimizing access control choice architectures in online social networks the way users manage access to their information and computers has a tremendous effect on the overall security and privacy of individuals and organizations  usually  access management is conducted using a choice architecture  a behavioral economics concept that describes the way decisions are framed to users  studies have consistently shown that the design of choice architectures  mainly the selection of default options  has a strong effect on the final decisions users make by nudging them toward certain behaviors  in this article  we propose a method for optimizing access control choice architectures in online social networks  we empirically evaluate the methodology on facebook  the world s largest online social network  by measuring how well the default options cover the existing user choices and preferences and toward which outcome the choice architecture nudges users  the evaluation includes two parts   a  collecting access control decisions made by 266 users of facebook for a period of 3 months  and  b  surveying 533 participants who were asked to express their preferences regarding default options  we demonstrate how optimal defaults can be algorithmically identified from users  decisions and preferences  and we measure how existing defaults address users  preferences compared with the optimal ones  we analyze how access control defaults can better serve existing users  and we discuss how our method can be used to establish a common measuring tool when examining the effects of default options  
 analyzing basketball games by a support vector machines with decision tree model support vector machines  svms  are an emerging and powerful technique in coping with classification problems  however  a lack of rule generation is a weakness of the svm model  especially in analyzing sporting results  this investigation developed a hybrid model integrating the svm technique and a decision tree approach  hsvmdt  to predict the results of basketball games  and to provide rules to aid coaches in developing strategies  the hsvmdt model employed the unique strength of svm and decision tree in generating rules and predicting the outcomes of games  with predicted outcomes of games  and rules yielded from the hsvmdt model  coaches can easily and quickly learn essential factors increasing the chances to win games  empirical results showed that the proposed hsvmdt model can obtain relatively satisfactory prediction accuracy and therefore is a promising alternative for analyzing the results of basketball competitions  
 analyzing engineering change of aircraft assembly tooling considering both duration and resource consumption aircraft assembly tooling is developed according to the constraints of geometric information and technical requirements of aircraft  and frequent aircraft changes can cause assembly tooling tasks to change frequently  assembly tooling parts are large in amount and complex in structure  due to the complex dependencies among the tasks of assembly tooling  change in one task can cause changes to many other tasks  which may require much time and resources to completely resolve them  however  long cycle and mass resource consumption for the engineering change would normally lead to high risk  high cost  high rework  and so on  the primary result of this work is the provision of a development support to find the optimal solution of assembly tooling change by examining the combined effects of duration and resource consumption  in this paper  engineering change progression of assembly tooling is modeled as a decrease of impact on affected tasks  which implies that the duration of certain changed task reduces gradually  besides  a deterministic simulation model is developed to analyze the change propagation schemes  the model explores the combined effects of task parallelism  resource constraints and change propagation during the engineering change process of assembly tooling  finally  a case study of an assembly tooling for the reinforced frame module is implemented and the analysis results suggest that the proposed method offers a valuable basis for providing targeted guidance on how to obtain the optimal engineering change scheme of assembly tooling   c  2017 elsevier ltd  all rights reserved  
 anatomy of the mind  a quick overview the recently published book   anatomy of the mind   explains psychological  cognitive  mechanisms  processes  and functionalities through a comprehensive computational theory of the human mind that is  a cognitive architecture  the goal of the work has been to develop a unified framework and then to develop process based mechanistic understanding of psychological phenomena within the unified framework  in this article  i will provide a quick overview of the work  
 annofin a hybrid algorithm to annotate financial text in this work  we study the problem of annotating a large volume of financial text by learning from a small set of human annotated training data  the training data is prepared by randomly selecting some text sentences from the large corpus of financial text  conventionally  bootstrapping algorithm is used to annotate large volume of unlabeled data by learning from a small set of annotated data  however  the small set of annotated data have to be carefully chosen as seed data  thus  our approach is a digress from the conventional approach of bootstrapping as we let the users randomly select the seed data  we show that our proposed algorithm has an accuracy of 73 56  in classifying the financial texts into the different categories   accounting    cost    employee    financing    sales    investments    operations    profit    regulations  and  irrelevant   even when the training data is just 30  of the total data set  additionally  the accuracy improves by an approximate average of 2  for an increase of the training data by 10  and the accuracy of our system is 77 91  when the training data is about 50  of the total data set  as a dictionary of hand chosen keywords prepared by domain experts are often used for financial text extraction  we assumed the existence of almost linearly separable hyperplanes between the different classes and therefore  we have used linear support vector machine along with a modified version of label propagation algorithm which exploits the notion of neighborhood  in euclidean space  for classification  we believe that our proposed techniques will be of help to early warning systems used in banks where large volumes of unstructured texts need to be processed for better insights about a company   c  2017 elsevier ltd  all rights reserved  
 annotation retrieval reinforcement by visual cognition modeling on manifold while content based image annotation and retrieval have been active research topics over the past decade  their correlation is not well exploited until recently  we argue that offline annotation and online retrieval models should be regarded as a unified long term learner to reinforce each other  based on this viewpoint  this paper presents an annotation retrieval reinforcement framework  in which a dual structured learning model is presented  the keyword image association is learned by semantic manifold modeling while the semantic correlation between keywords is learned by image word net modeling  in keyword image association modeling level  to effectively model keyword image manifold  we present a manifold co training algorithm to address the sample insufficiency problem  in manifold based image annotation  we view annotation process as semantic feature reduction in keyword space  based on which a biased fisher discriminant analysis  bfda  algorithm is presented for eigen feature  keyword  selection  in semantic correlation modeling level  a novel image word net is learned from annotation training set and users  retrieval log for  1  irrelevant annotated keyword pruning   2  semantic level retrieval enhancement  in retrieval  our framework can effectively reveal user target by improving traditional content based relevance feedback to linguistic level interaction using annotation information  based on which bfda is adopted for keyword selection  finally  user interaction logs are adopted for manifold model updating and annotation refinement  as presented in our experiments  proposed method outperforms state of the art annotation and retrieval algorithms in corel image databases with over 10 000 general purpose images   c  2016 elsevier b v  all rights reserved  
 anthropomorphic reasoning about neuromorphic agi safety one candidate approach to creating artificial general intelligence  agi  is to imitate the essential computations of human cognition  this process is sometimes called reverse engineering the brain  and the end product called neuromorphic   we argue that  unlike with other approaches to agi  anthropomorphic reasoning about behaviour and safety concerns is appropriate and crucial in a neuromorphic context  using such reasoning  we offer some initial ideas to make neuromorphic agi safer  in particular  we explore how basic drives that promote social interaction may be essential to the development of cognitive capabilities as well as serving as a focal point for human friendly outcomes  
 application of emotion affected associative memory based on mood congruency effects for a humanoid emotional factor plays an important role in communication  in the field of psychology  it is known that memory and emotions are closely related to each other  in this paper  we present the significance of emotional factors to associative memory in communication and apply it on human robot interaction problems  emotional models for the robot partner are developed  and an interactive robot system with a complex valued multi directional associative memory model is proposed  we utilize multi modal information such as object  gesture  voice  and facial expressions to associate the relationships in associative memory  and generate the emotional information for the robot partner  as a result  the robot partner is able to perform various actions depending on the emotional factors  results from the interactive experiments indicate possibility of suitable information for communication space being provided from the robot partner  
 applications of automated facial coding in media measurement facial coding has become a common tool in media measurement  with large companies  e g   unilever  using it to test all of their new video ad content  facial reactions capture the in the moment response of an individual and these data complement self report measures  two advancements in affective computing have made measurement possible at scale  1  computer vision algorithms are used to automatically code sign and message judgments based on facial muscle movements  2  video data are collected by recording responses in everyday environments via the viewer s own webcam over the internet  we present results of online facial coding studies of video ads  movie trailers  political content  and long form tv shows  we explain how these data can be used in market research  despite the ability to measure facial behavior in a scalable and quantifiable way  the interpretation of these data is still challenging without baselines and comparative measures  over the past four years we have collected and coded over two million responses to everyday media content  our huge dataset allows us to calculate reliable normative distributions of responses across different media types  we present these data and argue that this provides a context within which to interpret facial responses more accurately  
 applying behavioral economics in predictive analytics for b2b churn  findings from service quality data motivated by the long standing debate on rationality in behavioral economics and the potential of theory driven predictive analytics  this paper examines the link between service quality and b2b churn  using longitudinal b2b transactional data with service quality indicators provided by a large company  we present evidence that both rationality and bounded rationality assumptions play significant roles in predicting organizational decisions on churn  specifically  variables that relate to the assumed rationality of organizations appear to provide accurate predictions while  at the same time  variables that capture boundedly rational decision rules appear to play a role through  somatic states  that make organizations more sensitive to the rational variables  in addition to presenting a novel approach for predicting organizational decisions on churn  this paper offers theoretical and managerial insights as well as opportunities for future research at the intersection of behavioral economics and predictive analytics for decision making   c  2017 elsevier b v  all rights reserved  
 applying fuzzy logic of expert knowledge for accurate predictive algorithms of customer traffic flows in theme parks this study analyzes two forecasting models based on the application of fuzzy logic and evaluates their effectiveness in predicting visitor expenditure and length of stay at a popular theme park  the forecasting models are based on a set of more than 600 decision rules constructed in the form of a complex series of if then statements  these algorithms store expert knowledge  a descriptive instrument that records the individual visitor s time spent and expenditure distribution on activities in the e da world in taiwan was used to gather data  from a process of rigorous verification  the models developed are characterized by a high level of accuracy and efficiency  
 applying social computing to generate sound clouds human beings make decisions on a daily basis according to their social environment  social information given by such social contexts provides the basis for inferences  planning and coordination of any activity  social machines aim to incorporate this concept of social nature and make it possible to design digital systems that make information visible to the users  this paper presents a social machine implemented as a vo where humans and machines collaborate in a creative process to transform a picture into a musical sound cloud  the vo is implemented by a mas  and defines specialized roles for extracting sounds from the color pixels of the image  as a part of the social machine and in order to demonstrate the viability of the system  the prototype built from this model is evaluated by experts who rate the sounds produced following consonance criteria  
 approaches for credit scorecard calibration  an empirical analysis financial institutions use credit scorecards for risk management  a scorecard is a data driven model for predicting default probabilities  scorecard assessment concentrates on how well a scorecard discriminates good and bad risk  whether predicted and observed default probabilities agree  i e   calibration  is an equally important yet often overlooked dimension of scorecard performance  surprisingly  no attempt has been made to systematically explore different calibration methods and their implications in credit scoring  the goal of the paper is to integrate previous work on probability calibration  to re introduce available calibration techniques to the credit scoring community  and to empirically examine the extent to which they improve scorecards  more specifically  using real world credit scoring data  we first develop scorecards using different classifiers  next apply calibration methods to the classifier predictions  and then measure the degree to which they improve calibration  to evaluate performance  we measure the accuracy of predictions in terms of the brier score before and after calibration  and employ repeated measures analysis of variance to test for significant differences between group means  furthermore  we check calibration using reliability plots and decompose the brier score to clarify the origin of performance differences across calibrators  the observed results suggest that post processing scorecard predictions using a calibrator is beneficial  calibrators improve scorecard calibration while the discriminatory ability remains unaffected  generalized additive models are particularly suitable for calibrating classifier predictions   c  2017 elsevier b v  all rights reserved  
 approaches to pythagorean fuzzy stochastic multi criteria decision making based on prospect theory and regret theory with new distance measure and score function in this paper  we initiate a new axiomatic definition of pythagorean fuzzy distance measure  which is expressed by pythagorean fuzzy number that will reduce the information loss and remain more original information  then  the objective weights of various criteria are determined via grey system theory  combining objective weights with subjective weights  we present the combined weights  which can reflect both the subjective considerations of the decision maker and the objective information  meanwhile  a novel score function is proposed  later  we present two algorithms to solve stochastic multicriteria decision making problem  which takes prospect preference and regret aversion of decision makers into consideration in the decision process  finally  the effectiveness and feasibility of approach is demonstrated by a numerical example  
 approximate norta simulations for virtual sample generation we introduce an approximate variant of the norta method which aims at generating structured data from a given prior sample  the technique accommodates for any combinations of marginals  especially continuous discrete mixtures  and a wide range of correlation structures  we focus on the interesting case where the sample includes categorical data  both ordered and unordered  we provide an application in the financial industry through a test of our iterative newton like algorithm on a dataset comprising the results of a questionnaire  we show that the sampled data  similarly to the norta technique  matches both the marginal and correlation structures of the original dataset closely  consequently  analyses such as decision tree modeling or support vector machine classification and regression  can be carried out on the new  much larger  sample without altering the core properties of the original sample   c  2016 elsevier ltd  all rights reserved  
 approximating behavioral equivalence for scaling solutions of i dids interactive dynamic influence diagram  i did  is a recognized graphical framework for sequential multiagent decision making under uncertainty  i dids concisely represent the problem of how an individual agent should act in an uncertain environment shared with others of unknown types  i dids face the challenge of solving a large number of models that are ascribed to other agents  a known method for solving i dids is to group models of other agents that are behaviorally equivalent  identifying model equivalence requires solving models and comparing their solutions generally represented as policy trees  because the trees grow exponentially with the number of decision time steps  comparing entire policy trees becomes intractable  thereby limiting the scalability of previous i did techniques  in this article  our specific approaches focus on utilizing partial policy trees for comparison and determining the distance between updated beliefs at the leaves of the trees  we propose a principled way to determine how much of the policy trees to consider  which trades off solution quality for efficiency  we further improve on this technique by allowing the partial policy trees to have paths of differing lengths  we evaluate these approaches in multiple problem domains and demonstrate significantly improved scalability over previous approaches  
 archetypal analysis for data driven prototype identification prototypes  as rosch  1973  defined the term in the cognitive sciences field  are ideal exemplars that summarize and represent groups of objects  or categories  and that are typical  according to their internal resemblance and external dissimilarity vis a vis other groups or categories  in line with the cognitive approach  we propose a data driven procedure for identifying prototypes that is based on archetypal analysis and compositional data analysis  the procedure presented here exploits the properties of archetypes  both in terms of their external dissimilarity in relation to other points in the data set and in terms of their ability to represent the data through compositions in a simplex in which it is possible to cluster all of the observations  the proposed procedure is useful not only for the usual real data points  it may also be used for interval valued data  functional data  and relational data  and it provides well separated and clearly profiled prototypes   c  2016 wiley periodicals  inc  statistical analysis and data mining  the asa data science journal  2016 
 argumentation mining in user generated web discourse the goal of argumentation mining  an evolving research field in computational linguistics  is to design methods capable of analyzing people s argumentation  in this article  we go beyond the state of the art in several ways   i  we deal with actual web data and take up the challenges given by the variety of registers  multiple domains  and unrestricted noisy user generated web discourse   ii  we bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study   iii  we create a new gold standard corpus  90k tokens in 340 documents  and experiment with several machine learning methods to identify argument components  we offer the data  source codes  and annotation guidelines to the community under free licenses  our findings show that argumentation mining in user generated web discourse is a feasible but challenging task  
 artificial cognition for social human robot interaction  an implementation human robot interaction challenges artificial intelligence in many regards  dynamic  partially unknown environments that were not originally designed for robots  a broad variety of situations with rich semantics to understand and interpret  physical interactions with humans that requires fine  low latency yet socially acceptable control strategies  natural and multi modal communication which mandates common sense knowledge and the representation of possibly divergent mental models  this article is an attempt to characterise these challenges and to exhibit a set of key decisional issues that need to be addressed for a cognitive robot to successfully share space and tasks with a human  we identify first the needed individual and collaborative cognitive skills  geometric reasoning and situation assessment based on perspective taking and affordance analysis  acquisition and representation of knowledge models for multiple agents  humans and robots  with their specificities   situated  natural and multi modal dialogue  human aware task planning  human robot joint task achievement  the article discusses each of these abilities  presents working implementations  and shows how they combine in a coherent and original deliberative architecture for human robot interaction  supported by experimental results  we eventually show how explicit knowledge management  both symbolic and geometric  proves to be instrumental to richer and more natural human robot interactions by pushing for pervasive  human level semantics within the robot s deliberative system   c  2017 the authors  published by elsevier b v  
 assessing a fuzzy extension of rand index and related measures this empirical study extends the results of hullermeier  rifqi  henzgen  and senge  2012   it examines the ability of a generalization of the rand index and four related measures of similarity to recover the cluster structure of the data in the framework of fuzzy c means clustering  the index range is also used as a criterion statistic  a monte carlo simulation is conducted for both the null case and where the data have a well defined cluster structure  the fuzzy extension of the related measures is not so effective for imbalanced data  on the contrary  whether the index is dice  fowlkes and mallows  hurbert and arabie  or jaccard  it provides reliable results for noise data or for data containing fairly balanced clusters  the criticisms of the rand index in the context of crisp clustering can also be extended to its fuzzy version  
 assessing the equitable and sustainable well being of the italian provinces in recent years there has been an increasing interest in the measurement of well being of individuals and societies  influenced by the  beyond gdp  initiative  in 2012 the italian national institute of statistics  istat  and the national council for economics and labour launched the equitable and sustainable well being  bes  from the italian acronym of  benessere equo e sostenibile   project  a set of 134 indicators aimed at capturing the italian well being  lately  the debate on how to measure the well being moved from the national level to the local one  following this new trend  istat introduced a set of 88 indicators for the local well being  at nuts3 level   the so called  provinces  bes   based on this project  aim of the paper is to provide an exploratory analysis for detecting groups of italian provinces that share similar well being profiles  in particular  we first apply a factor analysis with the aim to reduce the high number of indicators and  grounded on these results  we then create groups of the italian provinces  applying the cluster analysis  in order to find similarity among them  finally  based on the result of the factor analysis  for each domain and for each italian province  we construct a composite indicator that is a linear combination of the estimated factor scores  with weights based on the gini index of concentration  
 assessing university enrollment and admission efforts via hierarchical classification and feature selection recruiting prospective students efficiently and effectively is a very important challenge for universities  mainly because of the increasing competition and the relevance of enrollment generated revenues  this work provides an intelligent system for modeling the student enrollment decisions problem  a nested logit classifier was constructed to predict which prospective students will eventually enroll in different bachelor degree programs of a small sized  private chilean university  feature selection is performed to identify the key features that influence the student decisions  such as socio demographic variables  gender  age  school type  among others   admission efforts  and admission test results  our results suggest that on campus activities are far more productive than career fairs and other efforts performed off campus  demonstrating the importance of bringing prospective students to the university  furthermore  variables such as gender  school type  and declared university and bachelor degree program preferences are shown to be relevant in successfully modeling the student s choice of university  
 assessing users  emotion at interaction time  a multimodal approach with multiple sensors users  emotional states influence decision making and are essential for the knowledge and explanation of users  behavior with computer applications  however  collecting emotional states during the interaction time with users is a onerous task because it requires very careful handling of the empirical observation  leading researchers to carry out assessments of emotional responses only at the end of the interaction  this paper reports our research in assessing users  behavior at interaction time and also describes the results of a case study which analyzed users  emotional responses while interacting with a game  we argue that capturing emotions during interaction time can help us in making changes on users  behavior  e g   changing from stressed to a less stressed state  or even suggesting an user to have a break  this can be all possible if both  1  emotions are captured during interaction and  2  changes are suggested at runtime  e g   through persuasion   the results of this study suggest that there are significant differences between emotional responses captured during the interaction and those declared at the end  
 assessment of fun in interactive systems  a survey fun is fundamental in life  since it fosters interaction and learning  but the design of fun is not trivial since it is subjective  it depends upon context  preferences and the history of users  that is why assessment is an important issue in the design process  traditional assessment methods involve observation and inquiring of users while interacting  while more recent methods involve data collection and physiological measurements  this article presents a survey on the existing methods for the assessment of fun from its constituent elements   attention  flow  immersion and emotion   c  2016 elsevier b v  all rights reserved  
 auction optimization using regression trees and linear models as integer programs in a sequential auction with multiple bidding agents  the problem of determining the ordering of the items to sell in order to maximize the expected revenue is highly challenging  the challenge is largely due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction  the main contribution of this paper is two fold  first  we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions  we learn regression models from historical auctions  which are subsequently used to predict the expected value of orderings for new auctions  given the learned models  we propose two types of optimization methods  a black box best first search approach  and a novel white box approach that maps learned regression models to integer linear programs  ilp   which can then be solved by any ilp solver  although the studied auction design problem is hard  our proposed optimization methods obtain good orderings with high revenues  our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ilp solver for optimization purposes  to this end  we provide efficient encodings of regression trees and linear regression models as ilp constraints  this new way of using learned models for optimization is promising  as the experimental results show  it significantly outperforms the black box best first search in nearly all settings   c  2015 elsevier b v  all rights reserved  
 audio visual synchronization in reading while listening to texts  effects on visual behavior and verbal learning reading while listening to texts  rwl  is a promising way to improve the learning benefits provided by a reading experience  in an exploratory study  we investigated the effect of synchronizing the highlighting of words  visual  with their auditory  speech  counterpart during a rwl task  forty french children from 3rd to 5th grade read short stories in their native language while hearing the story spoken by a narrator  in the non synchronized  s   condition the text was written in black on a white background  whereas in the synchronized  s   rwl  the text was written in grey and the words were dynamically written in black when they were aurally displayed  in a karaoke like fashion  the children were then unexpectedly tested on their memory for the orthographic form and semantic category of pseudowords that were included in the stories  the effect of synchronizing was null in the orthographic task and negative in the semantic task  children s preference was mainly for the s  condition  except for the poorest readers who tended to prefer the s  condition  in addition  the children s eye movements were recorded during reading  gaze was affected by synchronization  with fewer but longer fixations on words  and fewer regressive saccades in the s  condition compared to the s  condition  thus  the s  condition presumably captured the children s attention toward the currently heard word  which forced the children to be strictly aligned with the oral modality   c  2017 elsevier ltd  all rights reserved  
 auditing national cancer institute thesaurus neoplasm concepts in groups of high error concentration the national cancer institute thesaurus is an important knowledge resource that should ideally be error free  we investigated the occurrence of errors in the neoplasm subhierarchy  which is a part of the national cancer institute thesaurus disease  disorder or finding hierarchy  there are five key findings in this study   1  errors in the neoplasm subhierarchy are not uniformly distributed   2  a partial area taxonomy  which is a compact network for summarizing the structure and content of an ontology  helped uncover groups of concepts  called  smallpartial areas   in the neoplasm subhierarchy   3  the rate of errors in  small partial areas  is twice as large as in   large partial areas   44  versus 22    satisfying statistical significance  thus  we conclude that higher error concentrations exist in small partial areas   4  group based auditing can be used successfully to identify additional suspicious concepts in a small group  once a few members of the group are already known as erroneous   5  error correction propagation can be used successfully and with minimal effort to correct additional errors in the neoplasm subhierarchy that occur outside of an initial small group of erroneous concepts  we present examples of errors and examples of how corrections transform and simplify the partial area taxonomy  
 augmented reality  an ecological blend in this article we present ecological augmented reality  e ar   an approach that questions the theoretical assumptions of mainstream augmented reality  ar   the development of ar systems to date presupposes an information processing theory of perception that hinders the potential of the field  generally  in ar devices  virtual symbolic information is superimposed upon the environment in such a way that the real and the virtual may be processed  informationally speaking  in tandem  thus  we find information in reality itself  as well as virtual symbolic information  but by increasing the burden of symbolic crunching  ar devices run the risk of saturating the user of the technology  ar systems developed under the principles of an ecological psychology may contribute to new and better levels of performance and adaptation to the user s perceptual abilities  our proposal is to develop ar devices such that reality itself is augmented non symbolically by blending real and virtual layers information  although there are seldom ar devices in the market that are designed ecologically  two fields of research may well bring inspiration to ar developers  these are the design and manipulation of real objects  and ecological research in the field of sensory substitution  we consider them both in turn with an eye to putting forward a framework that eschews any type of information processing regarding the nature of our psychological processes  ultimately  our aim is to provide some guidelines for the exploration of an ecological trend in ar applications   c  2016 elsevier b v  all rights reserved  
 augmented reality based indoor navigation  a comparative analysis of handheld devices versus google glass navigation systems have been widely used in outdoor environments  but indoor navigation systems are still in early development stages  in this paper  we introduced an augmented reality  based indoor navigation application to assist people navigate in indoor environments  the application can be implemented on electronic devices such as a smartphone or a head mounted device  in particular  we examined google glass as a wearable head mounted device in comparison with handheld navigation aids including a smartphone and a paper map  we conducted both a technical assessment study and a human factors study  the technical assessment established the feasibility and reliability of the system  the human factors study evaluated human machine system performance measures including perceived accuracy  navigation time  subjective comfort  subjective workload  and route memory retention  the results showed that the wearable device was perceived to be more accurate  but other performance and workload results indicated that the wearable device was not significantly different from the handheld smartphone  we also found that both digital navigation aids were better than the paper map in terms of shorter navigation time and lower workload  but digital navigation aids resulted in worse route retention  these results could provide empirical evidence supporting future designs of indoor navigation systems  implications and future research were also discussed  
 augmenting feature model through customer preference mining by hybrid sentiment analysis a feature model is an essential tool to identify variability and commonality within a product line of an enterprise  assisting stakeholders to configure product lines and to discover opportunities for reuse  however  the number of product variants needed to satisfy individual customer needs is still an open question  as feature models do not incorporate any direct customer preference information  in this paper  we propose to incorporate customer preference information into feature models using sentiment analysis of user generated online product reviews  the proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough set technique  it is able to predict sentence sentiments for individual product features with acceptable accuracy  and thus augment a feature model by integrating positive and negative opinions of the customers  such opinionated customer preference information is regarded as one attribute of the features  which helps to decide the number of variants needed within a product line  finally  we demonstrate the feasibility and potential of the proposed method via an application case of kindle fire hd tablets  published by elsevier ltd  
 authenticating the writings of julius caesar in this paper  we shed new light on the authenticity of the corpus caesarianum  a group of five commentaries describing the campaigns of julius caesar  100 44 bc   the founder of the roman empire  while caesar himself has authored at least part of these commentaries  the authorship of the rest of the texts remains a puzzle that has persisted for nineteen centuries  in particular  the role of caesar s general aulus hirtius  who has claimed a role in shaping the corpus  has remained in contention  determining the authorship of documents is an increasingly important authentication problem in information and computer science  with valuable applications  ranging from the domain of art history to counter terrorism research  we describe two state of the art authorship verification systems and benchmark them on 6 present day evaluation corpora  as well as a latin benchmark dataset  regarding caesar s writings  our analyses allow us to establish that hirtius s claims to part of the corpus must be considered legitimate  we thus demonstrate how computational methods constitute a valuable methodological complement to traditional  expert based approaches to document authentication   c  2016 elsevier ltd  all rights reserved  
 autoextend  combining word embeddings with semantic resources we present autoextend  a system that combines word embeddings with semantic resources by learning embeddings for non word objects like synsets and entities and learning word embeddings that incorporate the semantic information from the resource  the method is based on encoding and decoding the word embeddings and is flexible in that it can take any word embeddings as input and does not need an additional training corpus  the obtained embeddings live in the same vector space as the input word embeddings  a sparse tensor formalization guarantees efficiency and parallelizability  we use wordnet  germanet  and freebase as semantic resources  autoextend achieves state of the art performance on word in context similarity and word sense disambiguation tasks  
 automated defect discovery for dishwasher appliances from online consumer reviews product defects can have a devastating impact on a firm s sales and reputation  especially in the era of social  media  the early detection of defects could not only protect consumers from financial losses  but could also mitigate financial damage to the manufacturer  previous work in automated defect discovery has had success in the automotive  consumer electronics  and toy industries  but so far there has been no application  to home appliances  in this study  we extend the text analytic framework conceived in earlier work to the discovery of underperformance in large home appliances  specifically dishwashers  we find that generic cross domain sentiment techniques can be strongly complemented by domain specific  smoke  and  sparkle  term lists that are highly correlated with potential defects  these findings can be highly beneficial to improving dishwasher appliance quality management methods   c  2016 elsevier ltd  all rights reserved  
 automated detection of engagement using video based estimation of facial expressions and heart rate we explored how computer vision techniques can be used to detect engagement while students  n   22  completed a structured writing activity  draft feedback review  similar to activities encountered in educational settings  students provided engagement annotations both concurrently during the writing activity and retrospectively from videos of their faces after the activity  we used computer vision techniques to extract three sets of features from videos  heart rate  animation units  from microsoft kinect face tracker   and local binary patterns in three orthogonal planes  lbp top   these features were used in supervised learning for detection of concurrent and retrospective self reported engagement  area under the roc curve  auc  was used to evaluate classifier accuracy using leave several students out cross validation  we achieved an auc    758 for concurrent annotations and auc    733 for retrospective annotations  the kinect face tracker features produced the best results among the individual channels  but the overall best results were found using a fusion of channels  
 automated domain bias correction and its application in text based personality analysis personality prediction based on textual data is one topic gaining attention recently for its potential in large scale personalized applications such as social media based marketing  however  when applying this technology in real world applications  users often encounter situations in which the personality traits derived from different sources  e g   social media posts versus emails  are inconsistent  varying results for the same individual renders the technology ineffective and untrustworthy  in this paper  we demonstrate the impact of domain differences in automated text based personality prediction  we also propose different approaches for domain error correction to meet different needs   a  single or multi domain correction and  b  outcome based or input feature based error correction  we conduct comprehensive experiments to evaluate the effectiveness of these methods  our findings demonstrate a significant improvement of prediction accuracy with the proposed methods   e g   20 30  relative error reduction using outcome based error correction or 48  increase of f1 score using feature based error correction   
 automated essay evaluation with semantic analysis essays are considered as the most useful tool to assess learning outcomes  guide students  learning process and to measure their progress  manual grading of students  essays is a time consuming process  but is nevertheless necessary  automated essay evaluation represents a practical solution to this task  however  its main weakness is the predominant focus on vocabulary and text syntax  and limited consideration of text semantics  in this work  we propose an extension of existing automated essay evaluation systems by incorporating additional semantic coherence and consistency attributes  we design the novel coherence attributes by transforming sequential parts of an essay into the semantic space and measuring changes between them to estimate coherence of the text  the novel consistency attributes detect semantic errors using information extraction and logic reasoning  the resulting system  named sage   semantic automated grader for essays  provides semantic feedback for the writer and achieves significantly higher grading accuracy compared with 9 other state of the art automated essay evaluation systems   c  2017 elsevier b v  all rights reserved  
 automatic detection and interpretation of nominal metaphor based on the theory of meaning automatic processing of metaphors can be explicitly divided into two subtasks  recognition and interpretation  this paper presents an approach to recognize nominal metaphorical references and to interpret metaphors by exploiting distributional semantics word embedding techniques and calculating semantic relatedness  in terms of detection  our idea is that nominal metaphors consist of source and target domains and that domains present in metaphors will be less related than domains present in non metaphors  we represent the meaning of the concept as a vector in high dimensional conceptual space derived from the corpus and compute the relatedness between the vectors to complete the task of detection  relatedness here is based on the semantics of concepts  thus  the model we present deals with metaphors where target and source have the same direct ancestors  such as  a surgeon is a butcher   then  using the relatedness between target and source domain  based on the properties of source domain and dynamic transfer of properties  we present an approach to interpret metaphors with dynamic transfer  based on the view that metaphor interpretation is the cooperation of source and target domains  we divide metaphor interpretation into two subtasks  properties extraction and properties transfer  creatively  we use annotations to express a non binary evaluation  and we take the degree of the annotators  acceptability to evaluate our interpretation of metaphors  
 automatic detection of satire in twitter  a psycholinguistic based approach in recent years  a substantial effort has been made to develop sophisticated methods that can be used to detect figurative language  and more specifically  irony and sarcasm  there is  however  an absence of new approaches and research works that analyze satirical texts  the recognition of satire by sentiment analysis and natural language processing  nlp  applications is extremely important because it can influence and change the meaning of a statement in varied and complex ways  we used this understanding as a basis to propose a method that employs a wide variety of psycholinguistic features and which detects satirical and non satirical text  we then went on to train a set of machine learning algorithms that would enable us to classify unknown data  finally  we conducted several experiments in order to detect the most relevant features that generate a better pattern as regards detecting satirical texts  we evaluated the effectiveness of our method by obtaining a corpus of satirical and non satirical news from mexican and spanish twitter accounts  our proposal obtained encouraging results  with an f measure of 85 5  for mexico and one of 84 0  for spain  moreover  the results of the experiment showed that there is no significant difference between mexican and spanish satire   c  2017 elsevier b v  all rights reserved  
 automatic measurement of anthropometric dimensions using frontal and lateral silhouettes anthropometric dimensions  such as lengths  heights  breadths  circumferences and their ratios are highly significant in healthcare  security  sports  clothing  tools and equipment industry  in this study  an automatic and precise method for anthropometric dimensions of human body using two dimensional images is proposed  the dimensions are obtained by using fiducial points that are detected from frontal and lateral views of body silhouettes  primary anthropometric dimensions  which include heights  breadths  depths and lengths  are obtained by calculating the difference between two relevant fiducial points  the secondary dimensions  ratios are obtained directly from primary dimensions  and circumference dimensions are estimated precisely using ellipsoid model  a total of 75  i e  51 primary and 24 secondary dimensions are obtained  which are three times the number acquired by the state of the art method  the accuracy of acquired dimensions is verified by comparing it with the manual measurements by using the standard parameter of maximum allowable error  it is found that mean absolute difference of all the dimensions  obtained by the proposed method  lie within the limits of maximum allowable error  more importantly  the mean absolute difference for the majority of dimensions  20 out of 24  is significantly less for proposed method as compared with the best method in existing literature  
 automatic negotiation  playing the domain instead of the opponent an automated negotiator is an intelligent agent whose task is to reach the best possible agreement  we explore a novel approach to developing a negotiation strategy  a domain based approach   specifically  we use two domain parameters  reservation value and discount factor  to cluster the domain into different regions  in each of which we employ a heuristic strategy based on the notions of temporal flexibility and bargaining strength  following the presentation of our cognitive and formal models  we show in an extensive experimental study that an agent based on that approach wins against the top agents of the automated negotiation competition of 2012 and 2013  and attained the second place in 2014  
 automatic pain assessment with facial activity descriptors pain is a primary symptom in medicine  and accurate assessment is needed for proper treatment  however  today s pain assessment methods are not sufficiently valid and reliable in many cases  automatic recognition systems may contribute to overcome this problem by facilitating objective and continuous assessment  in this article we propose a novel feature set for describing facial actions and their dynamics  which we call facial activity descriptors  we apply them to detect pain and estimate the pain intensity  the proposed method outperforms previous state of the art approaches in sequence level pain classification on both  the biovid heat pain and the unbc mcmaster shoulder pain expression database  we further discuss major challenges of pain recognition research  benefits of temporal integration  and shortcomings of widely used frame based pain intensity ground truth  
 automatic prediction of impressions in time and across varying context  personality  attractiveness and likeability in this paper  we propose a novel multimodal framework for automatically predicting the impressions of extroversion  agreeableness  conscientiousness  neuroticism  openness  attractiveness and likeability continuously in time and across varying situational contexts  differently from the existing works  we obtain visual only and audio only annotations continuously in time for the same set of subjects  for the first time in the literature  and compare them to their audio visual annotations  we propose a time continuous prediction approach that learns the temporal relationships rather than treating each time instant separately  our experiments show that the best prediction results are obtained when regression models are learned from audio visual annotations and visual cues  and from audio visual annotations and visual cues combined with audio cues at the decision level  continuously generated annotations have the potential to provide insight into better understanding which impressions can be formed and predicted more dynamically  varying with situational context  and which ones appear to be more static and stable over time  
 autonomous construction and exploitation of a spatial memory by a self motivated agent we propose an architecture for self motivated agents allowing them to construct their own knowledge of objects and of geometrical properties of space through interaction with their environment  self motivation is defined here as a tendency to experiment and to respond to behavioral opportunities afforded by the environment  interactions have predefined valences that specify inborn behavioral preferences  the long term goal is to design agents that construct their own knowledge of their environment through experience  rather than exploiting pre coded knowledge  over time  the agent learns relations between elements of the environment that afford its interactions  and its perception of these elements  in the form of data structures called signatures of interactions  these signatures allow the agent to attribute a low level semantics to elements that constitute its environment based on valences of interactions  without predefined knowledge about these elements and regardless of the number of element types  signatures of interaction are then used to localize elements in space and to construct data structures that characterize spatial properties of space  called signatures of places and signatures of presence  signatures of place and of presence characterize space using interactions rather than geometrical or topological properties  the agent uses these structures to maintain an egocentric representation of affordances of the surrounding environment  without any preconception about the elements that compose the environment  and without using notions of geometrical space  experiments with simulated agents show that they learn to behave in their environment  taking into account multiple surrounding objects  reaching or avoiding objects according to the valence of the interactions that they afford   c  2016 elsevier b v  all rights reserved  
 autonomous human robot proxemics  socially aware navigation based on interaction potential to enable situated human robot interaction  hri   an autonomous robot must both understand and control proxemics the social use of space to employ natural communication mechanisms analogous to those used by humans  this work presents a computational framework of proxemics based on data driven probabilistic models of how social signals  speech and gesture  are produced  by a human  and perceived  by a robot   the framework and models were implemented as autonomous proxemic behavior systems for sociable robots  including   1  a sampling based method for robot proxemic goal state estimation with respect to human robot distance and orientation parameters   2  a reactive proxemic controller for goal state realization  and  3  a cost based trajectory planner for maximizing automated robot speech and gesture recognition rates along a path to the goal state  evaluation results indicate that the goal state estimation and realization significantly improve upon past work in human robot proxemics with respect to  interaction potential  predicted automated speech and gesture recognition rates as the robot enters into and engages in face to face social encounters with a human user illustrating their efficacy to support richer robot perception and autonomy in hri  
 autoregressive metric based trimmed fuzzy clustering with an application to pm10 time series air quality measurement relies on the effectiveness of a network of monitoring stations  monitoring stations collect information about the evolution of air pollutants concentration  if shore stations supplies the same information  then some of them could be deemed as redundant  then  a clustering model for time series is useful to identify stations with similar features  time series of pollutant concentration can be classified using the autoregressive metric in the framework of standard clustering techniques  a serious drawback is related to the lack of robustness of standard procedures  in this paper  using a partitioning around medoids approach combined with a trimming based rule  a fuzzy model for cluster time series is proposed  the model provides a robust alternative to standard procedures  two simulation studies are carried out to evaluate the clustering performance of the proposed clustering model  finally  an empirical application to real time series of pm10 concentration in the lazio region is presented and discussed showing the practical usefulness of the proposed approach  
 awareness improves problem solving performance the brain s self monitoring of activities  including internal activities   a functionality that we refer to as awareness   has been suggested as a key element of consciousness  here we investigate whether the presence of an inner eye like process  monitor  that supervises the activities of a number of subsystems  operative agents  engaged in the solution of a problem can improve the problem solving efficiency of the system  the problem is to find the global maximum of a nk fitness landscape and the performance is measured by the time required to find that maximum  the operative agents explore blindly the fitness landscape and the monitor provides them with feedback on the quality  fitness  of the proposed solutions  this feedback is then used by the operative agents to bias their searches towards the fittest regions of the landscape  we find that a weak feedback between the monitor and the operative agents improves the performance of the system  regardless of the difficulty of the problem  which is gauged by the number of local maxima in the landscape  for easy problems  i e   landscapes without local maxima   the performance improves monotonically as the feedback strength increases  but for difficult problems  there is an optimal value of the feedback strength beyond which the system performance degrades very rapidly   c  2017 elsevier b v  all rights reserved  
 bank distress in the news  describing events through deep learning while many models are purposed for detecting the occurrence of significant events in financial systems  the task of providing qualitative detail on the developments is not usually as well automated  we present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events  supervised by only a  small set of event information  comprising entity names and dates  the model is leveraged by unsupervised learning of semantic vector representations on extensive text data  we demonstrate applicability to the study of financial risk based on news  6 6m articles   particularly bank distress and government interventions  243 events   where indices can signal the level of bank stress related reporting at the entity level  or aggregated at national or european level  while being coupled with explanations  thus  we exemplify how text  as timely  widely available and descriptive data  can serve as a useful complementary source of information for financial and systemic risk analytics   c  2017 elsevier b v  all rights reserved  
 bankruptcy forecasting using case based reasoning  the creperie approach bankruptcy prediction is a very important research trend  although statistical methods are mainly used in literature  techniques based on artificial intelligence are interesting from many points of view  among them  case based reasoning  cbr  could be useful to cluster enterprises according to opportune similarity metrics as well as suggest proper actions to take for avoiding bankruptcy in border line situations  in this paper  we present a new and still under development cbr approach to this problem  that seems to return better results than previous attempts  the approach is based on different kinds of similarity metrics and is focused on the implementation of innovative revise algorithms  in particular  the paper shows how the revise step is crucial to improve the accuracy of the bankruptcy prediction model   c  2016 elsevier ltd  all rights reserved  
 bankruptcy prediction for smes using relational data bankruptcy prediction has been a popular and challenging research area for decades  most prediction models are built using financial figures  stock market data and firm specific variables  we complement such traditional low dimensional data with high dimensional data on the company s directors and managers in the prediction models  this information is used to build a network between small and medium sized enterprises  smes   where two companies are related if they share a director or high level manager  a smoothed version of the weighted vote relational neighbour classifier is applied on the network and transforms the relationships between companies into bankruptcy prediction scores  thereby assuming that a company is more likely to file for bankruptcy if one of the related companies in its network has already failed  an ensemble model is built that combines the relational model s output scores with structured data and is applied on two data sets of belgian and uk smes  we find that the relational model gives improved predictions over a simple financial model when detecting the riskiest firms  the largest performance increase is found when the relational and financial data are combined  confirming the complementary nature of both data types   c  2017 elsevier b v  all rights reserved  
 baum 1  a spontaneous audio visual face database of affective and mental states in affective computing applications  access to labeled spontaneous affective data is essential for testing the designed algorithms under naturalistic and challenging conditions  most databases available today are acted or do not contain audio data  we present a spontaneous audio visual affective face database of affective and mental states  the video clips in the database are obtained by recording the subjects from the frontal view using a stereo camera and from the half profile view using a mono camera  the subjects are first shown a sequence of images and short video clips  which are not only meticulously fashioned but also timed to evoke a set of emotions and mental states  then  they express their ideas and feelings about the images and video clips they have watched in an unscripted and unguided way in turkish  the target emotions  include the six basic ones  happiness  anger  sadness  disgust  fear  surprise  as well as boredom and contempt  we also target several mental states  which are unsure  including confused  undecided   thinking  concentrating  and bothered  baseline experimental results on the baum 1 database show that recognition of affective and mental states under naturalistic conditions is quite challenging  the database is expected to enable further research on audio visual affect and mental state recognition under close to real scenarios  
 bayesian learning of dynamic multilayer networks a plethora of networks is being collected in a growing number of fields  including disease transmission  international relations  social interactions  and others  as data streams continue to grow  the complexity associated with these highly multidimensional connectivity data presents novel challenges  in this paper  we focus on the time varying interconnections among a set of actors in multiple contexts  called layers  current literature lacks flexible statistical models for dynamic multilayer networks  which can enhance quality in inference and prediction by efficiently borrowing information within each network  across time  and between layers  motivated by this gap  we develop a bayesian nonparametric model leveraging latent space representations  our formulation characterizes the edge probabilities as a function of shared and layer specific actors positions in a latent space  with these positions changing in time via gaussian processes  this representation facilitates dimensionality reduction and incorporates different sources of information in the observed data  in addition  we obtain tractable procedures for posterior computation  inference  and prediction  we provide theoretical results on the flexibility of our model  our methods are tested on simulations and infection studies monitoring dynamic face to face contacts among individuals in multiple days  where we perform better than current methods in inference and prediction  
 beautiful and damned  combined effect of content quality and social ties on user engagement user participation in online communities is driven by the intertwinement of the social network structure with the crowd generated content that flows along its links  these aspects are rarely explored jointly and at scale  by looking at how users generate and access pictures of varying beauty on flickr  we investigate how the production of quality impacts the dynamics of online social systems  we develop a deep learning computer vision model to score images according to their aesthetic value and we validate its output through crowdsourcing  by applying it to over 15 b flickr photos  we study for the first time how image beauty is distributed over a large scale social system  beautiful images are evenly distributed in the network  although only a small core of people get social recognition for them  to study the impact of exposure to quality on user engagement  we set up matching experiments aimed at detecting causality from observational data  exposure to beauty is double edged  following people who produce high quality content increases one s probability of uploading better photos  however  an excessive imbalance between the quality generated by a user and the user s neighbors leads to a decline in engagement  our analysis has practical implications for improving link recommender systems  
 behavioral cues help predict impact of advertising on future sales advertising aims to influence consumer preferences  appraisals  action tendencies  and behavior in order to increase sales  these are all components of emotion  in the past  they have been measured through self report or panel discussions  while informative  these approaches are difficult to scale to large numbers of consumers  fail to capture moment to moment changes in appraisals that may be predictive of sales  and depend on verbal mediation  we used web cam technology to sample non verbal responses to television commercials from four product categories in six different countries  for each participant  head pose  head motion  and more frequent facial expressions like smiling  surprise and disgust were automatically measured at each video frame and aggregated across subjects  dynamic features from the aggregated series were input to simple linear ensemble classifier with 10 fold cross validation to predict product sales  sales were predicted with roc auc   0 75  95  ci  0 727 0 773  and predictions for unseen categories were consistent for all  but one product groups  roc auc varies between 0 74 and 0 83  except for confections with 0 61   predictions for unseen countries showed similar pattern  roc auc varied between 0 71 and 0 89  with the exception of russia with roc auc 0 53  in comparison with previous attempts  our approach yielded higher overall performance and greater generalization over not modeled factors like country or category  these findings support the feasibility  efficiency  and predictive validity of sales predictions from large scale sampling of viewers  moment to moment responses to commercial media   c  2017 elsevier b v  all rights reserved  
 behavioral rationality as a foundation for public policy studies cognitive processing applies to a single human being  yet most key social processes are organized in collectives  and connecting them is not a simple task  i review the basic ways in which social and behavioral scientists have tried to link the individual with the system  illustrating the progress that has been made in this endeavor  we are closer than we ever have been to producing a behavioral model that integrates cognitive science yet does not produce a confusing overabundance of findings that have systems level implications  i conclude that we may well be at a tuning point in which the fully rational model of human choice  currently used as the microfoundation for economics and the study of political institutions  is replaced by a more robust behavioral model of choice that relies on developments in the cognitive sciences   c  2017 published by elsevier b v  
 beyond bodily anticipation  internal simulations in social interaction there is a long history of implementing internal simulation mechanisms in robotics  typically for the purpose of predicting the outcomes of motor commands before executing them  in the literature on human cognition  however  the relevance of such mechanisms goes beyond that of prediction  they also provide foundational aspects of social cognition and interaction  in this paper  we present a review of internal simulation mechanisms from this perspective  we contrast the roles they play in human cognition  in particular in the context of social interaction  with robotic implementations  we further discuss work in social robotics  emphasising in particular that a substantial effort currently goes into evaluating social robot systems  but that social robots to date are still limited in their abilities  we further discuss episodic simulations  which are functionally distinct from the type of internal simulations we consider here  and note their role in prospective thought in particular  overall  we conclude that one of the necessary next steps on the road to social robots may be to develop social abilities from the bottom up using internal simulations  by reviewing how these aspects all tie together in human cognition  we hope to clarify how this may be achieved   c  2016 elsevier b v  all rights reserved  
 beyond social graphs  mining patterns underlying social interactions this work aims at discovering and extracting relevant patterns underlying social interactions  to do so  some knowledge extracted from facebook  a social networking site  is formalised by means of an extended social graph  a data structure which goes beyond the original concept of a social graph by also incorporating information on interests  when the extended social graph is built  state of the art techniques are applied over it in order to discover communities  once these social communities are found  statistical techniques will look for relevant patterns common to each of those  in such a way that each cluster of users is characterised by a set of common features  the resulting knowledge will be used to develop and evaluate a social recommender system  which aims at suggesting users in a social network with possible friends or interests  
 big data analytics for security and criminal investigations applications of various data analytics technologies to security and criminal investigation during the past three decades have demonstrated the inception  growth  and maturation of criminal analytics  we first identify five cutting edge data mining technologies such as link analysis  intelligent agents  text mining  neural networks  and machine learning  then  we explore their recent applications to the criminal analytics domain  and discuss the challenges arising from these innovative applications  we also extend our study to big data analytics which provides some state of the art technologies to reshape criminal investigations  in this paper  we review the recent literature  and examine the potentials of big data analytics for security intelligence under a criminal analytics framework  we examine some common data sources  analytics methods  and applications related to two important aspects of social network analysis namely  structural analysis and positional analysis that lay the foundation of criminal analytics  another contribution of this paper is that we also advocate a novel criminal analytics methodology that is underpinned by big data analytics  we discuss the merits and challenges of applying big data analytics to the criminal analytics domain  finally  we highlight the future research directions of big data analytics enhanced criminal investigations   c  2017 john wiley   sons  ltd 
 big data technologies and management  what conceptual modeling can do the era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision making and knowledge discovery activities  in this paper  the five vs of big data  volume  velocity  variety  veracity  and value  are reviewed  as well as new technologies  including nosql databases that have emerged to accommodate the needs of big data initiatives  the role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data  
 biological autonomy  a philosophical and theoretical enquiry this paper discusses moreno and mossio s book biological autonomy  a philosophical and theoretical enquiry  the book provides an up to date overview of the authors  work within the organizational approach to mind and life  which is linked to the work of maturana and varela but which is here developed in new ways and with a strong focus on the autonomy of living systems  after an overview of the book  the paper focuses on the choice of the guiding concepts for this enterprise   autonomy  agency  organism and cognition   and discusses whether these notions are still up to the task of formulating the key issues to be targeted by the organizational approach  
 biotam  a technology acceptance model for biometric authentication systems the increasing demand on biometric authentication systems  bass  has brought the need of secure and privacy preserving solutions accepted by a wider community of users  the decision makers pay a great attention to how people react to bass and their opinions about the features and procedures of the system  in this work  a generic biometric technology acceptance model  biotam  is proposed  biotam encounters trust as an objective measure of privacy security tradeoff  public willingness and user confidence  biotam takes into account social and human factors which prominently affect the wider dissemination and easy penetration of bass  to scrutinise people s behavioural intention to use a bas  biotam melds traditional technology acceptance model constructs and the trust model offered as a new construct  in order to inspire stakeholders on how biotam can be used to assess a bas  a sample case study is investigated  
 blood inventory routing problem under uncertainty the problem of delivering blood products from community blood centers to the demand points including hospital blood banks falls within the context of perishable inventory routing problems  pirp   this is due to the fact that the delivery should be made on the right time with the right delivery quantity at the right place such that the total possible perished items as well as routing and inventory costs are minimized  however  some unique characteristics of blood logistics including assigned and unassigned inventories  crossmatch release period  transfusion to crossmatch ratio and older first policy have made the problem more difficult than the routine pirps and thus proposing a new modeling of the problem is required  in this paper  we first develop a mixed integer programming formulation for blood inventory routing problem  then  to cope with uncertainties  a novel robust possibilistic programming  rpp  approach is proposed  afterward  a novel iterative branch and cut is developed to solve a number of numerical examples to optimality  finally  by implementing a test scenario on the data inspired from a real iranian blood supply chain  the significance and applicability of the proposed model and rpp approach is proven  
 bonferroni means with distance measures and the adequacy coefficient in entrepreneurial group theory the aim of the paper is to develop new aggregation operators using bonferroni means  owa operators and some distance measure  we introduce the bon owaac and bon owaimam operators  we are able to include coefficient adequacy and the maximum and minimum levels in the same formulation with bonferroni means and an owa operator  the main advantages of using these operators are that they allow consideration of continuous aggregations  multiple comparisons between each argument and distance measures in the same formulation  an application is developed using these new algorithms in combination with moore s families and galois lattices to solve group decision making problems  the professional and personal interests of the entrepreneurs who share co working spaces are taken as an example for establishing relationships and groups  according to the professional and personal profile affinities for each entrepreneur  the results show dissimilarity and fuzzy relationships and the maximum similarity sub relations to establish relationships and groups using moore s families and galois lattice  finally  this new type of distance family can be used for applications in areas such as sports teams  strategy marketing and teamwork   c  2016 elsevier b v  all rights reserved  
 boosting hankel matrices for face emotion recognition and pain detection studies in psychology have shown that the dynamics of emotional expressions play an important role in face emotion recognition in humans  motivated by these studies  in this paper the dynamics of face expressions are modeled and used for automatic emotion recognition and pain detection  given a temporal sequence of face images  several appearance based descriptors are computed at each frame  over the sequence  the descriptors corresponding to the same feature type and spatial scale define a time series  the hankel matrix built upon each time series is used to represent the dynamics of face expressions with respect to the used feature scale pair  the set of hankel matrices obtained by varying the feature type and the scale is used within a boosting approach to train a strong classifier  during training  random subspace projection is adopted for feature and scale selection  experiments on two challenging publicly available datasets show that the dynamics of appearance based face expression representations can be used to discriminate among different emotion classes and  within a boosting approach  attain state of the art average accuracy values in classification   c  2016 elsevier inc  all rights reserved  
 brainstorm  a psychosocial game suite design for non invasive cross generational cognitive capabilities data collection currently available traditional as well as videogame based cognitive assessment techniques are inappropriate due to several reasons  this paper presents a novel psychosocial game suite  brainstorm  for non invasive cross generational cognitive capabilities data collection  which additionally provides cross generational social support  a motivation behind the development of presented game suite is to provide an entertaining and exciting platform for its target users in order to collect gameplay based cognitive capabilities data in a non invasive manner  an extensive evaluation of the presented game suite demonstrated high acceptability and attraction for its target users  besides  the data collection process is successfully reported as transparent and non invasive  
 building multi domain conversational systems from single domain resources current advances in the development of mobile and smart devices have generated a growing demand for natural human machine interaction and favored the intelligent assistant metaphor  in which a single interface gives access to a wide range of functionalities and services  conversational systems constitute an important enabling technology in this paradigm  however  they are usually defined to interact in semantic restricted domains in which users are offered a limited number of options and functionalities  the design of multi domain systems implies that a single conversational system is able to assist the user in a variety of tasks  in this paper we propose an architecture for the development of multi domain conversational systems that allows   1  integrating available multi and single domain speech recognition and understanding modules   2  combining available system in the different domains implied so that it is not necessary to generate new expensive resources for the multi domain system   3  achieving better domain recognition rates to select the appropriate interaction management strategies  we have evaluated our proposal combining three systems in different domains to show that the proposed architecture can satisfactory deal with multi domain dialogs   c  2017 elsevier b v  all rights reserved  
 building semantic kernels for cross document knowledge discovery using wikipedia research into text mining has progressed over the past decade  one of the main challenges now is gauging the difficulty of taking advantage of outside knowledge in the discovery process  in this work  to address the limitations of the traditional bag of words model and expand the search scope beyond the document collections at hand  we present a new text mining approach incorporating wikipedia as the background knowledge  various semantic kernels are built out of the extensive knowledge derived from wikipedia and applied to the search scenario of detecting potential semantic relationships between topics  we demonstrate the effectiveness of our approach through comparing with competitive baselines  as well as alternative solutions where only part of wikipedia resources  e g   the wiki article contents or the associated wiki categories  is considered  
 bump competition and lattice solutions in two dimensional neural fields some forms of competition among activity bumps in a two dimensional neural field are studied  first  threshold dynamics is included and rivalry evolutions are considered  the relations between parameters and dominance durations can match experimental observations about ageing  next  the threshold dynamics is omitted from the model and we focus on the properties of the steady state  from noisy inputs  hexagonal grids are formed by a symmetry breaking process  particular issues about solution existence and stability conditions are considered  we speculate that they affect the possibility of producing basis grids which may be combined to form feature maps   c  2017 elsevier ltd  all rights reserved  
 business failure prediction based on two stage selective ensemble with manifold learning algorithm and kernel based fuzzy self organizing map more and more models and algorithms are used to predict business failure  many of them are not suitable for the complicated distribution of financial data  which leads to unsatisfactory prediction performance  the manifold learning algorithm is a valid method to preprocess financial data because of the good performance on dimensionality reduction for any data distribution  the kernel based method is introduced to improve the disadvantage of fuzzy self organizing map  fsom  which is the limitation of spherical data distribution  therefore  this study adopts manifold learning algorithm to select feature subsets  and employs the kernel based fsom  kfsom  to compose base classifiers  and proposes the two stage selective ensemble model for business failure prediction  bfp   first  three manifold learning algorithms  which are isomap  laplacian eigenmaps and locally linear embedding  are adopted to select three feature subsets from original financial data  then  kfsom uses three kinds of kernel functions respectively  which are gaussian  polynomial and sigmoid  to obtain three classifiers  hence  three feature subsets are computed by kfsoms with three kernel functions respectively to acquire nine base classifiers  last  nine base classifiers are integrated by the two stage selective ensemble method  in the first stage  nine base classifiers are ranked according to three standards  the stepwise forward selection approach is adopted to selectively integrate nine base classifiers according to different standards  in the second stage  three selective ensembles in the first stage are integrated again to acquire the final result  in the empirical research  this work employs financial data from chinese listed companies to predict business failure  and makes comparative analysis with previous methods  it is the conclusion that the two stage selective ensemble with manifold learning algorithm and kfsom is good at predicting business failure   c  2017 elsevier b v  all rights reserved  
 can bounded and self interested agents be teammates  application to planning in ad hoc teams planning for ad hoc teamwork is challenging because it involves agents collaborating without any prior coordination or communication  the focus is on principled methods for a single agent to cooperate with others  this motivates investigating the ad hoc teamwork problem in the context of self interested decision making frameworks  agents engaged in individual decision making in multiagent settings face the task of having to reason about other agents  actions  which may in turn involve reasoning about others  an established approximation that operationalizes this approach is to bound the infinite nesting from below by introducing level 0 models  for the purposes of this study  individual  self interested decision making in multiagent settings is modeled using interactive dynamic influence diagrams  i did   these are graphical models with the benefit that they naturally offer a factored representation of the problem  allowing agents to ascribe dynamic models to others and reason about them  we demonstrate that an implication of bounded  finitely nested reasoning by a self interested agent is that we may not obtain optimal team solutions in cooperative settings  if it is part of a team  we address this limitation by including models at level 0 whose solutions involve reinforcement learning  we show how the learning is integrated into planning in the context of i dids  this facilitates optimal teammate behavior  and we demonstrate its applicability to ad hoc teamwork on several problem domains and configurations  
 can competitive advantage be achieved through knowledge management  a case study on smes unlike most knowledge management  km  studies which focus on large enterprises  this study focuses on smes in malaysia which represent 99 2  of the total business establishments  the largest percentage of establishments in the country  the tridimensional relationship between km practices  technological innovation  ti  and competitive advantage  ca  was examined in this case study  survey approach was conducted to gather data from managers of the manufacturing smes and 195 samples were usable for statistical analysis using partial least square structural equation modeling  pls sem  artificial neural network  ann   the use of the combined pls sem and ann analysis can provide a significant methodological contribution and substantial impacts to the world of expert and intelligent systems and could be the next methodological research paradigm  findings validated that km has a direct positive and significant relation with both ti and ca  while ti positively and significantly affects ca  most outstandingly  the mediating role of ti that connects km and ca has been proven to be positive and significant  this paper utilizes samples that were collected from malaysian smes only  therefore the findings cannot be generalized to represent the larger firms  nevertheless  conclusions garnered from the present research can help both practitioners of the manufacturing smes and scholars in implementing the correct km strategies  so that both ti and ca can be enhanced and improved   c  2016 elsevier ltd  all rights reserved  
 can digital games help us identify our skills to manage abstractions  in this work we present our progress in the field of intelligent user profiling  our objective is to build a user profile that captures users  skills rather than classical users  interests  thus  we propose a novel approach to learn users  skills by observing their behavior during a very common activity  playing games  specifically  we automatically identify users  skills to manage abstractions by using digital games  abstraction skills identification is important because it is related to several behavioral tendencies such as career preferences  aptitudes  and learning styles  traditional skills identification is based on questionnaires whose application implies many complications  including non intentional influences in the way questions are formulated  difficulty to motivate people to fill them out  and lack of awareness of the consequences or future uses of questionnaires  to address these limitations  we built a user profile that collects users  actions when playing digital games  then  we built and trained a hierarchical naive bayes network to infer users  skills to manage abstractions  the experiments carried out show that digital games can help us to identify abstraction skills with a promising accuracy  
 can machine translation systems be evaluated by the crowd alone crowd sourced assessments of machine translation quality allow evaluations to be carried out cheaply and on a large scale  it is essential  however  that the crowd s work be filtered to avoid contamination of results through the inclusion of false assessments  one method is to filter via agreement with experts  but even amongst experts agreement levels may not be high  in this paper  we present a new methodology for crowd sourcing human assessments of translation quality  which allows individual workers to develop their own individual assessment strategy  agreement with experts is no longer required  and a worker is deemed reliable if they are consistent relative to their own previous work  individual translations are assessed in isolation from all others in the form of direct estimates of translation quality  this allows more meaningful statistics to be computed for systems and enables significance to be determined on smaller sets of assessments  we demonstrate the methodology s feasibility in large scale human evaluation through replication of the human evaluation component of workshop on statistical machine translation shared translation task for two language pairs  spanish to english and english to spanish  results for measurement based solely on crowd sourced assessments show system rankings in line with those of the original evaluation  comparison of results produced by the relative preference approach and the direct estimate method described here demonstrate that the direct estimate method has a substantially increased ability to identify significant differences between translation systems  
 capturing emotion reactivity through physiology measurement as a foundation for affective engineering in engineering design science and engineering practices this paper presents the theoretical and practical fundamentals of using physiology sensors to capture human emotion reactivity in a products or systems engineering context  we aim to underline the complexity of regulating  internal and external  effects on the human body and highly individual physiological  emotion  responses and provide a starting point for engineering researchers entering the field  although great advances have been made in scenarios involving human machine interactions  the critical elements the actions and responses of the human remain far beyond automatic control  because of the irrational behavior of human subjects  these  re actions  which cannot be satisfactorily modeled  stem mostly from the fact that human behavior is regulated by emotions  the physiological measurement of the latter can thus be a potential door to future advances for the community  in this paper  following a brief overview of the foundations and ongoing discussions in psychology and neuroscience  various emotion related physiological responses are explained on the basis of a systematic review of the autonomic nervous system and its regulation of the human body  based on sympathetic and parasympathetic nervous system responses  various sensor measurements that are relevant in an engineering context  such as electrocardiography  electroencephalography  electromyography  pulse oximetry  blood pressure measurements  respiratory transducer  body temperature measurements  galvanic skin response measurements  and others  are explained  after providing an overview of ongoing engineering and human computer interaction projects  we discuss engineering specific challenges and experiment setups in terms of their usability and appropriateness for data analysis  we identify current limitations associated with the use of physiology sensors and discuss developments in this area  such as software based facial affect coding and near infrared spectroscopy  the key to truly understanding user experience and designing systems and products that integrate emotional states dynamically lies in understanding and measuring physiology  this paper serves as a call for the advancement of affective engineering research  
 cascaded re ranking modelling of translation hypotheses using extreme learning machines in statistical machine translation  smt   re ranking of huge amount of randomly generated translation hypotheses is one of the essential components in determining the quality of translation result  in this work  a novel re ranking modelling framework called cascaded re ranking modelling  crm  is proposed by cascading a classification model and a regression model  the proposed crm effectively and efficiently selects the good but rare hypotheses in order to alleviate simultaneously the issues of translation quality and computational cost  crm can be partnered with any classifier such as support vector machines  svm  and extreme learning machine  elm   compared to other state of the art methods  experimental results show that crm partnered with elm  crm elm  can raise at most 11 6  of translation quality over the popular benchmark chinese english corpus  iwslt 2014  and french english parallel corpus  wmt 2015  with extremely fast training time for huge corpus   c  2017 elsevier b v  all rights reserved  
 causality on cross sectional data  stable specification search in constrained structural equation modeling causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms  generally discovery algorithms can be divided into two approaches  constraint based and score based  the constraint based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable  the score based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships  but it is unable to detect common confounders of the observed variables  a drawback of both score based and constrained based approaches is the inherent instability in structure estimation  with finite samples small changes in the data can lead to completely different optimal structures  the present work introduces a new hypothesis free score based causal discovery algorithm  called stable specification search  that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms  structure search is performed over structural equation models  our approach uses exploratory search but allows incorporation of prior background knowledge  we validated our approach on one simulated data set  which we compare to the known ground truth  and two real world data sets for chronic fatigue syndrome and attention deficit hyperactivity disorder  which we compare to earlier medical studies  the results on the simulated data set show significant improvement over alternative approaches and the results on the real word data sets show consistency with the hypothesis driven models constructed by medical experts   c  2016 elsevier b v  all rights reserved  
 certifiable trust in autonomous systems  making the intractable tangible this article discusses verification and validation  v v  of autonomous systems  a concept that will prove to be difficult for systems that were designed to execute decision initiative  v v of such systems should include evaluations of the trustworthiness of the system based on transparency inputs and scenario based training  transparency facets should be used to establish shared awareness and shared intent among the designer  tester  and user of the system  the transparency facets will allow the human to understand the goals  social intent  contextual awareness  task limitations  analytical underpinnings  and team based orientation of the system in an attempt to verify its trustworthiness  scenario based training can then be used to validate that programming in a variety of situations that test the behavioral repertoire of the system  this novel method should be used to analyze behavioral adherence to a set of governing principles coded into the system  
 challenges of smart business process management  an introduction to the special issue this paper describes the foundations of smart business process management and serves as an editorial to the corresponding special issue  to this end  we introduce a framework that distinguishes three levels of business process management  multiprocess management  process model management  and process instance management  for each of these levels we identify major contributions of prior research and describe in how far papers assembled in this special issue extend our understanding of smart business process management   c  2017 elsevier b v  all rights reserved  
 chaotic analysis of embodied and situated agents embodied and situated view of cognition is a transdisciplinary framework which stresses the importance of real time and dynamical interaction of an agent with the surrounding environment  this article presents a series of evolutionary robotics experiments that operationalize such concept  training miniature two wheeled mobile robots to autonomously solve a temporal task  in order to provide a numerical description of the robots  behavior  chaotic measures are estimated on the attractor reconstructed from the recorded positions of the agent  chaos theory provides a rigorous mathematical framework consistent with an antireductionist approach  useful for understanding embodied and situated systems while avoiding a decomposition of the integrated system brain body environment  time series are analyzed in detail using nonlinear mathematical tools in order to verify the presence of low dimensional deterministic dynamical systems  a fundamental prerequisite for chaos theory  in particular  the recorded time series are evaluated with nonlinear prediction error to unveil deterministic dynamics  cross prediction error to determine the stationarity of the signal  and surrogate data testing to verify the existence of nonlinear components in the underlying system  estimators for quantifying level of chaos and fractal dimension are applied to suitable datasets  results show that robots governed by a chaotic dynamic are more efficient at adapting to environments never experience during evolution  demonstrating robustness towards novel and unpredictable situations  furthermore  chaotic measures  in particular fractal dimension  are correlated with the performance if robots exhibit a similar behavioral strategy   c  2017 elsevier b v  all rights reserved  
 characterisation of voice quality of parkinson s disease using differential phonological posterior features change in voice quality  vq  is one of the first precursors of parkinson s disease pd   specifically  impacted phonation and articulation causes the patient to have a breathy  husky semiwhisper and hoarse voice  a goal of this paper is to characterise a vq spectrum the composition of non modal phonations   of voiceinpd  the paper relates non modal healthy phonations  breathy  creaky  tense  falsetto and harsh  with disordered phonation in pd  first  statistics are learned to differentiate the modal and non modal phonations  statistics are compute dusing phonological posteriors  the probabilities of phonological features inferred from the speech signal using a deep learning approach  second  statistics of disordered speech are learned from pd speech data comprising 50 patients and 50 healthy controls   third  euclidean distance is used to calculate similarity of non modal and disordered statistics  and the inverse of the distances is used to obtain the composition of non modal phonation in pd  thus  pathological voice quality is characterised using healthy non modal voice quality  base eigenspace   the obtained results are interpreted as the voice of an average patient with pd and can be characterised by the voice quality spectrum composed of 30  breathy voice  23  creaky voice  20  tense voice  15  falsetto voice and 12  harsh voice  in addition  the proposed features were applied for prediction of the dysarthria level according to the frenchay assessment score related to the larynx  and significant improvement is obtained for reading speech task  the proposed characterisation of vq might also be applied to other kinds of pathological speech   c  2017 elsevier ltd  allrights reserved  
 characterizing attentive behavior in intelligent environments learning styles are strongly connected with learning and when it comes to acquiring new knowledge  attention is one the most important mechanisms  the learner s attention affects learning results and can define the success or failure of a student  when students are carrying out learning activities using new technologies  it is extremely important that the teacher has some feedback from the students  work in order to detect potential learning problems at an early stage and then to choose the appropriate teaching methods  in this paper we present a nonintrusive distributed system for monitoring the attention level in students  it is especially suited for classes working at the computer  the presented system is able to provide real time information about each student as well as information about the class  and make predictions about the best learning style for a student using an ensemble of neural networks  it can be very useful for teachers to identify potentially distracting events and this system might be very useful to the teacher to implement more suited teaching strategies   c  2017 published by elsevier b v  
 cheavd  a chinese natural emotional audio visual database this paper presents a recently collected natural  multimodal  rich annotated emotion database  casia chinese natural emotional audio visual database  cheavd   which aims to provide a basic resource for the research on multimodal multimedia interaction  this corpus contains 140 min emotional segments extracted from films  tv plays and talk shows  238 speakers  aging from child to elderly  constitute broad coverage of speaker diversity  which makes this database a valuable addition to the existing emotional databases  in total  26 non prototypical emotional states  including the basic six  are labeled by four native speakers  in contrast to other existing emotional databases  we provide multi emotion labels and fake suppressed emotion labels  to our best knowledge  this database is the first large scale chinese natural emotion corpus dealing with multimodal and natural emotion  and free to research use  automatic emotion recognition with long short term memory recurrent neural networks  lstm rnn  is performed on this corpus  experiments show that an average accuracy of 56   could be achieved on six major emotion states  
 chemical reaction optimization with unified tabu search for the vehicle routing problem this study proposes a new approach combining the chemical reaction optimization framework with the unified tabu search  uts  heuristic to solve the capacitated vehicle routing problem  cvrp   the cvrp is one of the most well studied problems not only because of its real life applications but also due to the fact that the cvrp could be used to evaluate the efficiency of new algorithms and optimization methods  chemical reaction optimization  cro  is a new optimization framework mimicking the nature of chemical reactions  the cro method has proved to be very effective for solving np hard optimization problems such as the quadratic assignment problem  neural network training  the knapsack problem  and the traveling salesman problem  we also present the design of elementary chemical reaction operations  the adaptation of the uts algorithm to educate solutions in these operations  finally  a thorough testing against well known benchmark problems has been conducted  experimental results show that the proposed algorithm is efficient and highly competitive in comparison with several prominent algorithms for this problem  the presented methodology may be a fine approach for developing similar algorithms to address other routing variants  
 citation function  polarity and influence classification current methods for assessing the impact of authors and scientific media employ tools such as h index  co citation and pagerank  these tools are primarily based on citation counting  which considers all citations to be equal  this type of methods can produce perverse incentives to publish controversial or incomplete papers  as mixed or negative reviews often generate larger citation counts and better indexes  regardless of whether the citations were critical or exerted minimal influence on the citing document  passing citations that are employed to establish background  which do not have a real impact on the citing paper  are common in scientific literature  however  these citations have equal weight in impact evaluations  notable researchers have emphasized the need to correct this situation by developing estimation methods that consider the different roles of quotations in citing papers  to accomplish this type of evaluation  a context citation analysis should be applied to determine the nature of the citations  we propose that citations should be categorized using four dimensions   function  polarity  aspects and influence   as these dimensions provide adequate information that can be employed toward the generation of a qualitative method to measure the impact of a given publication in a citing paper  in this paper  we used interchangeably the words influence and impact  we present a method for obtaining this information using our proposed classification scheme and manually annotated corpus  which is marked with meaningful keywords and labels to help identify the characteristics or properties that constitute what we call aspects  we develop a classification scheme which considers purpose definition shared by previous works  our contribution is to abstract purpose classes from several other schemes and divide a complex structure in more manageable parts  to attain a simple system that combines low granularity dimensions but nevertheless produces a fine grained classification  for annotators  the classification process is simple because in a first step  the coders distinguish only four primary classes  and in a second pass  they add the information contained in aspects keyword and labels to obtain the more specific functions  this way  we gain a high granularity labeling that gives enough information about the citations to characterize and classify them  and we achieve this detailed coding with a straightforward process where the level of human error could be minimized  
 classification of movement and inhibition using a hybrid bci brain computer interfaces  bcis  are an emerging technology that are capable of turning brain electrical activity into commands for an external device  motor imagery  mi  when a person imagines a motion without executing it is widely employed in bci devices for motor control because of the endogenous origin of its neural control mechanisms  and the similarity in brain activation to actual movements  challenges with translating a mi bci into a practical device used outside laboratories include the extensive training required  often due to poor user engagement and visual feedback response delays  poor user flexibility freedom to time the execution inhibition of their movements  and to control the movement type  right arm vs  left leg  and characteristics  reaching vs  grabbing   and high false positive rates of motion control  solutions to improve sensorimotor activation and user performance of mi bcis have been explored  virtual reality  vr  motor execution tasks have replaced simpler visual feedback  smiling faces  arrows  and have solved this problem to an extent  hybrid bcis  hbcis  implementing an additional control signal to mi have improved user control capabilities to a limited extent  these hbcis either fail to allow the patients to gain asynchronous control of their movements  or have a high false positive rate  we propose an immersive vr environment which provides visual feedback that is both engaging and immediate  but also uniquely engages a different cognitive process in the patient that generates event related potentials  erps   these erps provide a key executive function for the users to execute inhibit movements  additionally  we propose signal processing strategies and machine learning algorithms to move bcis toward developing long term signal stability in patients with distinctive brain signals and capabilities to control motor signals  the hbci itself and the vr environment we propose would help to move bci technology outside laboratory environments for motor rehabilitation in hospitals  and potentially for controlling a prosthetic  
 classifying a person s degree of accessibility from natural body language during social human robot interactions for social robots to be successfully integrated and accepted within society  they need to be able to interpret human social cues that are displayed through natural modes of communication  in particular  a key challenge in the design of social robots is developing the robot s ability to recognize a person s affective states  emotions  moods  and attitudes  in order to respond appropriately during social human robot interactions  hris   in this paper  we present and discuss social hri experiments we have conducted to investigate the development of an accessibility aware social robot able to autonomously determine a person s degree of accessibility  rapport  openness  toward the robot based on the person s natural static body language  in particular  we present two one on one hri experiments to  1  determine the performance of our automated system in being able to recognize and classify a person s accessibility levels and 2  investigate how people interact with an accessibility aware robot which determines its own behaviors based on a person s speech and accessibility levels  
 classifying news versus opinions in newspapers  linguistic features for domain independence newspaper text can be broadly divided in the classes opinion   editorials  commentary  letters to the editor  and neutral   reports   we describe a classification system for performing this separation  which uses a set of linguistically motivated features  working with various english newspaper corpora  we demonstrate that it significantly outperforms bag of lemma and pos tag models  we conclude that the linguistic features constitute the best method for achieving robustness against change of newspaper or domain  
 cluster analysis application for understanding sme manufacturing strategies small and medium size enterprises   sme  manufacturing strategy configurations are identified in a small developed economy with the aim to explore how sme manufacturing strategy configurations affect business stability and performance during a period of macroeconomic shock  drawing on a survey based dataset  our two step cluster analysis results suggest that three distinctive manufacturing strategy configurations can be observed among the smes of the finnish manufacturing sector  namely  responsive niche innovators  subcontractors  and engineer servers  furthermore  we are able to establish a link between the strategy configurations and business stability and performance  the results support conclusions that the nature of manufacturing strategy taxonomies are driven by the business context  and that volume flexibility  design flexibility and service provision capabilities enable better business outcomes during macroeconomic shocks  in comparison to the more easily achievable conformance quality as well as delivery speed and dependability  in light of this research  best performing cluster under the macroeconomic shock is the engineer servers  emphasizing flexibility oriented broad product line and after sales service  while having less priority concerning low price and volume flexibility  the results offer important insights for managers  but also for other stakeholders in the form of for example expert systems development for sme funding decisions  crown copyright  c  2016 published by elsevier ltd  all rights reserved  
 cluster based approach to discriminate the user s state whether a user is embarrassed or thinking to an answer to a prompt spoken dialog systems are employed in various devices to help users operate them  an advantage of a spoken dialog system is that the user can make input utterances freely  but the system sometimes makes it difficult for the user to speak to it  the system should estimate the state of a user who encounters a problem when starting a dialog and then give appropriate help before the user abandons the dialog  based on this assumption  our research aims to construct a system which responds to a user who does not reply to the system  in this paper  we propose a method of discriminating the user s state based on vector quantization of non verbal information such as prosodic features  facial feature points  and gaze  the experimental results showed that the proposed method outperforms the conventional approaches and achieves a discrimination ratio of 72 0   then  we examined sequential discrimination for responding to the user at an appropriate timing  the results indicate that the discrimination ratio reached equal to the end of the session at around 6 0 s  
 cluster based hierarchical demand forecasting for perishable goods demand forecasting is of particular importance for retailers in the context of supply chains of perishable goods and fresh food  such goods are daily produced and delivered as they need to be provided as fresh as possible and quickly deteriorate  demand underestimation and overestimation negatively affect the revenues of the retailer  stock outs have an undesired impact on consumers while unsold items need to be discarded at the end of the day  we propose a dss that supports day to operations by providing hierarchical forecasts at different organizational levels based on most recent point of sales data  it identifies article clusters that are used to extend the hierarchy based on intra day sales pattern  we apply multivariate arima models to forecast the daily demand to support operational decisions  we evaluate the approach with point of sales data of an industrialized bakery chain and show that it is possible to increase the availability while limiting the loss at the same time  the cluster analysis reveals that substitutable items have similar intra day sales pattern which makes it reasonable to forecast the demand at an aggregated level  the accuracy of top down forecasts is comparable to direct forecasts which allows reducing the computational costs   c  2017 elsevier ltd  all rights reserved  
 clustering colors regier  kay  and khetarpal report the results of computer simulations that cluster color stimuli on the basis of their coordinates in cielab space  one of two commonly used perceptual color spaces  regier and coauthors find partitions of those stimuli that are strikingly similar to the way actual color lexicons partition color space  they do not argue for the custom made clustering method used in their simulations  nor for the assumption of cielab space  the present paper aims to answer the question to what extent their computational results depend on these assumptions  it does this by applying a great variety of known clustering methods to regier et al  s stimuli  and by assuming not only cielab space but also cieluv space  the other main color space   c  2017 elsevier b v  all rights reserved  
 clustering nominal data using unsupervised binary decision trees  comparisons with the state of the art methods in this work  we propose an extension of cubt  clustering using unsupervised binary trees  to nominal data  for this purpose  we primarily use heterogeneity criteria and dissimilarity measures based on mutual information  entropy and hamming distance  we show that for this type of data  cubt outperforms most of the existing methods  we also provide and justify some guidelines and heuristics to tune the parameters in cubt  extensive comparisons are done with other well known approaches using simulations  and two examples of real datasets applications are given   c  2017 elsevier ltd  all rights reserved  
 clustering of micro messages using similarity upper approximation microblogging platforms like twitter  tumblr and plurk have radically changed our lives  the presence of millions of people has made these platforms a preferred channel for communication  a large amount of user generated content  on these platforms  has attracted researchers and practitioners to mine and extract information nuggets  for information extraction  clustering is an important and widely used mining operation  this paper addresses the issue of clustering of micro messages and corresponding users based on the text content of micro messages that reflect their primitive interest  in this paper  we performed modification of the similarity upper approximation based clustering algorithm for clustering of micro messages  we compared the performance of the modified similarity upper approximation based clustering algorithm with state of the art clustering algorithms such as partition around medoids  hierarchical agglomerative clustering  affinity propagation clustering and dbscan  experiments were performed on micro messages collected from twitter  experimental results show the effectiveness of the proposed algorithm  
 clustering retail products based on customer behaviour the categorization of retail products is essential for the business decision making process  it is a common practice to classify products based on their quantitative and qualitative characteristics  in this paper  we use a purely data driven approach  our clustering of products is based exclusively on the customer behaviour  we propose a method for clustering retail products using market basket data  our model is formulated as an optimization problem which is solved by a genetic algorithm  it is demonstrated on simulated data how our method behaves in different settings  the application using real data from a czech drugstore company shows that our method leads to similar results in comparison with the classification by experts  the number of clusters is a parameter of our algorithm  we demonstrate that if more clusters are allowed than the original number of categories is  the method yields additional information about the structure of the product categorization   c  2017 elsevier b v  all rights reserved  
 clusterwise linear regression modeling with soft scale constraints constrained approaches to maximum likelihood estimation in the context of finite mixtures of normals have been presented in the literature  a fully data dependent soft constrained method for maximum likelihood estimation of clusterwise linear regression is proposed  which extends previous work in equivariant data driven estimation of finite mixtures of normals  the method imposes soft scale bounds based on the homoscedastic variance and a cross validated tuning parameter c  in our simulation studies and real data examples we show that the selected c will produce an output model with clusterwise linear regressions and clustering as a most suited to the data solution in between the homoscedastic and the heteroscedastic models   c  2017 elsevier inc  all rights reserved  
 co evolutionary multi task learning with predictive recurrence for multi step chaotic time series prediction multi task learning employs a shared representation of knowledge for learning several instances of the same problem  multi step time series problem is one of the most challenging problems for machine learning methods  the performance of a prediction model face challenges for higher prediction horizons due to the accumulation of errors  cooperative coevolution employs in a divide and conquer approach for training neural networks and has been very promising for single step ahead time series prediction  recently  co evolutionary multi task learning has been proposed for dynamic time series prediction  in this paper  we adapt co evolutionary multi task learning for multi step prediction where predictive recurrence is developed to feature knowledge from previous states for future prediction horizon  the goal of the paper is to present a network architecture with predictive recurrence which is capable of mult istep prediction through a form of multi task learning  we employ cooperative neuro evolution and an evolutionary algorithm as baselines for comparison  the results show that the proposed method provides the best generalization performance in most cases  comparison of results with the literature has shown to be promising which motivates further application of the approach for related real world problems   c  2017 elsevier b v  all rights reserved  
 coarse and fine identification of collusive clique in financial market collusive transactions refer to the activity whereby traders use carefully designed trade to illegally manipulate the market  they do this by increasing specific trading volumes  thus creating a false impression that a market is more active than it actually is  the traders involved in the collusive transactions are termed as collusive clique  the collusive clique and its activities can cause substantial damage to the market s integrity and attract much attention of the regulators around the world in recent years  much of the current research focused on the detection based on a number of assumptions of how a normal market behaves  there is  clearly  a lack of effective decision support tools with which to identify potential collusive clique in a real life setting  the study in this paper examined the structures of the traders in all transactions  and proposed two approaches to detect potential collusive clique with their activities  the first approach targeted on the overall collusive trend of the traders  this is particularly useful when regulators seek a general overview of how traders gather together for their transactions  the second approach accurately detected the parcel passing style collusive transactions on the market through analysing the relations of the traders and transacted volumes  the proposed two approaches  on one hand  provided a complete cover for collusive transaction identifications  which can fulfil the different types of requirements of the regulation  i e  mifid ii  on the other hand  showed a novel application of well known computational algorithms on solving real and complex financial problem  the proposed two approaches are evaluated using real financial data drawn from the nyse and cme group  experimental results suggested that those approaches successfully identified all primary collusive clique scenarios in all selected datasets and thus showed the effectiveness and stableness of the novel application   c  2016 elsevier ltd  all rights reserved  
 cognitive adaptations to criminal justice lead to  paranoid  norm obedience people often cooperate and obey norms in situations where it is clear they cannot be caught and punished  such behavior does not serve their self interest  as they are foregoing opportunities to exploit others without any negative consequences  hence  it is not clear how this behavior could have evolved  some previous explanations invoked the existence of other regarding preferences  moral motivation  or intrinsic concern for social norms  in this study  we develop an agent based model illustrating that none of these is necessary for the emergence of norm abiding behavior  our model suggests evolutionary pressure against norm violators may lead to the emergence of a bias  causing agents to be extremely sensitive to the probability of being caught  because of this  they often incorrectly classify anonymous situations as non anonymous ones and obey social norms due to the fear of being punished  in our simulations  we show that cooperation is promoted by  1  the number of interactions actually observed   2  the strength of punishments against norm violators  and most importantly   3  the uncertainty in agent classifications  
 cognitive control explains the mutual transfer between dimensional change card sorting and first order false belief understanding  a computational modeling study on transfer of skills while most 3 year olds fail both in the false belief task of theory of mind and dimensional change card sorting task of cognitive control  most 4 year olds are able to pass these tasks  different theories have been constructed to explain this co development  to investigate the direction of the developmental relationship between false belief reasoning and cognitive control  kloo and perner  2003  trained 3 year olds on the false belief task in one condition and on the dimensional change card sorting task in another condition  they found that there is a mutual transfer between the two tasks  meaning that training children with the dimensional change card sorting task with feedback significantly improved children s performance on the false belief task and vice versa  in this study  we aim to provide an explanation for the underlying mechanisms of this mutual transfer by constructing computational cognitive models  in contrast to the previous theories  our models show that the common element in the two tasks is two competing strategies  only one of which leads to a correct answer  providing children with explicit feedback trains them to use a strategy of control instead of using a simpler reactive strategy  therefore  we propose that children start to pass the false belief and cognitive control tasks once they learn to be flexible in their behavior depending on the current goal   c  2017 elsevier b v  all rights reserved  
 cognitive load and issue engagement in congressional discourse like all human actors  politicians possess limited cognitive capacity  in ordinary interactions  this limitation discourages political decision makers from addressing high dimensional policy problems unless incentivized to do so by exogenous  focusing events   public policy researchers have documented this pattern extensively  and have argued that cognitive constraints help explain the  stick slip  dynamics that characterize macro level policymaking  however  data and measurement limitations have prevented these studies from examining individual level information processing patterns  in this paper  i develop a text based approach designed to measure diversity of attention at an individual level  which i apply to an original dataset of congressional hearing transcripts surrounding the 2008 2009 financial crisis  i find that individual speakers engaged with a more diverse set of topics during the crisis than before its onset  and became more focused as the crisis subsided   c  2017 elsevier b v  all rights reserved  
 cognitive load measurement in a virtual reality based driving system for autism intervention autism spectrum disorder  asd  is a highly prevalent neurodevelopmental disorder with enormous individual and social cost  in this paper  a novel virtual reality  vr  based driving system was introduced to teach driving skills to adolescents with asd  this driving system is capable of gathering eye gaze  electroencephalography  and peripheral physiology data in addition to driving performance data  the objective of this paper is to fuse multimodal information to measure cognitive load during driving such that driving tasks can be individualized for optimal skill learning  individualization of asd intervention is an important criterion due to the spectrum nature of the disorder  twenty adolescents with asd participated in our study and the data collected were used for systematic feature extraction and classification of cognitive loads based on five well known machine learning methods  subsequently  three information fusion schemes feature level fusion  decision level fusion and hybrid level fusion were explored  results indicate that multimodal information fusion can be used to measure cognitive load with high accuracy  such a mechanism is essential since it will allow individualization of driving skill training based on cognitive load  which will facilitate acceptance of this driving system for clinical use and eventual commercialization  
 cognitive map self organization from subjective visuomotor experiences in a hierarchical recurrent neural network animals develop and use cognitive maps  which are internal models of the external environment  to understand the spatial characteristics of their natural environment  previous studies have shown that a hierarchical structure of recurrent neural networks contributes to the extraction of high level concepts in sequential sensorimotor experiences  however  the previous studies did not focus on the spatial aspects of these experiences and did not acquire cognitive maps  we modified previous models and trained the proposed model with the visuomotor experiences of an agent in a simulated two dimensional environment  the proposed model was trained to predict future visual and motion inputs even when only one modality was provided  crossmodal prediction   the proposed model correctly predicted visual images  even when the agent experienced unknown paths  comparisons of the crossmodal predictions of the models under different conditions revealed that the crossmodal predictions related to motion resulted in self organization of the cognitive map  further experiments of mental simulation abilities showed that two way crossmodal predictions  from vision and motion only  were required for consistent generation of vision and motion  these results indicated that predictive learning involving integrated vision and motion was necessary for self organization of spatial recognition with a cognitive map  
 cognitive pilot aircraft interface for single pilot operations considering the foreseen expansion of the air transportation system within the next two decades and the opportunities offered by higher levels of automation  single pilot operations  spo  are regarded as viable alternatives to conventional two pilot operations for commercial transport aircraft  in comparison with current operations  spo require higher cognitive efforts  which potentially result in increased human error rates  this article proposes a novel cognitive pilot aircraft interface  cpai  concept  which introduces adaptive knowledge based system functionalities to assist single pilots in the accomplishment of mission essential and safety critical tasks in modern commercial transport aircraft  the proposed cpai system implementation is based on real time detection of the pilot s physiological and cognitive states  allowing the avoidance of pilot errors and supporting enhanced synergies between the human and the avionics systems  these synergies yield significant improvements in the overall performance and safety levels  a cpai working process consisting of sensing  estimation and reconfiguration steps is developed to support the assessment of physiological and external conditions  a dynamic allocation of tasks and adaptive alerting  suitable mathematical models are introduced to estimate the mental demand associated to each piloting task and to assess the pilot cognitive states  suitably implemented decision logics allow a continuous and optimal adjustment of the automation levels as a function of the estimated cognitive states  representative numerical simulation test cases provide a preliminary validation of the cpai concept  in particular  the continuous adaptation of the flight deck s automation successfully maintains the pilot s task load within an optimal range  mitigating the onset of hazardous fatigue levels  it is anticipated that by including suitably designed psychophysiological based integrity augmentation  pbia  functionalities the cpai system will allow to fulfil the evolving aircraft certification requirements and hence support the implementation of spo in commercial transport aircraft   c  2016 elsevier b v  all rights reserved  
 collaborative language grounding toward situated human robot dialogue to enable situated human robot dialogue  techniques to support grounded language communication are essential  one particular challenge is to ground human language to a robot s internal representation of the physical world  although copresent in a shared environment  humans and robots have mismatched capabilities in reasoning  perception  and action  their representations of the shared environment and joint tasks are  significantly misaligned  humans and robots will need to make extra effort to  bridge the gap and strive for a common ground of the shared world  only then is the robot able to engage in language communication and joint tasks  thus computational models for language grounding will need to take collaboration into consideration  a robot not only needs to incorporate collaborative effort from human partners to better connect human language to its own representation  but also needs to make extra collaborative effort to communicate its representation in language that humans can understand  to address these issues  the language and interaction research group  lair  at michigan state university has investigated multiple aspects of collaborative language grounding  this article gives a brief introduction to this research effort and discusses several collaborative approaches to grounding language to perception and action  
 collective decision optimization algorithm  a new heuristic optimization method recently  inspired by nature  diversiform successful and effective optimization methods have been proposed for solving many complex and challenging applications in different domains  this paper proposes a new meta heuristic technique  collective decision optimization algorithm  cdoa   for training artificial neural networks  it simulates the social behavior of human based on their decision making characteristics including experience based phase  others  based phase  group thinking based phase  leader based phase and innovation based phase  different corresponding operators are designed in the methodology  experimental results carried out on a comprehensive set of benchmark functions and two nonlinear function approximation examples demonstrate that cdoa is competitive with respect to other state of art optimization algorithms  
 combining forces  data fusion across man and machine for biometric analysis through the hummingbird framework outlined here  we seek to encourage a novel multidisciplinary approach to biometric analysis with the goal of enhancing both understanding and accuracy of identification   c  2016 elsevier b v  all rights reserved  
 combining lexical and syntactic features for detecting content dense texts in news content dense news report important factual information about an event in direct  succinct manner  information seeking applications such as information extraction  question answering and summarization normally assume all text they deal with is content dense  here we empirically test this assumption on news articles from the business  u s  international relations  sports and science journalism domains  our findings clearly indicate that about half of the news texts in our study are in fact not content dense and motivate the development of a supervised content density detector  we heuristically label a large training corpus for the task and train a two layer classifying model based on lexical and unlexicalized syntactic features  on manually annotated data  we compare the performance of domain specific classifiers  trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together  our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label  domain specific classifiers are more accurate for domains in which content dense texts are typically fewer  domain independent classifiers reproduce better naive crowdsourced judgements  classification prediction is high across all conditions  around 80   
 combining sentence similarities measures to identify paraphrases paraphrase identification consists in the process of verifying if two sentences are semantically equivalent or not  it is applied in many natural language tasks  such as text summarization  information retrieval  text categorization  and machine translation  in general  methods for assessing paraphrase identification perform three steps  first  they represent sentences as vectors using bag of words or syntactic information of the words present the sentence  next  this representation is used to measure different similarities between two sentences  in the third step  these similarities are given as input to a machine learning algorithm that classifies these two sentences as paraphrase or not  however  two important problems in the area of paraphrase identification are not handled   i  the meaning problem  two sentences sharing the same meaning  composed of different words  and  ii  the word order problem  the order of the words in the sentences may change the meaning of the text  this paper proposes a paraphrase identification system that represents each pair of sentence as a combination of different similarity measures  these measures extract lexical  syntactic and semantic components of the sentences encompassed in a graph  the proposed method was benchmarked using the microsoft paraphrase corpus  which is the publicly available standard dataset for the task  different machine learning algorithms were applied to classify a sentence pair as paraphrase or not  the results show that the proposed method outperforms state of the art systems   c  2017 elsevier ltd  all rights reserved  
 common and distinct components in data fusion in many areas of science  multiple sets of data are collected pertaining to the same system  examples are food products that are characterized by different sets of variables  bioprocesses that are online sampled with different instruments  or biological systems of which different genomic measurements are obtained  data fusion is concerned with analyzing such sets of data simultaneously to arrive at a global view of the system under study  one of the upcoming areas of data fusion is exploring whether the data sets have something in common or not  this gives insight into common and distinct variation in each data set  thereby facilitating understanding of the relationships between the data sets  unfortunately  research on methods to distinguish common and distinct components is fragmented  both in terminology and in methods  there is no common ground that hampers comparing methods and understanding their relative merits  this paper provides a unifying framework for this subfield of data fusion by using rigorous arguments from linear algebra  the most frequently used methods for distinguishing common and distinct components are explained in this framework  and some practical examples are given of these methods in the areas of medical biology and food science  this paper presents a general mathematical framework for defining common and distinct components in data fusion  it places the currently most used methods in this framework and derives new properties of those methods  some of the methods are illustrated with two real life examples  it also discusses unsolved problems in the area and hints at possible new directions of research  
 communicating intent to develop shared situation awareness and engender trust in human agent teams this paper addresses issues related to integrating autonomy enabled  intelligent agents into collaborative  human machine teams  interaction with intelligent machine agents capable of making independent  goal directed decisions in human machine teaming operations constitutes a major change from traditional human machine interaction involving teleoperation  communicating the machine agent s intent to human counterparts becomes increasingly important as independent machine decisions become subject to human trust and mental models  the authors present findings from their research that suggest existing user display technologies  tailored with context specific information and the human s knowledge level of the machine agent s decision process  can mitigate misperceptions of the appropriateness of agent behavioral responses  this is important because misperceptions on the part of human team members increases the likelihood of trust degradation and unnecessary interventions  ultimately leading to disuse of the agent  examples of possible issues associated with communicating agent intent  as well as potential implications for trust calibration are provided  published by elsevier b v  
 community detection in attributed networks based on heterogeneous vertex interactions community detection is attracting more attention on social network analysis  it is to cluster densely connected nodes into communities  in attributed networks where nodes have attributes  community detection should take both topology and attributes into account  traditional community detection algorithms only focus on the topological structure  they do not take advantage of attributes so their performance is limited  besides  most community detection algorithms for attributed networks are far from satisfactory because of accuracy and algorithm complexity  moreover  most of the algorithms depend on users to specify the community number  which also impacts the performance  based on a high performance community detection algorithm named attractor  we propose hetero attractor which can detect communities in attributed networks  it expands the sociological model of attractor and generates a heterogeneous network from the attributed network  hetero attractor analyzes the new network based on the interactions between vertices  by these interactions  the topological information and attribute information not only play a role in the community detection but also interact with each other to reach a balanced result  it also develops a novel way to analyze the heterogeneous network  the experiments demonstrate that our algorithm performs better by utilizing the attribute information  and outperforms other methods both in terms of accuracy as well as scalability  with a maximum promotion of 60  in accuracy  
 company event popularity for financial markets using twitter and sentiment analysis the growing number of twitter users makes it a valuable source of information to study what is happening right now  users often use twitter to report real life events  here we are only interested in following the financial community  this paper focuses on detecting events popularity through sentiment analysis of tweets published by the financial community on the twitter universe  the detection of events popularity on twitter makes this a non trivial task due to noisy content that often are the tweets  this work aims to filter out all the noisy tweets in order to analyze only the tweets that influence the financial market  more specifically the thirty companies that compose the dow jones average  to perform these tasks  in this paper it is proposed a methodology that starts from the financial community of twitter and then filters the collected tweets  makes the sentiment analysis of the tweets and finally detects the important events in the life of companies   c  2016 elsevier ltd  all rights reserved  
 comparing alternatives to account for unobserved heterogeneity in direct marketing models we are dealing with mailing decisions of a direct marketing company and focus on assessing three alternative approaches to model unobserved heterogeneity  which are based on finite mixtures  continuous mixtures  and a mixture of dirichlet processes  mdp   respectively  models are estimated by markov chain monte carlo  mcmc  simulation  based on pseudo bayes factors  psbf   we find that a finite mixture model turns out to be superior both to models based on either a mdp or a continuous mixture  whereas the mdp finds similar estimates compared to the finite mixture approach  estimates of the continuous mixture differ for some variables  according to the finite mixture  type of mailing has an effect on purchase behavior  in addition  some customers show supersaturation effects of mailings  due to different coefficient estimates  managerial implications differ depending on which model they relate  in particular  a continuous mixture model would recommend more mailings than a finite mixture approach   c  2017 elsevier b v  all rights reserved  
 comparing dynamics of fluency and inter limb coordination in climbing activities using multi scale jensen shannon embedding and clustering this paper reports the results of two studies carried out in a controlled environment aiming to understand relationships between movement patterns of coordination that emerge during climbing and performance outcomes  it involves a recent method of nonlinear dimensionality reduction  multi scale jensen shannon neighbor embedding  lee et al   2015   which has been applied to recordings of movement sensors in order to visualize coordination patterns adapted by climbers  initial clustering at the climb scale provides details linking behavioral patterns with climbing fluency smoothness  i e   the performance outcome   further clustering on shorter time intervals  where individual actions within a climb are analyzed  enables more detailed exploratory data analysis of behavior  results suggest that the nature of individual learning curves  the global  trial to trial performance  corresponded to certain behavioral patterns  the within trial motor behavior   we highlight and discuss three distinctive learning curves and their corresponding relationship to behavioral pattern emergence  namely  no improvement and a lack of new motor behavior emergence  sudden improvement and the emergence of new motor behaviors  and gradual improvement and a lack of new motor behavior emergence  
 comparing human behavior models in repeated stackelberg security games  an extended study several competing human behavior models have been proposed to model boundedly rational adversaries in repeated stackelberg security games  ssg   however  these existing models fail to address three main issues which are detrimental to defender performance  first  while they attempt to learn adversary behavior models from adversaries  past actions   attacks on targets    they fail to take into account adversaries  future adaptation based on successes or failures of these past actions  second  existing algorithms fail to learn a reliable model of the adversary unless there exists sufficient data collected by exposing enough of the attack surface a situation that often arises in initial rounds of the repeated ssg  third  current leading models have failed to include probability weighting functions  even though it is well known that human beings  weighting of probability is typically nonlinear  to address these limitations of existing models  this article provides three main contributions  our first contribution is a new human behavior model  sharp  which mitigates these three limitations as follows   i  sharp reasons based on success or failure of the adversary s past actions on exposed portions of the attack surface to model adversary adaptivity   ii  sharp reasons about similarity between exposed and unexposed areas of   the attack surface  and also incorporates a discounting parameter to mitigate adversary s lack of exposure to enough of the attack surface  and  iii  sharp integrates a non linear probability weighting function to capture the adversary s true weighting of probability  our second contribution is a first  repeated measures study    at least in the context of ssgs   of competing human behavior models  this study  where each experiment lasted a period of multiple weeks with individual sets of human subjects on the amazon  mechanical turk platform  illustrates the strengths and weaknesses of different models and shows the advantages of sharp  our third major contribution is to demonstrate sharp s superiority by conducting real world human subjects experiments at the bukit barisan seletan national park in indonesia against wildlife security experts   c  2016 elsevier b v  all rights reserved  
 comparing paradigms for strategy learning of route choice with traffic information under uncertainty this paper aims to model the traveller s day to day route choice in the case of an advanced traveller information system  atis  through two learning paradigms  reinforcement based and belief based  the reinforcement learning approach is adopted in both a basic version and an extended one  similarly  the belief learning approach is adopted in both a joint strategy fictitious play version and in a bayesian learning one  all the models are compared and validated based on data collected by means of a stated preference experiment  the models explicitly account for the reliability of the information system  as this interacts with the inherent dispersion of network travel times and determines the overall level of uncertainty affecting the travellers  adaptive learning behaviour  the experiment is then designed to simulate different levels of reliability for the atis  results show that for intermediate and high levels of information accuracy  joint strategy fictitious play best predicts the respondents  route choice behaviour under information provision  suggesting that a best reply strategy is used by travellers for their route choices  in low information accuracy  the result suggests the payoff variability moves the choice behaviour toward randomness  the proposed approach provides useful tools to model travellers  adaptive route choice behaviour and contributes to the support of effective atis design   c  2017 elsevier ltd  all rights reserved  
 competence assessment as an expert system for human resource management  a mathematical approach efficient human resource management needs accurate assessment and representation of available competences as well as effective mapping of required competences for specific jobs and positions  in this regard  appropriate definition and identification of competence gaps express differences between acquired and required competences  using a detailed quantification scheme together with a mathematical approach is a way to support accurate competence analytics  which can be applied in a wide variety of sectors and fields  this article describes the combined use of software technologies and mathematical and statistical methods for assessing and analyzing competences in human resource information systems  based on a standard competence model  which is called a professional  innovative and social competence tree  the proposed framework offers flexible tools to experts in real enterprise environments  either for evaluation of employees towards an optimal job assignment and vocational training or for recruitment processes  the system has been tested with real human resource data sets in the frame of the european project called comprofits   c  2016 elsevier ltd  all rights reserved  
 competences based performance model of multi skilled workers with learning and forgetting the relationship between performance and experience is non linear  thus planning models that seek to manage workforce development through task assignment are difficult to solve  this gets even more complicated when taking into account multi skilled workers that are capable of performing a variety of tasks  in this paper we develop a competences based analytical model of the performance of multi skilled workers undertaking repetitive tasks  taking into account learning and forgetting  a learning curve can be used to estimate improvement when repeating the same operation  inverse phenomenon is forgetting  which can occur due to interruption in the production process  the performance evaluation algorithm  pea  was developed for two cases  fixed shift duration and fixed production output  the aim was to build a tool that better describes the capabilities of workers to perform repetitive tasks by binding together hierarchical competences modeled as a weighted digraph together with a learning and forgetting curve model  lfcm  to express individual learning rates   c  2017 elsevier ltd  all rights reserved  
 complexity analysis of taxi duopoly game with heterogeneous business operation modes and differentiated products this paper has considered an automobile industry duopoly model with representative firms of kuaiche and taxi  the impact of product differentiation degree  market share of adaptive player and price adjustment speed of bounded rationality player on stability region and nash equilibrium points of system have been analyzed  numerical simulation has illustrated that product differentiation has increased the possibility of chaos  but chaos has existed not only in a fierce competitive market but also a weak competitive market  another finding is that generally speaking  the increase of market share and product differentiation degree has increased equilibrium price of kuaiche and decreased equilibrium price of taxi  this means cash burning war strategy of kuaiche has worked  we choose different price adjustment speed to show dependence on initials only when the system is in chaos  we find suitable control factors to restrain and eliminate chaos  
 composite quantile regression neural network with applications in recent years  there has been growing interest in neural network to explore complex patterns  we consider an extension of this framework in composite quantile regression setup and propose a novel composite quantile regression neural network  cqrnn  model  we further construct a differential approximation to the quantile regression loss function  and develop an estimation procedure using standard gradient based optimization algorithms  the cqrnn model is flexible and efficient to explore potential nonlinear relationships among variables  which we demonstrate both in monte carlo simulation studies and three real world applications  it enhances the nonlinear processing capacity of ann and enables us to achieve desired results for handling different types of data  in addition  our method also provides an idea to bridge the gap between composite quantile regression and intelligent methods such as anns  svm  etc   which is helpful to improve their robustness  goodness of fit and predictive ability   c  2017 elsevier ltd  all rights reserved  
 computational aspects of nearly single peaked electorates manipulation  bribery  and control are well studied ways of changing the outcome of an election  many voting rules are  in the general case  computationally resistant to some of these manipulative actions  however when restricted to single peaked electorates  these rules suddenly become easy to manipulate  recently  faliszewski  hemaspaandra  and hemaspaandra studied the computational complexity of strategic behavior in nearly single peaked electorates  these are electorates that are not single peaked but close to it according to some distance measure  in this paper we introduce several new distance measures regarding single peakedness  we prove that determining whether a given pro file is nearly single peaked is np complete in many cases  for one case we present a polynomial time algorithm  in case the single peaked axis is given  we show that determining the distance is always possible in polynomial time  furthermore  we explore the relations between the new notions introduced in this paper and existing notions from the literature  
 computational aspects of strategic behaviour in elections with top truncated ballots understanding when and how computational complexity can be used to protect elections against different manipulative actions has been a highly active research area over the past two decades  much of this literature  however  makes the assumption that the voters or agents specify a complete preference ordering over the set of candidates  there are many multiagent systems applications  and even real world elections  where this assumption is not warranted  and this in turn raises a series of questions on the impact of partial voting on the complexity of manipulative actions  in this paper  we focus on two of these questions  first  we address the question of how hard it is to manipulate elections when the agents specify only top truncated ballots  here  in particular  we look at the weighted manipulation problem both constructive and destructive manipulation when the voters are allowed to specify top truncated ballots  and we provide general results for all scoring rules  for elimination versions of all scoring rules  for the plurality with runoff rule  for a family of election systems known as copeland  and for the maximin protocol  the second question we address is the impact of top truncated voting on the complexity of manipulative actions in electorates with structured preference profiles  in particular  we consider electorates that are single peaked and we show how  for many voting protocols  allowing top truncated voting reimposes the  hardness shields that normally vanish in such electorates  
 computational evaluation of a mip model for multi port stowage planning problems in this paper  we consider the problem of determining stowage plans for containers into ships having to visit a given number of ports in their circular route  the problem is denoted multi port master bay plan problem  mp mbpp   in practice  the mp mbpp consists in determining how to stow a given set of containers  split into different groups  according to their size  type  class of weight and destination  into bay locations  either on the deck or in the stow  some structural and operational constraints  related to the containers  the ship and the maritime terminals  have to be satisfied  the single port mbpp is a np hard optimization problem  and has been proposed in the literature from 2001  from then  some variants of the problem have been presented  together with the related solution methods  mainly aimed at including in the corresponding models realistic features  required as a consequence of the naval gigantism  as a novel issue  in the present work  we look for stowage plans where the set of containers to be loaded on board at each port of the route consists of standard  reefer and open top ones  hatches positions in the ships are considered too  we present a new mixed integer programming  mip  model for the mp mbpp able to manage realistic scenarios and find stowage plans for containerships up to 18 000 teus  the model is finalized to be solved with a commercial mip solver  the reported computational experimentation shows that the model is very efficient and could be fruitfully used for facing real size instances of the problem  
 computational intelligent hybrid model for detecting disruptive trading activity the term  disruptive trading behaviour  was first proposed by the u s  commodity futures trading commission and is now widely used by us and eu regulation  mifid ii  to describe activities that create a misleading appearance of market liquidity or depth or an artificial price movement upward or downward according to their own purposes  such activities  identified as a new form of financial fraud in eu regulations  damage the proper functioning and integrity of capital markets and are hence extremely harmful  while existing studies have explored this issue  they have  in most cases  either focused on empirical analysis of such cases or proposed detection models based on certain assumptions of the market  effective methods that can analyse and detect such disruptive activities based on direct studies of trading behaviours have not been studied to date  there exists  accordingly  a knowledge gap in the literature  this paper seeks to address that gap and provides a hybrid model composed of two data mining based detection modules that effectively identify disruptive trading behaviours  the hybrid model is designed to work in an on line scheme  the limit order stream is transformed  calculated and extracted as a feature stream  one detection module   single order detection   detects disruptive behaviours by identifying abnormal patterns of every single trading order  another module   order sequence detection   approaches the problem by examining the contextual relationships of a sequence of trading orders using an extended hidden markov model  which identifies whether sequential changes from the extracted features are manipulative activities  or not   both models were evaluated using huge volumes of real tick data from the nasdaq which demonstrated that both are able to identify a range of disruptive trading behaviours and  furthermore  that they outperform the selected traditional benchmark models  thus  this hybrid model is shown to make a substantial contribution to the literature on financial market surveillance and to offer a practical and effective approach for the identification of disruptive trading behaviour   c  2016 elsevier b v  all rights reserved  
 computational modeling of players  emotional response patterns to the story events of video games this study suggests an approach for the computational modeling of players  emotional response patterns to story events in video games  we propose what is termed the dynamic narrative emotion model for analyzing the emotional response patterns of video game players by combining and reconstructing the occ cognitive emotion model and d  price s emotional intensity equation  based on this model  we compared with two emotional response patterns of players to story events in both commercially successful and unsuccessful video games  the analysis was conducted with 360 emotional response values extracted from the playing experiences of 10 player s  the results showed that responses from commercially successful games are 3 3 times higher in terms of the frequency of emotion transitions  1 3 times in terms of the number of emotion types  and twice as high in terms of the distance of an emotion transition compared to those of unsuccessful games  the results of this study can be applied by game designers with two implications for creating story driven video games  first  to differentiate the emotional responses patterns of players in successful games from unsuccessful games  and second  to develop emotional transition strategies when designing story events in accordance with feedback from the players  emotional response patterns while playing the games  
 computational models of affordance in robotics  a taxonomy and systematic classification j  j  gibson s concept of affordance  one of the central pillars of ecological psychology  is a truly remarkable idea that provides a concise theory of animal perception predicated on environmental interaction  it is thus not surprising that this idea has also found its way into robotics research as one of the underlying theories for action perception  the success of the theory in this regard has meant that existing research is both abundant and diffuse by virtue of the pursuit of multiple different paths and techniques with the common goal of enabling robots to learn  perceive  and act upon affordances  up until now  there has existed no systematic investigation of existing work in this field  motivated by this circumstance  in this article  we begin by defining a taxonomy for computational models of affordances rooted in a comprehensive analysis of the most prominent theoretical ideas of import in the field  subsequently  after performing a systematic literature review  we provide a classification of existing research within our proposed taxonomy  finally  by both quantitatively and qualitatively assessing the data resulting from the classification process  we highlight gaps in the research terrain and outline open questions for the investigation of affordances in robotics that we believe will help inform future work  prioritize research goals  and potentially advance the field toward greater robot autonomy  
 computational models of ethical decision making  a coherence driven reflective equilibrium model there are scientific and technical challenges that must be addressed in developing systems that interact with humans and work along with other agents in complex  dynamic  and uncertain environments where ethical concerns may arise  in such systems relationships between users and autonomous components will be driven as much by issues such as trust  responsibility  and acceptability  as technical ones such as planning and coordination  this paper provides a comprehensive review and classification of existing methods in machine ethics  resulting in delineation of specific challenges and issues  to address the identified challenges  we introduce a method that leverages the method of reflective equilibrium and the multi coherence theory as a unifying constraint satisfaction framework to simultaneously assess multiple ethical principles and manage ethical conflicts in a context sensitive manner   c  2017 elsevier b v  all rights reserved  
 computer vision analysis for children s social play classification in peer play scenarios labeling children s social play behavior is an important process in children s peer play analysis which is traditionally done by experienced coders  with the growing volume of data  automatic methods for labeling are increasingly required  this paper presents a novel method to classify children s social play behavior in peer play scenarios into three categories  solitary play  parallel play and group play   based on the two key cues attentiveness and proximity proposed in  the play observation scale   unary features and pairwise features are calculated to describe the relationships between a child and the whole context  and the interactions between two children  inspired by the recent studies in social behavior analysis and interaction recognition  children s activities are classified by support vector machine  svm  and hidden conditional random field  hcrf   this method is evaluated by a dataset of children s peer play scenarios collected by psychology researchers and the results show this method has a good performance in the dataset  
 computer vision for assistive technologies in the last decades there has been a tremendous increase in demand for assistive technologies  at  useful to overcome functional limitations of individuals and to improve their quality of life  as a consequence  different research papers addressing the development of assistive technologies have appeared into the literature pushing the need to organize and categorize them taking into account the application assistive aims  several surveys address the categorization problem for works concerning a specific need  hence giving the overview on the state of the art technologies supporting the related function for the individual  unfortunately  this  user need oriented  way of categorization considers each technology as a whole and then a deep and critical explanation of the technical knowledge used to build the operative tasks as well as a discussion on their cross contextual applicability is completely missing making thus existing surveys unlikely to be technically inspiring for functional improvements and to explore new technological frontiers  to overcome this critical drawback  in this paper an original  task oriented  way to categorize the state of the art of the at works has been introduced  it relies on the split of the final assistive goals into tasks that are then used as pointers to the works in literature in which each of them has been used as a component  in particular this paper concentrates on a set of cross application computer vision tasks that are set as the pivots to establish a categorization of the at already used to assist some of the user s needs  for each task the paper analyzes the computer vision algorithms recently involved in the development of at and  finally  it tries to catch a glimpse of the possible paths in the short and medium term that could allow a real improvement of the assistive outcomes  the potential impact on the assessment of at considering users  medical  economical and social perspective is also addressed   c  2016 elsevier inc  all rights reserved  
 computing semantic similarity of concepts in knowledge graphs this paper presents a method for measuring the semantic similarity between concepts in knowledge graphs  kgs  such as wordnet and dbpedia  previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts  e g   path length and depth   or only on the information content  ic  of concepts  we propose a semantic similarity method  namely wpath  to combine these two approaches  using ic to weight the shortest path length between concepts  conventional corpus based ic is computed from the distributions of concepts over textual corpus  which is required to prepare a domain corpus containing annotated concepts and has high computational cost  as instances are already extracted from textual corpus and annotated by concepts in kgs  graph based ic is proposed to compute ic based on the distributions of concepts over instances  through experiments performed on well known word similarity datasets  we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods  moreover  in a real category classification evaluation  the wpath method has shown the best performance in terms of accuracy and f score  
 conceptual spaces for cognitive architectures  a lingua franca for different levels of representation during the last decades  many cognitive architectures  cm  have been realized adopting different assumptions about the organization and the representation of their knowledge level  some of them  e g  soar  laird  2012   adopt a classical symbolic approach  some  e g  leabra o reilly and munakata  2000   are based on a purely connectionist model  while others  e g  clarion  sun  2006   adopt a hybrid approach combining connectionist and symbolic representational levels  additionally  some attempts  e g  bisoar  trying to extend the representational capacities of cas by integrating diagrammatical representations and reasoning are also available  kurup   chandrasekaran  2007   in this paper we propose a reflection on the role that conceptual spaces  a framework developed by gardenfors  2000  more than fifteen years ago  can play in the current development of the knowledge level in cognitive systems and architectures  in particular  we claim that conceptual spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic  sub symbolic and diagrammatic approaches  by overcoming some of their typical problems  and to integrate them on a common ground  in doing so we extend and detail some of the arguments explored by gardenfors  1997  for defending the need of a conceptual  intermediate  representation level between the symbolic and the sub symbolic one  in particular we focus on the advantages offered by conceptual spaces  with respect to symbolic and sub symbolic approaches  in dealing with the problem of compositionality of representations based on typicality traits  additionally  we argue that conceptual spaces could offer a unifying framework for interpreting many kinds of diagrammatic and analogical representations  as a consequence  their adoption could also favor the integration of diagrammatical representation and reasoning in cas   c  2016 elsevier b v  all rights reserved  
 conditionals  counterfactuals  and rational reasoning  an experimental study on basic principles we present a unified approach for investigating rational reasoning about basic argument forms involving indicative conditionals  counterfactuals  and basic quantified statements within coherence based probability logic  after introducing the rationality framework  we present an interactive view on the relation between normative and empirical work  then  we report a new experiment which shows that people interpret indicative conditionals and counterfactuals by coherent conditional probability assertions and negate conditionals by negating their consequents  the data support the conditional probability interpretation of conditionals and the narrow scope reading of the negation of conditionals  finally  we argue that coherent conditional probabilities are important for probabilistic analyses of conditionals  nonmonotonic reasoning  quantified statements  and paradoxes  
 conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability we propose a general framework for topic specific summarization of large text corpora  and illustrate how it can be used for analysis in two quite different contexts  an occupational safety and health administration  osha  database of fatality and catastrophe reports  to facilitate surveillance for patterns in circumstances leading to injury or death   and legal decisions on workers  compensation claims  to explore relevant case law   our summarization framework  built on sparse classification methods  is a compromise between simple word frequency based methods currently in wide use  and more heavyweight  model intensive methods such as latent dirichlet allocation  lda   for a particular topic of interest  e g   mental health disability  or carbon monoxide exposure   we regress a labeling of documents onto the high dimensional counts of all the other words and phrases in the documents  the resulting small set of phrases found as predictive are then harvested as the summary  using a branch and bound approach  this method can incorporate phrases of arbitrary length  which allows for potentially rich summarization  we discuss how focus on the purpose of the summaries can inform choices of tuning parameters and model constraints  we evaluate this tool by comparing the computational time and summary statistics of the resulting word lists to three other methods in the literature  we also present a new r package  textreg  overall  we argue that sparse methods have much to offer in text analysis and is a branch of research that should be considered further in this context   c  2016 wiley periodicals  inc  
 confidence curves  an alternative to null hypothesis significance testing for the comparison of classifiers null hypothesis significance testing is routinely used for comparing the performance of machine learning algorithms  here  we provide a detailed account of the major underrated problems that this common practice entails  for example  omnibus tests  such as the widely used friedman test  are not appropriate for the comparison of multiple classifiers over diverse data sets  in contrast to the view that significance tests are essential to a sound and objective interpretation of classification results  our study suggests that no such tests are needed  instead  greater emphasis should be placed on the magnitude of the performance difference and the investigator s informed judgment  as an effective tool for this purpose  we propose confidence curves  which depict nested confidence intervals at all levels for the performance difference  these curves enable us to assess the compatibility of an infinite number of null hypotheses with the experimental results  we benchmarked several classifiers on multiple data sets and analyzed the results with both significance tests and confidence curves  our conclusion is that confidence curves effectively summarize the key information needed for a meaningful interpretation of classification results while avoiding the intrinsic pitfalls of significance tests  
 connected word recognition using a cascaded neuro computational model we propose a novel framework for processing a continuous speech stream that contains a varying number of words  as well as non speech periods  speech samples are segmented into word tokens and non speech periods  an augmented version of an earlier proposed  cascaded neuro computational model is used for recognising individual words within the stream  simulation studies using both a multi speaker dependent and speaker independent digit string database show that the proposed method yields a recognition performance comparable to that obtained by a benchmark approach using hidden markov models with embedded training  
 constrained coalition formation on valuation structures  formal framework  applications  and islands of tractability coalition structure generation is the problem of partitioning the agents of a given environment into disjoint and exhaustive coalitions so that the whole available worth is maximized  while this problem has been classically studied in settings where all coalitions are allowed to form  it has been recently reconsidered in the literature moving from the observation that environments often forbid the formation of certain coalitions  by following this latter perspective  a model for coalition structure generation is proposed where constraints of two different kinds can be expressed simultaneously  indeed  the model is based on the concept of valuation structure  which consists of a set of pivotal agents that are pairwise incompatible  plus an interaction graph prescribing that a coalition c can form only if the subgraph induced over the nodes agents in c is connected  it is shown that valuation structures can be used to model a number of relevant problems arising in real world application domains  then  the complexity of coalition structure generation over valuation structures is studied  by assuming that the functions associating each coalition with its worth are given as input according to some compact encoding rather than explicitly listing all exponentially many associations  in particular  islands of tractability are identified based on the topological properties of the underlying interaction graphs and on suitable algebraic properties of the given worth functions  finally  stability issues over valuation structures are studied too  by considering the core as the prototypical solution concept   c  2017 elsevier b v  all rights reserved  
 constructing a language from scratch  combining bottom up and top down learning processes in a computational model of language acquisition we present a computational model that allows us to study the interplay of different processes involved in first language acquisition  we build on the assumption that language acquisition is usage driven and assume that there are different processes in language acquisition operating at different levels  bottom up processing allows a learner to identify regularities in the linguistic input received  while top down processing exploits prior experience and previous knowledge to guide choices made during bottom up processing  to shed light on the interplay between top down and bottom up processing in language acquisition  we present a computational model of language acquisition that is based on bootstrapping mechanisms and is usage based in that it relies on discovered regularities to segment speech into word like units  based on this initial segmentation  our model induces a construction grammar that in turn acts as a top down prior that guides the segmentation of new sentences into words  we spell out in detail these processes and their interplay  showing that top down processing increases both understanding performance and segmentation accuracy  our model thus contributes to a better understanding of the interplay between bottom up and top down processes in first language acquisition and thus to a better understanding of the mechanisms and architecture involved in language acquisition  
 construction and evaluation of structured association map for visual exploration of association rules the association rule mining is one of the most popular data mining techniques  however  the users often experience difficulties in interpreting and exploiting the association rules extracted from large transaction data with high dimensionality  the primary reasons for such difficulties are two folds  firstly  too many association rules can be produced by the conventional association rule mining algorithms  and secondly  some association rules can be partly overlapped  this problem can be addressed if the user can select the relevant items to be used in association rule mining  however  there are often quite complex relations among the items in large transaction data  in this context  this paper aims to propose a novel visual exploration tool  structured association map  sam   which enables the users to find the group of the relevant items in a visual way  the appearance of sam is similar with the well known cluster heat map  however  the items in sam are sorted in more intelligent way so that the users can easily find the interesting area formed by a set of associated items  which are likely to constitute interesting many to many association rules  moreover  this paper introduces an index called s2c  designed to evaluate the quality of sam  and explains the sam based association analysis procedure in a comprehensive manner  for illustration  this procedure is applied to a mass health examination result data set  and the experiment results demonstrate that sam with high s2c value helps to reduce the complexities of association analysis significantly and it enables to focus on the specific region of the search space of association rule mining while avoiding the irrelevant association rules   c  2017 elsevier ltd  all rights reserved  
 constructive decision theory we sketch a theory of decision that allows us to construct both goals and degrees of belief  before choosing an action  we create and weight goals  we represent our beliefs about the consequences of each action by constructing a belief function on the set of possible consequences  then we combine the beliefs and goals to see the value that will be secured and the value that will be excluded by each action  this approach is more constructive than bayesian decision theory  because as savage s problem of small worlds teaches us  that theory assumes preferences that antedate deliberation   c  2016 elsevier inc  all rights reserved  
 content based filtering for recommendation systems using multiattribute networks content based filtering  cbf   one of the most successful recommendation techniques  is based on correlations between contents  cbf uses item information  represented as attributes  to calculate the similarities between items  in this study  we propose a novel cbf method that uses a multiattribute network to effectively reflect several attributes when calculating correlations to recommend items to users  in the network analysis  we measure the similarities between directly and indirectly linked items  moreover  our proposed method employs centrality and clustering techniques to consider the mutual relationships among items  as well as determine the structural patterns of these interactions  this mechanism ensures that a variety of items are recommended to the user  which improves the performance  we compared the proposed approach with existing approaches using movielens data  and found that our approach outperformed existing methods in terms of accuracy and robustness  our proposed method can address the sparsity problem and over specialization problem that frequently affect recommender systems  furthermore  the proposed method depends only on ratings data obtained from a user s own past information  and so it is not affected by the cold start problem   c  2017 elsevier ltd  all rights reserved  
 content based methods in peer assessment of open response questions to grade students as authors and as graders massive open online courses  moocs  use different types of assignments in order to evaluate student knowledge  multiple choice tests are particularly apt given the possibility for automatic assessment of large numbers of assignments  however  certain skills require open responses that cannot be assessed automatically yet their evaluation by instructors or teaching assistants is unfeasible given the large number of students  a potentially effective solution is peer assessment whereby students grade the answers of other students  however  to avoid bias due to inexperience  such grades must be filtered  we describe a factorization approach to grading  as a scalable method capable of dealing with very high volumes of data  our method is also capable of representing open response content using a vector space model of the answers  since reliable peer assessment requires students to make coherent assessments  students can be motivated by their assessments reflecting not only their own answers but also their efforts as graders  the method described is able to tackle both these aspects simultaneously  finally  for a real world university setting in spain  we compared grades obtained by our method and grades awarded by university instructors  with results indicating a notable improvement from using a content based approach  there was no evidence that instructor grading would have led to more accurate grading outcomes than the assessment produced by our models   c  2016 elsevier b v  all rights reserved  
 context aware obstacle detection for navigation by visually impaired this paper presents a context aware smartphone based based visual obstacle detection approach to aid visually impaired people in navigating indoor environments  the approach is based on processing two consecutive frames  images   computing optical flow  and tracking certain points to detect obstacles  the frame rate of the video stream is determined using a context aware data fusion technique for the sensors on smartphones  through an efficient and novel algorithm  a point dataset on each consecutive frames is designed and evaluated to check whether the points belong to an obstacle  in addition to determining the points based on the texture in each frame  our algorithm also considers the heading of user movement to find critical areas on the image plane  we validated the algorithm through experiments by comparing it against two comparable algorithms  the experiments were conducted in different indoor settings and the results based on precision  recall  accuracy  and f measure were compared and analyzed  the results show that  in comparison to the other two widely used algorithms for this process  our algorithm is more precise  we also considered time to contact parameter for clustering the points and presented the improvement of the performance of clustering by using this parameter   c  2017 elsevier b v  all rights reserved  
 continuance of protective security behavior  a longitudinal study previous research has established continuance models that explain and predict an individual s behaviors when engaged with hedonic or functional systems  or with other environments that provide productivity enhancing outcomes  however  within the context of information security  these models are not applicable and fail to accurately assess the circumstances in which an individual engages in protective security behaviors beyond an initial adoption  this research addresses this gap and establishes a model for explaining an individual s continued engagement in protective security behaviors  which is a significant problem in securing enterprise information resources  within this model  protection motivation theory  pmt  is considered an underlying theoretical motivation for continuance intention using constructs such as perceived threat severity  perceived threat susceptibility  self efficacy  and response efficacy as direct antecedents of behavioral intents and indirect predictors of continuance behavior  furthermore  the introduction of perceived extraneous circumstances is used to reconcile the  acceptance discontinuance anomaly   a novel research methodology for measuring actual security behavior continuance was developed for this investigation  experimental results indicate support for all of the proposed relationships  with the exception of response efficacy continuance intent  nearly half of the variance in the dependent variable  continuance behavior  was explained by the model  this is the first comprehensive empirical investigation of protective security behavior continuance intention  the findings have practical implications for security administrators and security technology solution providers  and they have theoretical ramifications in the area of behavioral information security and protection motivation theory   c  2016 elsevier b v  all rights reserved  
 continuous estimation of emotions in speech by dynamic cooperative speaker models research on automatic emotion recognition from speech has recently focused on the prediction of time continuous dimensions  e g   arousal and valence  of spontaneous and realistic expressions of emotion  as found in real life interactions  however  the automatic prediction of such emotions poses several challenges  such as the subjectivity found in the definition of a gold standard from a pool of raters and the issue of data scarcity in training models  in this work  we introduce a novel emotion recognition system  based on ensembles of single speaker regression models  the estimation of emotion is provided by combining a subset of the initial pool of single speaker regression models selecting those that are most concordant among them  the proposed approach allows the addition or removal of speakers from the ensemble without the necessity to re build the entire recognition system  the simplicity of this aggregation strategy  coupled with the flexibility assured by the modular architecture  and the promising results observed on the recola database highlight the potential implications of the proposed method in a real life scenario and in particular in web based applications  
 contract designing for a supply chain with uncertain information based on confidence level we consider a contract design problem for two competing heterogeneous suppliers working with a common retailer  the retailer s type low volume or high volume is unknown to the suppliers  one supplier has a high variable cost and a low fixed cost  whereas the other has a low variable cost and a high fixed cost  and their variable costs are uncertain  they sell the same products to the retailer  and each supplier offers the retailer a menu of contracts  the retailer chooses the contract that maximizes her alternative profit based on her confidence level instead of her expected profit  in this setting  we find that the retailer s optimal order quantity is determined by the inverse distribution of the external demand and the confidence level  furthermore  higher confidence levels correlate with lower order quantities  we also show that the equilibrium contract menus depend on the magnitudes of the confidence level and the high fixed cost  importantly  if the confidence level of the supply chain tends to be 0 or 1  the supplier with the low fixed cost possesses a competitive advantage over the other supplier  in some cases  the supplier with the low fixed cost may choose not to serve the high volume retailer to avoid excessive information rent   c  2016 elsevier b v  all rights reserved  
 controlled school choice with soft bounds and overlapping types school choice programs are implemented to give students parents an opportunity to choose the public school the students attend  controlled school choice programs need to provide choices for students parents while maintaining distributional constraints on the composition of students  typically in terms of socioeconomic status  previous works show that setting soft bounds  which flexibly change the priorities of students based on their types  is more appropriate than setting hard bounds  which strictly limit the number of accepted students for each type  we consider a case where soft bounds are imposed and one student can belong to multiple types  e g    financially distressed  and  minority  types  we first show that when we apply a model that is a straightforward extension of an existing model for disjoint types  there is a chance that no stable matching exists  thus we propose an alternative model and an alternative stability definition  where a school has reserved seats for each type  we show that a stable matching is guaranteed to exist in this model and develop a mechanism called deferred acceptance for overlapping types  da ot   the da ot mechanism is strategy proof and obtains the student optimal matching within all stable matchings  furthermore  we introduce an extended model that can handle both type specific ceilings and floors and propose a extended mechanism da ot  to handle the extended model  computer simulation results illustrate that da ot outperforms an artificial cap mechanism where we set a hard bound for each type in each school  da ot  can achieve stability in the extended model without sacrificing students  welfare  
 contusion and recovery of individual cognition based on catastrophe theory  a computational model originated in emergent behavior characterized by interactions between individuals and cognitive processes  sudden changes in behavior are common phenomena under the information pressure being perceived by individuals  particularly those whose cognition is weak to negative information   said behavioral changes are thus related to reactive individual behavior   to probe its underlying mechanism  we introduce an icr model that accounts for the sudden changes in individual cognitions and behaviors  to ensure that our model is stable in different types of network environments  verification results show that the model indeed accurately describes the various catastrophe paths of individual cognition  exploration of our model also shows it can be applied in polarization of group behavior  and the confidence intervals of the resilience of individuals were analyzed to identify reversal  patterns of polarized group behavior  as a discussion of the results  it shows that the proposed model has wonderful prospect to support neural network training in individual behaviors among networks   c  2016 elsevier b v  all rights reserved  
 cooperation and strategy coexistence in a tag based multi agent system with contingent mobility understanding how to enhance cooperation and coordination in distributed  open  and dynamic multiagent systems has been a grand challenge across disciplines  knowledge employed in such systems is often limited and heuristic in nature such that cooperation promoting mechanisms based on trust or reputation become largely unreliable  although recent studies within the context of tag based systems reported the emergence of stable cooperation in such uncertain environments  they were limited exclusively to only static interaction structures  consequently  it remains unknown whether and under what conditions tag based interactions can promote cooperation in dynamic mobile systems  we herein combine the methods of game theory  evolutionary computing  and agent based simulation to study the emergence of tag mediated cooperation in a mobile network with resource diversity  in a series of extensive monte carlo simulations  we find that tag based interactions can give rise to high levels of cooperation even in the presence of different types of contingent mobility  our model reveals that agent migrations within the system and the invasion of new agents from the outside can have similar effects on the evolution of dominant strategies  interestingly enough  we observe a previously unreported coexistence of conditional and unconditional strategies in our tag based model with costly migrations  in contrast to earlier studies  we show that this mobility driven strategy coexistence in our model is not affected by resource limitations or other game specific factors  our findings highlight a striking robustness of tag based cooperation under different mobility regimes  with important consequences for the future design of cooperation enforcing protocols in large scale  decentralized  and self organizing systems such as peer to peer or mobile ad hoc networks   c  2016 elsevier b v  all rights reserved  
 cooperative group optimization with ants  cgo as   leverage optimization with mixed individual and social learning we present cgo as  a generalized ant system  as  implemented in the framework of cooperative group optimization  cgo   to show the leveraged optimization with a mixed individual and social learning  ant colony is a simple yet efficient natural system for understanding the effects of primary intelligence on optimization  however  existing as algorithms are mostly focusing on their capability of using social heuristic cues while ignoring their individual learning  cgo can integrate the advantages of a cooperative group and a low level algorithm portfolio design  and the agents of cgo can explore both individual and social search  in cgo as  each ant  agent  is added with an individual memory  and is implemented with a novel search strategy to use individual and social cues in a controlled proportion  the presented cgo as is therefore especially useful in exposing the power of the mixed individual and social learning for improving optimization  the optimization performance is tested with instances of the traveling salesman problem  tsp   the results prove that a cooperative ant group using both individual and social learning obtains a better performance than the systems solely using either individual or social learning  the best performance is achieved under the condition when agents use individual memory as their primary information source  and simultaneously use social memory as their searching guidance  in comparison with existing as systems  cgo as retains a faster learning speed toward those higher quality solutions  especially in the later learning cycles  the leverage in optimization by cgo as is highly possible due to its inherent feature of adaptively maintaining the population diversity in the individual memory of agents  and of accelerating the learning process with accumulated knowledge in the social memory   c  2016 elsevier b  v  all rights reserved  
 coordinated policy action and flexible coalitional psychology  how evolution made humans so good at politics the observation that politics makes strange bedfellows may be hackneyed  but it is also often true  politicians and other actors in the policy process routinely align themselves on specific issues with actors with whom they otherwise have broad disagreements  this fits with social psychological research showing that humans have a coalitional psychology that is remarkably flexible  allowing us to feel strong bonds toward the coalitions to which we belong but to also break those bonds and move on to new coalitions when circumstances change  how is this flexibility possible  here we examine the possible ways in which evolutionary forces helped shape our species  trademark flexible coalitional psychology  focusing in particular on gene culture coevolution and cultural group selection  we conclude with some examples of coordinated policy action among erstwhile foes in contemporary politics   c  2017 elsevier b v  all rights reserved  
 coordinating a three level supply chain under generalized parametric interval valued distribution of uncertain demand in uncertain supply chain management problem  the optimal order quantity often depends heavily on the distribution of uncertain demand  when the exact possibility distribution is unavailable  it is required to develop a novel method to characterize the uncertain demand and deal with the corresponding supply chain coordination problem  this paper addresses the coordination problem for a three level supply chain in a single period model  where the uncertain demand is characterized by generalized parametric interval valued possibility distribution  we define the lambda selection of uncertain demand  and discuss its parametric possibility distribution and mean value  in addition  we construct l s measure by parametric possibility distribution of the lambda selection  and use it to define the l s integral of uncertain profits under different scenarios  under the risk neutral criterion  we demonstrate that the mean supply chain profit in centralized decision is greater than the total mean supply chain profit in decentralized decision  then a three level supply chain with revenue sharing contract and return policy is studied and the analytical expressions of the optimal order quantity for different members are derived  under the variable parametric possibility distribution of demand  we provide the sufficient conditions to ensure the three level supply chain can be fully coordinated and show the total mean profit of the channel can be allocated with any specified ratios among the members  finally  we provide some managerial implications in a practical supply chain coordination problem  the computational results demonstrate the efficiency of the proposed parametric credibilistic optimization method  
 coordinating open fleets  a taxi assignment example nowadays  vehicles of modern fleets are endowed with advanced devices that allow the operators of a control center to have global knowledge about fleet status  including existing incidents  fleet management systems support real time decision making at the control center so as to maximize fleet performance  in this paper  setting out from our experience in dynamic coordination of fleet management systems  we focus on fleets that are open  dynamic and highly autonomous  furthermore  we propose how to cope with the scalability problem as the number of vehicles grows  we present our proposed architecture for open fleet management systems and use the case of taxi services as example of our approach  we carried out some experiments  which showed our proposed algorithm outperform the most common assignment method both in waiting times of clients and taxi costs  
 cost efficiency measurement with fuzzy data in dea being a nonparametric method  data envelopment analysis  dea  is confined to measuring the efficiency of a collection of decision making units  dmus  consuming multiple crisp inputs to produce multiple crisp outputs  since not all data in the real world have determined values  and input and output values for dmus are often subject to fluctuation  the concept of fuzziness has been introduced to deal with such imprecise data  this study intends to evaluate the cost efficiency of dmus in three different scenarios  with one distinct model proposed for each scenario  the main idea is to use the alpha cut method and the extension principle to convert the fuzzy cost efficiency model into a family of conventional crisp dea models by obtaining one lower bound and one upper bound for the cost efficiency score of a dmu for any alpha varying between 0 and 1  when the lower and upper bounds are invertible with respect to alpha  the membership function of the fuzzy efficiency of a dmu which falls within the scope of parametric programming problems can be obtained by finding the inverse of the lower and upper bounds as well as employing the extension principle  in this case  the value of fuzzy cost efficiency varies between 0 and 1  otherwise  the cost efficiency scores of dmus can be specified as intervals  which are actually alpha cuts of the fuzzy membership function  by collecting results for different alpha values  furthermore  we demonstrate that farrell s decomposition also holds for cost efficiency with fuzzy data  in addition  all dmus are divided into three classes in each scenario  with cost efficient and cost inefficient dmus falling into independent classes  in other words  units which are cost inefficient in the upper bound for any alpha ranging between 0 and 1 will definitely be cost inefficient in the lower bound  too  and the units will be cost efficient in the upper bound if they are cost efficient in the lower bound for a specific alpha varying between 0 and 1  moreover  the upper bound of units that are cost inefficient in the lower bound could not be judged in terms of cost efficiency  finally  a practical example dealing with data on all branches of the national bank of iran across ardabil province  iran  during 2012 2014 is provided to demonstrate the applicability of the proposed method  
 cost estimation of building individual cooperative housing with crowdfunding model  case of beijing  china the crowdfunding model is to quickly gather petty cash funds from the mass and to organize and develop construction projects spontaneously  and it might help the housing buyers to reduce the expense that originally paid to real estate developers in general develop models and probably cut down the housing price  this paper aims to estimate the building costs of individual cooperative housing with crowdfunding model  a reference cost model of commercial building is firstly constructed and estimated  according to which the costs of housing projects can be estimated and the sales price can also be decomposed into different parts  and then the reference cost of eight typical housing construction projects in major districts of beijing in china  is calculated using the constructed model and compared with the corresponding sales prices  the results indicate that the proportion of total construction cost is 10 15   of sales prices and building housing with crowdfunding model can save about 20 30   of expense  and the spread of the individual cooperative hosing with crowdfunding model is expected to impose downwards pressure on housing price and help to solve the housing problems of medium income and low income families in major cities in china  therefore  it is considerable to support the development of individual cooperative housing with crowdfunding model  
 cost heterogeneity and peak prediction in collective actions the peak of participants indicates the success probability of collective actions  the mathematical model is built to explore the mechanism and prediction values of peaks  besides of utility heterogeneity  cost heterogeneity is added into to simulate the situation of multiple heterogeneities in reality  each simulation is run one hundred times repeatedly to get stable expectations and standard deviations of peaks under each combination of parameter values  based on results of simulation  effects of related factors on peaks is investigated and estimated statistically  making it possible to predict peaks  in addition to forecasting the mean of peaks  the variability of peaks is estimated as well  therefore  the distribution of peaks is predicted  utility heterogeneity  cost heterogeneity and the jointness of supply  j  exert significant effects on the distribution of peaks  it indicates that both utility heterogeneity and cost heterogeneity reduce the values and increase the variability  standard deviation  of peaks  facilitating chain actions among individuals  heterogeneity promotes the outbreak of collective actions  however  it reduces the peaks and decreases the success probability of collective actions  while homogeneity increases the peak of participants and enhances the success chance of collective actions   c  2017 elsevier ltd  all rights reserved  
 coto  a novel approach for fuzzy aggregation of semantic similarity measures semantic similarity measurement aims to determine the likeness between two text expressions that use different lexicographies for representing the same real object or idea  there are a lot of semantic similarity measures for addressing this problem  however  the best results have been achieved when aggregating a number of simple similarity measures  this means that after the various similarity values have been calculated  the overall similarity for a pair of text expressions is computed using an aggregation function of these individual semantic similarity values  this aggregation is often computed by means of statistical functions  in this work  we present coto  consensus or trade off  a solution based on fuzzy logic that is able to outperform these traditional approaches   c  2016 elsevier b v  all rights reserved  
 coupled hmm based multimodal fusion for mood disorder detection through elicited audio visual signals mood disorders encompass a wide array of mood issues  including unipolar depression  ud  and bipolar disorder  bd   in diagnostic evaluation on the outpatients with mood disorder  a high percentage of bd patients are initially misdiagnosed as having ud  it is crucial to establish an accurate distinction between bd and ud to make a correct and early diagnosis  leading to improvements in treatment and course of illness  in this study  eliciting emotional videos are firstly used to elicit the patients  emotions  after watching each video clips  their facial expressions and speech responses are collected when they are interviewing with a clinician  in mood disorder detection  the facial action unit  au  profiles and speech emotion profiles  eps  are obtained  respectively  by using the support vector machines  svms  which are built via facial features and speech features adapted from two selected databases using a denoising autoencoder based method  finally  a coupled hidden markov model  chmm  based fusion method is proposed to characterize the temporal information  the chmm is modified to fuse the aus and the eps with respect to six emotional videos  experimental results show the promising advantage and efficacy of the chmm based fusion approach for mood disorder detection  
 creating affective autonomous characters using planning in partially observable stochastic domains the ability to reason about and respond to their own emotional states can enhance the believability of non player characters  npcs   in this paper  we use a partially observable markov decision process  pomdp  based framework to model emotion over time  a two level appraisal model  involving quick and reactive vs  slow and deliberate appraisals  is proposed for the creation of affective autonomous characters based on pomdps  wherein the probability of goal satisfaction is used in an appraisal and reappraisal process for emotion generation  we not only extend probabilistic computation tree logic  pctl  for reasoning about the properties of emotional states based on pomdps but also illustrate how four reactive  primary  emotions and nine deliberate  secondary  emotions can be derived by combining pctl with the belief desire theory of emotion  the results of an empirical study suggest that the proposed model can be used to create characters that appear to be more believable and more intelligent  
 creating investment scheme with state space modeling this paper proposes a unified approach to creating investment strategies with various desirable properties for investors  particularly  we provide a new interpretation and the resulting formulations for state space models to attain our investment objectives  which are possibly specified as generating additional returns over benchmark stock indexes or achieving target risk adjusted returns  our state space models with particle filtering algorithm are employed to develop expert systems for investment strategies in highly complex financial markets  more concretely  in our state space framework  we apply a system model to representing portfolio weight processes with various constraints  as well as the standard underlying state variables such as volatility processes  further  we formulate an observation model to stand for target value processes with non linear functions of observed and latent variables  numerical experiments demonstrate the effectiveness of our methodology through creating excess returns over s p 500 and generating investment portfolios with fine risk return profiles   c  2017 elsevier ltd  all rights reserved  
 credibility in social media  opinions  news  and health information a survey in the social web scenario  where large amounts of user generated content diffuse through social media  the risk of running into misinformation is not negligible  for this reason  assessing and mining the credibility of both sources of information and information itself constitute nowadays a fundamental issue  credibility  also referred as believability  is a quality perceived by individuals  who are not always able to discern with their cognitive capacities genuine information from the fake one  for this reason  in the recent years several approaches have been proposed to automatically assess credibility in social media  most of them are based on data driven models  i e   they employ machine learning techniques to identify misinformation  but recently also model driven approaches are emerging  as well as graph based approaches focusing on credibility propagation  since multiple social applications have been developed for different aims and in different contexts  several solutions have been considered to address the issue of credibility assessment in social media  three of the main tasks facing this issue and considered in this article concern   1  the detection of opinion spam in review sites   2  the detection of fake news and spam in microblogging  and  3  the credibility assessment of online health information  despite the high number of interesting solutions proposed in the literature to tackle the above three tasks  some issues remain unsolved  they mainly concern both the absence of predefined benchmarks and gold standard datasets  and the difficulty of collecting and mining large amount of data  which has not yet received the attention it deserves   c  2017 john wiley   sons  ltd 
 criminal prediction using naive bayes theory the paper introduces a solution to the criminal prediction problem using na   ve bayes theory  the criminal prediction problem is stated as finding the most likely criminal of a particular crime incident when the history of crime incidents is given with the incident level crime data  the incident level crime data are assumed to be given as a crime dataset where the incident date and location  crime type  criminal id and the acquaintances are the attributes or crime parameters considered in the paper  the acquaintances are the suspects whose names are either directly involved in the incident or indirectly the acquaintances of the criminal  acquiring the crime dataset is a difficult process in practice due to confidentiality principle  so the crime dataset is generated synthetically using the state of the art methods  the proposed system is tested for the criminal prediction problem using the cross validation  and the experimental results show that the proposed system provides high scores in finding of suspected criminals  
 crm technology  implementation project and consulting services as determinants of success the success of a customer relationship management  crm  strategy depends on the adequate use of technology  including crm software  this paper or offers empirical evidence regarding critical success factors for crm software adoption  namely  implementation project management and services provided by information technology consultants  data analyses involving 208 business customers of an international crm software provider show that  1  successful management of the implementation project is a fundamental prerequisite for the firm to take full advantage of crm software   2  perceptions of tangible offers reliability  responsiveness  assurance  empathy  and training services provided by crm consultants affect crm success  and  3  the successful management of an implementation project mediates the erects of consulting service quality on successful crm software adoption  these findings point to the relevance of service in the crm software industry from the perspectives of both the professionals involved and the potential adopters of crm software  
 cross association analysis of eeg and emg signals according to movement intention state rehabilitation within three months plays a significant role in the recovery of damaged motor functions following the onset of a stroke  to increase the effectiveness of rehabilitation  it is important to perform rehabilitative exercises with movement intention  this study analyzed the association between electroencephalogram  eeg  and electromyogram  emg  signals in healthy individuals in an attempt to verify the differences between the two signals in corticomuscular connectivity as well as the time delay in the flow of information in accordance with the presence of movement intention  to examine the relationship between the brain and muscles  coherence and mutual information analyses were performed on the eeg signals in the motor cortex and emg signals in the flexor digitorum superficialis muscle during grasping training  coherence and mutual information between eeg and emg signals were significantly higher and the time delay of information flow was shorter when subjects performed active exercise with movement intention than when they performed passive exercise without movement intention  these findings could be applied to the rehabilitation of stroke patients to develop a rehabilitative training system with heightened effectiveness through verification of the presence of movement intention in the patients   c  2017 the authors  published by elsevier b v  
 cross dataset and cross cultural music mood prediction  a case on western and chinese pop songs in music mood prediction  regression models are built to predict values on several mood representing dimensions such as valence  level of pleasure  and arousal  level of energy   many studies have shown that music mood is generally predictable based on music acoustic features  but these experiments were mostly conducted on datasets with homogeneous music  little research has been done to explore the generalizability of mood regression models cross datasets  especially those with music in different cultures  in the increasingly global market of music listening  generalizable models are highly desirable for automated processing  searching and managing music collections with heterogeneous characteristics  in this study  we evaluated mood regression models built on fifteen acoustic features in five mood related musical aspects  with a focus on cross dataset generalizability  specifically  three distinct datasets were involved in a series of five experiments to examine the effects of dataset size  reliability of annotations and cultural backgrounds of music and annotators on mood regression performances and model generalizability  the results reveal that the size of the training dataset and the annotation reliability of the testing dataset affect mood regression performances  when both factors are controlled  regression models are generalizable between datasets sharing a common cultural background of music or annotators  
 cross entropy based multi objective uncertain portfolio selection problem in most real life investment situations future security returns are represented mainly based on expert s judgments due to the occurrence of unexpected incidents in economic and social changes or lack of historical data  in order to tackle such uncertainties  the returns of the securities are evaluated by the experts instead of historical data  in this study  a multi objective uncertain portfolio selection model has been proposed by defining average return as expected value  risk as variance and divergence among security returns as cross entropy where the security returns are considered as uncertain variables  the transformed deterministic form of the proposed model is presented by considering security returns as linear uncertain variables  the deterministic model is then solved by using two multi objective genetic algorithms  mogas   namely  nondominated sorting genetic algorithm ii  nsga ii  and archive based hybrid scatter search  abyss   we use a dataset from the shenzhen stock exchange to illustrate the performance of the algorithms  finally  a comparative study is performed in terms of certain performance matrices among nsga ii and abyss  
 cross linguistic cognitive modeling of verbal morphology acquisition how children acquire and process inflectional morphology is still an open question  despite the fact that english past tense acquisition has been studied and modeled in depth  the current approaches do not account for many of the errors made by humans  moreover  not much work has been done with highly inflected languages  like spanish  however  the modeling of any linguistic phenomenon in different languages is very important in order to understand the general cognitive processes underlying each particular phenomenon  this paper presents an act r dual mechanism model that accomplishes the task of acquiring verbal morphology systems from one of the simplest systems  the english one  to one of the most complex systems  the spanish one   by using a double analogy process of stem and suffix  the model proposed was able to match all types of errors that developing children make  from a sample of them   both in english and spanish  the models for both languages used very similar parameters  the introduced approach not only shows how children could acquire a highly inflected morphology system in terms of dual mechanism theories but  given its cross linguistic character  also sheds light on the possible general processes involved in the acquisition and processing of inflectional morphology  
 crowd sourcing prosodic annotation much of what is known about prosody is based on native speaker intuitions of idealized speech  or on prosodic annotations from trained annotators whose auditory impressions are augmented by visual evidence from speech waveforms  spectrograms and pitch tracks  expanding the prosodic data currently available to cover more languages  and to cover a broader range of unscripted speech styles  is prohibitive due to the time  money and human expertise needed for prosodic annotation  we describe an alternative approach to prosodic data collection  with coarse grained annotations from a cohort of untrained annotators performing rapid prosody transcription  rpt  using lmeds  an open source software tool we developed to enable large scale  crowd sourced data collection with rpt  results from three rpt experiments are reported  the reliability of rpt is analysed comparing kappa statistics for lab based and crowd sourced annotations for american english  comparing annotators from the same  us  versus different  indian  dialect groups  and comparing each rpt annotator with a tobi annotation  results show better reliability for same dialect annotators  us   and the best overall reliability from crowd sourced us annotators  though lab based annotations are the most similar to tobi annotations  a generalized additive mixed model is used to test differences among annotator groups in the factors that predict prosodic annotation  results show that a common set of acoustic and contextual factors predict prosodic labels for all annotator groups  with only small differences among the rpt groups  but with larger effects on prosodic marking for tobi annotators  the findings suggest methods for optimizing the efficiency of rpt annotations  overall  crowd sourced prosodic annotation is shown to be efficient  and to rely on established cues to prosody  supporting its use for prosody research across languages  dialects  speaker populations  and speech genres   c  2017 elsevier ltd  all rights reserved  
 crowdsourced healthcare knowledge creation using patients  health experience ontologies in this research  we developed chekc framework for creation and integration of crowdsourced healthcare knowledge using experience ontologies  the purpose is to provide patients  healthcare information which contains similar healthcare experiences including conditions and symptoms and integrates the features and relations in the particular patients  data according to users  queries  to do this  we developed three modules and ontologies  the modules are crowdsourced health data manipulation module  chmm   health ontology based relevant patient finding module  hrfm   and ontology guided healthcare knowledge integration module  okim   chmm is developed to transform patients  data to structured cases with problem solution  the cases are stored in chekc patient ontology  hrfm is developed to find relevant cases according to the user s query using chekc upper ontology  to do this  ensemble semantic similarity is proposed using semantic similarity and fuzzy c means clustering and the relevant cases are stored in interim ontology  okim is developed for the integration of the relevant cases using swrl rule base  however  it is not guaranteed to find suitable rules and generate necessary knowledge from the rule base  to relieve the problem  ontology guided knowledge integration is proposed  which supports the inferring relations among classes in chekc interim ontology  chekc framework provides the integrated healthcare information and knowledge which are generated through the illustrated processes using the selected similar healthcare cases with users  query  in particular  the cases are constructed by crowdsourcing on healthcare featured social media and are based on patients  healthcare experiences from the perspectives of patients  through the conducting of two experiments  we proved the effectiveness of chekc framework  the conducted experiments proved the efficiency of chekc framework by the reduction in search volumes and the improvement in accuracy of query results  
 cultural transmission and evolution of melodic structures in multi generational signaling games it has been proposed that languages evolve by adapting to the perceptual and cognitive constraints of the human brain  developing  in the course of cultural transmission  structural regularities that maximize or optimize learnability and ease of processing  to what extent would perceptual and cognitive constraints similarly affect the evolution of musical systems  we conducted an experiment on the cultural evolution of artificial melodic systems  using multi generational signaling games as a laboratory model of cultural transmission  signaling systems  using five tone sequences as signals  and basic and compound emotions as meanings  were transmitted from senders to receivers along diffusion chains in which the receiver in each game became the sender in the next game  during transmission  structural regularities accumulated in the signaling systems  following principles of proximity  symmetry  and good continuation  although the compositionality of signaling systems did not increase significantly across generations  we did observe a significant increase in similarity among signals from the same set  we suggest that our experiment tapped into the cognitive and perceptual constraints operative in the cultural evolution of musical systems  which may differ from the mechanisms at play in language evolution and change  
 current research in eye movement biometrics  an analysis based on bioeye 2015 competition on the onset of the second decade of research in eye movement biometrics  the already demonstrated results strongly support the promising perspectives of the field  this paper presents a description of the research conducted in eye movement biometrics based on an extended analysis of the characteristics and results of the  bioeye 2015  competition on biometrics via eye movements   this extended presentation can contribute to the understanding of the current level of research in eye movement biometrics  covering areas such as the previous work in the field  the procedures for the creation of a database of eye movement recordings  and the different approaches that can be used for the analysis of eye movements  also  the presented results from bioeye 2015 competition can demonstrate the potential identification accuracy that can be achieved under easier and more difficult scenarios  based on the provided presentation  we discuss topics related to the current status in eye movement biometrics and suggest possible directions for the future research in the field   c  2016 elsevier b v  all rights reserved  
 current research trends and application areas of fuzzy and hybrid methods to the risk assessment of construction projects fuzzy and hybrid methods have been increasingly used in construction risk management research and this study aims to compile and analyse the basic concepts and methods applied in this field to date  a content analysis is made of a comprehensive literature review of publications during 2005 2017  it is found that the nature of complex projects is such that most risks are interdependent of each other  therefore  a fuzzy structured method such as the fuzzy analytical network process  fanp  has frequently been used for different complex projects  however  the application of fanp is limited because of the tedious and lengthy calculations required for the pairwise comparisons needed and an inability to incorporate new information into the risk structure  to overcome this constraint  a fuzzy bayesian belief network  fbbn  has been increasingly used for risk assessment  further project specific studies based on fbbn are recommended to justify its broader application  beyond fuzzy methods  the credal network an extended form of bayesian network  is found to have potential for risk assessment under uncertainty   c  2017 elsevier ltd  all rights reserved  
 cyberbullying detection based on semantic enhanced marginalized denoising auto encoder as a side effect of increasingly popular social media  cyberbullying has emerged as a serious problem afflicting children  adolescents and young adults  machine learning techniques make automatic detection of bullying messages in social media possible  and this could help to construct a healthy and safe social media environment  in this meaningful research area  one critical issue is robust and discriminative numerical representation learning of text messages  in this paper  we propose a new representation learning method to tackle this problem  our method named semantic enhanced marginalized denoising auto encoder  smsda  is developed via semantic extension of the popular deep learning model stacked denoising autoencoder  sda   the semantic extension consists of semantic dropout noise and sparsity constraints  where the semantic dropout noise is designed based on domain knowledge and the word embedding technique  our proposed method is able to exploit the hidden feature structure of bullying information and learn a robust and discriminative representation of text  comprehensive experiments on two public cyberbullying corpora  twitter and myspace  are conducted  and the results show that our proposed approaches outperform other baseline text representation learning methods  
 daehr  a discriminant analysis framework for electronic health record data and an application to early detection of mental health disorders electronic health records  ehr  provide a rich source of temporal data that present a unique opportunity to characterize disease patterns and risk of imminent disease  while many data mining tools have been adopted for ehr based disease early detection  linear discriminant analysis  lda  is one of the most commonly used statistical methods  however  it is difficult to train an accurate lda model for early disease diagnosis when too few patients are known to have the target disease  furthermore  ehr data are heterogeneous with significant noise  in such cases  the covariance matrices used in lda are usually singular and estimated with a large variance  this article presents daehr  an extension of the lda framework using electronic health record data to address these issues  beyond existing lda analyzers  we propose daehr to  1  eliminate the data noise caused by the manual encoding of ehr data and  2  lower the variance of parameter  covariance matrices  estimation for lda models when only a few patients  ehr are available for training  to achieve these two goals  we designed an iterative algorithm to improve the covariance matrix estimation with embedded data noise parameter variance reduction for lda  we evaluated daehr extensively using the college health surveillance network  a large  real world ehr dataset  specifically  our experiments compared the performance of lda to three baselines  i e   lda and its derivatives  in identifying college students at high risk for mental health disorders from 23 u s  universities  experimental results demonstrate daehr significantly outperforms the three baselines by achieving 1 4  19 4  higher accuracy and a 7 5  43 5  higher f1 score  
 data analysis for metropolitan economic and logistics development logistics industry is an integral sector encompassing transportation  warehousing  handling  circulation and processing  delivery and information technology  with the progress of economic globalization and integration  logistics industry has become a new momentum driving the fast development of national and regional economy  the close relationship between economic development and logistics advancement receives wide attention from the academia  however  current research on the coordination between economy and logistics mostly focuses on concept interpretation  and qualitative discussions  very rarely do scholars conduct quantitative analysis on the coordination of metropolitan economy and logistics  to fill this gap  we first examine whether there exist interactions between metropolitan logistics and economy by building evaluation index systems for metropolitan logistics and economy  then we introduce the entropy method and granger causality test to evaluate and test the level of logistics and economic development in five cities  beijing  shanghai  guangzhou  chongqing  and tianjin from 2009 to 2013  from the dimensions of regional economic investment  regional economic capacity and strength  we finally test the relationship between three economic subsystems and three logistics subsystems to further validate the relationship between metropolitan economy and logistics   c  2017 elsevier ltd  all rights reserved  
 data analytics enhanced component volatility model volatility modelling and forecasting have attracted many attentions in both finance and computation areas  recent advances in machine learning allow us to construct complex models on volatility forecasting  however  the machine learning algorithms have been used merely as additional tools to the existing econometrics models  the hybrid models that specifically capture the characteristics of the volatility data have not been developed yet  we propose a new hybrid model  which is constructed by a low pass filter  the autoregressive neural network and an autoregressive model  the volatility data is decomposed by the low pass filter into long and short term components  which are then modelled by the autoregressive neural network and an autoregressive model respectively  the total forecasting result is aggregated by the outputs of two models  the experimental evaluations using one hour and one day realized volatility across four major foreign exchanges showed that the proposed model significantly outperforms the component garch  egarch and neural network only models in all forecasting horizons   c  2017 elsevier ltd  all rights reserved  
 data envelopment analysis based on choquet integral data envelopment analysis  dea  is a widely used technique in decision making  the existing dea models always assume that the inputs  or outputs  of decision making units  dmus  are independent with each other  however  there exist positive or negative interactions between inputs  or outputs  of dmus  to reflect such interactions  choquet integral is applied to dea  self efficiency models based on choquet integral are first established  which can obtain more efficiency values than the existing ones  then  the idea is extended to the cross efficiency models  including the game cross efficiency models  the optimal analysis of dea is further investigated based on regret theory  to estimate the ranking intervals of dmus  several models are also established  it is founded that the models considering the interactions between inputs  or outputs  can obtain wider ranking intervals  
 data envelopment analysis with common weights in a fuzzy environment this work considers providing a common base for measuring the relative efficiency of a group of homogeneous decision making units in a fuzzy environment  the principle of compromise of the technique for order preference by similarity ideal solution is employed for solving the data envelopment analysis model with fuzzy objectives and constraints  an algorithm with the entropic regularization implementation for finding the compromise solution of the fuzzy data envelopment analysis model is developed  an illustrative example verifying the idea of this paper is provided  the contribution of this work is represented by the improvement of the discriminatory power of the fuzzy dea  gained through the common weight evaluation  
 data mining based variable assessment methodology for evaluating the contribution of knowledge services of a public research institute to business performance of firms this study proposes a methodology for assessing the contribution of knowledge services  kss  provided by a korean public research institute to the business performance of firms  a new methodology based on a data mining based variable assessment method in a regression model is proposed for the service level assessment  the contribution of the kss to firms  business performance is analyzed using their attributes and specific business performance indicators through the conditional variable permutation method in the random forest regression  this reduces the ambiguity in variable importance caused by the correlations among input variables  the proposed methodology is applied to the survey dataset collected from firms  the survey dataset is examined 1  for the whole data and 2  for a subset of the data  namely  small and medium sized enterprises  smes   the empirical results show behavioral properties of firms with regard to the given kss in general and smes in particular  practical and user friendly service product types increase the firms  expectation on business performance  also  flexibility in the service products helps firms acquire much needed knowledge and boosts their expectation on business performance  in particular  smes expect better business performance from the kss that help them create business plans and strategies   c  2017 elsevier ltd  all rights reserved  
 data quality assessment of maintenance reporting procedures today s largest and fastest growing companies  assets are no longer physical  but rather digital  software  algorithms      this is all the more true in the manufacturing  and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures  if quality of the reported data is too low  it can results in wrong decision making and loss of money  furthermore  various maintenance experts are involved and directly concerned about the quality of enterprises  daily maintenance data reporting  e g   maintenance planners  plant managers      each one having specific needs and responsibilities  to address this multi criteria decision making  mcdm  problem  and since data quality is hardly considered in existing expert maintenance systems  this paper develops a maintenance reporting quality assessment  mrqa  dashboard that enables any company stakeholder to easily   and in real time   assess rank company branch offices in terms of maintenance reporting quality  from a theoretical standpoint  ahp is used to integrate various data quality dimensions as well as expert preferences  a use case describes how the proposed mrqa dashboard is being used by a finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices   c  2016 elsevier ltd  all rights reserved  
 data driven agent based modeling  with application to rooftop solar adoption agent based modeling is commonly used for studying complex system properties emergent from interactions among agents  however  agent based models are often not developed explicitly for prediction  and are generally not validated as such  we therefore present a novel data driven agent based modeling framework  in which individual behavior model is learned by machine learning techniques  deployed in multi agent systems and validated using a holdout sequence of collective adoption decisions  we apply the framework to forecasting individual and aggregate residential rooftop solar adoption in san diego county and demonstrate that the resulting agent based model successfully forecasts solar adoption trends and provides a meaningful quantification of uncertainty about its predictions  meanwhile  we construct a second agent based model  with its parameters calibrated based on mean square error of its fitted aggregate adoption to the ground truth  our result suggests that our data driven agent based approach based on maximum likelihood estimation substantially outperforms the calibrated agent based model  seeing advantage over the state of the art modeling methodology  we utilize our agent based model to aid search for potentially better incentive structures aimed at spurring more solar adoption  although the impact of solar subsidies is rather limited in our case  our study still reveals that a simple heuristic search algorithm can lead to more effective incentive plans than the current solar subsidies in san diego county and a previously explored structure  finally  we examine an exclusive class of policies that gives away free systems to low income households  which are shown significantly more efficacious than any incentive based policies we have analyzed to date  
 data driven deep syntactic dependency parsing deep syntactic  dependency structures that capture the argumentative  attributive and coordinative relations between full words of a sentence have a great potential for a number of nlp applications  the abstraction degree of these structures is in between the output of a syntactic dependency parser  connected trees defined over all words of a sentence and language specific grammatical functions  and the output of a semantic parser  forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the frame structures of predicative elements and drop all attributive and coordinative dependencies   we propose a parser that provides deep syntactic structures  the parser has been tested on spanish  english and chinese  
 dealing with endogeneity in data envelopment analysis applications although the presence of the endogeneity is frequently observed in economic production processes  it tends to be overlooked when practitioners apply data envelopment analysis  dea   in this paper we deal with this issue in two ways  first  we provide a simple statistical heuristic procedure that enables practitioners to identify the presence of endogeneity in an empirical application  second  we propose the use of an instrumental input dea  ii dea  as a potential tool to address this problem and thus improve dea estimations  a monte carlo experiment confirms that the proposed ii dea approach outperforms standard dea in finite samples under the presence of high positive endogeneity  to illustrate our theoretical findings  we perform an empirical application on the education sector   c  2016 elsevier ltd  all rights reserved  
 deceptive text detection using continuous semantic space models we identify deceptive text by using different kinds of features  a continuous semantic space model based on latent dirichlet allocation topics  lda   one hot representation  ohr   syntactic information from syntactic n grams  sn   and lexicon based features using the linguistic inquiry and word count dictionary  liwc   several combinations of these features were tested to assess the best source s  for deceptive text identification  by selecting the appropriate features  we were able to obtain a benchmark level performance using a naive bayes classifier  we tested on three different available corpora  a corpus consisting of 800 reviews about hotels  a corpus consisting of 600 reviews about controversial topics  and a corpus consisting of 236 book reviews  we found that the merge of both lda features and ohr yielded the best results  obtaining accuracy above 80  in all tested datasets  additionally  this combination of features has the advantage that language specific resources are not required  e g  sn  liwc   compared to other reference works  additionally  we present an analysis on which features lead to either deceptive or truthful texts  finding that certain words can play different roles  sometimes even opposing ones  depending on the task being evaluated  
 decision mechanism for supplier selection under sustainability against the backdrop of responsible economic development  sustainable supply chain management  sscm  is key to achieving the sustainable development for enterprise and industry  in this regard  sustainable supplier selection is crucial in sscm  by integrating the three dimensions of sustainability  economic  environmental and social  this paper presents a new evaluation system for supplier selection from a sustainability perspective  specifically  we design a decision mechanism for sustainable supplier selection based on linguistic 2 tuple grey correlation degree  in this proposed mechanism  the hybrid attribute values whereby real numbers  interval numbers and linguistic fuzzy variables coexist are transformed into linguistic 2 tuples  a ranking method based on linguistic 2 tuple grey correlation degree is then presented to rank the suppliers  an application example is presented to highlight the implementation  availability and feasibility of the proposed decision making mechanism  
 decision support framework for selection of 3pl service providers  dominance based approach in combination with grey set theory since past two decades  logistics outsourcing has got immense importance in the context of supply chain management  logistics outsourcing or outsourcing of third party logistics  3pl  comprises involvement of outside firms to execute logistics activities of a firm efficiently  as deployment of 3pl providers has gained huge momentum in recent times  appropriate selection of 3pl providers seems indeed a necessity  the present work intends to propose a decision support framework for evaluation and selection of 3pl providers in pursuit of fulfilling various business needs  a decision support framework based on dominance measure concept integrated with grey set theory has been projected herein  result obtained thereof has been compared to that of grey topsis  a case empirical research has also been reported  the proposed dominance based decision support system in light of grey set theory has been conceptualized and is basically a simplified version of todim and promethee  it explores dominance between two alternatives with respect to a particular criterion  based on which a global dominance measure is computed to facilitate deriving ranking order of candidate 3pl providers  the proposed approach delineated in this research seems straightforward  easy to execute and can exclude complex and tedious computational steps of todim as well as promethee  
 decision support to customer decrement detection at the early stage for theme parks in recent years  a theme park drives significant attention in tourism industry due to the provision of quality and integrated service  and issuing annual pass cards help the theme park to differentiate long term customers from short term ones  customer value analysis is demanded for theme parks to identify potential customers as well as to appraise customer value through the setting of the annual pass  moreover  customer value often alters from time to time since theme park industry is relevantly competitive and innovation demanded than other industries  and customer preferences are frequently changed  this study provides an early warning system to support the theme park to detect  monitor and analyze the changes of customer value  by applying the aggregated approach based on rough set theory and recency  frequency and monetary architecture  the tourist satisfaction levels can be captured after the aforementioned approach is executed  in addition  the rule comparison approach is contributed to predicting customer behavior from technical viewpoint  this study aims at providing an early correction strategy for the theme park to avoid losing vip customers and identify latent customers   c  2017 elsevier b v  all rights reserved  
 decision theoretic modeling of affective and cognitive needs for product experience engineering  key issues and a conceptual framework user experience  ux  design plays a critical role in product experience engineering  to create a theoretical foundation of ux design  it is imperative to develop mathematical and computational models for elicitation  quantification  evaluation and reasoning of affective cognitive needs that are inherent in the fulfillment of user experience  this paper explores the key research issues for understanding how human users  subjective experience and affective prediction impact their choice behavior under uncertainty  a conceptual framework is envisioned by extending prospect theory in the field of behavioral economics to the modeling of user experience choice behavior  in which inference of affective influence is enacted through the shape parameters of prospect value functions  
 decision making and opinion formation in simple networks in many networked decision making settings  information about the world is distributed across multiple agents and agents  success depends on their ability to aggregate and reason about their local information over time  this paper presents a computational model of information aggregation in such settings in which agents  utilities depend on an unknown event  agents initially receive a noisy signal about the event and take actions repeatedly while observing the actions of their neighbors in the network at each round  such settings characterize many distributed systems such as sensor networks for intrusion detection and routing systems for internet traffic  using the model  we show that  1  agents converge in action and in knowledge for a general class of decision making rules and for all network structures   2  all networks converge to playing the same action regardless of the network structure  and  3  for particular network configurations  agents can converge to the correct action when using a well defined class of myopic decision rules  these theoretical results are also supported by a new simulation based open source empirical test bed for facilitating the study of information aggregation in general networks  
 decision making in a real time business simulation game  cultural and demographic aspects in small group dynamics simulated virtual realities or er a promising but currently underutilized source of data in studying cultural and demographic aspects of dynamic decision making  ddm  in small groups  this study focuses on one simulated reality  a clock driven business simulation game  which is used to teach operations management  the purpose of our study is to analyze the characteristics of the decision making groups  such as cultural orientation  education  gender and group size  and their relationship to group performance in a real time processed simulation game  our study examines decision making in small groups of two or three employees from a global manufacturing and service operations company  we aim at shedding new light on how such groups with diverse background profiles perform as decision making units  our results reveal that the profile of the decision making group influences the outcome of decision making  the final business result of the simulation game  in particular  the cultural and gender diversity  as well as group size seem to have intertwined effects on team performance  
 deep direct reinforcement learning for financial signal representation and trading can we train the computer to beat experienced traders for financial assert trading  in this paper  we try to address this challenge by introducing a recurrent deep neural network  nn  for real time financial signal representation and trading  our model is inspired by two biological related learning concepts of deep learning  dl  and reinforcement learning  rl   in the framework  the dl part automatically senses the dynamic market condition for informative feature learning  then  the rl module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment  the learning system is implemented in a complex nn that exhibits both the deep and recurrent structures  hence  we propose a task aware backpropagation through time method to cope with the gradient vanishing issue in deep training  the robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions  
 deep learning and punctuated equilibrium theory deep learning is associated with the latest success stories in ai  in particular  deep neural networks are applied in increasingly different fields to model complex processes  interestingly  the underlying algorithm of backpropagation was originally designed for political science models  the theoretical foundations of this approach are very similar to the concept of punctuated equilibrium theory  pet   the article discusses the concept of deep learning and shows parallels to pet  a showcase model demonstrates how deep learning can be used to provide a missing link in the study of the policy process  the connection between attention in the political system  as inputs  and budget shifts  as outputs    c  2017 elsevier b v  all rights reserved  
 deep learning networks for stock market analysis and prediction  methodology  data representations  and case studies we offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction  its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies  deep learning algorithms vary considerably in the choice of network structure  activation function  and other model parameters  and their performance is known to depend heavily on the method of data representation  our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction  using high frequency intraday stock returns as input data  we examine the effects of three unsupervised feature extraction methods principal component analysis  autoencoder  and the restricted boltzmann machine on the network s overall ability to predict future market behavior  empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance  the same cannot be said when the autoregressive model is applied to the residuals of the network  covariance estimation is also noticeably improved when the predictive network is applied to covariance based market structure analysis  our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction   c  2017 elsevier ltd  all rights reserved  
 deep neural network approaches for speech recognition with heterogeneous groups of speakers including children this paper introduces deep neural network  dnn  hidden markov model  hmm  based methods to tackle speech recognition in heterogeneous groups of speakers including children  we target three speaker groups consisting of children  adult males and adult females  two different kind of approaches are introduced here  approaches based on dnn adaptation and approaches relying on vocal tract length normalisation  vtln   first  the recent approach that consists in adapting a general dnn to domain language specific data is extended to target age gender groups in the context of dnn hmm  then  vtln is investigated by training a dnn hmm system by using either mel frequency cepstral coefficients normalised with standard vtln or mel frequency cepstral coefficients derived acoustic features combined with the posterior probabilities of the vtln warping factors  in this later  novel  approach the posterior probabilities of the warping factors are obtained with a separate dnn and the decoding can be operated in a single pass when the vtln approach requires two decoding passes  finally  the different approaches presented here are combined to take advantage of their complementarity  the combination of several approaches is shown to improve the baseline phone error rate performance by thirty per cent to thirty five per cent relative and the baseline word error rate performance by about ten per cent relative  
 deforming time in a nonadditive discount function the aim of this paper is to present a methodology to generate a partially subadditive  respectively superadditive  discount function starting from an entirely subadditive  respectively superadditive  discount model  to do this  we are going to use the concept of deformation of time in a discount function  focusing on stevens  power law  a deformation of time is a function that mathematically represents the perception from an individual or a group of individuals about how the calendar time evolves  this approach will be important when describing the treatment of addictions and other diseases in patients who show a certain degree of impulsivity in their intertemporal choice   c  2016 wiley periodicals  inc  
 design for product experience  a study on the analepsis construction of product use in the field of product design and development  product experience has started attracting a great deal of attention  however  assisting product designers to understand the concept of product experience  what sort of experiences they want for their products and how to include these into their design are topics that still need to be addressed  the aim of this study is to propose a design approach that begins with the use of narration and collects narration cases of product use for a specific product experience  this design approach guides designers through a four step process  to develop a design model that fits in with the product experience  in order to better understand how this design approach works  this study takes the concept of analepsis as an example of product experience to demonstrate the process  the definition of analepsis  a term used in narratology  is  an event that took place earlier than the point in the story where we are at any given moment   when the term is used in the product design field  it refers to use past experiences to solve current problems  this study explores the conceptual framework of the topic and proposes a product experience orientated design model for product design  an issue that has rarely been discussed before  directions for future research are also discussed  
 design of variable friction devices for shoe floor contact in rehabilitation training  high fidelity simulation environments are needed for reproducing the effects of slippery surfaces  in which potential balance failure conditions can be reproduced on demand  motivated by these requirements  this article considers the design of variable friction devices for use in the context of human walking on surfaces in which the coefficient of friction can be controlled dynamically  various designs are described  aiming at rendering low friction shoe floor contact  associated with slippery surfaces such as ice  as well as higher friction values more typical of surfaces such as pebbles  sand  or snow  these designs include an array of omnidirectional rolling elements  a combination of low  and high friction coverings whose contact pressure distribution is controlled  and modulation of low frequency vibration normal to the surface  our experimentation investigated the static coefficient of friction attainable with each of these designs  rolling elements were found to be the most slippery  providing a coefficient of friction as low as 0 03  but with significant drawbacks from the perspective of our design objectives  a controlled pressure distribution of low  and high friction coverings allowed for a minimum coefficient of friction of 0 06  the effects of vibration amplitude and frequency on sliding velocity were also explored  increases in amplitude resulted in higher velocities  but vibration frequencies greater than 25 hz reduced sliding velocities  to meet our design objectives  a novel approach involving a friction variation mechanism  embedded in a shoe sole  is proposed   c  2017 elsevier ltd  all rights reserved  
 designing  artificial  people to serve   the other side of the coin this paper addresses the issue of the ethical obligations of human beings towards the robots that will achieve the status of persons  in particular the text investigates the ethical status of designing such robot persons as servants  the author disagrees with steve petersen   who claims that we can design robot persons as servants without wronging them by implementing the desire to serve into them  following jurgen habermas critique of positive liberal eugenics  the author argues that any kind of intentional designing inevitably wrongs the designed beings regarding their freedom  autonomy  equality and identity  moreover  some unintended consequences of developing robot person servants are discussed  
 designing a source level debugger for cognitive agent programs when an agent program exhibits unexpected behaviour  a developer needs to locate the fault by debugging the agent s source code  the process of fault localisation requires an understanding of how code relates to the observed agent behaviour  the main aim of this paper is to design a source level debugger that supports single step execution of a cognitive agent program  cognitive agents execute a decision cycle in which they process events and derive a choice of action from their beliefs and goals  current state of the art debuggers for agent programs provide insight in how agent behaviour originates from this cycle but less so in how it relates to the program code  as relating source code to generated behaviour is an important part of the debugging task  arguably  a developer also needs to be able to suspend an agent program on code locations  we propose a design approach for single step execution of agent programs that supports both code based as well as cycle based suspension of an agent program  this approach results in a concrete stepping diagram ready for implementation and is illustrated by a diagram for both the goal and jason agent programming languages  and a corresponding full implementation of a source level debugger for goal in the eclipse development environment  the evaluation that was performed based on this implementation shows that agent programmers prefer a source level debugger over a purely cycle based debugger  
 designing and implementing transparency for real time inspection of autonomous robots the epsrc s principles of robotics advises the implementation of transparency in robotic systems  however research related to ai transparency is in its infancy  this paper introduces the reader of the importance of having transparent inspection of intelligent agents and provides guidance for good practice when developing such agents  by considering and expanding upon other prominent definitions found in literature  we provide a robust definition of transparency as a mechanism to expose the decision making of a robot  the paper continues by addressing potential design decisions developers need to consider when designing and developing transparent systems  finally  we describe our new interactive intelligence editor  designed to visualise  develop and debug real time intelligence  
 designing flexible service systems  application to machine tools service system design process is complex in nature and involves many stakeholders  the process becomes more critical in the context of machine tools due to market competition  complexity of services  competency requirements and cost constraints  the objective of this paper is to propose a generic approach for the design of a service system for machine tools which can support a variety of business models along with meeting customers  demand for flexibility  the proposed approach makes use of fundamental service design steps and offers flexibility to the customers to choose the services  service mechanisms  service levels and service frequencies as per their requirements considering the economic aspects  and thus underlines the involvement and participation of customers in the service design process  it also facilitates service designers to visualize the roles of stakeholders involved in the service process and highlights the importance of customer involvement for customer service assurance and better service experience  application of the approach is discussed through a case study conducted in association with a leading indian machine tool manufacturing group  
 designing utilization based spatial healthcare accessibility decision support systems  a case of a regional health plan in the u s   myriad healthcare reforms have begun to show some positive effects on enabling  potential access   one facet of healthcare access   having access   which is the availability and accessibility of health services for the surrounding populations  has not been adequately addressed  research regarding  having access  is presently championed by a family of methods called floating catchment area  fca   however  existing scholarship is limited in integrating non spatial factors within the fca methods  in this research  we propose a novel utilization based framework as the first attempt to adopt the behavioral model of health services use as a theoretical lens to integrate non spatial factors in spatial healthcare accessibility research  the framework employs a unique approach to derive categorical and factor weights for different population subgroup s healthcare needs using predictive analytics  the proposed framework is evaluated using a case study of a regional health plan  a spatial decision support system  sdss  instantiates the framework and enables decision makers to explore physician shortage areas  the sdss validates the practicality of the proposed utilization based framework and subsequently allows other fca methods to be implemented in real world applications   c  2017 elsevier b v  all rights reserved  
 detecting and predicting the topic change of knowledge based systems  a topic based bibliometric analysis from 1991 to 2016 the journal knowledge based systems  knosys  has been published for over 25 years  during which time its main foci have been extended to a broad range of studies in computer science and artificial intelligence  answering the questions   what is the knosys community interested in   and  how does such interest change over time   are important to both the editorial board and audience of knosys  this paper conducts a topic based bibliometric study to detect and predict the topic changes of knosys from 1991 to 2016  a latent dirichlet allocation model is used to profile the hotspots of knosys and predict possible future trends from a probabilistic perspective  a model of scientific evolutionary pathways applies a learning based process to detect the topic changes of knosys in sequential time slices  six main research areas of knosys are identified  i e   expert systems  machine learning  data mining  decision making  optimization  and fuzzy  and the results also indicate that the interest of knosys communities in the area of computational intelligence is raised  and the ability to construct practical systems through knowledge use and accurate prediction models is highly emphasized  such empirical insights can be used as a guide for knosys submissions   c  2017 elsevier b v  all rights reserved  
 detecting and quantifying extended landscape structure with spatial co occurrence surfaces the attribute adjacency matrix is a fundamental component of many metrics used to characterize landscape heterogeneity from land cover land use maps  since it quantifies adjacent pixel class co occurrence  it is unsuited to detect broader scale structure in land cover maps  this paper proposes a generalization of the adjacency matrix concept by incorporating lag distances into class co occurrence estimation  the spectrum of spatial structure is presented in the form of a spatial co occurrence surface  these surfaces can provide a wealth of information on landscape structure including the size and spatial distribution of patches of a single class as well as inter class spatial associations  
 detecting communities of authority and analyzing their influence in dynamic social networks users in real world social networks are organized into communities that differ from each other in terms of influence  authority  interest  size  etc  this article addresses the problems of detecting communities of authority and of estimating the influence of such communities in dynamic social networks  these are new issues that have not yet been addressed in the literature  and they are important in applications such as marketing and recommender systems  to facilitate the identification of communities of authority  our approach first detects communities sharing common interests  which we call  meta communities  by incorporating topic modeling based on users  community memberships  then  communities of authority are extracted with respect to each meta community  using a new measure based on the betweenness centrality  to assess the influence between communities over time  we propose a new model based on the granger causality method  through extensive experiments on a variety of social network datasets  we empirically demonstrate the suitability of our approach for community of authority detection and assessment of the influence between communities over time  
 detecting driving stress in physiological signals based on multimodal feature analysis and kernel classifiers monitoring driving status has great potential in helping us decline the occurrence probability of traffic accidents and the aim of this research is to develop a novel system for driving stress detection based on multimodal feature analysis and kernel based classifiers  physiological signals such as electrocardiogram  galvanic skin response and respiration were record from fourteen drives executed in a prescribed route at real drive environments  features were widely extracted from time  spectral and wavelet multi domains  in order to search for the optimal feature sets  sparse bayesian learning  sbl  and principal component analysis  pca  were combined and adopted  kernel based classifiers were employed to improve the accuracy of stress detection task  analysis i used features from 10 s intervals of data which were recorded during well defined rest  highway and city driving conditions to discriminate three levels of diving stress achieving an averaging accuracy over 99  at per drive level and 89  in cross drive validation  analysis ii made continuous stress evaluation throughout a complete driving test attaining a high coincidence with the true road situation especially at the switching interval of traffic conditions  experimental results reveal that different levels of driving stress can be characterized by specific set of physiological measures  these physiological measures could be applied to in vehicle intelligent systems in various approaches to help the drivers better manage their negative driving status  our design scheme for driving stress detection could also facilitate the development of similar in vehicle expert systems  such as driver s emotion management  driver s sleeping onset monitoring  and human computer interaction  hci    c  2017 published by elsevier ltd  
 detecting financial misstatements with fraud intention using multi class cost sensitive learning we develop multi class financial misstatement detection models to detect misstatements with fraud intention  hennes  leone and miller  2008  conducted a post event analysis of financial restatements and classified restatements as intentional or unintentional  using their results  along with non misstated firms  in the form of a three class target variable  we develop three multi class classifiers  multinomial logistic regression  support vector machine  and bayesian networks  as predictive tools to detect and classify misstatements according to the presence of fraud intention  to deal with class imbalance and asymmetric misclassification costs  we undertake cost sensitive learning using metacost  we evaluate features from previous studies of detecting fraudulent intention and material misstatements  features such as the short interest ratio and the firm efficiency measure show discriminatory potential  the yearly and quarterly context based feature set created further improves the performance of the classifiers   c  2016 elsevier ltd  all rights reserved  
 detecting financial restatements using data mining techniques financial restatements have been a major concern for the regulators  investors and market participants  most of the previous studies focus only on fraudulent  or intentional  restatements and the literature has largely ignored unintentional restatements  earlier studies have shown that large scale unintentional restatements can be equally detrimental and may erode investors  confidence  therefore it is important for us to pay a close to the significant unintentional restatements as well  a lack of focus on unintentional restatements could lead to a more relaxed internal control environment and lessen the efforts for curbing managerial oversights and instances of misreporting  in order to address this research gap  we focus on developing predictive models based on both intentional  fraudulent  and unintentional  erroneous  financial restatements using a comprehensive real dataset that includes 3 513 restatement cases over a period of 2001 to 2014  to the best of our knowledge it is the most comprehensive dataset used in the financial restatement predictive models  our study also makes contributions to the datamining literature by  i  focussing on various datamining techniques and presenting a comparative analysis   ii  ensuring the robustness of various predictive models over different time periods  we have employed all widely used data mining techniques in this area  namely  decision tree  dt   artificial neural network  ann   naive bayes  nb   support vector machine  svm   and bayesian belief network  bbn  classifier while developing the predictive models  we find that ann outperforms other data mining algorithms in our empirical setup in terms of accuracy and area under the roc curve  it is worth noting that our models remain consistent over the full sample period  2001 2014   pre financial crisis period  2001 2008   and post financial crisis period  2009 2014   we believe this study will benefit academics  regulators  policymakers and investors  in particular  regulators and policymakers can pay a close attention to the suspected firms and investors can take actions in advance to reduce their investment risks  the results can also help improving expert and intelligent systems by providing more insights on both intentional and unintentional financial restatements   c  2017 elsevier ltd  all rights reserved  
 detecting physical activity within lifelogs towards preventing obesity and aiding ambient assisted living obesity is a global health issue that affects 2 1 billion people worldwide and has an economic impact of approximately  2 trillion  it is a disease that can make the aging process worse by impairing physical function  which can lead to people becoming more frail and immobile  nevertheless  it is envisioned that technology can be used to aid in motivating behavioural changes to combat this preventable condition  the ubiquitous presence of wearable and mobile devices has enabled a continual stream of quantifiable data  e g  physiological signals  to be collected about ourselves  this data can then be used to monitor physical activity to aid in self reflection and motivation to alter behaviour  however  such information is susceptible to noise interference  which makes processing and extracting knowledge from such data challenging  this paper posits our approach that collects and processes physiological data that has been collected from tri axial accelerometers and a heart rate monitor  to detect physical activity  furthermore  an end user use case application has also been proposed that integrates these findings into a smartwatch visualization  this provides a method of visualising the results to the user so that they are able to gain an overview of their activity  the goal of the paper has been to evaluate the performance of supervised machine learning in distinguishing physical activity  this has been achieved by  i  focusing on wearable sensors to collect data and using our methodology to process this raw lifelogging data so that features can be extracted selected   ii  undertaking an evaluation between ten supervised learning classifiers to determine their accuracy in detecting human activity  to demonstrate the effectiveness of our method  this evaluation has been performed across a baseline method and two other methods   iii  undertaking an evaluation of the processing time of the approach and the smartwatch battery and network cost analysis between transferring data from the smartwatch to the phone  the results of the classifier evaluations indicate that our approach shows an improvement on existing studies  with accuracies of up to 99  and sensitivities of 100   
 detecting sexual predators in chats using behavioral features and imbalanced learning this paper presents a system developed for detecting sexual predators in online chat conversations using a two stage classification and behavioral features  a sexual predator is defined as a person who tries to obtain sexual favors in a predatory manner  usually with underage people  the proposed approach uses several text categorization methods and empirical behavioral features developed especially for the task at hand  after investigating various approaches for solving the sexual predator identification problem  we have found that a two stage classifier achieves the best results  in the first stage  we employ a support vector machine classifier to distinguish conversations having suspicious content from safe online discussions  this is useful as most chat conversations in real life do not contain a sexual predator  therefore it can be viewed as a filtering phase that enables the actual detection of predators to be done only for suspicious chats that contain a sexual predator with a very high degree  in the second stage  we detect which of the users in a suspicious discussion is an actual predator using a random forest classifier  the system was tested on the corpus provided by the pan 2012 workshop organizers and the results are encouraging because  as far as we know  our solution outperforms all previous approaches developed for solving this task  
 detecting stress based on social interactions in social networks psychological stress is threatening people s health  it is non trivial to detect stress timely for proactive care  with the popularity of social media  people are used to sharing their daily activities and interacting with friends on social media platforms  making it feasible to leverage online social network data for stress detection  in this paper  we find that users stress state is closely related to that of his her friends in social media  and we employ a large scale dataset from real world social platforms to systematically study the correlation of users  stress states and social interactions  we first define a set of stress related textual  visual  and social attributes from various aspects  and then propose a novel hybrid model   a factor graph model combined with convolutional neural network to leverage tweet content and social interaction information for stress detection  experimental results show that the proposed model can improve the detection performance by 6 9 percent in f1 score  by further analyzing the social interaction data  we also discover several intriguing phenomena  i e   the number of social structures of sparse connections  i e   with no delta connections  of stressed users is around 14 percent higher than that of non stressed users  indicating that the social structure of stressed users  friends tend to be less connected and less complicated than that of non stressed users  
 detecting variation of emotions in online activities online text sources form evolving large scale data repositories out of which valuable knowledge about human emotions can be derived  beyond the primary emotions which refer to the global emotional signals  deeper understanding of a wider spectrum of emotions is important to detect online public views and attitudes  the present work is motivated by the need to test and provide a system that categorizes emotion in online activities  such a system can be beneficial for online services  companies recommendations  and social support communities  the main contributions of this work are to   a  detect primary emotions  social ones  and those that characterize general affective states from online text sources   b  compare and validate different emotional analysis processes to highlight those that are most efficient  and  c  provide a proof of concept case study to monitor and validate online activity  both explicitly and implicitly  the proposed approaches are tested on three datasets collected from different sources  i e   news agencies  twitter  and facebook  and on different languages  i e   english and greek  study results demonstrate that the methodologies at hand succeed to detect a wider spectrum of emotions out of text sources   c  2017 elsevier ltd  all rights reserved  
 detecting wash trade in financial market using digraphs and dynamic programming a wash trade refers to the illegal activities of traders who utilize carefully designed limit orders to manually increase the trading volumes for creating a false impression of an active market  as one of the primary formats of market abuse  a wash trade can be extremely damaging to the proper functioning and integrity of capital markets  the existing work focuses on collusive clique detections based on certain assumptions of trading behaviors  effective approaches for analyzing and detecting wash trade in a real life market have yet to be developed  this paper analyzes and conceptualizes the basic structures of the trading collusion in a wash trade by using a directed graph of traders  a novel method is then proposed to detect the potential wash trade activities involved in a financial instrument by first recognizing the suspiciously matched orders and then further identifying the collusions among the traders who submit such orders  both steps are formulated as a simplified form of the knapsack problem  which can be solved by dynamic programming approaches  the proposed approach is evaluated on seven stock data sets from the nasdaq and the london stock exchange  the experimental results show that the proposed approach can effectively detect all primary wash trade scenarios across the selected data sets  
 developing  evaluating  and refining an automatic generator of diagnostic multiple choice cloze questions to assess children s comprehension while reading we describe the development  pilot testing  refinement  and four evaluations of diagnostic question generator  dqgen   which automatically generates multiple choice cloze  fill in theblank  questions to test children s comprehension while reading a given text  unlike previous methods  dqgen tests comprehension not only of an individual sentence but of the context preceding it  to test different aspects of comprehension  dqgen generates three types of distractors  ungrammatical distractors test syntax  nonsensical distractors test semantics  and locally plausible distractors test inter sentential processing  
 development and evaluation of a novel robotic platform for gait rehabilitation in patients with cerebral palsy  cpwalker the term cerebral palsy  cp  is a set of neurological disorders that appear in infancy or early childhood and permanently affect body movement and muscle coordination  the prevalence of cp is two three per 1000 births  emerging rehabilitation therapies through new strategies are needed to diminish the assistance required for these patients  promoting their functional capability  this paper presents a new robotic platform called cpwalker for gait rehabilitation in patients with cp  which allows them to start experiencing autonomous locomotion through novel robot based therapies  the platform  smart walker   exoskeleton  is controlled by a multimodal interface that gives high versatility  the therapeutic approach  as well as the details of the interactions may be defined through this interface  cpwalker concept aims to promote the earlier incorporation of patients with cp to the rehabilitation treatment and increases the level of intensity and frequency of the exercises  this will enable the maintenance of therapeutic methods on a daily basis  with the intention of leading to significant improvements in the treatment outcomes   c  2017 elsevier b v  all rights reserved  
 diegetic user interfaces for virtual environments with hmds  a user experience study with oculus rift research efforts  in the area of virtual reality  have mostly concentrated on the design and implementation of devices supporting user presence in immersive environments  much can be done  however  to further improve the experience of wearing a head mounted display  in order to forget being at home while playing a computer game or navigating a virtual environment  in fact  with the widespread availability  in the near future  of head mounted displays  such as oculus rift  not only the power of gaming platforms will increase  but also the support to the construction of graphical user interfaces  which more closely resemble reality  hence  although it may be exciting to navigate a virtual space wearing a head mounted display  user presence still depends on the functional fidelity of virtual objects  simply said  presence perception is highly influenced by the opportunity of interacting realistically with the environment at hand  hence  many graphical user interface  gui  components should be re engineered adopting a diegetic approach  as simple adaptations may result unnatural  awkward or out of place  such fact  urges  for example  re thinking how users receive and interact with information in virtual worlds  along this direction of work  this contribution moves a step forward analyzing the role of diegetic interfaces  in particular  it describes the design  the implementation and the performance  to the eyes of its users  of three different diegetic interfaces  tested utilizing the oculus rift display  the results that are here presented  although referring to three specific cases  are of wider scope as they have been obtained for two gui components that are adopted throughout many different entertainment virtual environments  namely a shell and a global control interface  
 direct trust assignment using social reputation and aging the exploitation of trust in virtual environments  on line services  intelligent pervasive environments and many other scenarios still deserve researchers  interest  in this work we investigate on how to assess the local  direct  trust a node receives from each of his neighbors  our proposal is social based and takes into account both positive and negative experiences as well as the history of past feedbacks and their aging  ensuring good stability also when a node receives hundreds of positive feedbacks briefly followed by few negative feedbacks  our results are illustrated by performing several simulations on different networks  
 disclosure risk reduction for generalized linear model output in a remote analysis system remote analysis systems allow analysts to obtain statistical results without providing direct access to confidential data stored in a secure server system  an attacking analyst could send queries to a remote server to obtain outputs of statistical analyses and use those outputs for a disclosure attack  statistical disclosure control  sdc  methods are used to modify remote analysis system  ras  outputs in the protection of confidential information  confidentiality protection through perturbation is one of the most commonly adopted sdc methods  in the case of generalized linear modelling  random noise is added to the estimated coefficients or to the associated estimating equation prior to getting estimates  this inflates the variances of estimators  and some efficiency and utility of estimators are lost  thus the application of any perturbation based sdc method could result in an inefficient estimator  with the danger of producing worthless inferences  to date  little attention has been given to systematically controlling the disclosure risk and utility in sdc methods for ras  in this paper  we develop a framework for the perturbation of estimating equations that enables an ras to release modified generalized linear model output in such a way that the disclosure risk is not only reduced but also a good utility is maintained  finally  we present some empirical results demonstrating the application of our framework for obtaining estimates from perturbed estimating equations of binary and count response models  
 discourse analysis based segregation of relevant document segments for knowledge acquisition documents are a useful source of expert knowledge in organizations and can be used to foresee  in an earlier stage of a product s life cycle  potential issues and solutions that might occur in later stages of its life cycle  in this research  these stages are  respectively  design and assembly  even if these documents are available online  it is rather difficult for users to access the knowledge contained in these documents  it is therefore desirable to automatically extract the knowledge contained in these documents and store them in a computer accessible or manipulable form  this paper describes an approach for the first step in this acquisition process  automatically identifying segments of documents that are relevant to aircraft assembly  so that they can be further processed for acquiring expert knowledge  such identification of relevant segments is necessary for avoiding processing of unrelated information that is costly and possibly distracting for domain relevance  the approach to extracting relevant segments has two steps  the first step is the identification of sentences that form a coherent segment of text  within which the topic does not shift  the second step is to classify segments that are within the topics of interest for knowledge acquisition  that is  aircraft assembly in this instance  these steps filter out segments that are unrelated  and therefore need not be processed for subsequent knowledge acquisition  the steps are implemented by understanding the contents of documents  using methods of discourse analysis  in particular  discourse representation theory  a list of discourse entities is obtained  the difference in discourse entities between sentences is used to distinguish between segments  the list of discourse entities in a segment is compared against a domain ontology for classification  the implementation and results of validation on sample texts for these steps are described  
 discover learning path for group users  a profile based approach with the explosion of knowledge and information in the big data era  learning new things efficiently is of crucial significance  despite recent development of e learning techniques which have broken the temporal and spatial barriers for learners  it is still very difficult to meet the requirement of efficient learning  as the key issues involve not only searching for learning resources but also identification of learning paths  people from diverse backgrounds  in most cases  also need to work as a group to acquire new knowledge or skills and complete certain tasks  as these tasks are normally assigned with time constraints  employment of e learning systems may be the optimal approach  in this research  we study the issue of identifying a suitable learning path for a group of learners rather than a single learner in an e learning environment  particularly  a profile based framework for the discovery of group learning paths is proposed by taking various learning related factors into consideration  we also conduct experiments on real learners to validate the effectiveness of the proposed approach   c  2017 elsevier b v  all rights reserved  
 discrete event simulation and virtual reality use in industry  new opportunities and future trends this paper reviews the area of combined discrete event simulation  des  and virtual reality  vr  use within industry  while establishing a state of the art for progress in this area  this paper makes the case for vr des as the vehicle of choice for complex data analysis through interactive simulation models  highlighting both its advantages and current limitations  this paper reviews active research topics such as vr and des real time integration  communication protocols  system design considerations  model validation  and applications of vr and des  while summarizing future research directions for this technology combination  the case is made for smart factory adoption of vr des as a new platform for scenario testing and decision making  it is put that in order for vr des to fully meet the visualization requirements of both industry 4 0 and industrial internet visions of digital manufacturing  further research is required in the areas of lower latency image processing  des delivery as a service  gesture recognition for vr des interaction  and linkage of des to real time data streams and big data sets  
 distributed fair allocation of indivisible goods distributed mechanisms for allocating indivisible goods are mechanisms lacking central control  in which agents can locally agree on deals to exchange some of the goods in their possession  we study convergence properties for such distributed mechanisms when used as fair division procedures  specifically  we identify sets of assumptions under which any sequence of deals meeting certain conditions will converge to a proportionally fair allocation and to an envy free allocation  respectively  we also introduce an extension of the basic framework where agents are vertices of a graph representing a social network that constrains which agents can interact with which other agents  and we prove a similar convergence result for envy freeness in this context  finally  when not all assumptions guaranteeing envy freeness are satisfied  we may want to minimise the degree of envy exhibited by an outcome  to this end  we introduce a generic framework for measuring the degree of envy in a society and establish the computational complexity of checking whether a given scenario allows for a deal that is beneficial to every agent involved and that will reduce overall envy   c  2016 elsevier b v  all rights reserved  
 distributionally robust games with an application to supply chain in this paper  we propose a distributionally robust optimization approach for n player  nonzero sum finite state action games with incomplete information where the payoff matrix is stochastic with an imprecise distribution which is assumed to be attached to an a prior known set  our model is different from the robust game theory which presents a robust optimization approach to game theory with the uncertain payoff matrix in a compact convex set without probabilistic information which can lead to overly conservative solutions  a distributionally robust approach is used to cope with our setting in the games by combining the stochastic optimization approach and the robust optimization approach which can be called the distributionally robust games  we show that the existence of the equilibria for the distributionally robust games  the computation method for equilibrium point  with the first  and second information about the uncertain payoff matrix  can be reformulated as semidefinite programming problems which can be tractably realized  a two echelon supply chain competition with demand uncertainty is analyzed by applying the distributionally robust game theory  
 do customer reviews drive purchase decisions  the moderating roles of review exposure and price customers read reviews to reduce the risk associated with a purchase decision  while prior studies have focused on the valence and volume of reviews  this study provides a more comprehensive understanding of how reviews influence customers by considering two additional factors exposure to reviews and price relative to other products in the category  data provided by two online retailers are used for the analysis  the results reveal a four way interaction with the effect of valence on purchase probability strongest when  1  there are many reviews   2  the customer reads reviews  and  3  the product is higher priced  the effects of valence are smaller  but still positive  in the other conditions  we develop theoretical explanations for the effects based on dual processing models and prospect theory  and provide a sensitivity analysis  we discuss implications for academics  manufacturers and online retailers   c  2017 elsevier b v  all rights reserved  
 dogs as behavior models for companion robots  how can human dog interactions assist social robotics  this position paper  re  presents a relatively new approach for the behavioral design of companion robots  the use of dogs  behavior as a model  this paper discusses the advantages of this approach compared to other prevalent approaches in the field of social robotics and analyzes its effectiveness through the review of three different experimental studies utilizing this concept  
 domain aware trust network extraction for trust propagation in large scale heterogeneous trust networks because users of online social networks  osns  may encounter others whom they have incomplete knowledge of or no previous experience with  trust propagation has become increasingly necessary in many real world applications  however  trust propagation in extracted trust networks often fails because few studies view trust as domain dependent  to address this gap in the research  this paper attempts to extract a domain aware trust network to achieve more accurate trust propagation  a directed multigraph is adopted to model the multiple trust relationships among users in a heterogeneous trust network  htn   a domain aware trust metric is then designed to measure the degree of trust between users considering their domain aware influential power in an osn  a domain aware trust network extraction approach is proposed in accordance with the trust model and domain aware trust metric  based on a real world dataset  prevailing trust propagation algorithms are applied in the extracted domain aware trust network  which validates our domain aware trust network extraction approach   c  2016 elsevier b v  all rights reserved  
 don t go in there  using the apex framework in the design of ambient assisted living systems an approach to design ambient assisted living systems is presented  which is based on apex  a framework for prototyping ubiquitous environments  the approach is illustrated through the design of a smart environment within a care home for older people  prototypes allow participants in the design process to experience the proposed design and enable developers to explore design alternatives rapidly  apex provides the means to explore alternative environment designs virtually  the prototypes developed with apex offered a mediating representation  allowing users to be involved in the design process  a group of residents in a city based care home were involved in the design  the paper describes the design process as well as lessons learned for the future design of aal systems  
 dopamine  inference  and uncertainty the hypothesis that the phasic dopamine response reports a reward prediction error has become deeply entrenched  however  dopamine neurons exhibit several notable deviations from this hypothesis  a coherent explanation for these deviations can be obtained by analyzing the dopamine response in terms of bayesian reinforcement learning  the key idea is that prediction errors are modulated by probabilistic beliefs about the relationship between cues and outcomes  updated through bayesian inference  this account can explain dopamine responses to inferred value in sensory preconditioning  the effects of cue preexposure  latent inhibition   and adaptive coding of prediction errors when rewards vary across orders of magnitude  we further postulate that orbitofrontal cortex transforms the stimulus representation through recurrent dynamics  such that a simple error driven learning rule operating on the transformed representation can implement the bayesian reinforcement learning update  
 drone aided healthcare services for patients with chronic diseases in rural areas this paper addresses the drone aided delivery and pickup planning of medication and test kits for patients with chronic diseases who are required to visit clinics for routine health examinations and or refill medicine in rural areas  for routine healthcare services  the work proposes two models  the first model is to find the optimal number of drone center locations using the set covering approach  and the second model is the multi depot vehicle routing problem with pickup and delivery requests minimizing the operating cost of drones in which drones deliver medicine to patients and pick up exam kits on the way back such as blood and urine samples  in order to improve computational performance of the proposed models  a preprocessing algorithm  a partition method  and a lagrangian relaxation  lr  method are developed as solution approaches  a cost benefit analysis method is developed as a tool to analyze the benefits of drone aided healthcare service  the work is tested on a numerical example to show its applicability  
 dropped personal pronoun recovery in chinese sms  in written chinese  personal pronouns are commonly dropped when they can be inferred from context  this practice is particularly common in informal genres like short message service messages sent via cell phones  restoring dropped personal pronouns can be a useful preprocessing step for information extraction  dropped personal pronoun recovery can be divided into two subtasks   1  detecting dropped personal pronoun slots and  2  determining the identity of the pronoun for each slot  we address a simpler version of restoring dropped personal pronouns wherein only the person numbers are identified  after applying a word segmenter  we used a linear chain conditional random field to predict which words were at the start of an independent clause  then  using the independent clause start information  as well as lexical and syntactic information  we applied a conditional random field or a maximum entropy classifier to predict whether a dropped personal pronoun immediately preceded each word and  if so  the person number of the dropped pronoun  we conducted a series of experiments using a manually annotated corpus of chinese short message service  our approaches substantially outperformed a rule based approach based partially on rules developed by chung and gildea  2010  effects of empty categories on machine translation  proceedings of the conference on empirical methods in natural language processing  emnlp   association for computational linguistics  pp  636 45   our approaches also outperformed  though by a considerably smaller margin  a machine learning approach based closely on work by yang  liu  and xue in  2015  recovering dropped pronouns from chinese text messages  proceedings of the 53rd annual meeting of the association for computational linguistics  acl   association for computational linguistics  pp  309 13   features derived from parsing largely did not help our approaches  we conclude that  given independent clause start information  the parse information we used was largely superfluous for identifying dropped personal pronouns  
 dual peccs  a cognitive system for conceptual representation and categorization in this article we present an advanced version of dual peccs  a cognitively inspired knowledge representation and reasoning system aimed at extending the capabilities of artificial systems in conceptual categorization tasks  it combines different sorts of common sense categorization  prototypical and exemplars based categorization  with standard monotonic categorization procedures  these different types of inferential procedures are reconciled according to the tenets coming from the dual process theory of reasoning  on the other hand  from a representational perspective  the system relies on the hypothesis of conceptual structures represented as heterogeneous proxytypes  dual peccs has been experimentally assessed in a task of conceptual categorization where a target concept illustrated by a simple common sense linguistic description had to be identified by resorting to a mix of categorization strategies  and its output has been compared to human responses  the obtained results suggest that our approach can be beneficial to improve the representational and reasoning conceptual capabilities of standard cognitive artificial systems  and   in addition   that it may be plausibly applied to different general computational models of cognition  the current version of the system  in fact  extends our previous work  in that dual  peccs is now integrated and tested into two cognitive architectures  act r and clarion  implementing different assumptions on the underlying invariant structures governing human cognition  such integration allowed us to extend our previous evaluation  
 dual channel supply chain competition with channel preference and sales effort under uncertain environment in this paper  we investigate a dual channel supply chain under uncertain environment  channel preference and sales effort are taken into account to explore their effects on supply chain members  profits with uncertain information  then we analyze the dual channel supply chain in centralized and decentralized cases  and give closed form expressions for equilibriums in the two cases  a series of numerical experiments are implemented to examine the impacts of uncertainty distributions of parameters on supply chain profits  we conclude that the total profit of the supply chain in the centralized case is invariably higher than that in the decentralized case under different uncertainty degrees of these parameters  it is shown that the supplier s profit first decreases then increases as the expected value of customers  preference to the direct channel increases  and the retailer s profit decreases with the increase of the expected value of customers  preference to the direct channel  in addition  the results indicate that the total profits in the centralized and decentralized cases  the supplier s profit and the retailer s profit all increase when the expected value of the retailer s sales effort elasticity increases  
 dual memory neural networks for modeling cognitive activities of humans via wearable sensors wearable devices  such as smart glasses and watches  allow for continuous recording of everyday life in a real world over an extended period of time or lifelong  this possibility helps better understand the cognitive behavior of humans in real life as well as build human aware intelligent agents for practical purposes  however  modeling the human cognitive activity from wearable sensor data stream is challenging because learning new information often results in loss of previously acquired information  causing a problem known as catastrophic forgetting  here we propose a deep learning neural network architecture that resolves the catastrophic forgetting problem  based on the neurocognitive theory of the complementary learning systems of the neocortex and hippocampus  we introduce a dual memory architecture  dma  that  on one hand  slowly acquires the structured knowledge representations and  on the other hand  rapidly learns the specifics of individual experiences  the dma system learns continuously through incremental feature adaptation and weight transfer  we evaluate the performance on two real life datasets  the cifar 10 image stream dataset and the 46 day lifelog dataset collected from google glass  showing that the proposed model outperforms other online learning methods   c  2017 elsevier ltd  all rights reserved  
 dual source procurement strategies for manufacturers with supply disruption risks the dual source procurement strategy does not necessarily reduce the cost of manufacturers under supply disruption risks  this paper differs from studies on the use of a backup supplier by manufactures  it focuses on the impact of inventory status  the recovery situation of production capacity  and the degree of supply disruption using the dual source procurement strategy when supply disruption occurs  we analyze different degrees of supply disruption situations  and then establish the manufacturer s profit functions in different situations  we further demonstrate the impact of each decision variable through numerical examples using the dual source procurement strategy  this study provides a theoretical basis for the manufacturer s decision on whether and how to use the dual source procurement strategy given varying degrees of supply disruption risks  
 dynamic brain network evolution in normal aging based on computational experiments the mechanisms of normal aging of the human brain are insufficiently understood at present  this lack of systematic understanding impedes the exploration of new treatments for age related diseases and approaches to extend our lifespan  the objective of this study was to develop a novel evolution model to simulate the dynamic alteration processes in functional brain networks that occur during normal aging  using computational experiments  six global topological properties and a nodal metric were applied to characterize functional magnetic resonance imaging data on the brain networks of individuals from three different age groups  comparing these real world results to our simulation results showed that our evolution model captures well the dynamic processes of normal aging in functional brain networks  our research shows that a tradeoff exists between the constraints on the degree distribution and the tendency toward clustered connections of functional brain networks during normal aging  these computational experiments provide a more comprehensive perspective that addresses dynamic alterations across a large time scale  which traditional research techniques cannot achieve  our model is therefore of profound significance for exploring the mechanisms of normal aging  
 dynamic evolution of government s public trust in online collective behaviour  a social computing approach there is a dearth of research on why public trust in government rises and falls over time following online collective behaviours  for the problem of dynamic process and micro macro evolution mechanism of this change in trust whenever it occurs over any specific campaign  in our research  a proposed social computing approach is employed to simulate the change of public trust in government on the basis of a heterogeneous network under three ideal network topologies including random network  scale free network  and small world network  the results show the dynamics of a change in public trust of the government exhibited in online collective behaviour can be dependent on the interplay between the participants and event  where the former mainly occurs on a social network layer  and the latter on an information layer  this leads to a significantly integrated role between macro network driving effect and micro group convergence effect  furthermore  several parameters have phase change phenomena in this process  while phase critical value and degree of impact vary from different network structures  the trigger contextual intensity is an important evolutionary power  and plays an integrated role in the evolution process of a shift in the public s trust of government  
 dynamic financial distress prediction with concept drift based on time weighting combined with adaboost support vector machine ensemble dynamic financial distress prediction  dfdp  is important for improving corporate financial risk management  however  earlier studies ignore the time weight of samples when constructing ensemble fdp models  this study ptoposes two new dfdp approaches based on time weighting and adaboost support vector machine  svm  ensemble  one is the double expert voting ensemble based on adaboost svm and timeboost svm  deve at   which externally combines the outputs of an error based decision expert and a time based decision expert  the other is adaboost svm internally integrated with time weighting  adasvm tw   which uses a novel error time based sample weight updating function in the adaboost iteration  these two approaches consider time weighting of samples in constructing adaboost based svm ensemble  and they are more suitable for dfdp in case of financial distress concept drift  empirical experiment is carried out with sample data of 932 chinese listed companies  7 financial ratios  and time moving process is simulated by dividing the sample data into 13 batches with one year as time step  experimental results show that both deve at and adasvm tw have significantly better dfdp performance than single svm  batch based ensemble with local weighted scheme  adaboost svm and timeboost svm  and they are more suitable for disposing concept drift of financial distress   c  2016 elsevier b v  all rights reserved  
 dynamic hierarchical models for monetary transmission monetary policies  either actual or perceived  cause changes in monetary interest rates  these changes impact the economy through financial institutions  which react to changes in the monetary policy with changes in their administered rates  on both deposits and lendings  in this paper we provide a dynamic modeling for describing how administered bank interest rates react in response to changes in money market rates  in a multicountry setting  in addition  by means of hierarchical equations  we take into account how such changes are affected by the macroeconomic fundamentals of each country  the paper applies the proposed models to interest rates on different loans  to corporates and families  in seven european economies  showing how the monetary policy and the specific situation of each country have differently impacted lendings across time  
 dynamic incomplete uninorm trust propagation and aggregation methods in social network in the social network  trust information features dynamic and incomplete  the experts  weight information is unknown beforehand  or there is not enough reliable sources  the dynamic process of trust propagation is analyzed  a new ts utowa operator for multipath propagation is introduced  and an innovative d utowa operator is proposed for trust aggregation in this paper  as for the efficiency of uninorm trust propagation  the average value of the other s values is adopted to estimate the incomplete sociomatrix vector  finally  a case about tourist route arrangment is discussed to demonstrate the efficiency and feasibility of the theory in this article  
 dynamic neural networks for gas turbine engine degradation prediction  health monitoring and prognosis in this paper  the problem of health monitoring and prognosis of aircraft gas turbine engines is considered by using computationally intelligent methodologies  two different dynamic neural networks  namely the nonlinear autoregressive with exogenous input neural networks and the elman neural networks  are developed and designed for this purpose  the proposed dynamic neural networks are designed to capture the dynamics of two main degradations in the gas turbine engine  namely the compressor fouling and the turbine erosion  the health status and condition of the engine in terms of the turbine output temperature  tt  are then predicted subject to occurrence of these deteriorations  various scenarios consisting of fouling and erosion separately as well as combined are considered  for each scenario  several neural networks are trained and their performance in predicting multiple flights ahead tts is evaluated  finally  the most suitable neural networks for achieving the best prediction are selected by using the normalized bayesian information criterion model selection  simulation results presented demonstrate and illustrate the effective performance of our proposed neural network based prediction and prognosis strategies  
 dynamic prediction of financial distress using malmquist dea creditors such as banks frequently use expert systems to support their decisions when issuing loans and credit assessment has been an important area of application of machine learning techniques for decades  in practice  banks are often required to provide the rationale behind their decisions in addition to being able to predict the performance of companies when assessing corporate applicants for loans  one solution is to use data envelopment analysis  dea  to evaluate multiple decision making units  dmus or companies  which are ranked according to the best practice in their industrial sector  a linear programming algorithm is employed to calculate corporate efficiency as a measure to distinguish healthy companies from those in financial distress  this paper extends the cross sectional dea models to time varying malmquist dea  since dynamic predictive models allow one to incorporate changes over time  this decision support system can adjust the efficiency frontier intelligently over time and make robust predictions  results based on a sample of 742 chinese listed companies observed over 10 years suggest that malmquist dea offers insights into the competitive position of a company in addition to accurate financial distress predictions based on the dea efficiency measures   c  2017 elsevier ltd  all rights reserved  
 dynamic repositioning to reduce lost demand in bike sharing systems bike sharing systems  bsss  are widely adopted in major cities of the world due to concerns associated with extensive private vehicle usage  namely  increased carbon emissions  traffic congestion and usage of non renewable resources  in a bss  base stations are strategically placed throughout a city and each station is stocked with a pre determined number of bikes at the beginning of the day  customers hire the bikes from one station and return them at another station  due to unpredictable movements of customers hiring bikes  there is either congestion  more than required  or starvation  fewer than required  of bikes at base stations  existing data has shown that congestion starvation is a common phenomenon that leads to a large number of unsatisfied customers resulting in a significant loss in customer demand  in order to tackle this problem  we propose an optimisation formulation to reposition bikes using vehicles while also considering the routes for vehicles and future expected demand  furthermore  we contribute two approaches that rely on decomposability in the problem  bike repositioning and vehicle routing  and aggregation of base stations to reduce the computation time significantly  finally  we demonstrate the utility of our approach by comparing against two benchmark approaches on two real world data sets of bike sharing systems  these approaches are evaluated using a simulation where the movements of customers are generated from real world data sets  
 dynamic supplier selection model under two echelon supply network with the rise in competition levels and rapid changes in customer preferences  companies feel the pressure to create an efficient and effective base of suppliers in order to achieve the competitive advantage for them  the selection parameters of suppliers do not remain constant with respect to time and moreover  with highly fluctuating market demand  the suppliers are also expected to respond to it dynamically  this paper addresses a specific dynamic supplier selection problem  dssp  under a two echelon supply network  tesn  for the decision maker to allocate optimum order quantities to different levels of suppliers  the problem here considers a tesn with an integrated approach where the original equipment manufacturer  oem  selects the first tier suppliers and in turn with their opinion decides for the second tier suppliers  second tier suppliers supply raw materials parts components to the first tier suppliers  and then the first tier suppliers supply the fabricated semi finished product to oem  in order to solve such a kind of problem  a mixed integer non linear programming  minlp  is proposed to minimize the total cost  tc  of procurement for satisfying the oem s demand  the problem incorporates parameters relevant to supplier s capacity  lead time  quality level of products  and transportation costs as a function of lead time  the model is validated through two cases with randomly generated data  and sensitivity analysis is conducted through taguchi method using lingo 15  this method not only helps to check the robustness of the parameters involved but also to set their optimum level  the analysis shows a significant reduction in the tc of procurement and the effect of each parameter on the tc are finally identified  the methodology adopted here can be extended to other organizations   c  2016 elsevier ltd  all rights reserved  
 dynamics of firm financial evolution and bankruptcy prediction the optimal forecasting horizon of bankruptcy prediction models is usually one year  beyond this point  their accuracy decreases as the horizon recedes  however  the ability of models to provide good mid term forecasts is an essential characteristic for financial institutions due to prudential reasons  this is why we have studied a method of improving their forecasts up to a 5 year horizon  for this purpose  we propose to quantize how firm financial health changes over time  typify these changes and design models that fit each type  our results show that  whatever the modeling technique used to design prediction models  model accuracy can be significantly improved when the horizon exceeds two years  they also show that when our method is used in combination with ensemble based models  model accuracy is always improved whatever the forecasting horizon  when compared to traditional models used by financial institutions  the method we propose in this article appears to be a reliable solution that makes it possible to solve a real problem most models are unable to overcome  and it can therefore help financial companies comply with the current recommendations made by the basel committee on banking supervision  it also provides the scientific community  which is interested in designing reliable failure models  with insights about how the evolution of firms  financial situations over time can be modeled and efficiently used to make forecasts   c  2017 elsevier ltd  all rights reserved  
 dynamics of metabolism and decision making during alcohol consumption  modeling and analysis heavy alcohol consumption is considered an important public health issue in the united states as over 88 000 people die every year from alcohol related causes  research is being conducted to understand the etiology of alcohol consumption and to develop strategies to decrease high risk consumption and its consequences  but there are still important gaps in determining the main factors that influence the consumption behaviors throughout the drinking event  there is a need for methodologies that allow us not only to identify such factors but also to have a comprehensive understanding of how they are connected and how they affect the dynamical evolution of a drinking event  in this paper  we use previous empirical findings from laboratory and field studies to build a mathematical model of the blood alcohol concentration dynamics in individuals that are in drinking events  we characterize these dynamics as the result of the interaction between a decision making system and the metabolic process for alcohol  we provide a model of the metabolic process for arbitrary alcohol intake patterns and a characterization of the mechanisms that drive the decision making process of a drinker during the drinking event  we use computational simulations and lyapunov stability theory to analyze the effects of the parameters of the model on the blood alcohol concentration dynamics that are characterized  also  we propose a methodology to inform the model using data collected in situ and to make estimations that provide additional information to the analysis  we show how this model allows us to analyze and predict previously observed behaviors  to design new approaches for the collection of data that improves the construction of the model  and help with the design of interventions  
 dynamics of perceptible agency  the case of social robots how do we perceive the agency of others  do the same rules apply when interacting with others who are radically different from ourselves  like other species or robots  we typically perceive other people and animals through their embodied behavior  as they dynamically engage various aspects of their affordance field  in second personal perception we also perceive social or interactional affordances of others  i discuss various aspects of perceptible agency  which might begin to give us some tools to understand interactions also with agents truly other than ourselves or   perhaps agents   like present and future social robots  robots have various kinds of physical and behavioral presence and thus make their agency if we want to call it that perceptible in ways that computers and other forms of ai do not  the largely dualist assumptions pertaining to the hidden bodies of traditional turing tests are discussed  as well as the social affordance effects of such indirect interactions as opposed to interactions in physically shared space  the question is what role various abilities to reveal  hide and dynamically control the body and broader behavior plays in heterogeneous tech or machine mediated interactions  i argue that the specifics and richness of perceptible agency matters to the kind of reciprocity we can obtain  
 early detection of deception and aggressiveness using profile based representations e communication represents a major threat to users who are exposed to a number of risks and potential attacks  detecting these risks with as much anticipation as possible is crucial for prevention  however  much research so far has focused on forensic tools that can be applied only when an attack has been performed this paper proposes a novel and effective methodology for the early detection of threats in written social media  the goal is to recognize a potential attack before it is consummated  and using a minimum amount of information  the proposed approach considers the use of profile based representations  pbrs  for this goal  pbrs have multiple benefits  including non sparsity  low dimensionality  and a proved discriminative power  moreover  representations for partial documents can be derived naturally with pbrs  which makes them suitable for the addressed problem  results include empirical evidence on the usefulness of pbrs in the early recognition setting for two tasks in which anticipation is critical  sexual predator detection and aggressive text identification  these results reveal  on the one hand  that pbrs achieve state of the art performance when using full length documents  i e   the classical task   and  on the other hand  that the proposed methodology outperforms previous work on early recognition of sexual predators by a considerable margin  while obtaining state of the art performance in aggressive text identification  to the best of our knowledge  these are the best results reported on early recognition for the approached problems  we foresee this work will pave the way for the development of novel methodologies for the problem and will motivate further research from the intelligent systems and text mining communities   c  2017 elsevier ltd  all rights reserved  
 early detection of university students with potential difficulties using data mining methods  this paper presents a new means of identifying freshmen s profiles likely to face major difficulties to complete their first academic year  academic failure is a relevant issue at a time when post secondary education is ever more critical to economic success  we aim at early detection of potential failure using student data available at registration  i e  school records and environmental factors  with a view to timely and efficient remediation and or study reorientation  we adapt three data mining methods  namely random forest  logistic regression and artificial neural network algorithms  we design algorithms to increase the accuracy of the prediction when some classes are of major interest  these algorithms are context independent and can be used in different fields  real data pertaining to undergraduates at the university of liege  belgium   illustrates our methodology   c  2017 elsevier b v  all rights reserved  
 effect of robot performance on human robot trust in time critical situations robots have the potential to save lives in high risk situations  such as emergency evacuations  to realize this potential  we must understand how factors such as the robot s performance  the riskiness of the situation  and the evacuee s motivation influence his or her decision to follow a robot  in this paper  we developed a set of experiments that tasked individuals with navigating a virtual maze using different methods to simulate an evacuation  participants chose whether or not to use the robot for guidance in each of two separate navigation rounds  the robot performed poorly in two of the three conditions  the participant s decision to use the robot and self reported trust in the robot served as dependent measures  a 53  drop in self reported trust was found when the robot performs poorly  self reports of trust were strongly correlatedwith the decision to use the robot for guidance  phi 90     0 745   we conclude that a mistake made by a robot will cause a person to have a significantly lower level of trust in it in later interactions  
 effective demand response for smart grids  evidence from a real world pilot we show how an electricity customer decision support system  dss  can be used to design effective demand response programs  designing an effective demand response  dr  program requires a deep understanding of energy consumer behavior and a precise estimation of the expected outcome  excessive demand shifting or a high price responsiveness might create new peaks during low demand periods  we combine insights from a real world pilot with simulations and investigate how we can design effective dr schemes  we evaluate our pricing recommendations against existing economic approaches in the literature and show that targeted recommendations are more beneficial for customers and for the grid  furthermore  we conduct robustness tests in which we apply our methods on two independent datasets and observe differences in peak demand and electricity cost reduction  dependent on individual characteristics  in addition  we examine the role of energy policy  as it varies across countries  and we find that the presence of competition in the electricity market creates lower prices and morecost savings for individuals  finally  we measure the economic value of our dss and show that our dss can result in up to 38  savings on household electricity bills  our results exhibit how the design of effective dr can be achieved and provide insights to energy policymakers with regard to understanding consumers  behavior and setting regulatory constraints   c  2016 elsevier b v  all rights reserved  
 effectiveness of the quantum mechanical formalism in cognitive modeling traditional approaches to cognitive psychology are founded on a classical vision of logic and probability theory  according to this perspective  the probabilistic aspects of human reasoning can be formalized in a kolmogorovian probability framework and reveal underlying boolean type logical structures  this vision has been seriously challenged by various discoveries in experimental psychology in the last three decades  meanwhile  growing research indicates that quantum theory provides the conceptual and mathematical framework to deal with these classically problematical situations  in this paper  we apply a general quantum based modeling scheme to represent two types of cognitive situations where deviations from classical probability occur in human decisions  namely   conceptual categorization  and  decision making   we show that our quantum theoretic modeling faithfully describes different sets of experimental data  explaining the observed deviations from classicality in terms of genuine quantum effects  these results may contribute to the development of applied disciplines where cognitive processes are involved  such as natural language processing  semantic analysis  and information retrieval  
 effects of agent timing on the human agent team as technology becomes more sophisticated  autonomous agents are applied more frequently to improve system performance  the current research employed a five step method  including modeling  simulation  and human experimentation to explore the effect of an artificial agent s timing on the performance of a human agent team within a highly dynamic task environment  agent timing significantly influenced the role assumed by the human within the team  further  agent timing changed system performance by approximately 40  within the experimental conditions  results indicate that an artificial agent s timing can be varied as a function of the task demands placed upon the human agent team to maintain an appropriate level of human activity and engagement  therefore  agent timing may be controlled to adapt autonomy to provide an apparent continuum along which to control human engagement in systems employing human agent teaming within dynamic environments  published by elsevier b v  
 effects of assurance mechanisms and consumer concerns on online purchase decisions  an empirical study online purchases are constantly challenged by potential threats which include compromised vendors  security and breaches of customer privacy  to mitigate these concerns  several information assurance mechanisms  e g   assurance statements and third party certifications  have gained attention in practice  despite the wide deployment of assurance mechanisms  it remains largely unknown as to how online consumers interpret these assurance mechanisms and how their concerns affect their purchase decisions  focusing on two information security assurance mechanisms  i e   assurance statements and third party assurance seals services  and three focal concerns of online consumers  i e   privacy  security  and product and service concerns   this study investigates how security assurance mechanisms influence purchase decisions through alleviating the three focal concerns  the empirical results of the study reveal the relative strength and weakness of the two assurance mechanisms and also uncover the mediating roles of the focal concerns between assurance mechanisms and online consumers  purchase intentions  post hoc analysis further shows that assurance seals supplement the effects of assurance statements on privacy concern and product and service concern  which is also a new finding to the literature  finally  results show that concerns on security and privacy displace concern on product and service  a non information security concern   when effects of the three concerns are concurrently considered by consumers  the implications of findings for both theory and practice are discussed   c  2016 published by elsevier b v  
 effects of cognitive effort on the resolution of overspecified descriptions studies in referring expression generation  reg  have shown different effects of referential overspecification on the resolution of certain descriptions  to further investigate effects of this kind  this article reports two eye tracking experiments that measure the time required to recognize target objects based on different kinds of information  results suggest that referential overspecification may be either helpful or detrimental to identification depending on the kind of information that is actually overspecified  an insight that may be useful for the design of more informed hearer oriented reg algorithms  
 effects of decision space information on maut based systems that support purchase decision processes this paper shows that decision makers often have a misconception of the decision space  the decision space is constituted by the relations among the attributes describing the alternatives available in a decision situation  the paper demonstrates that these misconceptions negatively affect the usage and perceptions of maut based decision support systems  to overcome these negative effects  this paper proposes to use a visualization method based on singular value decomposition to give decision makers insights into the attribute relations  in a laboratory experiment in cooperation with germany s largest internet real estate website  this paper moreover evaluates the proposed solution and shows that our solution improves decision makers  usage and perceptions of maui based decision support systems  we further show that information about the decision space ultimately affects variables relevant for the economic success of decision support system providers such as reuse intention and the probability to act as a promoter for the systems   c  2017 elsevier b v  all rights reserved  
 effects of depth perception cues and display types on presence and cybersickness in the elderly within a 3d virtual store as the population ages  home computers with an internet connection can provide the elderly with a new way to access information and services and manage internet shopping tasks  one of the primary advantages of virtual environment  ve  technology for online shopping is its ability to provide a three dimensional  3d  perspective to customers for a more realistic sense of the goods and the shopping environment  a sense of presence is one of the critical components required for an effective ve  however  side effects such as cybersickness may be caused by the display medium  when the quality of depth perception cues is poor  will the elderly s experience of cybersickness influence their feeling of presence and performance of goods searching during exposure within a 3d virtual store with 3d displays  an experiment addressed associations among presence  cybersickness  and performance in a 3d virtual store with autostereoscopic  stereoscopic and monocular displays with good and poor depth perception cues in an elderly sample  the results showed that the virtual store with an autostereoscopic display with high quality depth perception cues will produce good sense and realism in stereopsis to allow the elderly to experience presence within a virtual store  however  if the depth perception cues are poor  3d displays  and especially stereoscopic displays  are not recommended  elderly users may lose interest in a 3d virtual store due to even more serious cybersickness than that experienced with a monocular display  
 effects of different ambient environments on human responses and work performance the increasing concern on intelligent lighting designs for better life quality calls for the studies on thermal lighting environments  given this background  this study investigated the effects of various ambient environmental settings on the physiological responses  psychological responses and work performance of subjects  a total of eight male university students performed a non work task  30 min  and a work task  60 min  in various thermal lighting environments  the experimental conditions included a combination of five correlated colour temperatures  cct  and two indoor temperatures  temp   the galvanic skin response  heart rate  skin temperature  subjective evaluations and work performance were measured  the results showed that the ambient environments of cct5000 and cct6500 degrees k at 30 degrees c and cct4000 degrees k at 28 degrees c significantly affected subjects  physiology and psychology in both work and non work tasks  the future lighting environment designs may refer these findings of the present study  
 effects of morphological plasticity on evolution of virtual robots many animals are able to modify their morphology during their lifetime in response to changes in the environment  such modifications are often adaptivethey can improve individual s chances of survival and reproduction  in this paper we explore the effects of such morphological plasticity on body brain coevolution of virtual creatures  we propose a method where morphological plasticity is achieved through learning during individual s lifetime allowing each individual to quickly adapt its morphology to the current environment  we show that the resulting plasticity allows evolution of creatures better adapted to different simulated environments  we also show that evolution combined with the new learning rule reduces the total computational cost required to evolve an individual with a given target fitness compared to evolution without learning  
 effects of visual feedback on out of body illusory tactile sensation when interacting with augmented virtual objects funneling and saltation are the two major illusory feedback techniques employed by vibrotactile feedback  they elicit the sensation of a vibrotactile stimulus outside the user s body  originating from an externally held object that visually extends the body  this paper examines the synergy of associating the out of body illusory tactile sensation with different visual feedback to improve the user experience for interacting with the augmented virtual objects  there are two important types of visual feedback  the rendering of a  body extending  object  that appears attached to and connecting the two fingertips to create the illusion  and  interaction  with the object itself  withwhich the user interacts   two experiments were performed  for funneling and saltation  assessing the perceptual effects under four associated visual feedback conditions  with 1  no visual feedback  2  a body extending virtual object  3  a virtual interaction object  rendered at the illusion target location  and 4  both the body extending and interaction virtual objects  we hypothesized that rendering a body extending object will maintain an important role in eliciting the illusion itself  while showing the actual interaction object will improve user performance and experience through multimodal integration  our findings indicated that the effect of the interaction object was much stronger than that of the body extension  in the case of funneling  the visual body extension was not even necessary to elicit the out of body sensation  the effect of the body extension was marginal for funneling  these findings can be applied to tactile interaction design using only few actuators on a variety of media platforms including augmented content  
 efficiency factors in oecd banks  a ten year analysis this paper presents a performance assessment of 128 banks from 23 oecd countries from 2004 to 2013  using different financial criteria that emulate the camels rating system  a robust topsis approach for assessing bank efficiency is also developed and presented  first  alternative variable reduction techniques are employed to extract the major factors within each camels criterion  this is done to mitigate collinearity issues  then  topsis is used to measure bank performance based upon these factors  equally weighted  a comprehensive analysis based on a weighted linear optimization model for multi criteria classification is also performed  which detects any discrepancies from the original scores  lastly  censored quantile regressions are combined with bootstrapped topsis scores to produce a model for predicting the impact of different contextual variables on different efficiency quantiles  results reveal that the effects of ownership  trend  and origin of the bank may vary with respect to efficiency levels  whether high or low   c  2016 elsevier ltd  all rights reserved  
 efficiency measurement and decomposition in hybrid two stage dea with additional inputs the simulation and analysis of structure within decision making unit  dmu  is the basis on which network data envelopment analysis  dea  opens the  black box  and evaluates the efficiency of system with complex internal structure  the efficiency measurements of system with sub dmu in series or with sub dmus in parallel are two common cases in the theory development and applications of two stage dea  however  the research on parallel series hybrid system is not rich enough  the paper develops a set of dea models to treat a two stage system comprised of three sub dmus in hybrid form with additional inputs to the second stage  the proposed models simulate precisely the system s parallel series internal structure  employ synthetically additive and multiplicative dea approaches to estimate and decompose the efficiencies of system  and adopt a heuristic method to convert non linear program due to the additional inputs into a linear program  this approach gives more information about the sources of inefficiency by penetrating into the depth of system and modeling the efficiency formation mechanism  a model application is provided   c  2017 elsevier ltd  all rights reserved  
 efficient algorithms for the identification of top k structural hole spanners in large social networks recent studies show that individuals in a social network can be divided into different groups of densely connected communities  and these individuals who bridge different communities  referred to as structural hole spanners  have great potential to acquire resources information from communities and thus benefit from the access  structural hole spanners are crucial in many real applications such as community detections  diffusion controls  viral marketing  etc  in spite of their importance  little attention has been paid to them  particularly  how to accurately characterize the structural hole spanners and how to devise efficient yet scalable algorithms to find them in a large social network are fundamental issues  in this paper  we study the top k structural hole spanner problem  we first provide a novel model to measure the quality of structural hole spanners through exploiting the structural hole spanner properties  due to its np hardness  we then devise two efficient yet scalable algorithms  by developing innovative filtering techniques that can filter out unlikely solutions as quickly as possible  while the proposed techniques are built up on fast estimations of the upper and lower bounds on the cost of an optimal solution and make use of articulation points in real social networks  we finally conduct extensive experiments to validate the effectiveness of the proposed model  and to evaluate the performance of the proposed algorithms using real world datasets  the experimental results demonstrate that the proposed model can capture the characteristics of structural hole spanners accurately  and the structural hole spanners found by the proposed algorithms are much better than those by existing algorithms in all considered social networks  while the running times of the proposed algorithms are very fast  
 electronic social capital for self organising multi agent systems it is a recurring requirement in open systems  such as networks  distributed systems  and socio technical systems  that a group of agents must coordinate their behaviour for the common good  in those systems where agents are heterogeneous unexpected behaviour can occur due to errors or malice  agents whose practices free ride the system can be accepted to a certain level  however  not only do they put the stability of the system at risk  but they also compromise the agents that behave according to the system s rules  in social systems  it has been observed that social capital is an attribute of individuals that enhances their ability to solve collective action problems  sociologists have studied collective action through human societies and observed that social capital plays an important role in maintaining communities though time as well as in simplifying the decision making in them  in this work  we explore the use of electronic social capital for optimising self organised collective action  we developed a context independent electronic social capital framework to test this hypothesis  the framework comprises a set of handlers that capture events from the system and update three different forms of social capital  trustworthiness  networks  and institutions  later  a set of metrics are generated by the forms of social capital and used for decision making  the framework was tested in different scenarios such as two player games  n player games  and public goods games  the experimental results show that social capital optimises the outcomes  in terms of long term satisfaction and utility   reduces the complexity of decision making  and scales with the size of the population  this work proposes an alternative solution using electronic social capital to represent and reason with qualitative  instead of traditional quantitative  values  this solution could be embedded into socio technical systems to incentivise collective action without commodifying the resources or actions in the system  
 embedded heterogeneous feature selection for conjoint analysis  a svm approach using l1 penalty this paper presents a novel embedded feature selection approach for support vector machines  svm  in a choice based conjoint context  we extend the l1 svm formulation and adapt the rfe svm algorithm to conjoint analysis to encourage sparsity in consumer preferences  this sparsity can be attributed to consumers being selective about the attributes they consider when evaluating alternatives in choice tasks  given limited individual data in choice based conjoint  we control for heterogeneity by pooling information across consumers and shrinking the individual weights of the relevant attributes towards a population mean  we tested our approach through an extensive simulation study that shows that the proposed approach can capture the sparseness implied by irrelevant attributes  we also illustrate the characteristics and use of our approach on two real world choice based conjoint data sets  the results show that the proposed method has better predictive accuracy than competitive approaches  and that it provides additional information at an individual level  implications for product design decisions are discussed  
 embedding semantics in human resources management automation via sql among enterprise business processes  those related to hr management are characterized by conflicting issues  on one hand  the peculiarities of intellectual capital ask for rather expressive representation languages to convey as many facets as possible  on the other hand  such processes deal with huge amounts of resources to be managed  for handling hr management tasks  our approach combines the representation power of a logical language with the information processing efficiency of a dbms  it has been implemented in a fully functioning platform  i m p a k t   that we present here highlighting its peculiarities for three relevant business processes  skill matching  task team composition and company core competence identification  
 emergence of multimodal action representations from neural network self organization the integration of multisensory information plays a crucial role in autonomous robotics to forming robust and meaningful representations of the environment  in this work  we investigate how robust multimodal representations can naturally develop in a self organizing manner from co occurring multisensory inputs  we propose a hierarchical architecture with growing self organizing neural networks for learning human actions from audiovisual inputs  the hierarchical processing of visual inputs allows to obtain progressively specialized neurons encoding latent spatiotemporal dynamics of the input  consistent with neurophysiological evidence for increasingly large temporal receptive windows in the human cortex  associative links to bind unimodal representations are incrementally learned by a semi supervised algorithm with bidirectional connectivity  multimodal representations of actions are obtained using the co activation of action features from video sequences and labels from automatic speech recognition  experimental results on a dataset of 10 full body actions show that our system achieves state of the art classification performance without requiring the manual segmentation of training samples  and that congruent visual representations can be retrieved from recognized speech in the absence of visual stimuli  together  these results show that our hierarchical neural architecture accounts for the development of robust multimodal representations from dynamic audiovisual inputs   c  2016 the authors  published by elsevier b v  
 emergence of ultrafast sparsely synchronized rhythms and their responses to external stimuli in an inhomogeneous small world complex neuronal network we consider an inhomogeneous small world network  swn  composed of inhibitory short range  sr  and long range  lr  interneurons  and investigate the effect of network architecture on emergence of synchronized brain rhythms by varying the fraction of lr interneurons p long   the betweenness centralities of the lr and sr interneurons  characterizing the potentiality in controlling communication between other interneurons  are distinctly different  hence  in view of the betweenness  swns we consider are inhomogeneous  unlike the  canonical   watts strogatz swn with nearly the same betweenness centralities  for small p long   the load of communication traffic is much concentrated on a few lr interneurons  however  as p long  is increased  the number of lr connections  coming from lr interneurons  increases  and then the load of communication traffic is less concentrated on lr interneurons  which leads to better efficiency of global communication between interneurons  sparsely synchronized rhythms are thus found to emerge when passing a small critical value p long   c    similar or equal to 0 16   the population frequency of the sparsely synchronized rhythm is ultrafast  higher than 100 hz   while the mean firing rate of individual interneurons is much lower  similar to 30 hz  due to stochastic and intermittent neural discharges  these dynamical behaviors in the inhomogeneousswnare also compared with those in the homogeneous watts strogatz swn  in connection with their network topologies  particularly  we note that the main difference between the two types of swns lies in the distribution of betweenness centralities  unlike the case of the watts strogatz swn  dynamical responses to external stimuli vary depending on the type of stimulated interneurons in the inhomogeneous swn  we consider two cases of external time  periodic stimuli applied to sub populations of the lr and sr interneurons  respectively  dynamical responses  such as synchronization suppression and enhancement  to these two cases of stimuli are studied and discussed in relation to the betweenness centralities of stimulated interneurons  representing the effectiveness for transfer of stimulation effect in the whole network   c  2017 elsevier ltd  all rights reserved  
 emergency response community effectiveness  a simulation modeler for comparing emergency medical services with smartphone based samaritan response mobile emergency response applications involving location based alerts and physical response of networked members increasingly appear on smartphones to address a variety of emergencies  ems  emergency medical services  administrators  policy makers  and other decision makers need to determine when such systems present an effective addition to traditional emergency medical services  we developed a software tool  the emergency response community effectiveness modeler  ercem  that accepts parameters and compares the potential smartphone initiated samaritan member response to traditional ems response for a specific medical condition in a given geographic area  this study uses ems data from the national ems information system  nemsis  and analyses geographies based on rural urban commuting area  ruca  and economic research service  ers  urbanicity codes  to demonstrate ercem s capabilities  we input a full year of nemsis data documenting ems response incidents across the usa  we conducted three experiments to explore anaphylaxis  hypoglycemia and opioid overdose events across different population density characteristics  with further permutations to consider a series of potential app adoption levels  samaritan response behaviors  notification radii  etc  our model emphasizes how medical condition  prescription adherence levels  community network membership  and population density are key factors in determining the effectiveness of samaritan based emergency response communities  erc   we show how the efficacy of deploying mhealth apps for emergency response by volunteers can be modelled and studied in comparison to ems  a decision maker can utilize ercem to generate a detailed simulation of different emergency response scenarios to assess the efficacy of smartphone based samaritan response applications in varying geographic regions for a series of different conditions and treatments   c  2017 elsevier b v  all rights reserved  
 emerging approaches in literature based discovery  techniques and performance review literature based discovery systems aim at discovering valuable latent connections between previously disparate research areas  this is achieved by analyzing the contents of their respective literatures with the help of various intelligent computational techniques  in this paper  we review the progress of literature based discovery research  focusing on understanding their technical features and evaluating their performance  the present literature based discovery techniques can be divided into two general approaches  the traditional approach and the emerging approach  the traditional approach  which dominate the current research landscape  comprises mainly of techniques that rely on utilizing lexical statistics  knowledge based and visualization methods in order to address literature based discovery problems  on the other hand  we have also observed the births of new trends and unprecedented paradigm shifts among the recently emerging literature based discovery approach  these trends are likely to shape the future trajectory of the next generation literature based discovery systems  
 emerging trends word2vec my last column ended with some comments about kuhn and word2vec  word2vec has racked up plenty of citations because it satisifies both of kuhn s conditions for emerging trends   1  a few initial  promising  if not convincing  successes that motivate early adopters  students  to do more  as well as  2  leaving plenty of room for early adopters to contribute and benefit by doing so  the fact that google has so much to say on  how does word2vec work  makes it clear that the definitive answer to that question has yet to be written  it also helps citation counts to distribute code and data to make it that much easier for the next generation to take advantage of the opportunities  and cite your work in the process   
 emerging trends  i did it  i did it  i did it  but     there has been a trend for publications to report better and better numbers  but less and less insight  the literature is turning into a giant leaderboard  where publication depends on numbers and little else  such as insight and explanation   it is considered a feature that machine learning has become so powerful  and so opaque  that it is no longer necessary  or even relevant  to talk about how it works  insight is not only not required any more  but perhaps  insight is no longer even considered desirable  transparency is good and opacity is bad  a recent best seller  weapons of math destruction  is concerned that big data  and wmds  increase inequality and threaten democracy largely because of opacity  algorithms are being used to make lots of important decisions like who gets a loan and who goes to jail  if we tell the machine to maximize an objective function like making money  it will do exactly that  for better and for worse  who is responsible for the consequences  does it make it ok for machines to do bad things if no one knows what s happening and why  including those of us who created the machines  
 emerging trends  inflation our field has enjoyed amazing growth over the years  is this a good thing or a bad thing  or just a thing  good  growth sounds good  it is hard to imagine a politician arguing against jobs  there are more people working in the field than ever before  and they are publishing more and more  and creating more and more value  what could be wrong with that  bad  whatever you measure you get  we are all under too much pressure to publish too much too quickly  students are graduating these days with more publications than what used to be expected for tenure  so many people are publishing so much that no one has time to think great thoughts  or take time to learn about things that may not be directly relevant to the next publication  neutral  inflation is a fact of life  there are long term macro trends on publication rates that are beyond our control  these trends hold over tens and hundreds of years  and will continue over the foreseeable future  
 emothaw  a novel database for emotional state recognition from handwriting and drawing the detection of negative emotions through daily activities such as writing and drawing is useful for promoting wellbeing  the spread of human machine interfaces such as tablets makes the collection of handwriting and drawing samples easier  in this context  we present a first publicly available database which relates emotional states to handwriting and drawing  that we call emothaw  emotion recognition from handwriting and drawing   this database includes samples of 129 participants whose emotional states  namely anxiety  depression  and stress  are assessed by the depression anxiety stress scales  dass  questionnaire  seven tasks are recorded through a digitizing tablet  pentagons and house drawing  words copied in handprint  circles and clock drawing  and one sentence copied in cursive writing  records consist in pen positions  on paper and in air  time stamp  pressure  pen azimuth  and altitude  we report our analysis on this database  from collected data  we first compute measurements related to timing and ductus  we compute separate measurements according to the position of the writing device  on paper or inair  we analyze and classify this set of measurements  referred to as features  using a random forest approach  this latter is a machine learning method  1   based on an ensemble of decision trees  which includes a feature ranking process  we use this ranking process to identify the features which best reveal a targeted emotional state  we then build random forest classifiers associated with each emotional state  we provide accuracy  sensitivity  and specificity evaluation measures obtained from cross validation experiments  our results showthat anxiety and stress recognition perform better than depression recognition  
 emotion classification of youtube videos watching online videos is a major leisure activity among internet users  the largest video website  youtube  stores billions of videos on its servers  thus  previous studies have applied automatic video categorization methods to enable users to find videos corresponding to their needs  however  emotion has not been a factor considered in these classification methods  therefore  this study classified youtube videos into six emotion categories  i e   happiness  anger  disgust  fear  sadness  and surprise   through unsupervised and supervised learning methods  this study first categorized videos according to emotion  an ensemble model was subsequently applied to integrate the classification results of both methods  the experimental results confirm that the proposed method effectively facilitates the classification of youtube videos into suitable emotion categories   c  2017 elsevier b v  all rights reserved  
 emotion recognition in never seen languages using a novel ensemble method with emotion profiles over the last years  researchers have addressed emotional state identification because it is an important issue to achieve more natural speech interactive systems  there are several theories that explain emotional expressiveness as a result of natural evolution  as a social construction  or a combination of both  in this work  we propose a novel system to model each language independently  preserving the cultural properties  in a second stage  we use the concept of universality of emotions to map and predict emotions in never seen languages  features and classifiers widely tested for similar tasks were used to set the baselines  we developed a novel ensemble classifier to deal with multiple languages and tested it on never seen languages  furthermore  this ensemble uses the emotion profiles technique in order to map features from diverse languages in a more tractable space  the experiments were performed in a language independent scheme  results show that the proposed model improves the baseline accuracy  whereas its modular design allows the incorporation of a new language without having to train the whole system  
 emotion rendering in auditory simulations of imagined walking styles this paper investigated how different emotional states of a walker can be rendered and recognized by means of footstep sounds synthesis algorithms  in a first experiment  participants were asked to render  according to imagined walking scenarios  five emotions  aggressive  happy  neutral  sad  and tender  by manipulating the parameters of synthetic footstep sounds simulating various combinations of surface materials and shoes types  results allowed to identify  for the involved emotions and sound conditions  the mean values and ranges of variation of two parameters  sound level and temporal distance between consecutive steps  results were in accordance with those reported in previous studies on real walking  suggesting that expression of emotions in walking is independent from the real or imagined motor activity  in a second experiment participants were asked to identify the emotions portrayed by walking sounds synthesized by setting the synthesis engine parameters to the mean values found in the first experiment  results showed that the involved algorithms were successful in conveying the emotional information at a level comparable with previous studies  both experiments involved musicians and non musicians  in both experiments  a similar general trend was found between the two groups  
 emotion rendering in plantar vibro tactile simulations of imagined walking styles this paper investigates the production and identification of emotional states of a walker using plantar vibro tactile simulations  in a first experiment  participants were asked to render  according to imagined walking scenarios  five emotions  aggressive  happy  neutral  sad  and tender  by manipulating the parameters of synthetic footstep vibrations simulating various combinations of surface materials and shoes  results allowed to identify  for the involved emotions and vibration conditions  the mean values and ranges of variation of two parameters  vibration amplitude and temporal distance between consecutive steps  results were in accordance with those reported in previous studies on real walking  suggesting that the plantar vibro tactile expression of emotions in walking is independent of the real or imagined motor activity  in a second experiment  participants were asked to identify the emotions portrayed by walking vibrations synthesized by setting the synthesis engine parameters to the mean values found in the first experiment  results showed that the involved algorithms were successful in conveying the emotional information at a level comparable with previous studies  results of both experiments revealed strong similarities with those of an analogous study on footstep sounds suggesting that emotionally expressive walking styles are consistently produced and recognized at auditory and plantar vibro tactile level  
 emotion modulated attention improves expression recognition  a deep learning model spatial attention in humans and animals involves the visual pathway and the superior colliculus  which integrate multimodal information  recent research has shown that affective stimuli play an important role in attentional mechanisms  and behavioral studies show that the focus of attention in a given region of the visual field is increased when affective stimuli are present  this work proposes a neurocomputational model that learns to attend to emotional expressions and to modulate emotion recognition  our model consists of a deep architecture which implements convolutional neural networks to learn the location of emotional expressions in a cluttered scene  we performed a number of experiments for detecting regions of interest  based on emotion stimuli  and show that the attention model improves emotion expression recognition when used as emotional attention modulator  finally  we analyze the internal representations of the learned neural filters and discuss their role in the performance of our model   c  2017 the authors  published by elsevier b v  
 empirical mode decomposition based ensemble deep learning for load demand time series forecasting load demand forecasting is a critical process in the planning of electric utilities  an ensemble method composed of empirical mode decomposition  emd  algorithm and deep learning approach is presented in this work  for this purpose  the load demand series were first decomposed into several intrinsic mode functions  imfs   then a deep belief network  dbn  including two restricted boltzmann machines  rbms  was used to model each of the extracted imfs  so that the tendencies of these imfs can be accurately predicted  finally  the prediction results of all imfs can be combined by either unbiased or weighted summation to obtain an aggregated output for load demand  the electricity load demand data sets from australian energy market operator  aemo  are used to test the effectiveness of the proposed emd based dbn approach  simulation results demonstrated attractiveness of the proposed method compared with nine forecasting methods   c  2017 elsevier b v  all rights reserved  
 employees  information security policy compliance  a norm activation perspective this study explores the role of norms in employees  compliance with an organizational information security policy  isp   drawing upon norm activation theory  social norms theory  and ethical climate literature  we propose a model to examine how isp related personal norms are developed and then activated to affect employees  isp compliance behavior  we collected our data through amazon mechanical turk for hypothesis testing  the results show that isp related personal norms lead to isp compliance behavior  and the effect is strengthened by isp related ascription of personal responsibility  social norms related to isp  including injunctive and subjective norms   awareness of consequences  and ascription of personal responsibility shape personal norms  social norms related to isp are the product of principle ethical climate in an organization   c  2016 elsevier b v  all rights reserved  
 enabling effective workflow model reuse  a data centric approach with increasingly widespread adoption of workflow technology as a standard solution to business process management  a large number of workflow models have been put in use in companies in the era of electronic commerce  these workflow models form a valuable resource for workflow domain knowledge  which should be reused to support workflow model design  however  current workflow modeling approaches do not facilitate workflow model reuse as a fundamental requirement  leading to a research gap in effective workflow model reuse  in this paper  we propose a novel approach called data centric workflow model reuse framework  dwmr  to provide a solution to workflow model reuse  dwmr compliments existing control flow focused workflow modeling approaches by explicitly storing workflow data information  such as data dependency  data task relationships  and data similarity scores  dwmr also provides data driven workflow model search and composition algorithms to satisfy user query requirements by automatically combining multiple workflow models  we demonstrate the feasibility of the dwmr approach by applying it to data from a well known industry workflow model repository   c  2016 elsevier b v  all rights reserved  
 enabling personalised medical support for chronic disease management through a hybrid robot cloud approach information and communication technology and personal robots could play a fundamental role in efficiently managing chronic diseases and avoiding improper medications  they could support senior citizens with reminders  thus promoting their independent living and quality of life  especially in the presence of several chronic diseases  multimorbidity   in this context  this article proposes a service model for personalised medical support that is able to provide adequate healthcare service by means of a hybrid robot cloud approach  this service was quantitatively and qualitatively tested to assess the technical feasibility and user acceptability level of the service  the service was tested with 23 older people  65 86 years  in the domocasa lab  italy   this study demonstrated the feasibility of the proposed hybrid cloud solution and the usability and acceptability were positively evaluated thus confirming the ability to utilise these innovative technologies for active and healthy ageing  
 enabling robotic social intelligence by engineering human social cognitive mechanisms for effective human robot interaction  we argue that robots must gain social cognitive mechanisms that allow them to function naturally and intuitively during social interactions with humans  however  a lack of consensus on social cognitive processes poses a challenge for how to design such mechanisms for artificial cognitive systems  we discuss a recent integrative perspective of social cognition to provide a systematic theoretical underpinning for computational instantiations of these mechanisms  we highlight several commitments of our approach that we refer to as engineering human social cognition  we then provide a series of recommendations to facilitate the development of the perceptual  motor  and cognitive architecture for this proposed artificial cognitive system in future work  for each recommendation  we highlight their relation to the discussed social cognitive mechanisms  provide the rationale for these recommendations and potential benefits  and detail examples of associated computational formalisms that could be leveraged to instantiate our recommendations  overall  the goal of this paper is to outline an interdisciplinary and multi theoretic approach to facilitate the design of robots that will one day function  and be perceived  as socially interactive and effective teammates   c  2016 published by elsevier b v  
 encouraging eco driving with visual  auditory  and vibrotactile stimuli this paper presents an experimental evaluation of an in vehicle eco driving support system that provided auditory  visual  and vibrotactile stimuli for the discouragement of harsh accelerations  and to encourage maximization of the duration of the coasting phase of the vehicle  behavior when driving normally was compared to that exhibited when participants were asked to drive economically  and to that exhibited when provided with feedback in the three sensory modes  individually and in all combinations thereof  results suggest that participants were already aware that harsh accelerations are to be avoided when eco driving  however  additional eco driving support  particularly that which involved the auditory and haptic modalities  further discouraged these behaviors  the eco driving information also supported significantly greater coasting distances  when approaching slowing events   a behavior not spontaneously produced by participants when asked to drive economically  few differences were seen between the effects of the different modes and combinations  however  results taken together suggest that visual only information may be less effective at encouraging compliance across all participants  the auditory stimulus  although it encouraged compliance  was not well received by participants  
 endogenous disjoint change traditionally  models of macro political change  like those of macro economics  have been based on extremely unrealistic assumptions about human cognition  ranging from fully informed and rational to completely random  following from the work of herbert simon and contemporaries  a new tradition has developed based on more realistic ideas of human cognition  this paper lays out the need for such models in understanding endogenously produced disjointed change in public policies  it documents the commonality of such changes at the macro level and points to the contributions of many recent works in developing realistic models of human cognition providing the basis of a new and more fruitful literature   c  2017 elsevier b v  all rights reserved  
 engineering the emergence of norms  a review complex systems often exhibit emergent behaviour  unexpected macro level behaviour caused by the interaction of micro level components  in multiagent systems  these micro level components may be autonomous agents and the emergent behaviour may be expressed as norms patterns of behaviour that arise among the agents in response to their environment and each other  these emergent norms may be beneficial  e g  by encouraging cooperative behaviour   or detrimental  but in either case it is useful to recognize these norms as they emerge and either encourage or discourage their establishment  we term this process engineering the emergence of norms and have identified three steps  the identification of a possible norm  evaluation of its benefit and its encouragement  or discouragement   this paper is an attempt to provide a survey of existing research related to these steps  we also provide an analysis of the approaches based upon their suitability for a variety of normative systems  we examine the requirements for agents to have autonomy over their choice of norms  the degree of observability required in the system  and the norm enforcement methods  the paper concludes with an discussion of open issues  
 enhanced just noticeable difference model for images with pattern complexity the just noticeable difference  jnd  in an image  which reveals the visibility limitation of the human visual system  hvs   is widely used for visual redundancy estimation in signal processing  to determine the jnd threshold with the current schemes  the spatial masking effect is estimated as the contrast masking  and this cannot accurately account for the complicated interaction among visual contents  research on cognitive science indicates that the hvs is highly adapted to extract the repeated patterns for visual content representation  inspired by this  we formulate the pattern complexity as another factor to determine the total masking effect  the interaction is relatively straightforward with a limited masking effect in a regular pattern  and is complicated with a strong masking effect in an irregular pattern  from the orientation selectivity mechanism in the primary visual cortex  the response of each local receptive field can be considered as a pattern  therefore  in this paper  the orientation that each pixel presents is regarded as the fundamental element of a pattern  and the pattern complexity is calculated as the diversity of the orientation in a local region  finally  considering both pattern complexity and luminance contrast  a novel spatial masking estimation function is deduced  and an improved jnd estimation model is built  experimental results on comparing with the latest jnd models demonstrate the effectiveness of the proposed model  which performs highly consistent with the human perception the source code of the proposed model is publicly available at http   web xidian edu cn wjj en index html  
 enhanced visual data mining process for dynamic decision making data mining has great potential in extracting useful knowledge from large amount of temporal data for dynamic decision making  moreover  integrating visualization in data mining  known as visual data mining  allows combining the human ability of exploration with the analytical processing capacity of computers for effective problem solving  to design and develop visual data mining tools  an appropriate process must be followed  in this context  the goal of this paper is to enhance existing visualization processes by adapting it under the temporal dimension of data  the data mining tasks and the cognitive control aspects  the proposed process aims to model the visual data mining methods for supporting the dynamic decision making  we illustrate the steps of our proposed process by considering the design of the visualization of the temporal association rules technique  this technique was developed to assist physicians to fight against nosocomial infections in the intensive care unit  actually  an evaluation study in situ was performed to assess the automatic prediction results as well as the visual representations  at the end  the test of the efficiency of our process using utility and usability evaluation shows satisfactory   c  2016 elsevier b v  all rights reserved  
 enhancing comparison shopping agents through ordering and gradual information disclosure the plethora of comparison shopping agents  csas  in today s markets enables buyers to query more than a single csa when shopping  thus expanding the list of sellers whose prices they obtain  this potentially decreases the chance of a purchase within any single interaction between a buyer and a csa  and consequently decreases each csas  expected revenue per query  obviously  a csa can improve its competence in such settings by acquiring more sellers  prices  potentially resulting in a more attractive  best price   in this paper we suggest a complementary approach that improves the attractiveness of the best result returned based on intelligently controlling the order according to which they are presented to the user  in a way that utilizes several known cognitive biases of human buyers  the advantage of this approach is in its ability to affect the buyer s tendency to terminate her search for a better price  hence avoid querying further csas  without spending valuable resources on finding additional prices to present  the effectiveness of our method is demonstrated using real data  collected from four csas for five products  our experiments confirm that the suggested method effectively influence people in a way that is highly advantageous to the csa compared to the common method for presenting the prices  furthermore  we experimentally show that all of the components of our method are essential to its success  
 enhancing deep learning sentiment analysis with ensemble techniques in social applications deep learning techniques for sentiment analysis have become very popular  they provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques  i e   surface methods   traditional surface approaches are based on complex manually extracted features  and this extraction process is a fundamental question in feature driven methods  these long established approaches can yield strong baselines  and their predictive capabilities can be used in conjunction with the arising deep learning methods  in this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features  the contributions of this paper are sixfold  first  we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm  this classifier serves as a baseline to compare to subsequent results  second  we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in sentiment analysis  third  we also propose two models for combining both surface and deep features to merge information from several sources  fourth  we introduce a taxonomy for classifying the different models found in the literature  as well as the ones we propose  fifth  we conduct several experiments to compare the performance of these models with the deep learning baseline  for this  we use seven public datasets that were extracted from the microblogging and movie reviews domain  finally  as a result  a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on fl score   c  2017 the authors  published by elsevier ltd  
 enhancing effectiveness of dimension reduction in text classification nowadays  text is one prevalent forms of data and text classification is a widely used data mining task  which has various application fields  one mass produced instance of text is email  as a communication medium  despite having a lot of advantages  email suffers from a serious problem  the number of spam emails has steadily increased in the recent years  leading to considerable irritation  therefore  spam detection has emerged as a separate field of text classification  a primary challenge of text classification  which is more severe in spam detection and impedes the process  is high dimensionality of feature space  various dimension reduction methods have been proposed that produce a lower dimensional space compared to the original  these methods are divided mainly into two groups  feature selection and feature extraction  this research deals with dimension reduction in the text classification task and especially performs experiments in the spam detection field  we employ information gain  ig  and chi square statistic  chi  as well known feature selection methods  also  we propose a new feature extraction method called sprinkled semantic feature space  ssfs   furthermore  this paper presents a new hybrid method called ig ssfs  in ig ssfs  we combine the selection and extraction processes to reap the benefits from both  to evaluate the mentioned methods in the spam detection field  experiments are conducted on some well known email datasets  according to the results  ssfs demonstrated superior effectiveness over the basic selection methods in terms of improving classifiers  performance  and ig ssfs further enhanced the performance despite consuming less processing time  
 enhancing intelligence in multimodal emotion assessments computer systems are a part of everyday life  since they influence human behavior and stimulate changes in the emotional states of the users  the assessment of users  emotions during their interaction with computer systems can help to provide tailorable website interfaces and better recommendations systems  however  emotions are complex and difficult to identify or assess  previous studies have shown that  in a real world scenario  the use of single sensors do not provide an accurate emotional assessment  hence  in this study  we propose a framework that takes into account multiple sensors so that conclusions can be drawn about the emotional state of the user at the time of interaction  the proposed multi sensing approach includes several inputs from users  such as speech  facial movements  and everyday activities   and uses an artificial intelligent strategy to map these different responses into one or more emotional states  the componential emotion theory and scherer s emotional semantic space are used to underpin the theoretical framework  the experimental results show that the combination of outputs generated by multiple sensors provides a more accurate assessment of emotional states than when the sensors are treated individually  
 enhancing quality of learning experience through intelligent agent in e learning in this paper an interactive recommending agent is proposed which helps an e learner to enhance the quality of learning experience resulting in efficient achievement of learning objectives  the agent achieves this with the help of a fuzzy rule base working on a variety of learning materials and recommending the appropriate learning path through them  in a learner centric environment the learning behaviour of a learner may vary to a great extent due to the characteristics of the learner and his environment  students are often misled while choosing the appropriate path of web learning tools owing to non availability of a human teacher guide  by the response of a learner to different positive and negative motivation factors the proposed system employs a fuzzy machine that is fed with realization parameters e g  satisfied  depressed etc  the fuzzy machine working on the paradigm of fuzzy inference system processes these realization parameters with the help of a fuzzy rule base to produce the crisp measures of the learner s cognitive states in terms of belief  behaviour and attitude  on the basis of these defuzzified crisp diagnostic parameters the proposed system will enhanced the quality of learning experience of an e learner  to ensure this the system will provide more detailed discussion on the subject matter along with some additional learning tools  learners often get confused to select the proper tools among various  therefore the proposed system will also suggest most popular path among those learners with the same understanding  this recommendation comes from the analysis of data mining result  the system was tested with a wide variety of school level students  the response obtained indicates that it is able to enhance the quality of learning experience through its recommendation  
 enhancing relative ratio method for mcdm via attitudinal distance measures of interval valued hesitant fuzzy sets this paper is devoted to develop an enhanced relative ratio  rr  method for tackling with multi criteria decision making  mcdm  problem in interval valued hesitant fuzzy setting  first  a hesitant fuzzy cowa operator is defined  based on which we construct an attitudinal distance measure between two ivhfes and develop subsequently  three  some weighted  including weighted  ordered weighted and synergetic weighted  attitudinal distance measures between two collections of  for  ivhfes  or two ivhfss   the measures can not only alleviate the operational load  but also efficiently avoid information loss and distortion  as well as reflect the dms  decision attitude in the computing process  next  under the framework of relative ratio method  we use the attitudinal distance measures to establish an enhanced interval valued hesitant fuzzy relative ratio method  which adheres to the principle of compromise that the chosen alternative should have the shortest distance from the positive ideal solution and the greatest distance from the negative one simultaneously  to range alternatives  the established method provides us a very flexible and useful way to deal with fuzzy mcdm problems under interval valued hesitant fuzzy setting for it considers the dms  attitudinal character  moreover  in the methodology  several approaches for determining the interval valued hesitant fuzzy positive negative ideal solutions are introduced  finally  the implementation process and the applicability of our method are illustrated by a real example concerning watershed ecological risk evaluation  and comparisons are made with the four similar methods  
 enriching conflict resolution environments with the provision of context information it is a common affair to settle disputes out of courts nowadays  through negotiation  mediation or any other mean  this has also been implemented over telecommunication means under the so called online dispute resolution methods  however  this new technology supported approach is impersonal and cold  leaving aside important issues such as the disputants  body language  stress level or emotional response while being based on forms  e mails or chat rooms  to overcome this shortcoming  in this paper  it is proposed the creation of intelligent environments for conflict resolution that can complement the existing tools with important knowledge about the context of interaction  this will allow decision makers to take better framed decisions based not only on figures but also on important contextual information  similar to what happens when parties communicate in the physical presence of each other  
 ensuring the canonicity of process models process models play an important role for specifying requirements of business related software  however  the usefulness of process models is highly dependent on their quality  recognizing this  researches have proposed various techniques for the automated quality assurance of process models  a considerable shortcoming of these techniques is the assumption that each activity label consistently refers to a single stream of action  if  however  activities textually describe control flow related aspects such as decisions or conditions  the analysis results of these tools are distorted  due to the ambiguity that is associated with this misuse of natural language  also humans struggle with drawing valid conclusions from such inconsistently specified activities  in this paper  we therefore introduce the notion of canonicity to prevent the mixing of natural language and modeling language  we identify and formalize non canonical patterns  which we then use to define automated techniques for detecting and refactoring activities that do not comply with it  we evaluated these techniques by the help of four process model collections from industry  which confirmed the applicability and accuracy of these techniques  
 entropy kemira approach for mcdm problem solution in human resources selection task entropy kemira approach is proposed for criteria ranking and weights determining when solving multiple criteria decision making  mcdm  problem in human resources selection task  for the first time the method is applied in the case of three groups of criteria  weights are calculated by solving optimization problem of maximizing the number of elements  which are  best  according to all three criteria  and minimizing the number of  doubtful  elements  the algorithm of problem solution is presented in the paper  the numerical experiment with three groups of evaluation criteria describing 11 life goals was accomplished  
 environmental effects on simulated emotional and moody agents psychological models have been used to simulate emotions within agents as part of the decision making process  the body of this work has focussed on applying the process of decision making using emotions to social dilemmas  notably the prisoner s dilemma  previous work has focussed on agents which do not move around  with an initial analysis on how mobility and the environment can affect the decisions chosen  additionally simulated mood has been introduced to the decision making process  exploring simulated emotions and mood to inform the decision making process in multi agent systems allows us to explore in further detail how outside influences can have an effect on different strategies  we expand and clarify aspects of how agents are affected by environmental differences  we show how emotional characters settle on an outcome without deviation by providing a formal proof  we validate how the addition of mood increases cooperation  while also showing how small groups achieve this quicker than large groups  once pure defectors are added  to test the resilience of the cooperation achieved  we see that while agents with a low starting mood achieve a payoff closest to the pure defectors  they are reduced in numbers the most by the pure defectors  
 epistemic informational structural realism the paper surveys floridi s attempt for laying down informational structural realism  isr   after considering a number of reactions to the pars destruens of floridi s attack on the digital ontology  i show that floridi s enterprise for enriching the isr by borrowing elements from the ontic form of structural realism  in the pars construens  is blighted by a haunting inconsistency  isr has been originally developed by floridi as a restricted and level dependent form of structural realism which remains mainly bonded within the borders of a kantian perspective  i argue that this perspective doesn t mesh nicely with the ontic interpretation that floridi attached to the isr  i substantiate this claim through the assessment of floridi s strategy for reconciling the epistemic and ontic forms of the sr  as well as by close examination of his use of method of levels of abstraction and his notion of semantic information  my proposal is that the isr could be defended best against the mentioned and similar objections by being interpreted as an extension of the epistemic sr  
 ergonomic evaluation of video game playing this study investigated the effect of display type  play rest schedule and game type on visual fatigue  heart rate and mental workload for both genders during 1 h of wii game playing  twenty subjects participated in the experiment  two display types  a 32 in crt display and a 32 in plasma display   two different play rest schedules  10 min play with 5 min rest  repeated four times  20 min play with 10 min rest  repeated two times  and two nintendo  wii sports  games  boxing and tennis  were used for evaluation  the study results showed that the display and game type had a significant effect on all the measurements  the use of a plasma display to play video games improved the critical flicker fusion  cff  frequency threshold  however  it also caused a greater subjective eye fatigue rating  increased heart rate  as well as higher mental workload  subjects with a 5 min break for every 10 min played had a lower subjective eye fatigue rating  heart rate and mental workload than with a 10 min break for every 20 min played  the gender effect was not significant on any of the measurements  implications of the results regarding video game playing are discussed in terms of display and game type  playing video games with frequent short breaks is suggested for reducing visual fatigue  especially for intensive video games  
 error analysis methods for group decision making based on hesitant fuzzy preference relation hesitant fuzzy preference relation  hfpr  is an effective way to depict the decision makers  preferences over the objects  alternatives or attributes  in the process of group decision making  each component of the hfpr is characterized by several possible values and can express the decision makers  hesitant information comprehensively  to make a decision with the hfpr  it is very necessary to find a proper technique for deriving the priority weights from the hfpr  in this paper  we use the error analysis as a tool to develop several straightforward methods for the priorities of the hfpr  we first define the expected value and the average value of each hesitant fuzzy element in the hfpr  then based on the error analysis  we come up with the interval midpoint method  the average value method  and the difference method to derive the priority weights from the hfpr  after that  we discuss the relations among these methods  and utilize them and the possibility degree formula to develop an approach to decision making with the hfpr  finally  we demonstrate the effectiveness and practicality of our approach through a case study concerning the investment problem in liquor enterprise  
 estimation of human walking speed by doppler radar for elderly care this paper presents a human walking speed estimation algorithm using a doppler radar system for in home passive gait assessment of elderly adults  the fast fourier transform  fft  has been a common approach to obtain the gait speed estimation  the proposed algorithm analyzes the received radar signal by counting its zero crossing periods in the time domain and applies an interquartile range filter to improve its robustness with respect to the gait signature dynamics in the radar data  the proposed algorithm provides better accuracy and does not seem to produce extreme incorrect speed estimates compared to the previously developed fft method  it has been applied to the doppler radar system that is deployed in in home environments to continuously monitor the daily activities of the senior residences and estimate their gait speeds  improving the accuracy of gait parameters can better facilitate the assessment of the physical conditions of the seniors  
 estimation of passenger car equivalents for single lane roundabouts using a microsimulation based procedure passenger car equivalents for heavy vehicles are required to carry out capacity calculations and perform operational analysis of any road entity  roadway segments or intersections   at single lane roundabouts  the constraints to the vehicular trajectories imposed by the curvilinear geometric design and the driver s gap acceptance behaviour are expected to produce an impact of the heavy vehicles on the quality of traffic flow different from that produced on freeways and two lane highways or other at grade intersections  this is also because entering flow is opposed by the circulating flow which has priority and travels in an anticlockwise direction on a single lane path around the central island  this paper addresses the question of how to estimate the passenger car equivalents for heavy vehicles on single lane roundabouts  first  a comparison was performed between the empirical capacity functions based on a meta analytic estimation of the critical and the follow up headways and the simulation outputs manually obtained for a single lane roundabout built in aimsun microscopic simulator  a genetic algorithm based calibration procedure  therefore  was used to reach a better convergence between the simulation outputs and the empirical capacities  based on the calibrated model  the passenger car equivalents were determined by comparing the capacity functions built for a fleet of passenger cars with the capacity functions calculated for different percentages of heavy vehicles  differently from hcm 2010 which assumes a heavy vehicle to be equivalent to two passenger cars and sets as 2 0 the passenger car equivalents for heavy vehicles for roundabouts  a higher pce effect would be expected on the quality of traffic conditions when the traffic stream contains a high number of heavy vehicles  this effect should be accounted for when calculating capacity and level of services   c  2017 elsevier ltd  all rights reserved  
 ethics of healthcare robotics  towards responsible research and innovation how can we best identify  understand  and deal with ethical and societal issues raised by healthcare robotics  this paper argues that next to ethical analysis  classic technology assessment  and philosophical speculation we need forms of reflection  dialogue  and experiment that come  quite literally  much closer to innovation practices and contexts of use  the authors discuss a number of ways how to achieve that  informed by their experience with  embedded  ethics in technical projects and with various tools and methods of responsible research and innovation  the paper identifies  internal  and  external  forms of dialogical research and innovation  reflections on the possibilities and limitations of these forms of ethical technological innovation  and explores a number of ways how they can be supported by policy at national and supranational level   c  2016 the authors  published by elsevier b v  
 european union regulations on algorithmic decision making and a  right to explanation  we summarize the potential impact that the european union s new general data protection regulation will have on the routine use of machine learning algorithms  slated to take effect as law across the european union in 2018  it will place restrictions on automated individual decision making  that is  algorithms that make decisions based on user level predictors  that  significantly affect  users  when put into practice  the law may also effectively create a right to explanation  whereby a user can ask for an explanation of an algorithmic decision that significantly affects them  we argue that while this law may pose large challenges for industry  it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks that avoid discrimination and enable explanation  
 evacuation route optimization architecture considering human factor rapidly changing detrimental safety conditions on an evacuation route might cause casualties and provoke panic  this is why we need to take into account these conditions for real time route optimization  the objective of this paper is to facilitate efficient people evacuation through dynamic optimization of evacuation routes based on the safety conditions on the route and stress related evacuees  responses to the same  we propose a multi agent based evacuation route optimization architecture for smart space networks that considers the influence of stress on human reactions to the recommended routes  this architecture is meant as a real time decision support in route selection that recommends routes to evacuees  possibly through smart phone or smart building displays  the functioning of the architecture is shown on an example of a simple smart space network  
 evaluating intelligent knowledge systems  experiences with a user adaptive assistant agent this article examines experiences in evaluating a user adaptive personal assistant agent designed to assist a busy knowledge worker in time management  we examine the managerial and technical challenges of designing adequate evaluation and the tension of collecting adequate data without a fully functional  deployed system  the calo project was a seminal multi institution effort to develop a personalized cognitive assistant  it included a significant attempt to rigorously quantify learning capability  which this article discusses for the first time  and ultimately the project led to multiple spin outs including siri  retrospection on negative and positive experiences over the 6 years of the project underscores best practice in evaluating user adaptive systems  lessons for knowledge system evaluation include  the interests of multiple stakeholders  early consideration of evaluation and deployment  layered evaluation at system and component levels  characteristics of technology and domains that determine the appropriateness of controlled evaluations  implications of  in the wild  versus variations of  in the lab  evaluation  and the effect of technology enabled functionality and its impact upon existing tools and work practices  in the conclusion  we discuss through the lessons illustrated from this case study of intelligent knowledge system evaluation how development and infusion of innovative technology must be supported by adequate evaluation of its efficacy  
 evaluating taiwanese bank efficiency using the two stage range dea model this paper develops a range directional distance data envelopment analysis  dea  model to simultaneously deal with the problems of negative data and undesirable outputs in the study of performance measurement with two stage dea  we report on the development of this model to handle both positive and negative data in a dea framework and accommodate the problem of undesirable intermediate outputs in the first stage of operational processes  unlike previous twostage dea models we allow for a nonuniform abatement factor imposing on stage 1  production technology  such a model is then applied to evaluate taiwanese bank efficiencies both at the operational stage and profitability stage in banking activities based on a data set consisting of 35 domestic banks in taiwan in the period 2007  the results indicate that  by the range directional two stage data envelopment analysis model  the operational efficiency was smaller than the profitability efficiency  many banks generated too many performing loans in which independent banks should reduce more performing loans than financial holding company subsidiary banks  both the ratio of investments to loans and the ratio of nonperforming loans to performing loans did not have significant contributions to the efficiency  this paper is able to provide information for bank operators and researchers on the managerial and strategic implications of how negative data and undesirable outputs affect efficiency and how to measure efficiency appropriately  
 evaluating the efficiency of cloud services using modified data envelopment analysis and modified super efficiency data envelopment analysis several cloud services with comparable functionality are now available to customers at different prices and performance levels  often  there may be trade offs among different functional and non functional requirements fulfilled by different cloud providers  hence  it is difficult to evaluate the relative performances of the cloud services and their ranking based on various quality of service attributes  in this paper  we propose a modified data envelopment analysis and a modified super efficiency data envelopment analysis for evaluating the cloud services and their efficiencies considering user preferences  we compare these methods of cloud service selection based on sensitivity analysis  adequacy to changes in dmus  adequacy to support decision making and modeling of uncertainty  the comparison helps customers to choose a cloud service that is most suitable to their requirements and also creates a healthy competition among the cloud service providers  
 evaluating the sensitivity to virtual characters facial asymmetry in emotion synthesis the use of expressive virtual characters is an effective complementary means of communication for social networks offering multi user 3d chatting environment  in such contexts  the facial expression channel offers a rich medium to translate the ongoing emotions conveyed by the text based exchanges  however  until recently  only purely symmetric facial expressions have been considered for that purpose  in this article we examine human sensitivity to facial asymmetry in the expression of both basic and complex emotions  the rationale for introducing asymmetry in the display of facial expressions stems from two well established observations in cognitive neuroscience  first that the expression of basic emotions generally displays a small asymmetry  second that more complex emotions such as ambivalent feeling may reflect in the partial display of different  potentially opposite  emotions on each side of the face  a frequent occurrence of this second case results from the conflict between the truly felt emotion and the one that should be displayed due to social conventions  our main hypothesis is that a much larger expressive and emotional space can only be automatically synthesized by means of facial asymmetry when modeling emotions with a general valence arousal dominance dimensional approach  besides  we want also to explore the general human sensitivity to the introduction of a small degree of asymmetry into the expression of basic emotions  we conducted an experiment by presenting 64 pairs of static facial expressions  one symmetric and one asymmetric  illustrating eight emotions  three basic and five complex ones  alternatively for a male and a female character  each emotion was presented four times by swapping the symmetric and asymmetric positions and by mirroring the asymmetrical expression  participants were asked to grade  on a continuous scale  the correctness of each facial expression with respect to a short definition  results confirm the potential of introducing facial asymmetry for a subset of the complex emotions  guidelines are proposed for designers of embodied conversational agent and emotionally reflective avatars  
 evaluation in artificial intelligence  from task oriented to ability oriented measurement the evaluation of artificial intelligence systems and components is crucial for the progress of the discipline  in this paper we describe and critically assess the different ways ai systems are evaluated  and the role of components and techniques in these systems  we first focus on the traditional task oriented evaluation approach  we identify three kinds of evaluation  human discrimination  problem benchmarks and peer confrontation  we describe some of the limitations of the many evaluation schemes and competitions in these three categories  and follow the progression of some of these tests  we then focus on a less customary  and challenging  ability oriented evaluation approach  where a system is characterised by its  cognitive  abilities  rather than by the tasks it is designed to solve  we discuss several possibilities  the adaptation of cognitive tests used for humans and animals  the development of tests derived from algorithmic information theory or more integrated approaches under the perspective of universal psychometrics  we analyse some evaluation tests from ai that are better positioned for an ability oriented evaluation and discuss how their problems and limitations can possibly be addressed with some of the tools and ideas that appear within the paper  finally  we enumerate a series of lessons learnt and generic guidelines to be used when an ai evaluation scheme is under consideration  
 evaluation of co evolutionary neural network architectures for time series prediction with mobile application in finance the fusion of soft computing methods such as neural networks and evolutionary algorithms have given a very promising performance for time series prediction problems  in order to fully harness their strengths for wider impact  effective real world implementation of prediction systems must incorporate the use of innovative technologies such as mobile computing  recently  co evolutionary algorithms have shown to be very promising for training neural networks for time series prediction  cooperative coevolution decomposes a problem into subcomponents that are evolved in isolation and cooperation typically involves fitness evaluation  the challenge has been in developing effective subcomponent decomposition methods for different neural network architectures  in this paper  we evaluate the performance of two problem decomposition methods for training feedforward and recurrent neural networks for chaotic time series problems  we further apply them for financial prediction problems selected from the nasdaq stock exchange  we highlight the challenges in real time implementation and present a mobile application framework for financial time series prediction  the results  in general  show that recurrent neural networks have better generalisation ability when compared to feedforward networks for real world time series problems   c  2016 elsevier b v  all rights reserved  
 evaluation of cognitive behavior among deaf subjects with video game as intervention previous research on video gaming mainly focused on negative impact on users  in this study  the positive impact of action video gaming among congenitally deaf subjects is investigated  subjects possessed high level of visual information processing capacity  a 96 h of action video game play resulted in the improvement of mean reaction time  mrt   post   413 21 ms  pre   453 39 ms  and certain cognitive functions  higher heart rate variability  hrv  subjects demonstrated faster and significant improvement in mrt  p   0 0045  after intervention  the alpha and beta eeg band powers were found increased in parietal  6 33   4 28   and occipital  8 38   38   lobes respectively  the theta band power increased in frontal  32   and parietal  13   lobes  these results can reflect enhancement in certain cognitive performances such as visual perception  attention  memory  and motor skills  the ratio index beta  alpha   theta  increased in frontal and occipital lobes while beta theta increased in frontal and temporal lobes  these results may also suggest improvements in attention processing capacity and neural activity  these results implies improvements in certain aspects of cognition among deaf subjects  however  to validate these results a further study on larger number of samples with advanced computerized cognitive battery testing can be employed   c  2016 elsevier b v  all rights reserved  
 evaluation of e commerce websites  an integrated approach under a single valued trapezoidal neutrosophic environment e commerce website evaluation is recognized as a complex multi criteria decision making  mcdm  problem involving vast amounts of imprecise and inconsistent evaluation data  single valued trapezoidal neutrosophic numbers  svtnns   which are elements in single valued trapezoidal neutrosophic sets  svtnss   have a strong capacity to model such complex evaluation information  however  only few studies simultaneously consider the imprecise and inconsistent information inherent in the evaluation data  moreover  much literature overlooks the different priority levels and interrelationships among criteria  to bridge this gap  this paper outlines a novel integrated decision system consisting of the following three modules   1  information acquisition   2  the single valued trapezoidal neutrosophic decision making trial and evaluation laboratory  svtn dematel  module  and  3  the integration module  in this study  we used the information acquisition module to gather the svtnn information provided by experts  applied the svtn dematel module to analyze the causal relationships among criteria  and proposed the integration module for information fusion with consideration of interdependencies and different priority levels of criteria  furthermore  we conducted a case study to illustrate the effectiveness and feasibility of the proposal along with the sensitivity and comparison analyses to verify its stability and superiority  finally  conclusions and future research directions were drawn   c  2017 elsevier b v  all rights reserved  
 evaluation of fused imagery using eye movement based measures of perceptual processing human performance measures were used to evaluate the perceptual processing efficiency of infrared and fused infrared images  in two experiments  eye movements were recorded while subjects searched for and identified human targets in forested scenes presented on a computer monitor  the scenes were photographed simultaneously using short wave infrared  swir   long wave infrared  lwir   and visible  vis  spectrum cameras  fused images were created through two way combinations of these single band images  in experiment 1 the single band sensors were contrasted with a simple average fusion scheme  swir lwir   analysis of subjects  eye movements revealed differences between sensors in measures of central processing  gaze duration  response accuracy  and peripheral selection  detection interval  saccade amplitude   in experiment 2 this methodology was applied to compare three two way combinations of sensors  swir lwir  swir vis  vis lwir   produced by state of the art fusion methods  peripheral selection for fused images tended to exhibit a compromise between the performance levels of component sensor images  while measures of central processing showed evidence that fused images matched or exceeded the performance level of component single band sensor images  stimulus analysis was conducted to link measures of central and peripheral processing efficiency to image characteristics  e g  target contrast  target background contrast   and these image characteristics were able to account for a moderate amount of the variance in the performance across fusion conditions  these findings demonstrate the utility of eye movement measures for evaluating the perceptual efficiency of fused imagery  crown copyright  c  2017 published by elsevier b v  all rights reserved  
 evaluation of on line trading systems  markov switching vs time varying parameter models automatic trading systems  to support the decisions of investors in financial markets  are increasingly used nowadays  such systems process data on line and provide signals of buy and sell in correspondence of pits and peaks of the market  real time detection of turning points in financial time series is a challenging issue and can only be performed with sequential methods  this paper considers non linear and non stationary dynamic models used in statistics and econometrics  and evaluates their performance  in particular  it compares markov switching  ms  regression and time varying parameter  tvp  methods  the latter extend moving average  ma  techniques which are widely used by traders  the novel approach of this paper is to select the coefficients of the detection methods by optimizing the profit objective functions of the trading activity  using statistical estimates as initial values  the paper also develops a sequential approach  based on sliding windows  to cope with the time variability of ms coefficients  an extensive application to the daily standard   poor 500 index  the world s leading indicator of stock values  in the period 1999 2015  provides evidence in favor of models with a few parameters  this seems a natural consequence of the complexity of the gain maximization problem  which usually admits multiple local solutions  directions for further research are represented by multivariate detection methods and the development of recursive algorithms for gain optimization   c  2016 elsevier b v  all rights reserved  
 evaluation of research proposals for grant funding using interval valued intuitionistic fuzzy sets it is a well known fact that the most appealing external funding for a project is grant funding  therefore  evaluation of research proposal task needs an elaborate approach so as not to finance inconvenient projects  it is indispensable to establish a detailed study for submitted research proposals to have a clearer picture of the grant funding candidates  this study has contributed to research proposal evaluation using a multicriteria approach based on interval valued intuitionistic fuzzy sets  an interval valued intuitionistic fuzzy preference relation matrix is initially constructed to determine the relative importance of criteria based on pairwise comparisons in the presence of insufficient information about the criteria  the proposed evaluation method for grand funding allocation problem is composed of six main criteria and 24 sub criteria  a sensitivity analysis is applied to see the robustness of the decision made  
 evaluation of sequential adaptive testing with real data simulation  a case study computer based testing systems take advantage of the interaction between computers and individuals to sequentially customize the presented test items to the test taker s ability estimate  administering such sequential adaptive tests has many benefits including personalized tests  accurate measurement  item security  and substantial cost reduction  however  the design of such intelligent tests is a complex process and it is important to explore the impact of various parameters and options on the performance before switching from traditional tests in a particular environment  although monte carlo simulation is a typical tool for achieving this purpose  it depends on generating pseudo random samples  which may fail to effectively represent the environment under study and thus incorrect inferences can be drawn  this paper presents a comprehensive case study to evaluate and compare the performance of a number of sequential adaptive testing procedures but using post hoc simulation  where items of a real conventional test are re administered adaptively  the comparisons are based on the number of administered items  standard error of measurement  item exposure rates  and correlation between adaptive and non adaptive estimates  it is found that the results varies based on the settings  however  bayesian estimation with adaptive item selection can lead to greater savings in terms of the number of test items without jeopardizing the estimated ability  it also has the lowest average exposure rate for each item  
 evaluation of the hotels e services quality under the user s experience while the servqual scale has met with greater success than other initiatives in the internet context  the various adaptations and changes made to the measurement scales often make it difficult to compare results over time  a key aspect that companies must take into account when implementing their market oriented strategies  even when the time horizons are the same  it is often impossible to aggregate the results if different types of surveys and measurement scales are used  a practice which is  at the same time  customary  moreover  the wide range of data collection methodologies and measurement scales used by different companies in the same market prevents comparing the results of surveys to evaluate service quality  in this paper  we present a linguistic multi criteria decision making model for aggregating these heterogeneous questionnaires with opinions about quality of the e service offered by the hotels through several websites taking into account the experience on www of such users  the study found that all the scales have been slightly better evaluated for travel 2 0 websites in general that for the studied hotel and established a ranking depending on the website from best to worst score on all servqual scales  hotel tripadvisor webpage  hotel facebook profile and official hotel blog  
 evaluation of the relationship between brand measures and customer satisfaction by using data mining techniques different measures have been proposed to study brands  in this paper  it is studied whether regression methods can capture the relationship between customer satisfaction and brand measures  it is also investigated whether a combination of these brand measures is useful for the prediction of customer satisfaction  various regression methods were employed and it was found that generally there was a high correlation    0 7  between the combination of brand measures and customer satisfaction  attribute selection methods were used to find out the most important components among all the components of different brand measures  results suggest that a small subset of all the components  7 out of 111  gives almost the same prediction accuracy as with all the components of different brand measures  this subset of components consists of components from different brand measures  the results emphasize that various brand measures should be combined to improve the prediction accuracy of customer satisfaction  experiments also suggest that while various regression methods produce good results  support vector machine regression method generally perform best for this problem  
 evaluative language beyond bags of words  linguistic insights and computational applications the study of evaluation  affect  and subjectivity is a multidisciplinary enterprise  including sociology  psychology  economics  linguistics  and computer science  a number of excellent computational linguistics and linguistic surveys of the field exist  most surveys  however  do not bring the two disciplines together to show how methods from linguistics can benefit computational sentiment analysis systems  in this survey  we show how incorporating linguistic insights  discourse information  and other contextual phenomena  in combination with the statistical exploitation of data  can result in an improvement over approaches that take advantage of only one of these perspectives  we first provide a comprehensive introduction to evaluative language from both a linguistic and computational perspective  we then argue that the standard computational definition of the concept of evaluative language neglects the dynamic nature of evaluation  in which the interpretation of a given evaluation depends on linguistic and extra linguistic contextual factors  we thus propose a dynamic definition that incorporates update functions  the update functions allow for different contextual aspects to be incorporated into the calculation of sentiment for evaluative words or expressions  and can be applied at all levels of discourse  we explore each level and highlight which linguistic aspects contribute to accurate extraction of sentiment  we end the review by outlining what we believe the future directions of sentiment analysis are  and the role that discourse and contextual information need to play  
 event based knowledge reconciliation using frame embeddings and frame similarity this paper proposes an evolution over mergilo  a tool for reconciling knowledge graphs extracted from text  using graph alignment and word similarity  the reconciled knowledge graphs are typically used for multi document summarization  or to detect knowledge evolution across document series  the main point of improvement focuses on event reconciliation i e   reconciling knowledge graphs generated by text about two similar events described differently  in order to gather a complete semantic representation of events  we use fred semantic web machine reader  jointly with framester  a linguistic linked data hub represented using a novel formal semantics for frames  framester is used to enhance the extracted event knowledge with semantic frames  we extend mergilo with similarities based on the graph structure of semantic frames and the subsumption hierarchy of semantic roles as defined in framester  with an effective evaluation strategy similarly as used for mergilo  we show the improvement of the new approach  mergilo plus semantic frame role similarities  over the baseline   c  2017 elsevier b v  all rights reserved  
 every team deserves a second chance  an extended study on predicting team performance voting among different agents is a powerful tool in problem solving  and it has been widely applied to improve the performance in finding the correct answer to complex problems  we present a novel benefit of voting  that has not been observed before  we can use the voting patterns to assess the performance of a team and predict their final outcome  this prediction can be executed at any moment during problem solving and it is completely domain independent  hence  it can be used to identify when a team is failing  allowing an operator to take remedial procedures  such as changing team members  the voting rule  or increasing the allocation of resources   we present three main theoretical results   1  we show a theoretical explanation of why our prediction method works   2  contrary to what would be expected based on a simpler explanation using classical voting models  we show that we can make accurate predictions irrespective of the strength  i e   performance  of the teams  and that in fact  the prediction can work better for diverse teams composed of different agents than uniform teams made of copies of the best agent   3  we show that the quality of our prediction increases with the size of the action space  we perform extensive experimentation in two different domains  computer go and ensemble learning  in computer go  we obtain high quality predictions about the final outcome of games  we analyze the prediction accuracy for three different teams with different levels of diversity and strength  and show that the prediction works significantly better for a diverse team  additionally  we show that our method still works well when trained with games against one adversary  but tested with games against another  showing the generality of the learned functions  moreover  we evaluate four different board sizes  and experimentally confirm better predictions in larger board sizes  we analyze in detail the learned prediction functions  and how they change according to each team and action space size  in order to show that our method is domain independent  we also present results in ensemble learning  where we make online predictions about the performance of a team of classifiers  while they are voting to classify sets of items  we study a set of classical classification algorithms from machine learning  in a data set of hand written digits  and we are able to make high quality predictions about the final performance of two different teams  since our approach is domain independent  it can be easily applied to a variety of other domains  
 evolution of commitment and level of participation in public goods games before engaging in a group venture agents may require commitments from other members in the group  and based on the level of acceptance  participation  they can then decide whether it is worthwhile joining the group effort  here  we show in the context of public goods games and using stochastic evolutionary game theory modelling  which implies imitation and mutation dynamics  that arranging prior commitments while imposing a minimal participation when interacting in groups induces agents to behave cooperatively  our analytical and numerical results show that if the cost of arranging the commitment is sufficiently small compared to the cost of cooperation  commitment arranging behavior is frequent  leading to a high level of cooperation in the population  moreover  an optimal participation level emerges depending both on the dilemma at stake and on the cost of arranging the commitment  namely  the harsher the common good dilemma is  and the costlier it becomes to arrange the commitment  the more participants should explicitly commit to the agreement to ensure the success of the joint venture  furthermore  considering that commitment deals may last for more than one encounter  we show that commitment proposers can be lenient in case of short term agreements  yet should be strict in case of long term interactions  
 evolutionary computing applied to customer relationship management  a survey customer relationship management  crm  is a customer centric business strategy which a company employs to improve customer experience and satisfaction by customizing products and services to customers  needs  this strategy  when implemented in totality eventually increases the revenue of the company  traditionally  data mining  dm  techniques have been applied to solve various analytical crm tasks  in turn  optimization techniques have long been used for training some of the dm techniques  however  during the past few years  evolutionary techniques have become so powerful and versatile that they can be deployed as a substitute for some dm techniques  this trend caught the attention of the researchers working in the analytical crm area as they too started solving the crm tasks using evolutionary techniques alone  in this context  we present a survey of evolutionary computing techniques applied to crm tasks  in this paper  we surveyed 78 papers that were published during 1998 and 2015  where the application of evolutionary computing  ec  techniques to analytical crm tasks is the main focus  the survey includes papers involving evolutionary computing techniques applied to the analytical crm tasks under single as well as multi objective optimization framework  the purpose of the survey is to let the reader realize the versatility and power of ec techniques in solving analytical crm tasks in the service industry and suggesting future directions   c  2016 elsevier ltd  all rights reserved  
 evolutionary dynamics of strategies for threshold snowdrift games on complex networks the puzzle of altruistic cooperation attracts many concerns of researchers in multiple subjects nowadays  in this work we establish punishment in the framework of a threshold multiple player snowdrift game employed as the scenario for the cooperative dilemma problem  we show by analysis that given this assumption  punishing free riders can significantly influence the evolution dynamics  and the results are driven by the specific components of the punishing rule  intriguingly larger thresholds of the game provide a more favorable scenario for the coexistence of the cooperators and defectors under a broad value range of parameters  furthermore  we provide a two layer network framework for describing the individual interactions  by extending the threshold snowdrift games to the double layers networks  here  cooperators are best supported on complex networks by applying the threshold  moreover  the interlinks between the two layers are conducive to reinforce the network reciprocity   c  2017 elsevier b v  all rights reserved  
 evolutionary fuzzy intelligent system for multi objective supply chain network designs  an agent based optimization state of the art supply chain network designing and programming is a momentous issue that many practitioners have focused on and contributed numerous novelties for this prompt  this paper puts forward a fuzzy multi agent system according to which compatible with the decision makers  interests and environmental survey  identifies the parameters of the mathematical model  an embedded optimization party including evolutionary based optimizer intelligent agents  obtains non dominated potential solutions  the output of these optimizer agents during the calibration process is an underpinning for evaluating the performance of the party  the system makes the policy of optimization complying with the results evaluation as well as the decision makers  elaborated desires  afterwards  in step with this policy  it sets a pool from obtained pareto fronts and aggregates them to extract a set of the best individuals  it interactively represents this set to the decision makers and catches their desired circumstance amongst these optional solutions  proposing the network graph and program which its generic morphography is determined for decision makers is contrived as the system last stage  the main competencies of this system could be contemplated regarding the facts that it interactively fulfills the decision makers  utilities relying on its robustness in optimization  self tuning  training loop  ambient intelligence and consciousness toward the changes in environment  
 evolving cooperation in spatial population with punishment by using pso algorithm understanding the effects of punishment in multiplayer spatial games  however  is a formidable challenge  in this paper  we present a multiplayer evolutionary game model in which agents play iterative games in spatial populations with punishment  two kinds of spatial structure are used  regular and random connected network  key model parameters include the number of players  the interaction topology  the punishment and the cost to benefit ratio  the simulation results reveal that the punishment can promote the levels of cooperative behaviors to some extent  the cost to benefit ratio and the number of players is important factors in determining the strategy evolution  while punishment improves cooperation in these new environments  the impact of punishment is different in two social dilemma games  the spatial structures add the complex of the evolution and the random connected network can better isolate the defectors  
 evolving possibilistic fuzzy modeling for realized volatility forecasting with jumps equity asset volatility modeling and forecasting provide key information for risk management  portfolio construction  financial decision making  and derivative pricing  realized volatility models outperform autoregressive conditional heteroskedasticity and stochastic volatility models in out of sample forecasting  gain in forecasting performance is achieved when models comprise volatility jump components  this paper suggests evolving possibilistic fuzzy modeling to forecast realized volatility with jumps  the modeling approach is based on an extension of the possibilistic fuzzy c means clustering and on functional fuzzy rule based models  it employs memberships and typicalities to recursively update cluster centers  the evolving nature of the model allows adding or removing clusters using statistical distance like criteria to update the model as dictated by input data  the possibilistic model improves robustness to noisy data and outliers  an essential requirement in financial markets volatility modeling and forecasting  computational experiments and statistical analysis are done using value at risk estimates to evaluate and compare the performance of the evolving possibilistic fuzzy modeling with the heterogeneous autoregressive model  neural networks and current state of the art evolving fuzzy models  the experiments use actual data from s p 500 and nasdaq  u s    ftse  u k    dax  germany   ibex  spain   and ibovespa  brazil   major equity market indexes in global markets  the results show that the evolving possibilistic fuzzy model is highly efficient to model realized volatility with jumps in terms of forecasting accuracy  
 evolving trading strategies using directional changes the majority of forecasting methods use a physical time scale for studying price fluctuations of financial markets  making the flow of physical time discontinuous  therefore  using a physical time scale may expose companies to risks  due to ignorance of some significant activities  in this paper  an alternative and original approach is explored to capture important activities in the market  the main idea is to use an event based time scale based on a new way of summarising data  called directional changes  combined with a genetic algorithm  the proposed approach aims to find a trading strategy that maximises profitability in foreign exchange markets  in order to evaluate its efficiency and robustness  we run rigorous experiments on 255 datasets from six different currency pairs  consisting of intra day data from the foreign exchange spot market  the results from these experiments indicate that our proposed approach is able to generate new and profitable trading strategies  significantly outperforming other traditional types of trading strategies  such as technical analysis and buy and hold   c  2016 elsevier ltd  all rights reserved  
 examining incentives to share demand information with your channel partner in this paper  we examine the information sharing behavior of firms in a distribution channel context  channel alliance initiatives like ecr and category management often involve pooling of information available with manufacturers and retailers  such pooling of information should lead to better decision making and hence it is desirable  however  in practice  category management is often implemented with an intriguing institution of category captain  that involves the retailer entering into an alliance with only one  of many  supplier in a category  we first analyze the information sharing incentives of a manufacturer and retailer in a bilateral monopoly and identify the importance of quality of information available with the firms and the degree of complementarity of resources in determining the effectiveness of information sharing  we then show how these forces might lead to the emergence of the category captain phenomena in a model with competing manufacturers selling through a common retailer  
 expectation effect of perceptual experience in sensory modality transitions  modeling with information theory a user s experience of a product involves a set of state transitions  for example  the state of a sensory modality may shift from vision to touch to perceive a quality of a product  between such state transitions  users expect experiences of the subsequent states as well as experience the current state event  a discrepancy between prior expectation and posterior experience evokes emotions  such as surprise  satisfaction  and disappointment  affecting the perceived product value  a noteworthy psychological phenomenon is that expectation affects perceived experience  this phenomenon  called the expectation effect  is a key to designing the affective experience of a product  although experimental findings of this effect exist in a variety of disciplines  general and theoretical models of the effect are largely unexplored  in this paper  we propose a theoretical model of the expectation effect using information theory and affective expectation model  we hypothesize that shannon s entropy of prior subjective probability distributions of posterior experience determines the occurrence of an expectation effect  and the amount of information gained after experiencing a posterior event positively relates to the intensity of the expectation effect  furthermore  we hypothesize that the conscious awareness of expectation discrepancy discriminates the two types of expectation effect  assimilation and contrast  to verify these hypotheses  we conducted an experiment using the tactile quality of surface texture  in the experiment  we extracted the visual expectation effect on tactile roughness and analyzed the causes of the effect based on these hypotheses  the experimental results validated the proposed model  
 expectile regression neural network model with applications recently  nonlinear expectile regression becomes popular because it can not only explore nonlinear relationships among variables  but also describe the complete distribution of a response variable conditional on covariates information  in contrast  the traditional nonlinear expectile regression mainly confronts two shortcomings  first  it is difficult to select an appropriate form of nonlinear function  second  it ignores the interaction effects among covariates  in this paper  we develop a new expectile regression neural network  ernn  model by adding a neural network structure to expectile regression approach  the proposed ernn model is flexible and can be used to explore potential nonlinear effects of covariates on expectiles of the response  the ernn model can be easily estimated through standard gradient based optimization algorithms and output conditional expectile functions directly  the advantage of ernn model is illustrated by monte carlo simulation studies  the numerical results show that the ernn model outperforms the conventional expectile regression and support vector machine models in terms of predictive ability with both in sample and out of sample test  we also apply the ernn model to the predictions of concrete compressive strength and housing price  it turns out the marginal contribution of each predictor to the conditional expectile of a response is useful for decision making   c  2017 elsevier b v  all rights reserved  
 explaining machine learning models in sales predictions a complexity of business dynamics often forces decision makers to make decisions based on subjective mental models  reflecting their experience  however  research has shown that companies perform better when they apply data driven decision making  this creates an incentive to introduce intelligent  data based decision models  which are comprehensive and support the interactive evaluation of decision options necessary for the business environment  recently  a new general explanation methodology has been proposed  which supports the explanation of state of the art black box prediction models  uniform explanations are generated on the level of model individual instance and support what if analysis  we present a novel use of this methodology inside an intelligent system in a real world case of business to business  b2b  sales forecasting  a complex task frequently done judgmentally  users can validate their assumptions with the presented explanations and test their hypotheses using the presented what if parallel graph representation  the results demonstrate effectiveness and usability of the methodology  a significant advantage of the presented method is the possibility to evaluate seller s actions and to outline general recommendations in sales strategy  this flexibility of the approach and easy to follow explanations are suitable for many different applications  our well documented real world case shows how to solve a decision support problem  namely that the best performing black box models are inaccessible to human interaction and analysis  this could extend the use of the intelligent systems to areas where they were so far neglected due to their insistence on comprehensible models  a separation of the machine learning model selection from model explanation is another significant benefit for expert and intelligent systems  explanations unconnected to a particular prediction model positively influence acceptance of new and complex models in the business environment through their easy assessment and switching   c  2016 elsevier ltd  all rights reserved  
 explanatory dialogues with argumentative faculties over inconsistent knowledge bases we introduce a formal model of explanatory dialogue called eds  we extend this model by including argumentation capacities to facilitate knowledge acquisition in inconsistent knowledge bases  to prove the relevance of such model we provide the dalek  dialectical explanation in knowledge bases  framework that implements this model  we show the usefulness of the framework on a real world application in the domain of durum wheat sustainability improvement within the anr  french national agency  funded dur dur project  the preliminary pilot evaluation of the framework with agronomy experts gives a promising indication on the impact of explanation dialogues on the improvement of the knowledge s content   c  2017 elsevier ltd  all rights reserved  
 exploration of cognition affect and type 1 type 2 dichotomies in a computational model of decision making this paper studies the role of cognition and affect in decision making as well as notions of type 1 and 2 processes and behaviors typically used in dual process theories  in order to demonstrate that there is no 1  1 correspondence between types of observed behavior and internal processes causing them  and that type 1 and type 2 processes can be produced by a single system  we implemented a computational model integrating affective and cognitive processing  our model is based on the model of marinier  laird  and lewis  2009   we modified it by increasing the agent s visual field  adding a gofai style cognitive module  sub goal management  and expanding the environment by a high threat tile  to which the agent responds with a hard wired automatic reaction  this allowed us to generate and observe different types of behavior and study interesting interactions between cognitive and affective control  by comparing our re implementation to the modified agent  we demonstrated clear cases of type 1  fast  automatic  and type 2  slow  deliberative  behavior  providing further evidence for the   single system  two processes  hypothesis   c  2016 elsevier b v  all rights reserved  
 exploratory analysis of older adults  sedentary behavior in the primary living area using kinect depth data we describe case studies of clinically significant changes in sedentary behavior of older adults captured with a novel computer vision algorithm for depth data  an unobtrusive microsoft kinect sensor continuously recorded older adults  activity in the primary living spaces of tigerplace apartments  using the depth data from a period of ten months  we develop a context aware algorithm to detect person specific postural changes  sit to stand and stand to sit events  that define sedentary behavior  the robustness of our algorithm was validated over 33 120 minutes of data for 5 residents against manual analysis of raw depth data as the ground truth  with a strong correlation  r   0 937  p   0 001  and mean error of 17 minutes day  our findings are highlighted in two case studies of sedentary activity and its relationship to clinical assessments of functional decline  our findings show strong potential for future research towards a generalizable platform to automatically study sedentary behavior patterns with an in home activity monitoring system  
 exploratory querying of sparql endpoints in space and time the linked data web provides a simple and flexible way of accessing information resources in a self descriptive format  this offers a realistic chance of perforating existing data silos  however  in order to do so  space  time and other semantic concepts need to function as dimensions for effectively exploring  querying and filtering contents  while triple stores  sparql endpoints  and rdf were designed for machine access  large burdens are still placed on a user to simultaneously explore and query the contents of a given endpoint according to these dimensions  first  one has to know the semantic concepts and the type of knowledge contained in an endpoint a priori in order to query content effectively  second  one has to be able to write and understand sparql and rdf  and third  one has to understand complex data type literals for space and time  in this article  we propose a way to deal with these challenges by interactive visual query construction  i e   by letting query results feedback into both  space time  exploration and filtering  and thus enabling exploratory querying  we propose design principles for spex  spatio temporal content explorer   a tool which helps people unfamiliar with the content of sparql endpoints or their syntax to explore the latter in space and time  in a preliminary user study on a repository of historical maps  we found that our feedback principles were effective  however  that successful question answering still requires improvements regarding space time filtering  vocabulary explanation and the linking of space time windows with other displays  
 exploring patient perceptions of healthcare service quality through analysis of unstructured feedback mechanisms for collecting unstructured feedback  i e   text comments  from patients of healthcare providers have become commonplace  but analysis techniques to examine such feedback have not been frequently applied in this domain  to fill this gap  we apply a text mining methodology to a large set of textual feedback of physicians by their patients and relate the textual commentary to their numeric ratings  while perceptions of healthcare service quality in the form of numeric ratings are easy to aggregate  freeform textual commentary presents more challenges to extracting useful information  our methodology explores aggregation of the textual commentary using a topic analysis procedure  i e   latent dirichlet allocation  and a sentiment tool  i e   diction   we then explore how the extracted topic areas and expressed sentiments relate to the physicians  quantitative ratings of service quality from both patients and other physicians  we analyze 23 537 numeric ratings plus textual feedback provided by patients of 3 712 physicians who have also been recommended by other physicians  and determine process quality satisfaction is an important driver of patient perceived quality  whereas clinical quality better reflects physician perceived quality  our findings lead us to suggest that to maximize the usefulness of online reviews of physicians  potential patients should parse them for particular quality elements they wish to assess and interpret them within the scope of those quality elements   c  2016 elsevier ltd  all rights reserved  
 exploring the ordered weighted averaging operator knowledge domain  a bibliometric analysis ordered weighted averaging  owa  operator has been received increasingly widespread interest since its appearance in 1988  recently  a topic search with the keywords ordered weighted averaging operator or owa operator on web of science  wos  found 1231 documents  as the publications about owa operator increase rapidly  thus a scientometric analysis of this research field and discovery of its knowledge domain becomes very important and necessary  this paper studies the publications about owa operator between 1988 and 2015  and it is based on 1213 bibliographic records obtained by using topic search from wos  the disciplinary distribution  most cited papers  influential journals  as well as influential authors are analyzed through citation and cocitation analysis  the emerging trends in owa operator research are explored by keywords and references burst detection analysis  the research methods and results in this paper are meaningful for researchers associated with owa operator field to understand the knowledge domain and establish their own future research direction  
 exploring trading strategies and their effects in the foreign exchange market one of the most critical issues that developers face in developing automatic systems for electronic markets is that of endowing the agents with appropriate trading strategies  in this article  we examine the problem in the foreign exchange  fx  market  and we use an agent based market simulation to examine which trading strategies lead to market states in which the stylized facts  statistical properties  of the simulation match those of the fx market transactions data  our goal is to explore the emergence of the stylized facts  when the simulated market is populated with agents using different strategies  a variation of the zero intelligence with a constraint strategy  the zero intelligence directional change event strategy  and a genetic programming based strategy  a series of experiments were conducted  and the results were compared with those of a high frequency fx transaction data set  our results show that the zero intelligence directional change event agents best reproduce and explain the properties observed in the fx market transactions data  our study suggests that the observed stylized facts could be the result of introducing a threshold that triggers the agents to respond to periodic patterns in the price time series  the results can be used to develop decision support systems for the fx market  
 extreme learning machines for credit scoring  an empirical evaluation classification algorithms are used in many domains to extract information from data  predict the entry probability of events of interest  and  eventually  support decision making  this paper explores the potential of extreme learning machines  elm   a recently proposed type of artificial neural network  for consumer credit risk management  elm possess some interesting properties  which might enable them to improve the quality of model based decision support  to test this  we empirically compare elm to established scoring techniques according to three performance criteria  ease of use  resource consumption  and predictive accuracy  the mathematical roots of elm suggest that they are especially suitable as a base model within ensemble classifiers  therefore  to obtain a holistic picture of their potential  we assess elm in isolation and in conjunction with different ensemble frameworks  the empirical results confirm the conceptual advantages of elm and indicate that they are a valuable alternative to other credit risk modelling methods   c  2017 elsevier ltd  all rights reserved  
 extreme punishments characterize weak pareto optimality in normal form games  we model the largely observed psychological phenomenon of systematic and extreme punishment after a deviation  regardless of the cost  after establishing basic properties  we show that this notion characterizes a weak form of pareto optimality  every pareto optimal outcome can also be sustained by the threat of extreme punishment  which cannot be achieved in general through nash equilibria strategies  nor with tit for tat strategies  
 face hallucination using linear models of coupled sparse support most face super resolution methods assume that low and high resolution manifolds have similar local geometrical structure  hence  learn local models on the low resolution manifold  e g   sparse or locally linear embedding models   which are then applied on the high resolution manifold  however  the low resolution manifold is distorted by the one to many relationship between low and high resolution patches  this paper presents the linear model of coupled sparse support  lm css  method  which learns linear models based on the local geometrical structure on the high resolution manifold rather than on the low resolution manifold  for this  in a first step  the low resolution patch is used to derive a globally optimal estimate of the high resolution patch  the approximated solution is shown to be close in the euclidean space to the ground truth  but is generally smooth and lacks the texture details needed by the state of the art face recognizers  unlike existing methods  the sparse support that best estimates the first approximated solution is found on the high resolution manifold  the derived support is then used to extract the atoms from the coupled low and high resolution dictionaries that are most suitable to learn an up scaling function for every facial region  the proposed solution was also extended to compute face super resolution of non frontal images  extensive experimental results conducted on a total of 1830 facial images show that the proposed method outperforms seven face super resolution and a state of the art cross resolution face recognition method in terms of both quality and recognition  
 facility location using gis enriched demographic and lifestyle data for a traveling entertainment troupe in bavaria  germany this paper presents the development and subsequent application of a facility location methodology for selecting good show locations for a traveling entertainment troupe in bavaria  germany  the troupe is headquartered at a theater in munich and wishes to expand its audience by offering traveling shows to select sites across bavaria  a spatial analysis of the region is completed via classic location theory modeling techniques  leading to the development of a multi criteria facility location approach for application  additionally  we use location analytics techniques on demographic and consumer spending data extracted from the business analyst web app  bawa  system for each of the 95 districts in bavaria  this data is integrated into a decision support system to weight consumer demand values with district lifestyle population patterns aggregated at the postal code level  lifestyle weighted demand is then used to identify locations that maximize the amount of customers within a given travel distance to a show while maintaining dispersion of selected facilities   c  2017 elsevier b v  all rights reserved  
 facility location with double peaked preferences we study the problem of locating a single facility on a real line based on the reports of self interested agents  when agents have double peaked preferences  with the peaks being on opposite sides of their locations  we observe that double peaked preferences capture real life scenarios and thus complement the well studied notion of single peaked preferences  as a motivating example  assume that the government plans to build a primary school along a street  an agent with single peaked preferences would prefer having the school built exactly next to her house  however  while that would make it very easy for her children to go to school  it would also introduce several problems  such as noise or parking congestion in the morning  a 5 min walking distance would be sufficiently far for such problems to no longer be much of a factor and at the same time sufficiently close for the school to be easily accessible by the children on foot  there are two positions  symmetrically  in each direction and those would be the agent s two peaks of her double peaked preference  motivated by natural scenarios like the one described above  we mainly focus on the case where peaks are equidistant from the agents  locations and discuss how our results extend to more general settings  we show that most of the results for single peaked preferences do not directly apply to this setting  which makes the problem more challenging  as our main contribution  we present a simple truthful in expectation mechanism that achieves an approximation ratio of for both the social and the maximum cost  where b is the distance of the agent from the peak and c is the minimum cost of an agent  for the latter case  we provide a 3 2 lower bound on the approximation ratio of any truthful in expectation mechanism  we also study deterministic mechanisms under some natural conditions  proving lower bounds and approximation guarantees  we prove that among a large class of reasonable strategyproof mechanisms  there is no deterministic mechanism that outperforms our truthful in expectation mechanism  in order to obtain this result  we first characterize mechanisms for two agents that satisfy two simple properties  we use the same characterization to prove that no mechanism in this class can be group strategyproof  
 factor augmented artificial neural network model this paper brings together two important developments in forecasting literature  the artificial neural networks and factor models  the paper introduces the factor augmented artificial neural network  faann  hybrid model in order to produce a more accurate forecasting  theoretical and empirical findings have indicated that integration of various models can be an effective way of improving on their predictive performance  especially when the models in the ensemble are quite different  the proposed model is used to forecast three time series variables using large south african monthly panel  namely  deposit rate  gold mining share prices and long term interest rate  using monthly data over the in sample period  training set  1992 1 2006 12  the variables are used to compute out of sample  testing set  results for 3  6 and 12 month ahead forecasts for the period of 2007 1 2011 12  the out of sample root mean square error findings show that the faann model yields substantial improvements over the autoregressive ar benchmark model and standard dynamic factor model  dfm   the diebold mariano test results also further confirm the superiority of the faann model forecast performance over the ar benchmark model and the dfm model forecasts  the superiority of the faann model is due to the ann flexibility to account for potentially complex nonlinear relationships that are not easily captured by linear models  
 fairness in examination timetabling  student preferences and extended formulations variations of the examination timetabling problem have been investigated by the research community for more than two decades  the common characteristic between all problems is the fact that the definitions and datasets used all originate from actual educational institutions  particularly universities  including specific examination criteria and the students involved  although much has been achieved and published on the state of the art problem modelling and optimisation  a lack of attention has been focussed on the students involved in the process  this work presents and utilises the results of an extensive survey seeking student preferences with regard to their individual examination timetables  with the aim of producing solutions which satisfy these preferences while still also satisfying all existing benchmark considerations  the study reveals one of the main concerns relates to fairness within the student s cohort  i e  a student considers fairness with respect to the examination timetables of their immediate peers  as highly important  considerations such as providing an equitable distribution of preparation time between all student cohort examinations  not just a majority  are used to form a measure of fairness  in order to satisfy this requirement  we propose an extension to the state of the art examination timetabling problem models widely used in the scientific literature  fairness is introduced as a new objective in addition to the standard objectives  creating a multi objective problem  several real world examination data models are extended and the benchmarks for each are used in experimentation to determine the effectiveness of a multi stage multi objective approach based on weighted tchebyceff scalarisation in improving fairness along with the other objectives  the results show that the proposed model and methods allow for the production of high quality timetable solutions while also providing a trade off between the standard soft constraints and a desired fairness for each student   c  2017 elsevier b v  all rights reserved  
 false name manipulation in weighted voting games  empirical and theoretical analysis weighted voting games are important in multiagent systems because of their usage in automated decision making  however  they are not immune from the vulnerability of false name manipulation by strategic agents that may be present in the games  false name manipulation involves an agent splitting its weight among several false identities in anticipation of power increase  previous works have considered false name manipulation using the well known shapley shubik and banzhaf power indices  bounds on the extent of power that a manipulator may gain exist when it splits into k   2 false identities for both the shapley shubik and banzhaf indices  the bounds when an agent splits into k   2 false identities  until now  have remained open for the two indices  this article answers this open problem by providing four nontrivial bounds when an agent splits into k   2 false identities for the two indices  furthermore  we propose a new bound on the extent of power that a manipulator may gain when it splits into several false identities in a class of games referred to as excess unanimity weighted voting games  finally  we complement our theoretical results with empirical evaluation  results from our experiments confirm the existence of beneficial splits into several false identities for the two indices  and also establish that splitting into more than two false identities is qualitatively different than the previously known splitting into exactly two false identities  
 fast robust sur with economical and actuarial applications the seemingly unrelated regression  sur  model is a generalization of a linear regression model consisting of more than one equation  where the error terms of these equations are contemporaneously correlated  the standard feasible generalized linear squares  fgls  estimator is efficient as it takes into account the covariance structure of the errors  but it is also very sensitive to outliers  the robust sur estimator of bilodeau and duchesne  canadian journal of statistics  28  277 288  2000  can accommodate outliers  but it is hard to compute  first  we propose a fast algorithm  fastsur  for its computation and show its good performance in a simulation study  we then provide diagnostics for outlier detection and illustrate them on a real dataset from economics  next  we apply our fastsur algorithm in the framework of stochastic loss reserving for general insurance  we focus on the general multivariate chain ladder  gmcl  model that employs sur to estimate its parameters  consequently  this multivariate stochastic reserving method takes into account the contemporaneous correlations among run off triangles and allows structural connections between these triangles  we plug in our fastsur algorithm into the gmcl model to obtain a robust version   c  2016 wiley periodicals  inc  
 fast simulated annealing hybridized with quenching for solving job shop scheduling problem various heuristic based methods are available in literature for optimally solving job shop scheduling problems  jssp   in this research work a novel approach is proposed which hybridizes fast simulated annealing  fsa  with quenching  the proposed algorithm uses fsa for global search and quenching for localized search in neighborhood of current solution  while tabu list is used to restrict search from revisiting previously explored solutions  fsa is started with a relatively higher temperature and as search progresses temperature is gradually reduced to a value close to zero  the overall best solution  bs  is maintained throughout execution of the algorithm  if no improvement is observed in bs for certain number of iterations then quenching cycle is invoked  during quenching cycle current temperature is reduced to nearly freezing point and iterations are increased by many folds  as a result of this change search becomes nearly greedy  the strength of the proposed algorithm is that even in quenching mode escape from local optima is possible due to use of cauchy probability distribution and non zero temperature  at the completion of quenching cycle previous values of search parameters are restored and fsa takes over  which moves search into another region of solution space  effectiveness of proposed algorithm is established by solving 88 well known benchmark problems taken form published work  the proposed algorithm was able to solve 45 problems optimally to their respective best known values in reasonable time  the proposed algorithm has been compared with 18 other published works  the experimental results show that the proposed algorithm is efficient in finding solution to jssp   c  2016 elsevier b v  all rights reserved  
 feature extraction and selection for arabic tweets authorship authentication in tweet authentication  we are concerned with correctly attributing a tweet to its true author based on its textual content  the more general problem of authenticating long documents has been studied before and the most common approach relies on the intuitive idea that each author has a unique style that can be captured using stylometric features  sf   inspired by the success of modern automatic document classification problem  some researchers followed the bag of words  bow  approach for authenticating long documents  in this work  we consider both approaches and their application on authenticating tweets  which represent additional challenges due to the limitation in their sizes  we focus on the arabic language due to its importance and the scarcity of works related on it  we create different sets of features from both approaches and compare the performance of different classifiers using them  we experiment with various feature selection techniques in order to extract the most discriminating features  to the best of our knowledge  this is the first study of its kind to combine these different sets of features for authorship analysis of arabic tweets  the results show that combining all the feature sets we compute yields the best results  
 feature extraction based on bio inspired model for robust emotion recognition emotional state identification is an important issue to achieve more natural speech interactive systems  ideally  these systems should also be able to work in real environments in which generally exist some kind of noise  several bio inspired representations have been applied to artificial systems for speech processing under noise conditions  in this work  an auditory signal representation is used to obtain a novel bio inspired set of features for emotional speech signals  these characteristics  together with other spectral and prosodic features  are used for emotion recognition under noise conditions  neural models were trained as classifiers and results were compared to the well known mel frequency cepstral coefficients  results show that using the proposed representations  it is possible to significantly improve the robustness of an emotion recognition system  the results were also validated in a speaker independent scheme and with two emotional speech corpora  
 feature level combination of skeleton joints and body parts for accurate aggressive and agitated behavior recognition this paper presents a novel and practical approach for aggressive and agitated behavior recognition using skeleton data  our approach is based on feature level combination of joint based features and body part based features  to characterize spatiotemporal information  our approach extracts first meaningful joint based features by computing pairwise distances of skeleton 3d joint positions at each time frame  then  distances between body parts as well as joint angles are computed to incorporate body part features  these features are then effectively combined using an ensemble learning method based on rotation forests  a singular value decomposition method is used for feature selection and dimensionality reduction  the proposed approach is validated using extensive experiments on variety of challenging 3d action datasets for human behavior recognition  we empirically demonstrate that our proposed approach accurately discriminates between behaviors and performs better than several state of the art algorithms  
 feature weighted clustering with inner product induced norm based dissimilarity measures  an optimization perspective the performance of a clustering algorithm can be improved by assigning appropriate weights to different features of the data  such feature weighting is likely to reduce the effect of noise and irrelevant features while enhancing the effect of the discriminative features simultaneously  for the clustering purpose  feature weighted dissimilarity measures are so far limited to euclidean  mahalanobis  and exponential distances  in this article  we introduce a novel feature weighting scheme for the general class of inner product induced norm  ipin  based weighted dissimilarity measures  this class has a wide range and includes the three above mentioned distances as special cases  we develop the general algorithms to solve the hard  k means  and fuzzy  fuzzy c means  partitional clustering problems and undertake in depth analyses of the convergence of the algorithms as well  in addition  we address issues like feasibility and uniqueness of the solutions of these problems in sufficient details  the novelty of the article lies in the introduction of a general feature weighting scheme for the generalized class of ipin based dissimilarity measures and a complete convergence analysis for the automated feature weighted clustering algorithms using such measures  
 fiart  adaptive resonance model of feature integration  proto objects formation and coherence theory of visual attention the adaptive resonance theory  art1  art2  etc   provides neural networks with means to model the parallel accumulation of features  followed by a serial search for matching feature patterns  this process reminds the psychology of visual attention  in particular  feature integration and coherence theories  this paper presents the art based neural architecture  fiart   inspired by them  the fiart consists of multifeature calculation layer  art1 modules and inter feature associations  one of the main problems in application of art to processing of natural images is necessity to adapt art to the diversity of visual features  this paper presents the bioplausible approach to resolve this problem  the fiart is a multifeature art architecture  based on separateart i  x  modules  each processing one visual feature  i  in one location  x  in input image  the art modules in fiart are connected by mutual excitation associative connections a i j   allowing to initiate the winner takes all competition between groupings of feature patterns corresponding to  proto objects  in the coherence theory of visual attention  the fiart neural network was verified on a set of simple images of textured objects  zebras  and showed its ability to cluster images of the same object and to form its internal representation  corresponding to the memorized image of the object in the brain  fiart was also compared to human behavioral studies of attention  textural features integration and proto objects formation  the fiart is an expansion of art1 to multifeature processing  as well as its application to modeling the feature integration theory of visual attention   c  2017 elsevier b v  all rights reserved  
 financial distress prediction using svm ensemble based on earnings manipulation and fuzzy integral financial distress prediction  fdp  has received considerable attention from both practitioners and researchers  this paper proposes a novel support vector machine  svm  classifier ensemble framework based on earnings manipulation and fuzzy integral for fdp named semfi  we use financial data from the previous three years to predict companies  current financial situation and divide the companies in each year into different categories according to whether they manipulate the earnings  then  svm is trained on different categories  the outputs of svm are combined by fuzzy integral which adopts a new fuzzy measure determination method  this method considers the fact that recent financial data are more valuable for fdp  additionally  when using the model trained by historical data for fdp  the external environment may have dramatically changed  therefore  a fuzzy measure dynamic adjustment method is proposed by considering the confidence of each single classifier s output  the consistency between each single classifier s output and the diversity among classifiers  to verify the performance of semfi  an empirical study using real financial data is conducted  the results indicate that the introduction of earnings manipulation  the new fuzzy measure determination and dynamic adjustment method to fdp can significantly enhance the prediction performance  
 financial time series forecasting using rough sets with time weighted rule voting in the paper we investigate experimentally the feasibility of rough sets in building profitable trend prediction models for financial time series  in order to improve the decision process for long time series  a novel time weighted rule voting method  which accounts for information aging  is proposed  the experiments have been performed using market data of multiple stock market indices  the classification efficiency and financial performance of the proposed rough sets models was verified and compared with that of support vector machines models and reference financial indices  the results showed that the rough sets approach with time weighted rule voting outperforms the classical rough sets and support vector machines decision systems and is profitable as compared to the buy and hold strategy  in addition  with the use of variable precision rough sets  the effectiveness of generated trading signals was further improved   c  2016 elsevier ltd  all rights reserved  
 financial time series prediction using hybrids of chaos theory  multi layer perceptron and multi objective evolutionary algorithms financial time series prediction is a complex and a challenging problem  in this paper  we propose two 3 stage hybrid prediction models wherein chaos theory is used to construct phase space  stage 1  followed by invoking multi layer perceptron  mlp   stage 2  and multi objective particle swarm optimization  mopso    elitist non dominated sorting genetic algorithm  nsga ii   stage 3  in tandem  in both of these hybrid models  stage 3 improves the prediction yielded by stage 2  the effectiveness of the proposed models is tested on financial datasets including the exchange rates data of us dollar  usd  versus japanese yen  jpy   british pound  gbp   euro  eur   and gold price in terms of usd  from the results  it is concluded that chaos mlp  nsga ii hybrid yielded better predictions than the other three stage hybrid models  chaos mlp mopso and chaos mlp pso  and two stage hybrid models  chaos pso  chaos mopso and chaos nsga ii in terms of both mean squared error  mse  and directional change statistic  dstat   theirs inequality coefficient computed also confirms the superiority of the chaos mlp nsga ii hybrid over the chaos mlp mopso across all datasets  finally  diebold mariano test indicates that the performance of chaos mlp nsga ii hybrid is statistically significant than the chaos mlp mopso and other hybrids across all datasets  the results of these models are also compared with the two stage hybrids found in literature  1 2   pradeepkumar and ravi  2014  2017   these results are encouraging and suggest further application of these hybrids to other financial and scientific time series prediction problems in the future  
 find the errors  get the better  enhancing machine translation via word confidence estimation this paper presents two novel ideas of improving the machine translation  mt  quality by applying the word level quality prediction for the second pass of decoding  in this manner  the word scores estimated by word confidence estimation systems help to reconsider the mt hypotheses for selecting a better candidate rather than accepting the current sub optimal one  in the first attempt  the selection scope is limited to the mt n best list  in which our proposed re ranking features are combined with those of the decoder for re scoring  then  the search space is enlarged over the entire search graph  storing many more hypotheses generated during the first pass of decoding  over all paths containing words of the n best list  we propose an algorithm to strengthen or weaken them depending on the estimated word quality  in both methods  the highest score candidate after the search becomes the official translation  the results obtained show that both approaches advance the mt quality over the one pass baseline  and the search graph re decoding achieves more gains  in bleu score  than n best list re ranking method  
 first and second order dynamics in a hierarchical som system for action recognition human recognition of the actions of other humans is very efficient and is based on patterns of movements  our theoretical starting point is that the dynamics of the joint movements is important to action categorization  on the basis of this theory  we present a novel action recognition system that employs a hierarchy of self organizing maps together with a custom supervised neural network that learns to categorize actions  the system preprocesses the input from a kinect like 3d camera to exploit the information not only about joint positions  but also their first and second order dynamics  we evaluate our system in two experiments with publicly available datasets  and compare its performance to the performance with less sophisticated preprocessing of the input  the results show that including the dynamics of the actions improves the performance  we also apply an attention mechanism that focuses on the parts of the body that are the most involved in performing the actions   c  2017 elsevier b v  all rights reserved  
 flash mobs  arab spring and protest movements  can we analyse group identities in online conversations  the internet has provided people with new ways of expressing not only their individuality but also their collectivity i e   their group affiliations  these group identities are the shared sense of belonging to a group  online contact with others who share the same group identity can lead to cooperation and  even  coordination of social action initiatives both online and offline  such social actions may be for the purposes of positive change  e g   the arab spring in 2010  or disruptive  e g   the england riots in 2011  stylometry and authorship attribution research has shown that it is possible to distinguish individuals based on their online language  in contrast  this work proposes and evaluates a model to analyse group identities online based on textual conversations amongst groups  we argue that textual features make it possible to automatically distinguish between different group identities and detect whether group identities are salient  i e   most prominent  in the context of a particular conversation  we show that the salience of group identities can be detected with 95  accuracy and group identities can be distinguished from others with 84  accuracy  we also identify the most relevant features that may enable mal actors to manipulate the actions of online groups  this has major implications for tools and techniques to drive positive social actions online or safeguard society from disruptive initiatives  at the same time  it poses privacy challenges given the potential ability to persuade or dissuade large groups online to move from rhetoric to action   c  2016 elsevier ltd  all rights reserved  
 flocking based evolutionary computation strategy for measuring centrality of online social networks centrality in social network is one of the major research topics in social network analysis  even though there are more than half a dozen methods to find centrality of a node  each of these methods has some drawbacks in one aspect or the other  this paper analyses different centrality calculation methods and proposes a new swarm based method named flocking based centrality for social network  fbcs   this new computation technique makes use of parameters that are more realistic and practical in online social networks  the interactions between nodes playa significant role in determining the centrality of node  the new method has been calculated both empirically as well as experimentally  the new method is tested  verified and validated for different sets of random networks and benchmark datasets  the method has been correlated with other state of the art centrality measures  the new centrality measure is found to be realistic and suits well with online social networks  the proposed method can be used in applications such as finding the most prestigious node and for discovering the node which can influence maximum number of users in an online social network  fbcs centrality has higher kendall s tau correlation when compared with other state of the art centrality methods  the robustness of the fbcs centrality is found to be better than other centrality measures   c  2017 elsevier b v  all rights reserved  
 follow the herd or be myself  an analysis of consistency in behavior of reviewers and helpfulness of their reviews sults emerge from the econometric analyses using publicly available data from tripadvisor com  first  reviewers  rating behavior is consistent over time and across products  furthermore  most of the variation in their future rating behavior can be explained by their rating behavior in the past rather than by the observed average rating  second  reviews by reviewers with higher absolute bias in rating in the past receive more helpful votes in future  we further divide the bias in rating into intrinsic bias  driven by intrinsic reviewer characteristics  and extrinsic bias  driven by influences beyond intrinsic bias  and document that intrinsic bias plays a more significant role in influencing helpful votes for reviews than extrinsic bias  our results are robust to different product categories and different definition of bias  overall our results indicate that in the online review context  the observed average rating or an attention grabbing strategy may not be as important as believed in the past  this study provides insights into reviewers  rating behavior and prescribes actionable items for online vendors so that they can proactively influence online opinion instead of passively responding to them  c  2016 elsevier b v  all rights reserved  
 force velocity assessment of caress like stimuli through the electrodermal activity processing  advantages of a convex optimization approach we propose the use of the convex optimization based eda  cvxeda  framework to automatically characterize the force and velocity of caressing stimuli through the analysis of the electrodermal activity  eda   cvxeda  in fact  solves a convex optimization problem that always guarantees the globally optimal solution  we show that this approach is especially suitable for the implementation in wearable monitoring systems  being more computationally efficient than a widely used eda processing algorithm  in addition  it ensures low memory consumption  due to a sparse representation of the eda phasic components  eda recordings were gathered from 32 healthy subjects  16 females  who participated in an experiment where a fabric based wearable haptic system conveyed them caress like stimuli by means of two motors  six types of stimuli  combining three levels of velocity and two of force  were randomly administered over time  performance was evaluated in terms of execution time of the algorithm  memory usage  and statistical significance in discerning the affective stimuli along force and velocity dimensions  experimental results revealed good performance of cvxedamodel for all of the considered metrics  
 forecasting daily stock market return using dimensionality reduction in financial markets  it is both important and challenging to forecast the daily direction of the stock market return  among the few studies that focus on predicting daily stock market returns  the data mining procedures utilized are either incomplete or inefficient  especially when a large amount of features are involved  this paper presents a complete and efficient data mining process to forecast the daily direction of the s p 500 index etf  spy  return based on 60 financial and economic features  three mature dimensionality reduction techniques  including principal component analysis  pca   fuzzy robust principal component analysis  frpca   and kernel based principal component analysis  kpca  are applied to the whole data set to simplify and rearrange the original data structure  corresponding to different levels of the dimensionality reduction  twelve new data sets are generated from the entire cleaned data using each of the three different dimensionality reduction methods  artificial neural networks  anns  are then used with the thirty six transformed data sets for classification to forecast the daily direction of future market returns  moreover  the three different dimensionality reduction methods are compared with respect to the natural data set  a group of hypothesis tests are then performed over the classification and simulation results to show that combining the anns with the pca gives slightly higher classification accuracy than the other two combinations  and that the trading strategies guided by the comprehensive classification mining procedures based on pca and anns gain significantly higher risk adjusted profits than the comparison benchmarks  while also being slightly higher than those strategies guided by the forecasts based on the frpca and kpca models   c  2016 elsevier ltd  all rights reserved  
 forecasting financial time series volatility using particle swarm optimization trained quantile regression neural network accurate forecasting of volatility from financial time series is paramount in financial decision making  this paper presents a novel  particle swarm optimization  pso  trained quantile regression neural network namely psoqrnn  to forecast volatility from financial time series  we compared the effectiveness of psoqrnn with that of the traditional volatility forecasting models  i e   generalized autoregressive conditional heteroskedasticity  garch  and three artificial neural networks  anns  including multi layer perceptron  mlp   general regression neural network  grnn   group method of data handling  gmdh   random forest  rf  and two quantile regression  qr  based hybrids including quantile regression neural network  qrnn  and quantile regression random forest  qrrf   the results indicate that the proposed psoqrnn outperformed these models in terms of mean squared error  mse   on a majority of the eight financial time series including exchange rates of usd versus jpy  gbp  eur and inr  gold price  crude oil price  standard and poor 500  s p 500  stock index and nse india stock index considered here  it was corroborated by the dieboldmariano test of statistical significance  it also performed well in terms of other important measures such as directional change statistic  dstat  and theil s inequality coefficient  the superior performance of psoqrnn can be attributed to the role played by pso in obtaining the better solutions  therefore  we conclude that the proposed psoqrnn can be used as a viable alternative in forecasting volatility   c  2017 elsevier b v  all rights reserved  
 forecasting price movements using technical indicators  investigating the impact of varying input window length the creation of a predictive system that correctly forecasts future changes of a stock price is crucial for investment management and algorithmic trading  the use of technical analysis for financial forecasting has been successfully employed by many researchers  input window length is a time frame parameter required to be set when calculating many technical indicators  this study explores how the performance of the predictive system depends on a combination of a forecast horizon and an input window length for forecasting variable horizons  technical indicators are used as input features for machine learning algorithms to forecast future directions of stock price movements  the dataset consists of ten years daily price time series for fifty stocks  the highest prediction performance is observed when the input window length is approximately equal to the forecast horizon  this novel pattern is studied using multiple performance metrics  prediction accuracy  winning rate  return per trade and sharpe ratio  crown copyright  c  2017 published by elsevier b v  all rights reserved  
 forecasting stochastic neural network based on financial empirical mode decomposition in an attempt to improve the forecasting accuracy of stock price fluctuations  a new one step ahead model is developed in this paper which combines empirical mode decomposition  emd  with stochastic time strength neural network  stnn   the emd is a processing technique introduced to extract all the oscillatory modes embedded in a series  and the stnn model is established for considering the weight of occurrence time of the historical data  the linear regression performs the predictive availability of the proposed model  and the effectiveness of emd stnn is revealed clearly through comparing the predicted results with the traditional model  moreover  a new evaluated method  q order multiscale complexity invariant distance  is applied to measure the predicted results of real stock index series  and the empirical results show that the proposed model indeed displays a good performance in forecasting stock market fluctuations   c  2017 elsevier ltd  all rights reserved  
 forecasting volatility of oil price using an artificial neural network garch model this paper builds on previous research and seeks to determine whether improvements can be achieved in the forecasting of oil price volatility by using a hybrid model and incorporating financial variables  the main conclusion is that the hybrid model increases the volatility forecasting precision by 30  over previous models as measured by a heteroscedasticity adjusted mean squared error  hmse  model  key financial variables included in the model that improved the prediction are the euro dollar and yen dollar exchange rates  and the djia and ftse stock market indexes   c  2016 elsevier ltd  all rights reserved  
 forecasting  clustering and patrolling criminal activities tools that perform pattern recognition analysis of crimes  comprising at the same time forecasting  clustering  and recommendations on real data such as patrolling routes  are not fully integrated  modules are developed separately  and thus  a single workflow providing all the steps necessary to perform this analysis has not been reported  in this paper  we propose forecasting criminal activity in a particular region by using supervised classification  then  to use this information to automatically cluster and find important hot spots  and finally  to optimize patrolling routes for personnel working in public security  the proposed forecasting model  cr omega vertical bar   is based on the family of kora omega logical combinatorial algorithms operating on large data volumes from several heterogeneous sources using an inductive learning process  we perform two analyses  punctual prediction and tendency analysis  which show that it is possible to punctually predict one out of four crimes to be perpetrated  crime family  in a specific space and time   and two out of three times the place of crime  despite of the noise of the dataset  the forecasted crimes are then clustered using a density based clustering algorithm  and finally route patrolling routes were crafted using an ant colony optimization algorithm  for three different patrolling requirements  we were always able to find optimal routes in shorter time compared to commonly used random walk algorithms  we present a case study based on real crime data from the municipality of cuautitlan izcalli  in mexico  
 formal distributional semantics  introduction to the special issue formal semantics and distributional semantics are two very influential semantic frameworks in computational linguistics  formal semantics is based on a symbolic tradition and centered around the inferential properties of language  distributional semantics is statistical and data driven  and focuses on aspects of meaning related to descriptive content  the two frameworks are complementary in their strengths  and this has motivated interest in combining them into an overarching semantic framework  a formal distributional semantics  given the fundamentally different natures of the two paradigms  however  building an integrative framework poses significant theoretical and engineering challenges  the present issue of computational linguistics advances the state of the art in formal distributional semantics  this introductory article explains the motivation behind it and summarizes the contributions of previous work on the topic  providing the necessary background for the articles that follow  
 formal nonmonotonic theories and properties of human defeasible reasoning the knowledge representation and reasoning of both humans and artificial systems often involves conditionals  a conditional connects a consequence which holds given a precondition  it can be easily recognized in natural languages with certain key words  like  if   in english  a vast amount of literature in both fields  both artificial intelligence and psychology  deals with the questions of how such conditionals can be best represented and how these conditionals can model human reasoning  on the other hand  findings in the psychology of reasoning  such as those in the suppression task  have led to a paradigm shift from the monotonicity assumptions in human inferences towards nonmonotonic reasoning  nonmonotonic reasoning is sensitive for information change  that is  inferences are drawn cautiously such that retraction of previous information is not required with the addition of new information  while many formalisms of nonmonotonic reasoning have been proposed in the field of artificial intelligence  their capability to model properties of human reasoning has not yet been extensively investigated  in this paper  we analyzed systematically from both a formal and an empirical perspective the power of formal nonmonotonic systems to model  i  possible explicit defeaters  as in the suppression task  and  ii  more implicit conditional rules that trigger nonmonotonic reasoning by the keywords in such rules  the results indicated that the classical evaluation for the correctness of inferences has to be extended in the three major aspects  i  regarding the inference system   ii  the knowledge base  and  iii  possible assumed exceptions for the rule  
 formalization of the semantics of iconic languages  an ontology based method and four semantic powered applications iconic languages can represent concepts by the combination of graphical components  such as colors or pictograms   there are numerous examples  from traffic signs to computer user interface icons  however  these languages do not associate formalized semantics to their icons  which raises various problems  inconsistent combinations of graphical components  different interpretations of a given icon by two persons  difficulties to map the icons to the concepts of existing termino ontological resources  etc  in this article  we describe a method that formalizes the semantics of an iconic language with an ontology  this method was initially developed for the vcm iconic language  visualization of concepts in medicine   which enables to represent the main medical concepts  antecedents  disorders  treatments  etc   by icons  we show that it can be generalized to other iconic languages  including traffic signs  we also describe four practical applications made possible by the formalization of the language semantics  the verification of icons consistency  the semi automatic alignment with terminologies  the automatic generation of a pictogram lexicon and the automatic generation of icon labels  the article also presents the vcm ontology  the implementation details of a semantic iconic server with fast response times  and the evaluation results obtained when evaluating the four applications   c  2017 elsevier b v  all rights reserved  
 formalizing cognitive acceptance of arguments  durum wheat selection interdisciplinary study in this paper we present an interdisciplinary approach that concerns the problem of argument acceptance in an agronomy setting  we propose a computational cognitive model for argument acceptance based on the dual model system in cognitive psychology  we apply it in an agronomy setting within a french national project on durum wheat  
 fostering knowledge reuse in communities of practice by using a trust model and agents currently  knowledge management is a key issue for companies as it gives them a competitive advantage  a community of practice  cop  is a means to encourage employees to manage knowledge and enables them to exchange knowledge and experience  members of these communities  however  are often geographically distributed  this hinders the development of feelings of trust between their members  which limits knowledge reuse  our proposal seeks to minimize the effect of lack of trust between cop members  thereby fostering the exchange of knowledge  to achieve this goal  we propose a trust model to calculate trust among cop members  along with a multi agent architecture to automatically manage the trust model in a cop  the agents calculate a trust value in each situation  taking the user s profile into account  we also present a tool that recommends sources of knowledge and documents that are trustworthy  
 framebase  enabling integration of heterogeneous knowledge large scale knowledge graphs such as those in the linked open data cloud are typically stored as subject predicateobject triples  however  many facts about the world involve more than two entities  while n ary relations can be converted to triples in a number of ways  unfortunately  the structurally different choices made in different knowledge sources significantly impede our ability to connect them  they also increase semantic heterogeneity  making it impossible to query the data concisely and without prior knowledge of each individual source  this article presents framebase  a wide coverage knowledge base schema that uses linguistic frames to represent and query n ary relations from other knowledge bases  providing multiple levels of granularity connected via logical entailment  overall  this provides a means for semantic integration from heterogeneous sources under a single schema and opens up possibilities to draw on natural language processing techniques for querying and data mining  
 framework of multiuser satisfaction for assessing interaction models within collaborative virtual environments collaborative virtual environments  ves  require interaction models for resolving conflicts and promoting multiuser collaboration  common models  such as the first come first serve  fcfs  model  which grants interaction opportunities to the most agile user  and the static priority model  which gives interaction opportunities to the user with the highest predefined priority  disregard the importance of perceiving equality in interaction  eii  among all users  one exception is the dynamic priority  dp  model  as proposed in our earlier work  which grants interaction opportunities to a user based on the recency of his her gained opportunities  to date  few research efforts have investigated the effect of interaction models on multiuser satisfaction  this paper hence presents an assessment of the dp model s effect on multiuser satisfaction within a collaborative ve  we first verified that the dp model allowed multiple users to perceive eii  we then conducted an experiment to examine the effect of the dp and fcfs models on multiuser satisfaction under a quasi practical scenario that mimicked a decision makingmeeting of experts  the framework of the examination was based on several metrics  which we proposed for the components of the iso iec 25010  2011 standard  this framework resolved issues with existing metrics that measure user satisfaction by analyzing individual experience  thus omitting eii desired by multiple users  the results of the experiment indicated that the dp model fulfilled the metrics of the framework significantly better than the fcfs model  this observation implies a potential application of the dp model in collaborative ves where multiuser satisfaction is the key to productive collaboration  
 framing qa as building and ranking intersentence answer justifications we propose a question answering  qa  approach for standardized science exams that both identifies correct answers and produces compelling human readable justifications for why those answers are correct  our method first identifies the actual information needed in a question using psycholinguistic concreteness norms  then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information  we then jointly rank answers and their justifications using a reranking perceptron that treats justification quality as a latent variable  we evaluate our method on 1 000 multiple choice questions from elementary school science exams  and empirically demonstrate that it performs better than several strong baselines  including neural network approaches  our best configuration answers 44  of the questions correctly  where the top justifications for 57  of these correct answers contain a compelling human readable justification that explains the inference required to arrive at the correct answer  we include a detailed characterization of the justification quality for both our method and a strong baseline  and show that information aggregation is key to addressing the information need in complex questions  
 from intelligence to rationality of minds and machines in contemporary society  the sciences of design and the role of information the presence of intelligence and rationality in artificial intelligence  ai  and the internet requires a new context of analysis in which herbert simon s approach to the sciences of the artificial is surpassed in order to grasp the role of information in our contemporary setting  this new framework requires taking into account some relevant aspects   i  in the historical endeavor of building up ai and the internet  minds and machines have interacted over the years and in many ways through the interrelation between scientific creativity and technological innovation   ii  philosophically  minds and machines can have epistemological  methodological and ontological differences  which are based on the distinct configuration of human intelligence and artificial intelligence  their comparison with rationality and its various forms is particularly relevant   iii  scientifically  ai and the internet belong to the sciences of the artificial  because they work on designs that search for specific aims  following selected processes in order to achieve expected results   iv  technologically  ai and the internet require the support of information and communication technologies  ict   these have an instrumental role regarding the existence of ai and the internet  also ict shape their diverse forms of configuration over the years  within this framework  this paper offers a new context of analysis that goes beyond simon s and follows four main steps   i  the interaction between scientific creativity and technological innovation as the philosophico methodological setting for artificial intelligence and the internet   ii  artificial intelligence and human intelligence as epistemological basis for machines and minds  where the differences between artificial intelligence and human intelligence are made explicit  under the consideration of  computational intelligence    and the analysis of minds and machines is made from the perspective of rationality   symbolic rationality   and  adaptive rationality      iii  intention and its difference with design of machine learning are considered to distinguish human intelligence from artificial intelligence  and  iv  the internal and external aspects of artificial designs in contemporary society are considered through the perspective of rationality  which leads to the transition from intelligence to rationality in the internet as well as to the historicity of information  how aims  processes  and results can be based on conceptual revolutions   
 fubini theorems for capacities capacity plays an important role in many areas  a capacity is usually studied under the assumption that it is concave  or convex   in this paper  we perform a further investigation on the fubini theorems for concave  or convex  capacities given by ghirardato  1997  and chateauneuf and lefort  2008   we extend fubini theorems for capacities to a larger class of functions  which are both mu 1  choquet integrable and mu 2  choquet integrable  not to restrict only on slice comonotonic functions  
 functional reward markov decision processes  theory and applications markov decision processes  mdp  have become one of the standard models for decision theoretic planning problems under uncertainty  in its standard form  rewards are assumed to be numerical additive scalars  in this paper  we propose a generalization of this model allowing rewards to be functional  the value of a history is recursively computed by composing the reward functions  we show that several variants of mdps presented in the literature can be instantiated in this setting  we then identify sufficient conditions on these reward functions for dynamic programming to be valid  we also discuss the infinite horizon case and the case where a maximum operator does not exist  in order to show the potential of our framework  we conclude the paper by presenting several illustrative examples  
 fused latent models for assessing product return propensity in online commerce in online shopping  product returns are very common  liberal return policies have been widely used to attract shoppers  however  returns often increase expense and inconvenience for all parties involved  customers  retailers  and manufacturers  despite the large fraction of purchases that are returned  there are few systematic studies to explain the underlying forces that drive return requests  and to assess the return propensity at the level of individual purchases  i e   a particular customer purchasing a particular product   rather than in aggregate  to this end  in this paper  we provide a systematic framework for personalized predictions of return propensity  these predictions can help retailers enhance inventory management  improve customer relationships  and reduce return fraud and abuse  specifically  we treat product returns as a result of inconsistency arising during a commercial transaction  we decompose this inconsistency into two components  one for the buying phase  e g   product does not match description  and another for the shipping phase  e g   product damaged during shipping   along these lines  we introduce a generalized return propensity latent model  rplm   we further propose a complete framework  called fused return propensity latent model  frplm   to jointly model the correlation among user profiles  product features  and return propensity  we present comprehensive experimental results with real world data to demonstrate the effectiveness of the proposed method for assessing return propensity   c  2016 elsevier b v  all rights reserved  
 fuzzy approaches to option price modeling the aim of this paper is to review the literature that has addressed direct and inverse problems in option pricing in a fuzzy setting  in a direct problem  the stochastic process for the underlying asset is assumed and the option prices are derived by no arbitrage or equilibrium conditions  in an inverse problem  the option prices are taken as given and used to infer the underlying asset process  models are divided into discrete time and continuous time ones  special attention is paid to real options  a particular class of nonfinancial options that are used to evaluate real investments  directions for future research are outlined  in particular in inverse problems  there is still room for promising research  both in discrete time and in continuous time  moreover  given that many proposed methods remain difficult to use in practice  there is mainly the need to apply the fuzzy models obtained on real market data and to compare the results with nonfuzzy techniques in order to assess the usefulness and the improvements in the modeling of imprecise data with fuzzy sets and fuzzy random variables  
 fuzzy best worst multi criteria decision making method and its applications considering the vagueness frequently representing in decision data due to the lack of complete information and the ambiguity arising from the qualitative judgment of decision makers  the crisp values of criteria may be inadequate to model the real life multi criteria decision making  mcdm  issues  in this paper  the latest mcdm method  namely best worst method  bwm  was extended to the fuzzy environment  the reference comparisons for the best criterion and for the worst criterion were described by linguistic terms of decision makers  which can be expressed in triangular fuzzy numbers  then  the graded mean integration representation  gmir  method was employed to calculate the weights of criteria and alternatives with respect to different criteria under fuzzy environment  according to the concept of bwm  the nonlinearly constrained optimization problem was built for determining the fuzzy weights of criteria and alternatives with respect to different criteria  the fuzzy ranking scores of alternatives can be derived from the fuzzy weights of alternatives with respect to different criteria multiplied by fuzzy weights of the corresponding criteria  and then the crisp ranking score of alternatives can be calculated by employing gmir method for optimal alternative selection  meanwhile  the consistency ratio was proposed for fuzzy bwm to check the reliability of fuzzy preference comparisons  three case studies were performed to illustrate the effectiveness and feasibility of the proposed fuzzy bwm  the results indicate the proposed fuzzy bwm can not only obtain reasonable preference ranking for alternatives but also has higher comparison consistency than the bwm   c  2017 elsevier b v  all rights reserved  
 fuzzy c means clustering and particle swarm optimization based scheme for common service center location allocation common service centers  cscs   which are also known as tele centers and rural kiosks  are important infrastructural options for any country aiming to provide e governance services in rural regions  their main objective is to provide adequate information and services to a country s rural areas  thereby increasing government citizen connectivity  within developing nations  such as india  many csc allocations are being planned  this study proposes a solution for allocating a csc for villages in a country according to their e governance plan  the fuzzy c means  fcm  algorithm was used for clustering the village dataset and finding a cluster center for csc allocation  and the particle swarm optimization  pso  algorithm was used for further optimizing the results obtained from the fcm algorithm based on population  in the context of other studies addressing similar issues  this study highlights the practical implementation of location modeling and analysis  an extensive analysis of the results obtained using a village dataset from india including four prominent states shows that the proposed solution reduces the average traveling costs of villagers by an average of 33   compared with those of allocating these cscs randomly in a sorted order and by an average of 11   relative to centroid allocation using the fcm based approach only  as compared to traditional approaches like p center and p median  the proposed scheme is better by 31   and 14    respectively  therefore  the proposed algorithm yields better results than classical fcm and other types of computing techniques  such as random search   linear programming  this scheme could be useful for government departments managing the allocation of cscs in various regions  this work should also be useful for researchers optimizing the location allocation schemes used for various applications worldwide  
 fuzzy color spaces  a conceptual approach to color vision in this paper  we introduce formal definitions of the concepts of fuzzy color and fuzzy color space  first  we formalize the notion of fuzzy color for representing the correspondence between computational representation of colors and perceptual color categories identified by a color name  second  we propose a methodology for learning fuzzy colors based on the paradigm of conceptual spaces  where prototypes are used for each category to be learnt  since the conceptual space approach yields crisp categorizations  we introduce a novel methodology for defining fuzzy boundaries of color categories on the basis of a voronoi tessellation of a color space  finally  we also formalize the notion of fuzzy color space as the collection of fuzzy colors corresponding to the color categories employed in a certain context application and or for a specific user  different typologies of fuzzy color spaces are proposed in order to be consistent with the nature of the categories we want to model  our approach is illustrated by defining fuzzy color spaces using rgb with the euclidean distance  examples based on the well known iscc nbs color naming system are presented  as well as others based on collections of color names and prototypes provided by users  the proposal is evaluated and compared with the most used approaches for color modeling  additionally  a website located at http   www jfcssoftware com including all experimentation data  software implementing our models  and additional materials is available to researchers in color modeling  
 fuzzy cross efficiency evaluation  a possibility approach cross efficiency evaluation is an extension of data envelopment analysis  dea  aimed at ranking decision making units  dmus  involved in a production process regarding their efficiency  as has been done with other enhancements and extensions of dea  in this paper we propose a fuzzy approach to the cross efficiency evaluation  specifically  we develop a fuzzy cross efficiency evaluation based on the possibility approach by lertworasirikul et al   fuzzy sets syst 139 379 394  2003a  to fuzzy dea  thus  a methodology for ranking dmus is presented that may be used when data are imprecise  in particular for fuzzy inputs and outputs being normal and convex  we prove some results that allow us to define  consistent  cross efficiencies  the ranking of dmus for a given possibility level results from an ordering of cross efficiency scores  which are real numbers  as in the crisp case  we also develop benevolent and aggressive fuzzy formulations in order to deal with the alternate optima for the weights  
 fuzzy decision making  a bibliometric based review fuzzy decision making consists in making decisions under complex and uncertain environments where the information can be assessed with fuzzy sets and systems  the aim of this study is to review the main contributions in this field by using a bibliometric approach  for doing so  the article uses a wide range of bibliometric indicators including the citations and the h index  moreover  it also uses the vos viewer software in order to map the main trends in this area  the work considers the leading journals  articles  authors and institutions  the results indicate that the usa was the traditional leader in this field with the most significant researcher  however  during the last years  this field is receiving more attention by asian authors that are starting to lead the field  this discipline has a strong potential and the expectations for the future is that it will continue to grow  
 fuzzy dynamical system scenario simulation based cross border financial contagion analysis  a perspective from international capital flows this paper focuses on investigating the cross border financial contagion based on a fuzzy dynamical system scenario simulation froma perspective of analyzing the volatility of international capital flows for a panel of 50 countries in emerging markets and advanced economies from 1980 to 2011  increasing evidence has shown that financial globalization has developed into a complex nonlinear dynamical system made up of economic subsystems with extensive financial connections and linkages  the contagion effects of the spread of bonanzas in the 50 countries are identified and analyzed  the hodrick prescott filter is employed to address the long term net capital inflow trend  the comovement of financial contagion between the source country of financial turbulence and the volatility affected country is described as a fuzzy dynamical system in which the driving and response systems are coupled  a fuzzy dynamical system scenario simulation model under a liberal economy is established by employing nonlinear differential equations to describe the contagion mechanism and the international capital flow volatility effects  the model is then extended to a dynamical system model with macroeconomic control  the coupling strength uncertainty is addressed by employing an interval type 2 fuzzy theory method  the properties of the volatility equilibrium point for the two models are discussed  and the volatility contagion principles based on locally asymptotic stability analysis are derived to explain the different volatility transmission patterns  policy suggestions are given in three situations for providing managerial insights for policymakers and the explorations of response strategies are also presented  the global financial crisis in 2008 is used as an experimental study to demonstrate the validity and effectiveness of the simulation and modeling method  
 fuzzy games  a complement consistent solution  axiomatizations and dynamic approaches by applying the supreme utilities under fuzzy behavior  we propose a new solution on fuzzy games  in order to present the rationality for this solution  we adopt an extended reduction to provide related axiomatizations and dynamics process  based on different viewpoint  we also define excess function to introduce alternative formulation and related dynamic process for this solution respectively  
 fuzzy inference suitability to determine the utilitarian quality of b2c websites the present paper proposes a new and intuitive model to evaluate the utilitarian e service quality on textile and fashion b2c websites in spain  the relevant variables to include into this model have been validated through an in depth literature review and a reliability study based on an empirical investigation  among the several methodologies used to develop the model  the paper proves that fuzzy inference systems  fis  have numerous advantages  the following are particularly relevant  the ease to manage non linear behaviours and the intuitive way of incorporating knowledge to evaluate b2c websites  this knowledge can stem both  from the expert know how or from the evaluation of consumer satisfaction on these websites  the knowledge is made explicit through if then rules which work with linguistic type concepts agreed upon by experts  closer to the human reasoning mode  finally  the usefulness of the proposed model and the defined fis suitability to evaluate the model are shown   c  2017 elsevier b v  all rights reserved  
 fuzzy inference systems and inventory allocation decisions  exploring the impact of priority rules on total costs and service levels inventory allocation decisions in a distribution system concern issues such as how much and where stock should be assigned to orders in a supply chain  when the inventory level of an inventory point is lower than the total number of items ordered by lower echelons in the chain  the decision of how many items to allocate to each  competing  order must take into consideration the trade off between cost and service level  this paper proposes a decision support system that makes use of fuzzy logic to consider inventory carrying  shortage and ordering costs as well as transportation costs  the proposed system is compared through simulation with three other inventory allocation decision support models in terms of cost and service levels achieved  conclusions are then drawn   c  2017 elsevier ltd  all rights reserved  
 fuzzy inventory models  a comprehensive review over the years since the advancement of inventory management and fuzzy set theories  a vast number of studies have been published to integrate these concepts  nonetheless  no comprehensive and systematic literature review can be found that analyzed the studies in this research stream  it motivated us to conduct this survey as a systematic and comprehensive review in the field of fuzzy inventory management to identify major achievements attained so far and shed light on future directions  first  the earlier review papers are presented to reveal the necessity of this study  and then methodology applied in collecting sample papers is described  followed by an in depth analysis of the papers  totally  a sample of 210 papers is identified and classified according to the common characteristics of the models  several aspects of the models are assessed that led to identification of some areas overlooked by researchers so far   c  2017 elsevier b v  all rights reserved  
 fuzzy knowledge based token ordering policies for bullwhip effect management in supply chains the  bullwhip effect  is a well known example of supply chain inefficiencies and refers to demand amplification as moving up toward upstream echelons in a supply chain  this paper concentrates on representing a robust token based ordering policy to facilitate information sharing in supply chains in order to manage the bullwhip effect  takagi sugeno kang and hybrid multiple input single output fuzzy models are proposed to model the mechanism of token ordering in the token based ordering policy  the main advantage of proposed fuzzy models is that they eliminate the exogenous and constant variables from the procedure of obtaining the optimal amount of tokens which should be ordered in every period  these fuzzy approaches model the mentioned mechanism through a push pull policy  a four echelon sc with fuzzy lead time and unlimited production capacity and inventory is considered to survey the outcomes  numerical experiments confirm the effectiveness of proposed policies in alleviating bwe  inventory costs and variations  
 fuzzy linguistic summarization with genetic algorithm  an application with operational and financial healthcare data it is now well recognized that knowledge extracted from rich healthcare data play a vital role for delivery  management and planning of healthcare services  so far  however  there is not much study done on the domain of operational and financial healthcare data since  up to now  a great deal of works are dedicated to clinical medical healthcare data for the purposes of diagnosis and treatment of diseases  in this paper  an attempt is made  by applying fuzzy linguistic summarization  for the first time to discover knowledge from operational and financial healthcare data  fuzzy linguistic summarization  in its simplest term  provides natural language based summaries from a dataset in a human consistent way along with a degree of truth attached to each summary  while basically valuable  its benefit can be increased by only generating summaries with a degree of truth above than an indicated threshold value  a genetic algorithm is developed within this context in order to eliminate less promising and useless linguistic summaries  we assess the proposed approach experimentally on a real data and evaluate the generated summaries to gain actionable insights from them  
 fuzzy measures of quality of life  a multidimensional and comparative approach it is unanimously recognised in the literature that the concept of quality of life should be measured within a multidimensional framework  that is able to add to the information usually provided by indices based on economic variables  this paper proposes to adopt a new brand approach for the measurement of quality of life  which is based on the so called fuzzy set approach  and for the first time implement it in a comparative context  the empirical analysis is based on the third wave of european quality of life survey  conducted in 2012 by eurofound  the fuzzy set approach to quality of life measurement  results to be consistent when compared to the traditional approach defined by eurofound  moreover  it results to also statistically robust  in conclusion  the fuzzy set approach provides quite significant added value for both data users and data analysts since it presents results which are easy to read  are concise  and it also facilitates comparison among dimensions  
 fuzzy multi objective modeling of effectiveness and user experience in online advertising the focus placed on maximizing user engagement in online advertising negatively affects the user experience because of advertising clutter and increasing intrusiveness  an intelligent decision support system providing balance between user experience and profits from online advertising based on the fuzzy multi objective optimization model is presented in this paper  the generalized mathematical model uses uncertain parameters for content descriptors that are difficult to be precisely defined and measured  such as the level of intrusiveness and the change in performance over time  the search for final decision solutions and the verification of the proposed model are based on experimental results from both perceptual studies  which are evaluating visibility and intrusiveness of marketing content as well as online campaigns providing interaction data for estimation of effectiveness  surprisingly  the online response to the most noticeable advertisements with highly perceived visibility and intrusiveness was relatively low  during the field study performed in order to compute the model parameters  the best results were achieved for advertising content with moderate visual influence on web users  simulations with the proposed model revealed that a growing level of persuasion can increase results only to a certain extent  above a saturation point  a strategy based on extensive visual effects  such as high frequency flashing  resulted in a very high increase of intrusiveness and a slightly better performance in terms of acquired interactions  proposed balanced content design with the use of intelligent decision support system creates directions towards sustainable advertising and a friendlier online environment   c  2016 elsevier ltd  all rights reserved  
 fuzzy topsis evaluation approach for business process management software acquisition at the present time  business process management software  bpms  implementation is on focus of organizations  to implement a bpms in a firm successfully  selecting a suitable bpms is critical  evaluation and selection of the bpms software is complex and a time consuming decision making process  this paper propounds an approach for dealing with such a decision  this approach introduces functional  non functional and fuzzy evaluation method for bpms evaluation  the presented bpm lifecycle based approach breaks down bpms evaluation and selection criteria into two broad categories namely functional and non functional requirements including totally 48 selection criteria  a facile fuzzy technique for order preference by similarity to ideal solution  ftopsis  is customized for bpms selection based on identified criteria  the provided approach is applied to a port and maritime organization in order to evaluate and acquire a bpms and the provided numerical example illustrates the applicability of the approach  the approach can assist practitioners to evaluate bpmss more effective  
 gaining consensus in a moderated group  a model with a twofold feedback mechanism the group analytic hierarchy process  ahp  is an effective tool to collect experts  wisdom to evaluate complex decision making problems  because judgments are always diverse in the real world  it is crucial to adequately support the consensus reaching process  in this paper  we develop a convergent group ahp consensus reaching model with a twofold feedback mechanism  which consists of both a judgment and a weighting feedback mechanism  in each round of this dynamic and interactive model  the most incompatible expert is asked to revise her judgment according to the judgment feedback mechanism  if the expert rejects the suggestion  her weight of importance will be adjusted downward based on the compatibility within the group by the weighting feedback mechanism  the proof of convergence of this consensus reaching model with the twofold mechanism is also provided and discussed  hence this proposed consensus reaching process supports the leader or client in reaching a successful decision with a dispersed group of experts  the proposed consensus reaching model is applied to the brake pad supplier selection problem of chery automobile co   ltd  the empirical example demonstrates that the proposed methodology provides an operational decision framework for companies to select suitable suppliers in the supplier involvement under the environment of collaborative product development  sicpd  through its successful application in that context   c  2016 elsevier ltd  all rights reserved  
 game design and analysis for price based demand response  an aggregate game approach in this paper  an aggregate game is adopted for the modeling and analysis of energy consumption control in smart grid  since the electricity users  cost functions depend on the aggregate energy consumption  which is unknown to the end users  an average consensus protocol is employed to estimate it  by neighboring communication among the users about their estimations on the aggregate energy consumption  nash seeking strategies are developed  convergence properties are explored for the proposed nash seeking strategies  for energy consumption game that may have multiple isolated nash equilibria  a local convergence result is derived  the convergence is established by utilizing singular perturbation analysis and lyapunov stability analysis  energy consumption control for a network of heating  ventilation  and air conditioning systems is investigated  based on the uniqueness of the nash equilibrium  it is shown that the players  actions converge to a neighborhood of the unique nash equilibrium nonlocally  more specially  if the unique nash equilibrium is an inner nash equilibrium  an exponential convergence result is obtained  energy consumption game with stubborn players is studied  in this case  the actions of the rational players can be driven to a neighborhood of their best response strategies by using the proposed method  numerical examples are presented to verify the effectiveness of the proposed methods  
 game factors influencing players to continue playing online pets online pet games emphasize the enjoyment of raising a pet  social interaction  and expression of the personality  players often obtain a sense of satisfaction and develop companionship with their pets  however  the question pertinent to why people continue playing with online pets is still unclear  the answer of this question can help developers improve game designs  the flow experience is a factor contributing to players  willingness to engage in a gaming activity  therefore  the purpose of this study is to adopt the concept of flow to help understand why people enjoy playing online pet games  in this study  the authors analyzed flow factors and identified crucial game elements that might elicit a flow experience by means of a questionnaire survey  a total of 180 valid questionnaires were collected  and the results demonstrated that role playing is an important flow factor for online pet games  in addition  social interaction  personality design  and creativity behavior are crucial game elements contributing to players  willingness to continue playing with online pets  
 generalized exponential moving average  ema  model with particle filtering and anomaly detection this paper proposes a generalized exponential moving average  ema  model  a new stochastic volatility model with time varying expected return in financial markets  in particular  we effectively apply a particle filter  pf  to sequential estimation of states and parameters in a state space framework  moreover  we develop three types of anomaly detectors  which are implemented easily in the pf algorithm to be used for investment decision  as a result  a simple investment strategy with our scheme is superior to the one based on the standard ema and well known traditional strategies such as equally weighted  minimum variance and risk parity portfolios  our dataset is monthly total returns of global financial assets such as stocks  bonds and reits  and investment performances are evaluated with various statistics  namely compound returns  sharpe ratios  sortino ratios and drawdowns   c  2016 elsevier ltd  all rights reserved  
 generalized regret based decision making we describe the basic regret decision making model for decision problems in which the payoff for a given alternative is uncertain and depends on the value of a variable called the state of nature  an important attribute of this model is the maximum payoff for the occurrence of a given state of nature  we note that the regret is based on the difference between the payoff we receive  given our choice of alternative  under the occurrence of a state of nature and the best payoff we could have received for that state of nature  we note the effective regret associated with an alternative is an aggregation of an alternative s regrets across all the possible states of nature  our objective is to select the alternative with minimum effective regret  we look at this for various methods of aggregating an alternative s individual regrets across the different states of nature and various types of information about the uncertainty associated with the states of nature  one issue limiting the use of regret decision making is its lack of indifference to irrelevant alternatives and the related openness to strategic manipulation by introducing alternatives to solely effect the determination of the maximum payoff  we begin to look at methods to reduce this effect   c  2017 elsevier ltd  all rights reserved  
 generating natural language descriptions using speaker dependent information  this paper discusses the issue of human variation in natural language referring expression generation  we introduce a model of content selection that takes speaker dependent information into account to produce descriptions that closely resemble those produced by each individual  as seen in a number of reference corpora  results show that our speaker dependent referring expression generation model outperforms alternatives that do not take human variation into account  or which do so less extensively  and suggest that the use of machine learning methods may be an ideal approach to mimic complex referential behaviour  
 generation of simple structured information retrieval functions by genetic algorithm without stagnation this paper investigates an approach to construct new ranking models for information retrieval  the ir ranking model depends on the document description  it includes the term frequency and document frequency  the model ranks documents upon a user request  the quality of the model is defined by the difference between the documents  which experts assess as relative to the request  and the ranked ones  to boost the model quality a modified genetic algorithm was developed  it generates models as superpositions of primitive functions and selects the best according to the quality criterion  the main impact of the research if the new technique to avoid stagnation and to control structural complexity of the consequently generated models  to solve problems of stagnation and complexity  a new criterion of model selection was introduced  it uses structural metric and penalty functions  which are defined in space of generated superpositions  to show that the newly discovered models outperform the other state of the art ir scoring models the authors perform a computational experiment on trec datasets  it shows that the resulted algorithm is significantly faster than the exhaustive one  it constructs better ranking models according to the map criterion  the obtained models are much simpler than the models  which were constructed with alternative approaches  the proposed technique is significant for developing the information retrieval systems based on expert assessments of the query document relevance   c  2017 elsevier ltd  all rights reserved  
 genetic local search algorithm for a new bi objective arc routing problem with profit collection and dispersion of vehicles we present a new bi objective arc routing problem in which routes must be constructed in order to maximize collected profit and a non linear dispersion metric  a dispersion metric calculated based on instantaneous positions  suitable to capture routing characteristics found when vehicles have to travel in hostile environments  is a novelty in the routing literature  the inherent combinatorial nature of this problem makes it difficult to solve using exact methods  we propose a multi objective genetic local search algorithm to solve the problem and compare the results with those obtained by a well known multi objective evolutionary algorithm  computational experiments were performed on a new set of benchmark instances  and the results evidence that local search plays an important role in providing good approximation sets  the proposed method can be adapted to other multi objective problems in which the exploitation provided by local search may improve the evolutionary procedures usually adopted   c  2017 elsevier ltd  all rights reserved  
 genetic programming optimization for a sentiment feedback strength based trading strategy this study is motivated by the empirical findings that news and social media twitter messages  tweets  exhibit persistent predictive power on financial market movement  based on the evidence that tweets are faster than news in revealing new market information  whereas news is regarded broadly a more reliable source of information than tweets  we propose a superior trading strategy based on the sentiment feedback strength between the news and tweets using generic programming optimization method  the key intuition behind this feedback strength based approach is that the joint momentum of the two sentiment series leads to significant market signals  which can be exploited to generate superior trading profits  with the trade off between information speed and its reliability  this study aims to develop an optimal trading strategy using investors  sentiment feedback strength with the objective to maximize risk adjusted return measured by the sterling ratio  we find that the sentiment feedback based strategies yield superior market returns with low maximum drawdown over the period from 2012 to 2015  in comparison  the strategies based on the sentiment feedback indicator generate over 14 7  sterling ratio compared with 10 4  and 13 6  from the technical indicator based strategies and the basic buy and hold strategy respectively  after considering transaction costs  the sentiment indicator based strategy outperforms the technical indicator based strategy consistently  backtesting shows that the advantage is statistically significant the result suggests that the sentiment feedback indicator provides support in controlling loss with lower maximum drawdown   c  2017 elsevier b v  all rights reserved  
 geography of online network ties  a predictive modelling approach internet platforms are increasingly enabling individuals to access and interact with a wider  globally dispersed group of peers  the promise of these platforms is that the geographic distance is no longer a barrier to forming network ties  however  whether these platforms truly alleviate the influence of geographic distance remains unexplored  in this study  we examine the role of geographic distance with machine learning approach using a unique dataset of the network ties between traders in an online social trading platform  specifically  we determine the extent to which  compared to other types of distances  geographic distance predicts the occurrences of the network ties in country dyads  using cluster analysis and predictive modelling  we show that not only the geographic distance and network ties exhibit an inverse association but also that geographic distance is the strongest predictor of such ties   c  2017 elsevier b v  all rights reserved  
 gesture triggered  dynamic gaze alignment architecture for intelligent elearning systems current elearning systems enable streaming of live lectures to distant students facilitating a live instructor student interaction  however  studies have shown that there exists a marked divide in local students   student present in the teacher s location  experience as compared to distant students   one of the major factors attributing to this rift is lack of gaze aligned interaction  in this paper  we present a system architecture that receives gesture triggers as input  and dynamically calculates the perspective angle to be captured of the speaking participant  for the listener  facilitating eye contact  the gesture triggers are calculated using microsoft kinect sensor which extracts skeleton joint information of the instructor  and performs gesture recognition with the acquired joint information real time  this serves as interaction initiation triggers for dynamic perspective correction for gaze alignment during a conversation  for evaluation  we constructed a five classroom test bed with dynamic perspective correction and user study results indicate a marked 42  enhancement in experience with the gaze correction in place  
 getting closer to the essence of music  the con espressione manifesto this text offers a personal and very subjective view on the current situation of music information research  mir   motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have  i try to sketch a number of challenges for the next decade of mir research  grouped around six simple truths about music that are probably generally agreed on but often ignored in everyday research  
 getting phished on social media the study experimentally simulated a level 1 social networking based phishing  snp  attack  where a phisher using a phony profile attempts to friend an individual on facebook  and a level 2 snp attack  where a phisher attempts to extract information directly  the results implicate the use of cognitive shortcuts triggered by the cues afforded in facebook s interface  individuals appeared to be using the phisher s friend count as a heuristic for judging the authenticity of a level 1 request  they  thus  responded to a phisher displaying a large friend count even in the absence of a profile picture  interestingly  the affordance of smartphones used to access social media an issue that has received little academic attention increased the odds of considering such requests sevenfold   c  2017 elsevier b v  all rights reserved  
 getting value from business intelligence systems  a review and research agenda much of the research on business intelligence  bi  has examined the ability of bi systems to help organizations address challenges and opportunities  however  the literature is fragmented and lacks an overarching framework to integrate findings and systematically guide research  moreover  researchers and practitioners continue to question the value of bi systems  this study reviews and synthesizes empirical information system  is  studies to team what we know  how well we know  and what we need to know about the processes of organizations obtaining business value from bi systems  the study aims to identify which parts of the bi business value process have been studied and are still most in need of research  and to propose specific research questions for the future  the findings show that organizations appear to obtain value from bi systems according to the process suggested by soh and markus  1995   as a chain of necessary conditions from bi investments to bi assets to bi impacts to organizational performance  however  researchers have not sufficiently studied the probabilistic processes that link the necessary conditions together  moreover  the research has not sufficiently covered all relevant levels of analysis  nor examined how the levels link up  overall  the paper identified many opportunities for researchers to provide a more complete picture of how organizations can and do obtain value from bi   c  2016 elsevier b v  all rights reserved  
 goaalll   using sentiment in the world cup to explore theories of emotion sporting events evoke strong emotions among fans and thus act as natural laboratories to explore emotions and how they unfold in the wild  computational tools  such as sentiment analysis  provide new ways to examine such dynamic emotional processes  in this article we use sentiment analysis to examine tweets posted during 2014 world cup  such analysis gives insight into how people respond to highly emotional events  and how these emotions are shaped by contextual factors  such as prior expectations  and how these emotions change as events unfold over time  here we report on some preliminary analysis of a world cup twitter corpus using sentiment analysis techniques  after performing initial tests of validation for sentiment analysis on data in this corpus  we show these tools can give new insights into existing theories of what makes a sporting match exciting  this analysis seems to suggest that  contrary to assumptions in sports economics  excitement relates to expressions of negative emotion  the results are discussed in terms of innovations in methodology and understanding the role of emotion for  tuning in  to real world events  we also discuss some challenges that such data present for existing sentiment analysis techniques and discuss future analysis   c  2017 elsevier b v  all rights reserved  
 governing the fiduciary relationship in information security services we propose a model of security governance that draws upon agency theory to conceptualize the infosec function as a fiduciary for business users  we introduce a ternary typology of governance and test the efficacy of governance on goal congruence and information asymmetry  our survey of information security managers finds that   1  governance enhances goal congruence but does not impact information asymmetry  and  2  perceived information security service effectiveness is positively related to both information asymmetry and goal congruence  contrary to what agency theory suggests  in the infosec context  information asymmetry can be exactly what is needed to enhance the principal s welfare  we conclude with a discussion of the theoretical and managerial implications of these findings   c  2016 elsevier b v  all rights reserved  
 graph aggregation graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs  each provided by a different source  one needs to perform graph aggregation in a wide variety of situations  e g   when applying a voting rule  graphs as preference orders   when consolidating conflicting views regarding the relationships between arguments in a debate  graphs as abstract argumentation frameworks   or when computing a consensus between several alternative clusterings of a given dataset  graphs as equivalence relations   in this paper  we introduce a formal framework for graph aggregation grounded in social choice theory  our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule  we consider both common properties of graphs  such as transitivity and reflexivity  and arbitrary properties expressible in certain fragments of modal logic  our results establish several connections between the types of properties preserved under aggregation and the choice theoretic axioms satisfied by the rules used  the most important of these results is a powerful impossibility theorem that generalises arrow s seminal result for the aggregation of preference orders to a large collection of different types of graphs   c  2017 elsevier b v  all rights reserved  
 greedy transition based dependency parsing with stack lstms we introduce a greedy transition based parser that learns to represent parser states using recurrent neural networks  our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networksthe stack long short term memory unit  lstm   like the conventional stack data structures used in transition based parsers  elements can be pushed to or popped from the top of the stack in constant time  but  in addition  an lstm maintains a continuous space embedding of the stack contents  our model captures three facets of the parser s state   i  unbounded look ahead into the buffer of incoming words   ii  the complete history of transition actions taken by the parser  and  iii  the complete contents of the stack of partially built tree fragments  including their internal structures  in addition  we compare two different word representations   i  standard word vectors based on look up tables and  ii  character based models of words  although standard word embedding models work well in all languages  the character based models improve the handling of out of vocabulary words  particularly in morphologically rich languages  finally  we discuss the use of dynamic oracles in training the parser  during training  dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned  making the model more robust to the kinds of errors that will be made at test time  training our model with dynamic oracles yields a linear time greedy parser with very competitive performance  
 grey stochastic multi criteria decision making based on regret theory and topsis extended grey numbers  egns   integrated with discrete grey numbers and continuous grey numbers  have a powerful capacity to express uncertainty and thus have been widely studied and applied to solve multi criteria decision making  mcdm  problems that involve uncertainty  considering stochastic mcdm problems with interval probabilities  we propose a grey stochastic mcdm approach based on regret theory and technique for order preference by similarity to ideal solution  topsis   in which the criteria values are expressed as egns  we also construct the utility value function  regret value function  and perceived utility value function of egns  and we rank the alternatives in accordance with classical topsis method  finally  we provide two examples to illustrate the method and make comparison analyses between the proposed approach and existing methods  the comparisons suggest that the proposed approach is feasible and usable  and it provides a new method to solve stochastic mcdm problems  it not only fully considers decision makers  bounded rationality for decision making  but also enriches and expands the application of regret theory  
 grey wolf optimization evolving kernel extreme learning machine  application to bankruptcy prediction this study proposes a new kernel extreme learning machine  kelm  parameter tuning strategy using a novel swarm intelligence algorithm called grey wolf optimization  gwo   gwo  which simulates the social hierarchy and hunting behavior of grey wolves in nature  is adopted to construct an effective kelm model for bankruptcy prediction  the derived model gwo kelm is rigorously compared with three competitive kelm methods  which are typical in a comprehensive set of methods including particle swarm optimization based kelm  genetic algorithm based kelm  grid search technique based kelm  extreme learning machine  improved extreme learning machine  support vector machines and random forest  on two real life datasets via 10 fold cross validation analysis  results obtained clearly confirm the superiority of the developed model in terms of classification accuracy  training  validation  test   type i error  type ii error  area under the receiver operating characteristic curve  auc  criterion as well as computational time  therefore  the proposed gwo kelm prediction model is promising to serve as a powerful early warning tool with excellent performance for bankruptcy prediction   c  2017 elsevier ltd  all rights reserved  
 ground truth bias in external cluster validity indices external cluster validity indices  cvis  are used to quantify the quality of a clustering by comparing the similarity between the clustering and a ground truth partition  however  some external cvis show a biased behavior when selecting the most similar clustering  users may consequently be misguided by such results  recognizing and understanding the bias behavior of cvis is therefore crucial  it has been noticed that  some external cvis exhibit a preferential bias towards a larger or smaller number of clusters which is monotonic  directly or inversely  in the number of clusters in candidate partitions  this type of bias is caused by the functional form of the cvi model  for example  the popular rand index  ri  exhibits a monotone increasing  ncinc  bias  while the jaccard index  ji  index suffers from a monotone decreasing  ncdec  bias  this type of bias has been previously recognized in the literature  in this work  we identify a new type of bias arising from the distribution of the ground truth  reference  partition against which candidate partitions are compared  we call this new type of bias ground truth  gt  bias  this type of bias occurs if a change in the reference partition causes a change in the bias status  e g   ncinc  ncdec  of a cvi  for example  ncinc bias in the ri can be changed to ncdec bias by skewing the distribution of clusters in the ground truth partition  it is important for users to be aware of this new type of biased behavior  since it may affect the interpretations of cvi results  the objective of this article is to study the empirical and theoretical implications of gt bias  to the best of our knowledge  this is the first extensive study of such a property for external cvis  our computational experiments show that 5 of 26 pair counting based cvis studied in this paper  which are all functions of the ri  exhibit gt bias  following the numerical examples  we provide a theoretical analysis of gt bias based on the relationship between the ri and quadratic entropy  specifically  we prove that the quadratic entropy of the ground truth partition provides a computable test which predicts the nc bias status of the ri  
 group decision making based on incomplete multiplicative and fuzzy preference relations the main aim of this paper is to investigate the group decision making on incomplete multiplicative and fuzzy preference relations without the requirement of satisfying reciprocity property  this paper introduces a new characterization of the multiplicative consistency condition  based on which a method to estimate unknown preference values in an incomplete multiplicative preference relation is proposed  apart from the multiplicative consistency property among three known preference values  the method proposed also takes the multiplicative consistency property among more than three values into account  in addition  two models for group decision making with incomplete multiplicative preference relations and incomplete fuzzy preference relations are presented  respectively  some properties of the collective preference relation are further discussed  numerical examples are provided to make a discussion and comparison with other similar methods   c  2016 elsevier b v  all rights reserved  
 group decision making with dual hesitant fuzzy preference relations due to the complexity and uncertainty of socioeconomic environments and cognitive diversity of group members  the cognitive information over alternatives provided by a decision organization consisting of several experts is usually uncertain and hesitant  hesitant fuzzy preference relations provide a useful means to represent the hesitant cognitions of the decision organization over alternatives  which describe the possible degrees that one alternative is preferred to another by using a set of discrete values  however  in order to depict the cognitions over alternatives more comprehensively  besides the degrees that one alternative is preferred to another  the decision organization would give the degrees that the alternative is non preferred to another  which may be a set of possible values  to effectively handle such common cases  in this paper  the dual hesitant fuzzy preference relation  dhfpr  is introduced and the methods for group decision making  gdm  with dhfprs are investigated  firstly  a new operator to aggregate dual hesitant fuzzy cognitive information is developed  which treats the membership and non membership information fairly  and can generate more neutral results than the existing dual hesitant fuzzy aggregation operators  since compatibility is a very effective tool to measure the consensus in gdm with preference relations  then two compatibility measures for dhfprs are proposed  after that  the developed aggregation operator and compatibility measures are applied to gdm with dhfprs and two gdm methods are designed  which can be applied to different decision making situations  each gdm method involves a consensus improving model with respect to dhfprs  the model in the first method reaches the desired consensus level by adjusting the group members  preference values  and the model in the second method improves the group consensus level by modifying the weights of group members according to their contributions to the group decision  which maintains the group members  original opinions and allows the group members not to compromise for reaching the desired consensus level  in actual applications  we may choose a proper method to solve the gdm problems with dhfprs in light of the actual situation  compared with the gdm methods with ivifprs  the proposed methods directly apply the original dhfprs to decision making and do not need to transform them into the ivifprs  which can avoid the loss and distortion of original information  and thus can generate more precise decision results  
 group decision opinion evolution and simulation based on community and individual influence power the reality of group decision problem is often a complex large group problem  and because of the influence of the factors such as personality  psychology  values and connection degree  group members can form different community organizations  the structure of community organizations and the individual influence index of member have a significant influence on the decision results  on the basis of individual influence index as well as recessive community organization  considering the individual heterogeneity and the community in group  the article put forward an individual interaction and opinion evolution model of dynamic group decision making which is more accord with the actual group behavior  and the feasibility and effectiveness of the model proposed in this paper are illustrated with simulation experiments by matlab platform  the simulation results show that the number of the community in group decision could affect the evolution of the group opinion  as well as the nature of the community  semi open type of community not only does not prevent the convergence point of view  it will facilitate decision making reached a certain extent  in addition  the change of community nature and amount will have an impact on the convergence point of view  
 h index manipulation by merging articles  models  theory  and experiments an author s profile on google scholar consists of indexed articles and associated data  such as the number of citations and the h index  the author is allowed to merge articles  this may affect the h index  we analyze the  parameterized  computational complexity of maximizing the h index using article merges  herein  to model realistic manipulation scenarios  we define a compatibility graph whose edges correspond to plausible merges  moreover  we consider several different measures for computing the citation count of a merged article  for the measure used by google scholar  we give an algorithm that maximizes the h index in linear time if the compatibility graph has constant size connected components  in contrast  if we allow to merge arbitrary articles  that is  for compatibility graphs that are cliques   then already increasing the h index by one is np hard  experiments on google scholar profiles of ai researchers show that the h index can be manipulated substantially only if one merges articles with highly dissimilar titles   c  2016 elsevier b v  all rights reserved  
 handwriting movement sonification  why and how  sonifying handwriting  i  e   transforming some characteristics of handwriting movement into sounds  aims to improve handwriting perception  evaluation  and control  the educative or clinical interest is to make hidden variables of handwriting  which are not accessible through a visual inspection of the trace  perceptible and useful for novice or poor writers  from a theoretical point of view  handwriting sonification might be a powerful tool for assessing multisensory integration in motor control and learning  in a brief introduction  the main studies on handwriting sonification are reported and the interest of handwriting movement sonification is justified  then  the project of sonification is described  with a specific focus on select variables of handwriting movement and associated sounds  the experimental validations of this sonification strategy are presented  followed by a conclusion and a few theoretical perspectives  
 harnessing music related visual stereotypes for music information retrieval over decades  music labels have shaped easily identifiable genres to improve recognition value and subsequently market sales of new music acts  referring to print magazines and later to music television as important distribution channels  the visual representation thus played and still plays a significant role in music marketing  visual stereotypes developed over decades that enable us to quickly identify referenced music only by sight without listening  despite the richness of music related visual information provided by music videos and album covers as well as t shirts  advertisements  and magazines  research towards harnessing this information to advance existing or approach new problems of music retrieval or recommendation is scarce or missing  in this article  we present our research on visual music computing that aims to extract stereotypical music related visual information from music videos  to provide comprehensive and reproducible results  we present the music video dataset  a thoroughly assembled suite of datasets with dedicated evaluation tasks that are aligned to current music information retrieval tasks  based on this dataset  we provide evaluations of conventional low level image processing and affect related features to provide an overview of the expressiveness of fundamental visual properties such as color  illumination  and contrasts  further  we introduce a high level approach based on visual concept detection to facilitate visual stereotypes  this approach decomposes the semantic content of music video frames into concrete concepts such as vehicles  tools  and so on  defined in a wide visual vocabulary  concepts are detected using convolutional neural networks and their frequency distributions as semantic descriptions for a music video  evaluations showed that these descriptions show good performance in predicting the music genre of a video and even outperform audio content descriptors on cross genre thematic tags  further  highly significant performance improvements were observed by augmenting audio based approaches through the introduced visual approach  
 harnessing the frontline employee sensing of capabilities for decision support the ability to sense developments in  operational  steady state  and dynamic  growth  capabilities provides early signals about how the firm adapts its operations to ongoing changes in the environment  frontline employees engage in the daily transactions and sense the firm s operating conditions and ability to deal with the environment that eventually will affect performance and strategic outcomes  the environmental sensing is a central cognitive feature and constitutes an information source for operations strategy decisions  drawing on aggregated judgmental time series forecasting techniques  this article develops a sensing instrument an employee sensed operational conduct  esoc  index for updated information as an essential decision support mechanism  this sensing capacity is firm specific and difficult to replicate once in place and thus can provide a basis for sustainable competitive advantage   c  2017 elsevier b v  all rights reserved  
 has computational creativity successfully made it  beyond the fence  in musical theatre  a significant test for software is to task it with replicating human performance  as done recently with creative software and the commercial project beyond the fence  undertaken for a television documentary computer says show   the remit of this project was to use computer software as much as possible to produce  the world s first computer generated musical   several creative systems were used to generate this musical  which was performed in london s west end in 2016  this paper considers the challenge of evaluating this project  current computational creativity evaluation methods are ill suited to evaluating projects that involve creative input from multiple systems and people  following recent inspiration within computational creativity research from interaction design  here the decide evaluation framework is applied to evaluate the beyond the fence project  evaluation finds that the project was reasonably successful at achieving the task of using computational generation to produce a credible musical  lessons have been learned for future computational creativity projects though  particularly for affording creative software more agency and enabling software to interact with other creative partners  upon reflection  the decide framework emerges as a useful evaluation  checklist   if not a tangible operational methodology  for evaluating multiple creative systems participating in a creative task  
 hashtag sense clustering based on temporal similarity hashtags are creative labels used in micro blogs to characterize the topic of a message discussion  regardless of the use for which they were originally intended  hashtags cannot be used as a means to cluster messages with similar content  first  because hashtags are created in a spontaneous and highly dynamic way by users in multiple languages  the same topic can be associated with different hashtags  and conversely  the same hashtag may refer to different topics in different time periods  second  contrary to common words  hashtag disambiguation is complicated by the fact that no sense catalogs  e g   wikipedia or wordnet  are available  and  furthermore  hashtag labels are difficult to analyze  as they often consist of acronyms  concatenated words  and so forth  a common way to determine the meaning of hashtags has been to analyze their context  but  as we have just pointed out  hashtags can have multiple and variable meanings  in this article  we propose a temporal sense clustering algorithm based on the idea that semantically related hashtags have similar and synchronous usage patterns  
 hesitant fuzzy multi attribute decision making based on the minimum deviation method as a powerful and efficient tool in expressing the indeterminate and fuzzy information  the hesitant fuzzy set  hfs  has shown its increasing importance  it was first proposed by torra and narukawa  permitting the membership degree of an element to a set of several possible values  in this paper  based on the idea of minimum deviation between the subjective and the objective preferences  we first develop two methods to determine the attribute weights under the hesitant fuzzy environment  to do so  we present the concept of hesitant fuzzy expected value and then establish several optimization models to gain the attribute weights  after that  we use the information aggregation techniques to integrate the hesitant fuzzy attribute values or their expected values  and then sort the alternatives by the overall values  moreover  we generalize these two methods to interval valued hfss  and a numerical example is utilized to show the detailed implementation procedure and effectiveness of our methods in solving multi attribute decision making problems with hesitant fuzzy or interval valued hesitant fuzzy information  
 hesitant fuzzy thermodynamic method for emergency decision making based on prospect theory due to the timeliness of emergency response and much unknown information in emergency situations  this paper proposes a method to deal with the emergency decision making  which can comprehensively reflect the emergency decision making process  by utilizing the hesitant fuzzy elements to represent the fuzziness of the objects and the hesitant thought of the experts  this paper introduces the negative exponential function into the prospect theory so as to portray the psychological behaviors of the experts  which transforms the hesitant fuzzy decision matrix into the hesitant fuzzy prospect decision matrix  hfpdm  according to the expectation levels  then  this paper applies the energy and the entropy in thermodynamics to take the quantity and the quality of the decision values into account  and defines the thermodynamic decision making parameters based on the hfpdm  accordingly  a whole procedure for emergency decision making is conducted  what is more  some experiments are designed to demonstrate and improve the validation of the emergency decision making procedure  last but not the least  this paper makes a case study about the emergency decision making in the firing and exploding at port group in tianjin binhai new area  which manifests the effectiveness and practicability of the proposed method  
 heterogeneity in generalized reinforcement learning and its relation to cognitive ability in this paper  we study the connections between working memory capacity  wmc  and learning in the context of economic guessing games  we apply a generalized version of reinforcement learning  popularly known as the experience weighted attraction  ewa  learning model  which has a connection to specific cognitive constructs  such as memory decay  the depreciation of past experience  counterfactual thinking  and choice intensity  through the estimates of the model  we examine behavioral differences among individuals due to different levels of wmc  in accordance with  miller s magic number   which is the constraint of working memory capacity  we consider two different sizes  granularities  of strategy space  one is larger  finer  and one is smaller  coarser   we find that constraining the ewa models by using levels  granules  within the limits of working memory allows for a better characterization of the data based on individual differences in wmc  using this level reinforcement version of ewa learning  also referred to as the ewa rule learning model  we find that working memory capacity can significantly affect learning behavior  our likelihood ratio test rejects the null that subjects with high wmc and subjects with low wmc follow the same ewa learning model  in addition  the parameter corresponding to  counterfactual thinking ability  is found to be reduced when working memory capacity is low   c  2016 elsevier b v  all rights reserved  
 heuristic information acquisition and restriction rules for decision support the research question addressed by this study was  what information should be presented to or hidden from decision makers in order to facilitate high performance in decision tasks  previous research on information search is limited because of its focus on analytic information acquisition methods  analytic because of the focus on maximizing expected utility  acquisition because of the focus on what information should be added or searched for  implementing these methods requires reliable assessments of probabilities  cue weights  and cue values and does not provide suggestions on how to restrict or remove information  in this work  we present four heuristics  or simple rules  for acquiring and restricting information that only require an understanding of the distribution of known and unknown information  information imbalance and complete attribute pairs   the rules were tested on a range of analytic and heuristic decision strategies within twooption decision tasks across 15 real world environments  though the rules are transparent and easy to communicate  create a balance of information between options and within cues  and require little information to perform  the simulation results show that the rules were generally effective across all environments  for almost every combination of rule and strategy  the heuristic restriction rules were shown to be more likely to increase rather than decrease accuracy  in every combination  the heuristic acquisition rules were shown to increase accuracy more than acquiring information that did not adhere to the rules  further statistical and mathematical analysis showed that rules are mediated by strategies  full information accuracy and estimates of missing information  
 heuristically repopulated bayesian ant colony optimization for treating missing values in large databases the incomplete datasets with missing values are unsuitable for making strategic decisions since they lead to biased results  this problem is even worse when the dataset is large and collected from many heterogeneous sources  the paper deals with missing scenarios which were not dealt together earlier  the proposed dual repopulated bayesian ant colony optimization  dpbaco  handles both ignorable and non ignorable missing values in heterogeneous attributes of large datasets the dpbaco integrates bayesian principles with ant colony optimization technique since both are simple and efficient to implement  after pheromone updation  repopulation of the solution pool is done by dividing the population into two based on their fitness values and generating new offsprings by performing crossover operation  the dpbaco algorithm is implemented on six large mixed attribute datasets for imputing both kinds of missing values  the empirical and statistical results show that dpbaco performs better than other existing methods at variable missing rates ranging from 5  to 50    c  2017 elsevier b v  all rights reserved  
 heuristics for the robust vehicle routing problem with time windows uncertainty is frequently present in logistics and transportation  where vehicle routing problems play a crucial role  however  due to the complexity inherent in dealing with uncertainty  most research has been devoted to deterministic problems  this paper considers a robust version of the vehicle routing problem with hard time windows  in which travel times are uncertain  a budget polytope uncertainty set describes the travel times  to limit the maximum number of sailing legs that can be delayed  this makes sure that improbable scenarios are not considered  while making sure that solutions are immune to delays on a given number of sailing legs  existing exact methods are only able to solve small instances of the problem and can be computationally demanding  with the aim of solving large instances with reduced running times  this paper proposes an efficient heuristic based on adaptive large neighborhood search  the computational study performed on instances with different uncertainty levels compares and analyzes the performance of four versions of the heuristic and shows how good quality solutions can be obtained within short computational times   c  2017 elsevier ltd  all rights reserved  
 heuristics based trust estimation in multiagent systems using temporal difference learning the application of multiagent system  mas  is becoming increasing popular as it allows agents in a system to pool resources together to achieve a common objective  a vital part of the mas is the teamwork cooperation through the sharing of information and resources among the agents to optimize their efforts in accomplishing given objectives  a critical part of the teamwork effort is the ability to trust each other when executing any task to ensure efficient and successful cooperation  this paper presents the development of a trust estimation model that could empirically evaluate the trust of an agent in mas  the proposed model is developed using temporal difference learning by incorporating the concept of markov games and heuristics to estimate trust  simulation experiments are conducted to test and evaluate the performance of the developed model against some of the recently reported model in the literature  the simulation experiments indicate that the developed model performs better in terms of accuracy and efficiency in estimating trust  
 hierarchical representation learning for kinship verification kinship verification has a number of applications such as organizing large collections of images and recognizing resemblances among humans  in this paper  first  a human study is conducted to understand the capabilities of human mind and to identify the discriminatory areas of a face that facilitate kinshipcues  the visual stimuli presented to the participants determine their ability to recognize kin relationship using the whole face as well as specific facial regions  the effect of participant gender and age and kin relation pair of the stimulus is analyzed using quantitative measures such as accuracy  discriminability index d   and perceptual information entropy  utilizing the information obtained from the human study  a hierarchical kinship verification via representation learning  kvrl  framework is utilized to learn the representation of different face regions in an unsupervised manner  we propose a novel approach for feature representation termed as filtered contractive deep belief networks  fcdbn   the proposed feature representation encodes relational information present in images using filters and contractive regularization penalty  a compact representation of facial images of kin is extracted as an output from the learned model and a multi layer neural network is utilized to verify the kin accurately  a new wvu kinship database is created  which consists of multiple images per subject to facilitate kinship verification  the results show that the proposed deep learning framework  kvrl fcdbn  yields the state of the art kinship verification accuracy on the wvu kinship database and on four existing benchmark data sets  furthermore  kinship information is used as a soft biometric modality to boost the performance of face verification via product of likelihood ratio and support vector machine based approaches  using the proposed kvrl fcdbn framework  an improvement of over 20  is observed in the performance of face verification  
 hopfield net spreading activation for grounding of abstract action words in cognitive robot human intelligence is the main inspiration for the cognitive robotics field  and language is one of the unique qualities of humans  the increasing interest in acquisition of language in cognitive agents robots with grounded phenomenon facilitates robotic services in real scenarios in real means through stepping into the shoes of human intelligence  in this paper  a cognitive robotics model is proposed for the grounding of a higher order concept in the sensorimotor experience of humanoid robot  in the proposed model  robot s conceptual knowledge is hierarchically organized in a semantic network by using the inputted linguistic description of these words  the meaning of the higher order word is indirectly grounded in sensorimotor primitives through a top down activation process  specifically  in the described model  the meaning of the abstract action word is acquired through the combination of already grounded action primitives  hence  the proposed model with the strength of symbolic computation extends the grounding mechanism to the abstract words category  and also provides a tool  i e  a practical way  to investigate the link that exists between sensorimotor representation and abstract conceptual knowledge of the cognitive robot  for verification of the proposed model  simulation  experiments have been conducted by using icub robot data   c  2017 elsevier b v  all rights reserved  
 how are religious concepts created  a form of cognition and its effects science that needs logical demonstration has failed to eliminate religious concepts  it is as if they have own validity that cannot be broken by scientific knowledge we trust the most at present  in this paper  i will attempt to establish a new cognitive theory to help explain the basis of belief in religious concepts  this form of cognition will be named simply unifying induction or unifying inductive cognition  as illustrations  i will consider some typical religious discourses involving concepts such as  all in one  or  one is everything   it is these typically religious discourses that science has not been able to easily sweep away by its logical scientific proofs  in the end  although we perhaps cannot know if the religious beings such as gods really exist or not  we may understand these concepts are very the creation of human cognition  it also has important implications for other disciplines such as robotics  developmental psychology  cognitive archaeology  the history of science  the study of religion and so on   c  2016 elsevier b v  all rights reserved  
 how do you smile  towards a comprehensive smile analysis system to better understand the expression of human smile  there have been considerable studies about automatic smile detection  despite all the research  few attention is paid to analyze a smile in a comprehensive way  in this paper  a smile analysis system is presented to detailedly measure a person s smile  which consists of three main modules  smile detection  smile intensity estimation and spontaneous versus posed  svp  smile recognition  firstly  our recent proposed feature  self similarity of gradients  gss   is employed to detect smiling facial images in unconstrained scenarios  secondly  the smile intensity is estimated in terms of different facial regions rather than merely the mouth region  which is also applied in the temporal phase segmentation of a smile  finally  in svp smile recognition module  a discriminative learning model  dlm  is proposed based on a local spatial temporal feature  which devotes to obtaining most robust and discrihainative patterns of interest  the first two modules are the bases of the last  preparing a deeper understanding of a smile  experiments on benchmark databases are carried out and compared with the state of the art methods respectively  which validate the advantages of our approach of svp smile recognition  moreover  a comprehensive analysis of human smile is given for the first time to the best of our knowledge  which could pave the way for computers that better assess the emotional states of their users and provide useful and important information in helping the research of psychology and behavior science  
 how e wom and local competition drive local retailers  decisions about daily deal offerings local retailers considering offering daily deals must take into account possible impacts of both electronic wordof mouth  e wom  and local competition  however  how e wom  local competition  and their interactions affect local retailers  decisions to offer daily deals remains unclear  here we examine these effects utilizing a data set that contains details of daily deals  online reviews  and local competition measures for restaurants in the chicago area  with a propensity score matching  psa  method  we show  1  local retailers with high ratings and high number of reviews were more likely to initiate daily deals  2  local retailers in an area with a low level of local competition were more likely to initiate daily deals  and 3  the strength and direction of the impact of e wom depend on the level of local competition  our results enhance understanding of local retailers  decisions to offer daily deals and yield important implications related to daily deal sites   c  2017 elsevier b v  all rights reserved  
 how hard is control in single crossing elections  election control problems model situations where some entity  traditionally called the election chair  wants to ensure some candidate s victory by either adding or deleting candidates or voters  the complexity of deciding if such control actions can be successful is well studied for many typical voting rules and  usually  such control problems are  complete  however  faliszewski et al   inf comput 209 2  89 107  2011  have shown that many control problems become polynomial time solvable when we consider single peaked elections  in this paper we show that a similar phenomenon applies to the case of single crossing elections  specifically  we consider the complexity of control by adding deleting candidates voters under plurality  condorcet  and approval voting  for each of these control types and each of the rules  we show that if the control type is  complete in general  it becomes polynomial time solvable for single crossing elections  
 how otto did not extend his mind  but might have  dynamic systems theory and social cultural group selection proponents of cognitive situationism argue that the human mind is embodied  embedded in both natural and social cultural environments and extended creating both extended and distributed cognition  anti situationists reject all or some of these claims  i argue that four major objections to extended cognition   1  the mark of the cognitive   2  the function identity fallacy   3  cognitive bloat  and  4  scientific irrelevance lose much of their sting in the case of distributed cognition  the extension of cognitive agency to a group of cognitive agents  such as a scientific research team  however  i claim that a crucial fifth challenge  that advocates of the extended mind commit the causal constitution fallacy  has yet to be satisfactorily addressed  i focus on spyridon palermos  use of dynamic systems theory to refute this charge and i argue that his appeal to dynamic systems theory as a way of understanding system constitution fails  instead  i suggest a social cultural group selection hypothesis for understanding system constitution  but  i leave it for another day to elaborate that hypothesis  empirical plausibility   c  2017 elsevier b v  all rights reserved  
 how to deal with unbelievable assertions we tackle the problem that arises when an agent receives unbelievable information  information is unbelievable if it conflicts with the agent s convictions  that is  what the agent considers knowledge  we propose two solutions based on modifying the information so that it is no longer unbelievable  in one solution  the source and the receiver of the information cooperatively resolve the conflict  for this purpose we introduce a dialogue protocol in which the receiver explains what is wrong with the information by using logical interpolation  and the source produces a new assertion accordingly  if such cooperation is not possible  we propose an alternative solution in which the receiver revises the new piece of information by its own convictions to make it acceptable  
 how to make a best seller  optimal product design problems we formalize and analyze the computational complexity of three problems which are at the keystone of any marketing management process  given the preferences of customers over the attribute values we may assign to our product  i e  its possible features   the attribute values of products sold by our competitors  and which attribute values are available to each producer  due to e g  technological limitations  legal issues  or availability of resources   we consider the following problems   a  select the attributes of our product in such a way that the number of customers is maximized   b  find out whether there is a feasible strategy guaranteeing that  at some point in the future before some deadline  we will reach a given average number of customers during some period of time  and  c  the same question as  b   though the number of steps before the deadline is restricted to be  at most  the number of attributes  we prove that these problems are poly apx complete  exptime complete  and pspace complete  respectively  after presenting these theoretical properties  heuristic methods based on genetic  swarm and minimax algorithms are proposed to suboptimally solve these problems  we report experimental results where these methods are applied to solve some artificially designed problem instances  and next we present a case study  based on real data  where these algorithms are applied to a particular kind of product  we automatically design the political platform of a political party to maximize its numbers of votes in an election  problem  a   and its number of supporters along time  problems  b  and  c    the problem instances solved in this case study are constructed from publicly released polls on political tendencies in spain   c  2017 elsevier b v  all rights reserved  
 how to measure memorability and social interestingness of images  a review an entire industry has been developed around keyword optimization for buyers of advertising space  however  the social media landscape has shifted to photo driven behaviors  and there is a need to overcome the challenge of analyzing the large amount of visual data that users post on the internet  we will address this analysis by providing a review on how to measure image and video interestingness and memorability from content that is tacked in real time on social networks  we will investigate state of the art methods that are used to analyze social media images and present experiments that were performed to obtain comparable results based on the studied proposals and to determine which are the best characteristics and classifiers  finally  we will discuss future research directions that could be beneficial to both users and companies  
 how visual cognition influences process model comprehension process analysts and other professionals extensively use process models to analyze business processes and identify performance improvement opportunities  therefore  it is important that such models can be easily and properly understood  previous research has mainly focused on two types of factors that are important in this context   i  properties of the model itself  and  ii  properties of the model reader  the work in this paper aims at determining how the performance of subjects varies across different types of comprehension tasks  which is a new angle  to reason about the complexity of comprehension tasks we take a theoretical perspective that is grounded in visual cognition  we test our hypotheses using a free simulation experiment that incorporates eye tracking technology  we find that model related and person related factors are fully mediated by variables of visual cognition  more over  in comparison  visual cognition variables provide a significantly higher explanatory power for the duration and efficiency of comprehension tasks  these insights shed a new perspective on what influences sense making of process models  shifting the attention from model and reader characteristics to the complexity of the problem solving task at hand  our work opens the way to investigate and develop effective strategies to support readers of process models  for example through the context sensitive use of visual cues   c  2017 elsevier b v  all rights reserved  
 human age classification using facial skin aging features and artificial neural network in this paper a novel method based on facial skin aging features and artificial neural network  ann  is proposed to classify the human face images into four age groups  the facial skin aging features are extracted by using local gabor binary pattern histogram  lgbph  and wrinkle analysis  the ann classifier is designed by using two layer feedforward backpropagation neural networks  the proposed age classification framework is trained and tested with face images from pal face database and shown considerable improvement in the age classification accuracy up to 94 17  and 93 75  for male and female respectively   c  2016 elsevier b v  all rights reserved  
 human brain function in path planning  a task study despite plenty of research being performed in the human movement science  less attention has been paid to the probable method used by the human brain in the higher level motor planning  the previous studies suggest that the human brain may use a predictive approach to anticipate physical dynamics of the body and the environment to plan a short and collision free movement trajectory  we propose that the human brain may use a model based prediction procedure in path planning in which a finite prediction horizon is used to estimate the future state of the body and the environment  a goal oriented driving task  gdt  in a virtual street was designed to consider the human path planning method in dynamic environments  two groups of experiments were presented to consider the ability of the human brain in estimation of a dynamic object location and planning a collision free path  the first group of study includes four gdts  with different conditions to evaluate how the human planning strategy would change by varying the configuration of the environment  in the second group  the changes of human planning in a visually obscured and blurred situation were considered  the results are in compliance with the theory of using a model based prediction approach by human brains and indicate that the subjects benefit from a prediction horizon to plan their paths  our studies provide evidence to introduce possible factors which may be used by the human brain during path planning in dynamic environments  
 human mental search  a new population based metaheuristic optimization algorithm population based metaheuristic algorithms have become popular in recent years with them getting used in different fields such as business  medicine  and agriculture  the present paper proposes a simple but efficient population based metaheuristic algorithm called human mental search  hms   hms algorithm mimics the exploration strategies of the bid space in online auctions  the three leading steps of hms algorithm are   1  the mental search that explores the region around each solution based on levy flight   2  grouping that determines a promising region  and  3  moving the solutions toward the best strategy  to evaluate the efficiency of hms algorithm  some test functions with different characteristics are studied  the results are compared with nine state of the art metaheuristic algorithms  moreover  some nonparametric statistical methods  including wilcoxon signed rank test and friedman test  are provided  the experimental results demonstrate that the hms algorithm can present competitive results compared to other algorithms  
 human computer negotiation in a three player market setting this paper proposes a novel agent design for a three player game involving human players and computer agents  the game is analogous to settings in which participants repeatedly negotiate over contracts  such as cell phones and credit card plans  the game comprises three players  two service providers who compete to sign contracts with a single customer player  the service providers compete to make repeated contract offers to the customer consisting of resource exchanges in the game  customers can join and leave contracts at will  we computed sub game perfect equilibrium strategies for all players that were based on making contracts involving commitments between the customer player and one of the service provider players  we conducted extensive empirical studies  spanning over 500 participants  comparing the performance of computer agents using different types of equilibrium strategies with that of people in three different countries  the u s   israel and china  that are characterized by cultural differences in how people make contracts in the game  two human participants played a single computer agent in various role configurations in the game  for the customer role  agents using equilibrium strategies were able to obtain a higher score than people playing the same role in three countries  for the service provider role  agents using equilibrium strategies that reasoned about possibly irrational behavior were able to obtain higher scores than people  as well as agents that did not reason about irrational behavior   this work shows that for particular market settings involving competition between service providers  equilibrium strategies can be a successful design paradigm for computer agents without relying on data driven approaches   c  2017 elsevier b v  all rights reserved  
 hybrid bbo pso and higher order spectral features for emotion and stress recognition from natural speech  the aim of the present study is to select a set of higher order spectral features for emotion stress recognition system  50 bispectral  28 features  and bicoherence  22 features  based higher order spectral features were extracted from speech signal and its glottal waveform  these features were combined with inter speech 2010 features to further improve the recognition rates  feature subset selection  fss  was carried out in this proposed work with the objective of maximizing emotion recognition rate for subject independent with minimum features  the fss contains two stages  multi cluster feature selection was adopted in stage 1 to reduce feature space and identify relevant feature subset from interspeech 2010 features  in stage 2  biogeography based optimization  bbo   particle swarm optimization  pso  and proposed bbo pso hybrid optimization were performed to further reduce the dimension of feature space and identify the most relevant feature subset  which has higher discrimination ability to distinguish different emotional states  the proposed method was tested in three different databases  berlin emotional speech database  bes   surrey audio visual expressed emotion database  savee  and speech under simulated and actual stress  susas  simulated domain  the proposed feature set was evaluated with subject independent  si   subject dependent  sd   gender dependent male  gd male   gender dependent female  gd female   text independent pairwise speech  tidps   and text independent multi style speech  tidmss  experiments by using svm and elm classifiers  from the results obtained  it is evident that the proposed method attained accuracies of 93 25   si   100   sd   93 75   gd male   and 97 58   gd female  for bes  62 38   si  and 76 19   sd  for savee  and 90 09   tidmss   97 04   tidps   angryvs  neutral   98 89   tidps   lombard vs  neutral   99 07   tidps   loud vs  neutral  for susas   c  2017 elsevier b v  all rights reserved  
 hybrid genetic algorithm and fuzzy clustering for bankruptcy prediction in the design of a financial bankruptcy prediction model  financial ratio selection and classifier design play major roles  methodology based on expert opinion  statistical theory and computational intelligence technique has been widely applied  in this study  a hybrid structure integrating statistical theory and computational intelligence technique was developed using genetic algorithm  ga  with statistical measurements and fuzzy logic based fitness functions for key ratio selection  a fuzzy clustering algorithm was used for the classifier design  in the experiments  two financial ratio sets  one extracted from the suggestions of other studies and the other obtained by using the ga toolbox in the sas statistical software package  were applied to examine the proposed ratio selection schemes  for classifier design  the developed fuzzy classifier was compared with the well known bpnn classifier frequently used in other studies  besides  comparison between the developed hybrid structure and other well applied structures was also given  experimental results based on one to four years of financial data prior to the occurrence of bankruptcy were used to evaluate the performance of the proposed prediction model   c  2017 elsevier b v  all rights reserved  
 hybrid grammars for parsing of discontinuous phrase structures and non projective dependency structures we explore the concept of hybrid grammars  which formalize and generalize a range of existing frameworks for dealing with discontinuous syntactic structures  covered are both discontinuous phrase structures and non projective dependency structures  technically  hybrid grammars are related to synchronous grammars  where one grammar component generates linear structures and another generates hierarchical structures  by coupling lexical elements of both components together  discontinuous structures result  several types of hybrid grammars are characterized  we also discuss grammar induction from treebanks  the main advantage over existing frameworks is the ability of hybrid grammars to separate discontinuity of the desired structures from time complexity of parsing  this permits exploration of a large variety of parsing algorithms for discontinuous structures  with different properties  this is confirmed by the reported experimental results  which show a wide variety of running time  accuracy  and frequency of parse failures  
 hybrid learning in signalling games lewis skyrms signalling games have been studied under a variety of low rationality learning dynamics  reinforcement dynamics are stable but slow and prone to evolving suboptimal signalling conventions  a low inertia trial and error dynamical like win stay lose randomise is fast and reliable at finding perfect signalling conventions but unstable in the context of noise or agent error  here we consider a low rationality hybrid of reinforcement and win stay lose randomise learning that exhibits the virtues of both  this hybrid dynamics is reliable  stable and exceptionally fast  
 hybrid multi agent strategy discovering algorithm for human behavior training in simulators through serious games is widely used in domains where it is too dangerous to train in a real environment  simulations can help to model complex social and psychological aspects and can enable repetitiveness during game based learning  which is especially important when opposing or cooperating humans can get hurt  when a trainee team interacts with other humans or software agents with human like performance  cognitive and psychological properties and interactions that arise in various situations play an important role in serious game training  therefore  special tools and methods that integrate physical and cognitive activities need to be developed in order to analyze the way trainees tackle the scenario  we have addressed these problems with the hybrid multi agent strategy discovering algorithm  hmasda   which builds upon an existing algorithm for physical strategy identification  masda  by adding the ability to process and consider cognitive models  to include the cognitive behavior of trainees  and to identify integrated policies based on their overall behavior  we introduced additional features that take into account the trainees  cognitive state  their well being  and their emotional reactions  using a predefined asymmetric conflict scenario  we demonstrate that it is possible to obtain physical and cognitive descriptions of the behavior that trainees display   c  2016 elsevier ltd  all rights reserved  
 identification and rating of developmental dysgraphia by handwriting analysis developmental dysgraphia  being observed among 10 30  of school aged children  is a disturbance or difficulty in the production of written language that has to do with the mechanics of writing  the objective of this study is to propose a method that can be used for automated diagnosis of this disorder  as well as for estimation of difficulty level as determined by the handwriting proficiency screening questionnaire  we used a digitizing tablet to acquire handwriting and consequently employed a complex parameterization in order to quantify its kinematic aspects and hidden complexities  we also introduced a simple intrawriter normalization that increased dysgraphia discrimination and hpsq estimation accuracies  using a random forest classifier  we reached 96  sensitivity and specificity  while in the case of automated rating by the hpsq total score  we reached 10  estimation error  this study proves that digital parameterization of pressure and altitude tilt patterns in children with dysgraphia can be used for preliminary diagnosis of this writing disorder  
 identification method for fuzzy forecasting models of time series in this paper  we propose a fuzzy forecasting methodology of time series  which is tested on two series  the price of electricity in new south wales  australia  and on the futures market index of taiwan  the method uses a triangular membership function in a fuzzification process  including an alpha cut  and applies the extended autocorrelation function  the identification algorithm enables optimization of the number of fuzzy sets to be used  to determine the optimal order for the fuzzy prediction model and estimate its parameters with greater accuracy  the fuzzy prediction models of time series found in the scientific literature are compared using mainly trivalent membership functions  0 0 5 and 1 as membership values   and the proposed method shows more accurate results   c  2016 elsevier b v  all rights reserved  
 identifying and avoiding confusion in dialogue with people with alzheimer s disease alzheimer s disease  ad  is an increasingly prevalent cognitive disorder in which memory  language  and executive function deteriorate  usually in that order  there is a growing need to support individuals with ad and other forms of dementia in their daily lives  and our goal is to do so through speech based interaction  given that 33  of conversations with people with middle stage ad involve a breakdown in communication  it is vital that automated dialogue systems be able to identify those breakdowns and  if possible  avoid them in this article  we discuss several linguistic features that are verbal indicators of confusion in ad  including vocabulary richness  parse tree structures  and acoustic cues  and apply several machine learning algorithms to identify dialogue relevant confusion from speech with up to 82  accuracy  we also learn dialogue strategies to avoid confusion in the first place  which is accomplished using a partially observable markov decision process and which obtains accuracies  up to 96 1   that are significantly higher than several baselines  this work represents a major step towards automated dialogue systems for individuals with dementia  
 identifying child abuse through text mining and machine learning in this paper  we describe how we used text mining and analysis to identify and predict cases of child abuse in a public health institution  such institutions in the netherlands try to identify and prevent different kinds of abuse  a significant part of the medical data that the institutions have on children is unstructured  found in the form of free text notes  we explore whether these consultation data contain meaningful patterns to determine abuse  then we train machine learning models on cases of abuse as determined by over 500 child specialists from a municipality in the netherlands  the resulting model achieves a high score in classifying cases of possible abuse  we methodologically evaluate and compare the performance of the classifiers  we then describe our implementation of the decision support api at a municipality in the netherlands   c  2017 elsevier ltd  all rights reserved  
 identifying desirable product specifications from target customers  chinese ewom in a  ercely competitive business environment  understanding target customers  product preferences and demands has become the basis for improving competitive advantage  in the past  an enterprise would understand its consumers  preferences and demands through interactions between salespersons and consumers or questionnaire surveys  as internet technology and the popularity of virtual communities have grown  more consumers are commenting about products on the internet  enabling enterprises to understand more objectively consumers  preferences and demands  therefore  the extraction and analysis of valuable decision supporting information from extensive target customers  chinese electronic word of mouth  ewom  is critical to improving an enterprise s competitiveness  this work develops a mechanism for identifying desirable product specifications from target customers  chinese ewom to provide enterprises with reference specifications in product planning  and thereby reduce the time to market and improve the target customers  satisfaction  this goal can be achieved by performing the following tasks   i  designing a process for identifying desirable product specifications from target customers  chinese ewom   ii  developing techniques related to desirable product specification identification from target customers  chinese ewom  and  iii  implementing a mechanism for identifying desirable product specifications from target customers  chinese ewom  developing techniques associated with identifying desirable product specifications from target customers  chinese ewom involves the selection and analysis of the target customers  chinese ewom and the evaluation of the desirable product specifications  
 identifying developmental dysgraphia characteristics utilizing handwriting classification methods diagnosis of a specific learning disability such as dysgraphia impacts children s academic progress and well being  dysgraphia is diagnosed by clinicians based on children s written product and educational staff s impressions  this process is time consuming and subjective  consequently  many children with mild dysgraphia remain undiagnosed  especially those from lower socioeconomic backgrounds  in this work  a method for automatic identification and characterization of dysgraphia in third grade children is described  the method is based on analyzing the child s writing dynamics by sampling the pressure the pen exerts on the paper as well as the pen s position and orientation by using a standard digital writing pad  ninety nine samples were collected from writers with dysgraphia and proficient writers  a wide range of features covering dynamic properties of the writing and typographic  i  e   visual  properties were extracted for each participant  machine learning methodologies were used to infer a statistical model  which is capable of discriminating dysgraphic products from proficient products with approximately 90  accuracy  the model was analyzed to conclude which handwriting features are most discriminative  since the model provides 90  sensitivity for a specificity of 90   it is the first step toward future use as an effective standard indicator for dysgraphia detection  
 identifying influencers in a social network  the value of real referral data individuals influence each other through social interactions and marketers aim to leverage this interpersonal influence to attract new customers  it still remains a challenge to identify those customers in a social network that have the most influence on their social connections  a common approach to the influence maximization problem is to simulate influence cascades through the network based on the existence of links in the network using diffusion models  our study contributes to the literature by evaluating these principles using real life referral behaviour data  a new ranking metric  called referral rank  is introduced that builds on the game theoretic concept of the shapley value for assigning each individual in the network a value that reflects the likelihood of referring new customers  we also explore whether these methods can be further improved by looking beyond the one hop neighbourhood of the influencers  experiments on a large telecommunication data set and referral data set demonstrate that using traditional simulation based methods to identify influencers in a social network can lead to suboptimal decisions as the results overestimate actual referral cascades  we also find that looking at the influence of the two hop neighbours of the customers improves the influence spread and product adoption  our findings suggest that companies can take two actions to improve their decision support system for identifying influential customers   1  improve the data by incorporating data that reflects the actual  referral behaviour of the customers or  2  extend the method by looking at the influence of the connections in the two hop neighbourhood of the customers   c  2016 elsevier b v  all rights reserved  
 identifying market behaviours using european stock index time series by a hybrid segmentation algorithm the discovery of useful patterns embodied in a time series is of fundamental relevance in many real applications  repetitive structures and common type of segments can also provide very useful information of patterns in financial time series  in this paper  we introduce a time series segmentation and characterization methodology combining a hybrid genetic algorithm and a clustering technique to automatically group common patterns from this kind of financial time series and address the problem of identifying stock market prices trends  this hybrid genetic algorithm includes a local search method aimed to improve the quality of the final solution  the local search algorithm is based on maximizing a likelihood ratio  assuming normality for the series and the subseries in which the original one is segmented  to do so  we select two stock market index time series  ibex35 spanish index  closing prices  and a weighted average time series of the ibex35  spanish   bel20  belgian   cac40  french  and dax  german  indexes  these are processed to obtain segments that are mapped into a five dimensional space composed of five statistical measures  with the purpose of grouping them according to their statistical properties  experimental results show that it is possible to discover homogeneous patterns in both time series  
 illusions in reasoning some philosophers argue that the principles of human reasoning are impeccable  and that mistakes are no more than momentary lapses in  information processing    this article makes a case to the contrary  it shows that human reasoners commit systematic fallacies  the theory of mental models predicts these errors  it postulates that individuals construct mental models of the possibilities to which the premises of an inference refer  but  their models usually represent what is true in a possibility  not what is false  this procedure reduces the load on working memory  and for the most part it yields valid inferences  however  as a computer program implementing the theory revealed  it leads to fallacious conclusions for certain inferences those for which it is crucial to represent what is false in a possibility  experiments demonstrate the variety of these fallacies and contrast them with control problems  which reasoners tend to get right  the fallacies can be compelling illusions  and they occur in reasoning based on sentential connectives such as  if   and  or    quantifiers such as  all the artists   and  some of the artists    on deontic relations such as  permitted   and  obligated    and causal relations such as  causes   and  allows    after we have reviewed the principal results  we consider the potential for alternative accounts to explain these illusory inferences  and we show how the illusions illuminate the nature of human rationality  
 image based search and retrieval for biface artefacts using features capturing archaeologically significant characteristics archaeologists are currently producing huge numbers of digitized photographs to record and preserve artefact finds  these images are used to identify and categorize artefacts and reason about connections between artefacts and perform outreach to the public  however  finding specific types of images within collections remains a major challenge  often  the metadata associated with images is sparse or is inconsistent  this makes keyword based exploratory search difficult  leaving researchers to rely on serendipity and slowing down the research process  we present an image based retrieval system that addresses this problem for biface artefacts  in order to identify artefact characteristics that need to be captured by image features  we conducted a contextual inquiry study with experts in bifaces  we then devised several descriptors for matching images of bifaces with similar artefacts  we evaluated the performance of these descriptors using measures that specifically look at the differences between the sets of images returned by the search system using different descriptors  through this nuanced approach  we have provided a comprehensive analysis of the strengths and weaknesses of the different descriptors and identified implications for design in the search systems for archaeology  
 imitative and direct learning as interacting factors in life history evolution the idea that lifetime learning can have a significant effect on life history evolution has recently been explored using a series of artificial life simulations  these involved populations of competing individuals evolving by natural selection to learn to perform well on simplified abstract tasks  with the learning consisting of identifying regularities in their environment  in reality  there is more to learning than that type of direct individual experience  because it often includes a substantial degree of social learning that involves various forms of imitation of what other individuals have learned before them  this article rectifies that omission by incorporating memes and imitative learning into revised versions of the previous approach  to do this reliably requires formulating and testing a general framework for meme based simulations that will enable more complete investigations of learning as a factor in any life history evolution scenarios  it does that by simulating imitative information transfer in terms of memes being passed between individuals  and developing a process for merging that information with the  possibly inconsistent  information acquired by direct experience  leading to a consistent overall body of learning  the proposed framework is tested on a range of learning variations and a representative set of life history factors to confirm the robustness of the approach  the simulations presented illustrate the types of interactions and tradeoffs that can emerge  and indicate the kinds of species specific models that could be developed with this approach in the future  
 impact of individual difference and investment heterogeneity on the collective cooperation in the spatial public goods game in order to deeply elucidate the evolution of cooperation  we present a novel spatial public goods game  pgg  model in which the individual difference and investment heterogeneity is simultaneously taken into account  we characterize the individual difference by classifying all players into two categories  a   type and b   type  where a   type cooperators will not give perfect contribution into a pgg group and their ratio will be controlled by the parameter epsilon  while b   type ones will always keep the unit investment into the group that they participate in  meanwhile  the heterogeneity can be implemented by a non uniform investment mechanism regarding a   type cooperators  where a contribution from a cooperative agent into a pgg group is represented as a power function of fraction of cooperators inside this group  extensive simulations indicate that the cooperation can be driven into a very high level by integrating these two components into the pgg model  furthermore  the full cooperation phase diagrams further highlight their role in the promotion of cooperation  current results will greatly enrich our understanding of collective cooperation behavior within many natural  social and even man made systems   c  2017 elsevier b v  all rights reserved  
 impact of product modularity on mass customization capability  an exploratory study of contextual factors this study examines how the impact of product modularity  pm  on the mass customization capability  mcc  is moderated by several contextual factors  such as the firms  information system capacity  isc   teamwork  tw   multifunctional employees  mfe   and organizational structure  flat or hierarchical   osf   data from 238 firms located in multiple countries across three different industry groups were analyzed to test the moderated regression models and the hypotheses  the results showed that the product modularity strongly impacts the mcc  compared to isc  the social contextual variables  such as tw  mfe  and osf  have stronger moderating effects on the impact of the product modularity on the mass customization capability  in addition  isc helps mcc solely for firms with flat organizational structures  overall  our study suggests that manufacturers who desire to become mass customizers should create flat  nimble organizations with employees who are trained in several different tasks and are adept at teamwork  
 impact of risk levels on optimal selling to heterogeneous retailers under dual uncertainties consider the optimal selling problem of a supplier who sells the same product to two competing retailers under two types of uncertainty the selling costs of retailers and external demand  the confidence level is used to characterize the risk caused by the two uncertainties and the profits of the participants in the supply chain channel  our results demonstrate that higher risk levels correlate with lower belief degree costs of the two retailers and higher belief degree sizes of the market  for the vertically integrated channel  the supplier always supplies a larger quantity to the retailer with the low belief degree cost than to the other retailer  whereas  for the decentralized channel  the optimal selling decision of the supplier sometimes violates the volume ranking suggested by the quantities of the vertically integrated channel  i e   when the belief degree cost differences between the two retailers are not significant and the competitive intensity is high  the supplier supplies more units of the product to the retailer with the high belief degree cost than to the other retailer  furthermore  a decrease in the risk borne by the channel or an increase in the competitive intensity often reduces the quantities supplied to the retailers  however  in some cases  increased risk or intense competition increases the quantity supplied to the retailer with the low belief degree cost or to the other retailer  finally  we design a contract menu of two part tariffs with quantity controls such that the optimal quantities supplied and retailer profits are the same for the relaxed and original models in the decentralized channel  
 impact of sensory preferences of individuals with autism on the recognition of emotions expressed by two robots  an avatar  and a human we design a personalized human robot environment for social learning for individuals with autism spectrum disorders  asd   in order to define an individual s profile  we posit that the individual s reliance on proprioceptive and kinematic visual cues should affect the way the individual suffering from asd interacts with a social agent  human robot virtual agent   in this paper  we assess the potential link between recognition performances of body facial expressions of emotion of increasing complexity  emotion recognition on platforms with different visual features  two mini humanoid robots  a virtual agent  and a human   and proprioceptive and visual cues integration of an individual  first  we describe the design of the embodi emo database containing videos of controlled body facial expressions of emotions from various platforms  we explain how we validated this database with typically developed  td  individuals  then  we investigate the relationship between emotion recognition and proprioceptive and visual profiles of td individuals and individuals with asd  for td individuals  our results indicate a relationship between profiles and emotion recognition  as expected  we show that td individuals that rely more heavily on visual cues yield better recognition scores  however  we found that td individuals relying on proprioception have better recognition scores  going against our hypothesis  finally  participants with asd relying more heavily on proprioceptive cues have lower emotion recognition scores on all conditions than participants relying on visual cues  
 implications of estimating confidence intervals on group fuzzy decision making scores group multi criteria decision making methodologies are widely used tools in a democratic environment  previous research work in this field has been done by aggregating results of analytical hierarchical process  ahp  and analytical network process  anp   further  elaboration of the group methodology has been done to include the fuzziness in the decision making environment in the ahp and anp analysis  the current aggregation methods in group fuzzy ahp and group fuzzy anp yield a certain rank based best option among the available alternatives and criteria according to aggregated mean score method  this research introduces the concept of calculating standard deviation and 95  confidence interval on the aggregated mean score of group fuzzy ahp  gfahp  and group fuzzy anp  gfanp   the standard deviation suggests the deviance in the group decision making from the mean scores of the group  and the 95  confidence interval  ci  gives upper and lower ci of the mean score  thus providing the decision makers an interval where the ranks obtained may be valid instead of a single absolute rank  tukey s hsd tests were done to show if the mean score of the alternatives were statistically significantly different from each other  the study uses arrow s theorem as a guiding principle which helps in understanding and making decisions in a group fuzzy environment with multiple alternatives where ranking and choosing the alternatives may not always yield a single best choice of alternative  the concept of confidence interval on the group fuzzy decision making scores has been presented by comparing its implication on gfahp and gfanp using a case study example of online purchase of cookware  perfume and a watches through shopping platforms  amazon  walmart and macy s with male and female participants  an important implication of the study is presented by the results which show that in many instances the ranks of the alternatives are not statistically different from each other  this study acts as a foundation for future research where the methodology used can be combined with delphi or other complex group argumentation methods to gain more meaningful outcomes in ranking alternatives   c  2016 elsevier ltd  all rights reserved  
 implicit intention communication in human robot interaction through visual behavior studies the emergence of assistive robots presents the possibility of restoring vital degrees of independence to the elderly and impaired in activities of daily living  adl   however  one of the main challenges is the lack of a means for effective and intuitive human robot interaction  hri   while humans can express their intentions in different ways  e g   physical gestures or motions  or speech or language patterns   gaze based implicit intention communication is still underdeveloped  in this study  a novel nonverbal implicit communication framework based on eye gaze is introduced for hri  in this framework  a user s eye gaze movements are proactively tracked and analyzed to infer the user s intention in adl  then  the inferred intention can be used to command assistive robots for proper service  the advantage of this framework is that gaze based communication can be handled by most of the people  as it requires very little effort  and most of the elderly and impaired retain visual capability  this framework is expected to simplify hri  consequently enhancing the adoption of assistive technologies and improving users  independence in daily living  the testing results of this framework confirmed that a human s subtle gaze cues on visualized objects could be effectively used for human intention communication  results also demonstrated that the gaze based intention communication is easy to learn and use  in this study  the relationship of visual behaviors with the mental process during human intention expression was studied for the first time to build a fundamental understanding of this process  these findings are expected to guide further design of accurate intention inference algorithms and intuitive hri  
 implicit visual learning  image recognition via dissipative learning model according to consciousness involvement  human s learning can be roughly classified into explicit learning and implicit learning  contrasting strongly to explicit learning with clear targets and rules  such as our school study of mathematics  learning is implicit when we acquire new information without intending to do so  research from psychology indicates that implicit learning is ubiquitous in our daily life  moreover  implicit learning plays an important role in human visual perception  but in the past 60 years  most of the well known machine learning models aimed to simulate explicit learning while the work of modeling implicit learning was relatively limited  especially for computer vision applications  this article proposes a novel unsupervised computational model for implicit visual learning by exploring dissipative system  which provides a unifying macroscopic theory to connect biology with physics  we test the proposed dissipative implicit learning model  dilm  on various datasets  the experiments show that dilm not only provides a good match to human behavior but also improves the explicit machine learning performance obviously on image classification tasks  
 importance sampling for credit portfolio risk with risk factors having t copula this paper proposes an efficient simulation method for calculating credit portfolio risk when risk factors have a heavy tailed distributions  in modeling heavy tails  its features of return on underlying asset are captured by multivariate t copula  moreover  we develop a three step importance sampling  is  procedure in the t copula credit portfolio risk measure model for further variance reduction  simultaneously  we apply the levenberg marquardt algorithm associated with nonlinear optimization technique to solve the problem that estimates the meanshift vector of the systematic risk factors after the probability measure change  numerical results show that those methods developed in the t copula model can produce large variance reduction relative to the plain monte carlo method  to estimate more accurately tail probability of credit portfolio loss distribution  
 improving cash logistics in bank branches by coupling machine learning and robust optimization this paper describes how machine learning and robust optimization techniques can greatly improve cash logistics operations  specifically  we seek to optimize the logistics followed by the different branches of a given bank  machine learning is used to forecast cash demands for each of the branches  taking into account past demands and calendar effects  these demand predictions are forwarded to a robust optimization model  whose outputs are the cash transports that each branch should request  these transports guarantee that demand is fulfilled up to the desired confidence level  while also satisfying additional constraints arising in this particular domain   c  2017 elsevier ltd  all rights reserved  
 improving handwriting based gender classification using ensemble classifiers this paper presents a system to predict gender of individuals from offline handwriting samples  the technique relies on extracting a set of textural features from handwriting samples of male and female writers and training multiple classifiers to learn to discriminate between the two gender classes  the features include local binary patterns  lbp   histogram of oriented gradients  hog   statistics computed from gray level co occurrence matrices  glcm  and features extracted through segmentation based fractal texture analysis  sfta   for classification  we employ artificial neural networks  ann   support vector machine  svm   nearest neighbor classifier  nn   decision trees  dt  and random forests  rf   classifiers are then combined using bagging  voting and stacking techniques to enhance the overall system performance  the realized classification rates are significantly better than those of the state of the art systems on this problem validating the ideas put forward in this study   c  2017 elsevier ltd  all rights reserved  
 improving mention detection for basque based on a deep error analysis this paper presents the improvement process of a mention detector for basque  the system is rule based and takes into account the characteristics of mentions in basque  a classification of error types is proposed based on the errors that occur during mention detection  a deep error analysis distinguishing error types and causes is presented and improvements are proposed  at the final stage  the system obtains an f measure of 74 57  under the exact matching protocol and of 80 57  under lenient matching  we also show the performance of the mention detector with gold standard data as input  in order to omit errors caused by the previous stages of linguistic processing  in this scenario  we obtain an f measure of 85 89  with strict matching and of 89 06  with lenient matching  i e   a difference of 11 32 and 8 49 percentage points  respectively  finally  how improvements in mention detection affect coreference resolution is analysed  
 improving shift reduce phrase structure parsing with constituent boundary information shift reduce parsing enjoys the property of efficiency because of the use of efficient parsing algorithms like greedy deterministic search and beam search  in addition  shift reduce parsing is much simpler and easy to implement compared with other parsing algorithms  in this article  we explore constituent boundary information to improve the performance of shift reduce phrase structure parsing  in previous work  constituent boundary information has been used to speed up chart parsers successfully  however  whether it is useful for improving parsing accuracy has not been investigated  we propose two different models to capture constituent boundary information  based on which two sets of novel features are designed for a shift reduce parser  the first model is a boundary prediction model that uses a classifier to predict the boundaries of constituents  we use automatically parsed data to train the classifier  the second one is a tree likelihood model that measures the validity of a constituent by its likelihood which is calculated on automatically parsed data  experimental results show that our proposed method outperforms a strong baseline by 0 8  and 1 6  in f score on english and chinese data  respectively  achieving the competitive parsing accuracies on chinese  84 8   and english  90 8    to our knowledge  this is the first time for shift reduce phrase structure parsing to advance the state of the art with constituent boundary information  
 incorporated risk metrics and hybrid ai techniques for risk management this study proposes a novel technique by extending balanced scorecards with risk management considerations  i e   risk metrics and insolvency risk  for corporate operating performance assessment and then establishes a fusion mechanism that incorporates hybrid filter wrapper subset selection  hfw   random vector functional link network  rvfln   and ant colony optimization  aco  for operating performance forecasting  the study executes hfw  which preserves the advantages of wrapper approaches  but prevents paying its tremendous computational cost  in order to determine the essential features for forecasting model construction  with the merits of rapid learning speed and no extra inherent parameters needed to be tuned  rvfln helps establish the forecasting model  however  rvfln has demonstrated that its superior forecasting performance comes with the challenge of being unable to represent the inherent decision logic for humans to comprehend  to cope with this task  the study conducts aco so as to extract the inherent knowledge from rvfln and represents it in human readable format  if the extracted knowledge is not comprehensive for decision makers  then they will not be able to interpret and verify it  in this circumstance  the decision makers probably will not trust enough the extracted knowledge and be prone to making unreliable judgments more easily  the introduced mechanism herein is examined by real cases and poses superior forecasting quality under numerous examinations  it is a promising alternative for corporate operating performance forecasting  
 incorporating android conversational agents in m learning apps smart mobile devices have fostered new learning scenarios that demand sophisticated interfaces  multimodal conversational agents have became a strong alternative to develop human machine interfaces that provide a more engaging and human like relationship between students and the system  the main developers of operating systems for such devices have provided application programming interfaces for developers to implement their own applications  including different solutions for developing graphical interfaces  sensor control and voice interaction  despite the usefulness of such resources  there are no strategies defined for coupling the multimodal interface with the possibilities that these devices offer to enhance mobile educative apps with intelligent communicative capabilities and adaptation to the user needs  in this paper  we present a practical m learning application that integrates features of android application programming interfaces on a modular architecture that emphasizes interaction management and context awareness to foster user adaptively  robustness and maintainability  
 incorporating sequential information in bankruptcy prediction with predictors based on markov for discrimination in this paper we make a contribution to the body literature that incorporates a dynamic view on bankruptcy into bankruptcy prediction modelling in addition to using financial ratios measured over multiple time periods  we introduce variables based on the markov for discrimination  mfd  model  mfd variables are able to extract the sequential information from time series of financial ratios and concentrate it in one score  our results obtained from multiple samples of belgian bankruptcy data show that using data collected from multiple time periods outperforms snap shot data that contains financial ratios measured at one point in time  in addition  we demonstrate that inclusion of mfd variables in non ensemble bankruptcy prediction models considered in the study can lead to better classification performance  the latter type of models  despite not achieving the top performance based on metric considered in our study  can still be used by practitioners who prefer simpler  more interpretable models   c  2017 elsevier b v  all rights reserved  
 increasing firm agility through the use of data analytics  the role of fit agility  which refers to a dynamic capability within firms to identify and effectively respond to threats and opportunities with speed  is considered as a main business imperative in modern business environments  while there is some evidence that information technology  it  capabilities can help organizations to be more agile  studies have reported mixed findings regarding such effects  in this study  we identify the conditions under which it capabilities translate into agility gains  we focus on a specific and critical it capability  the use of data analytics  which is often leveraged by firms to improve decision making and achieve agility gains  we leverage dynamic capability theory to understand the influence of data analytics use as a lower order dynamic capability on firm agility as a higher order dynamic capability  we also draw on the fit perspective to suggest that this impact will only accrue if there is a high degree of fit between several elements that are closely related to the use of data analytics tools within firms including the tools themselves  the users  the firm tasks  and the data  the proposed research model is empirically validated using survey data from 215 senior it professionals confirming the importance of high levels of fit between data analytics tools and key related elements  the findings provide the understanding of the impacts of data analytics use on firm agility  while also providing guidance to managers on how they could better leverage the use of such technologies  these findings could be more broadly used to inform the effective use of other forms of it in organizations   c  2017 elsevier b v  all rights reserved  
 independent hesitant fuzzy group decision making methods with application to person and post matching this paper aims to study an effective group decision making  gdm  method for dealing with the hesitant fuzzy context and apply it to solving person and post matching problem  we studied the hesitant fuzzy information aggregation problem in two folds  1  consider confidence levels of the decision makers  2  consider different priority levels of the decision makers or criteria  firstly  we propose some hamacher based aggregation operators considering different confidence levels and hamacher based aggregation operator considering different prioritized levels  then  special cases of these operators and numerical examples are analyzed  finally  we propose a new gdm method using these operators and applied to person and post matching problem  
 inducing probability distributions on the set of value functions by subjective stochastic ordinal regression ordinal regression methods of multiple criteria decision aiding  mcda  take into account one  several  or all value functions compatible with the indirect preference information provided by the decision maker  dm   when dealing with multiple criteria ranking problems  typically  this information is a series of holistic and certain judgments having the form of pairwise comparisons of some reference alternatives  indicating that alternative a is certainly either preferred to or indifferent with alternative b  in some decision situations  it might be useful  however  to additionally account for uncertain pairwise comparisons interpreted in the following way  although the preference of a over b is not certain  it is more credible than preference of b over a  to handle certain and uncertain preference information  we propose a new approach that builds a probability distribution over the space of all value functions compatible with the dm s certain holistic judgments  a didactic example shows the applicability of the proposed approach   c  2016 elsevier b v  all rights reserved  
 industrial energy and environment efficiency of chinese cities  an analysis based on range adjusted measure industrial energy and environment efficiency evaluation is essential in guiding national and environmental policy making  since the industrial sector is the largest energy consumer and major pollutants producer in china  this study utilizes the range adjusted measure   ram based models to evaluate the energy and environment efficiency of industrial sectors in 31 chinese major cities  the empirical results show that eastern chinese cities outperform their western counterparts in terms of industrial energy efficiency  and central chinese cities outperform their eastern counterparts in terms of industrial environment efficiency  under natural disposability  23 cities exhibit decreasing returns to scale  and under managerial disposability  18 cities exhibit increasing damages to scale  
 industry watch nlp in a post truth world we live in a post truth world  it now matters more whether people think something is true than whether something really is true  this is dangerous  and technology is at least partly to blame  so  as technologists  how can we help to fix this  
 industry watch  the pros and cons of listening devices vastly improved speech recognition  backed by a more slowly improving ability to make sense of the recognized speech  has brought state of the art nlp into our homes in the form of smart speakers and other devices that listen  there s no doubt these devices can be incredibly useful  but they also may also support incursions into our privacy  we look at where we are today  consider what might be coming  and express just a tad of caution  
 inequality  asymmetry and social welfare  and their relationship with the median mean ratio inequality  asymmetry  and the median mean ratio are related but different concepts  although they have been frequently treated as equivalent in the literature  in this paper  we find important connections between these three concepts under particular conditions  we show that the atkinson family and the generalized entropy class of inequality indices are simple functions of the median mean ratio when a distribution is symmetrizable by a power transformation  in such a case  the coefficient of asymmetry can be interpreted as the aversion inequality parameter of a social planner  and a social evaluation function a la kolm atkinson is shown to be equivalent to median income  a social evaluation function that depends only on mean income and inequality of opportunity is found as a by product  in addition  the hannah kay family of concentration indices  of which the herfindahl hirschman is a special case  is also obtained as a function of the median mean ratio  we illustrate these results empirically using the cps dataset for the u s   1992 2007  and the eu silc dataset for the european union  2005 2007   
 influence of speaker familiarity on blind and visually impaired children s and young adults  perception of synthetic voices in this paper  we evaluate how speaker familiarity influences the engagement times and performance of blind children and young adults when playing audio games made with different synthetic voices  we also show how speaker familiarity influences speaker and synthetic speech recognition  for the first experiment we develop synthetic voices of school children  their teachers and of speakers that are unfamiliar to them and use each of these voices to create variants of two audio games  a memory game and a labyrinth game  results show that pupils have significantly longer engagement times and better performance when playing games that use synthetic voices built with their own voices  these findings can be used to improve the design of audio games and lecture books for blind and visually impaired children and young adults  in the second experiment we show that blind children and young adults are better in recognizing synthetic voices than their visually impaired companions  we also show that the average familiarity with a speaker and the similarity between a speaker s synthetic and natural voice are correlated to the speaker s synthetic voice recognition rate   c  2017 elsevier ltd  all rights reserved  
 influential user weighted sentiment analysis on topic based microblogging community nowadays  social microblogging services have become a popular expression platform of what people think  people use these platforms to produce content on different topics from finance  politics and sports to sociological fields in real time  with the proliferation of social microblogging sites  the massive amount of opinion texts have become available in digital forms  thus enabling research on sentiment analysis to both deepen and broaden in different sociological fields  previous sentiment analysis research on microblogging services generally focused on text as the unique source of information  and did not consider the social microblogging service network information  inspired by the social network analysis research and sentiment analysis studies  we find that people s trust in a community have an important place in determining the community s sentiment polarity about a topic  when studies in the literature are examined  it is seen that trusted users in a community are actually influential users  hence  we propose a novel sentiment analysis approach that takes into account the social network information as well  we concentrate on the effect of influential users on the sentiment polarity of a topic based microblogging community  our approach extends the classical sentiment analysis methods  which only consider text content  by adding a novel pagerank based influential user finding algorithm  we have carried out a comprehensive empirical study of two real world twitter datasets to analyze the correlation between the mood of the financial social community and the behavior of the stock exchange of turkey  namely bist100  using pearson correlation coefficient method  experimental results validate our assumptions and show that the proposed sentiment analysis method is more effective in finding topic based microblogging community s sentiment polarity   c  2017 elsevier ltd  all rights reserved  
 information and the internet  an analysis from the perspective of the science of the artificial this paper provides a novel philosophical approach to the role of information on the internet  the link information internet is analyzed from the perspective of the sciences of the artificial  to highlight aspects of this field that herbert simon did not consider  the analysis follows three steps   1  the study of the development of artificial intelligence as the support of internet for communication processes  this analysis is made to clarify the new communicative designs   2  the role creativity in the new communication designs is studied  in this regard  there is an interplay between the scientific creativity of human beings making designs and technological innovation of information and communication technologies   3  the consideration of the transverse and longitudinal novelty that exist in the types of digital communication  they are based on ai built up as a science of the artificial  these types of novelty depend on the interaction between scientific creativity and technological innovation  a central aim of this paper on communication sciences from the perspective of sciences of design is to overcome simona   s theoretical schemes  his view is mainly focused on structural complexity  holistic complexity and near decomposability   but communicative designs of the internet phenomena require the dynamic complexity  in addition  communication on the internet is based on an internal external duality  which goes beyond simon s approach on the artificial  thus  the analysis takes into account the new types of communication  such as social networks  and their different levels  micro  meso  and macro   
 information quality assessment for facility management assessing the quality of building information models  bims  is an important yet challenging task within the construction industry as projects are increasingly being delivered with bim  this is particularly essential for facility management  fm  users as downstream information consumers that depend on the quality of models developed in the previous project phases  the research presented in this paper addresses this challenge by introducing a framework for information quality assessment  iqa  of bims for fm uses  the iqa framework is the outcome of an extensive study of two large owner organizations involving numerous bim projects  the framework is structured based on the essential fm subjects  assets  spaces  and systems  and the model characteristics  objects  attributes  relationships  and spatial information  the framework is then operationalized through the development and evaluation of information quality  iq  tests using bim model checking tools across three projects with different levels of detail and complexity  the proposed iqa framework and associated tests advance the state of knowledge about bim quality in terms of methods to represent and evaluate conformance to owner requirements   c  2017 elsevier ltd  all rights reserved  
 information representation in decision making  the impact of cognitive style and depletion effects although the literature on information representation in decision support has argued for a long time that the way in which information is presented to decision makers should fit both task characteristics and the cognitive style of decision makers  the latter aspect has received much less attention in empirical research  most studies that took into account cognitive style used rather general instruments to measure it  which do not focus on the specifics of managerial decision making  in this paper  we describe an experiment that uses an instrument specifically developed for a managerial context to study the relationship between cognitive style and decision performance when using tabular or graphical representations  we also take into account that having to deal with a misfitting information representation depletes cognitive resources  and thus might not only impede the solution of the current problem  but also impact subsequent problems  our results confirm that a mismatch between information representation and cognitive style indeed has effects that last beyond the solution of the current decision problem   c  2017 elsevier b v  all rights reserved  
 information systems and task demand  an exploratory pupillometry study of computerized decision making information systems  is  play an important role in successful execution of organizational decisions  and the ensuing tasks that rely on those decisions  because decision making models show that cognitive load has a significant impact on how people use information systems  objective measurement of cognitive load becomes both relevant and important in is research  in this paper  we manipulate task demand during a decision making task in four different ways  we then investigate how increasing task demand affects a user s pupil data during interaction with a computerized decision aid  our results suggest that pupillometry has the potential to serve as a reliable  objective  continuous and unobtrusive measure of task demand and that the adaptive decision making theory may serve as a suitable framework for studying user pupillary responses in the is domain   c  2017 elsevier b v  all rights reserved  
 insights from a machine learning model for predicting the hospital length of stay  los  at the time of admission a model that accurately predicts  at the time of admission  the length of stay  los  for hospitalized patients could be an effective tool for healthcare providers  it could enable early interventions to prevent complications  enabling more efficient utilization of manpower and facilities in hospitals  in this study  we apply a regression tree  cubist  model for predicting the los  based on static inputs  that is  values that are known at the time of admission and that do not change during patient s hospital stay  the model was trained and validated on de identified administrative data from the veterans health administration  vha  hospitals in pittsburgh  pa  we chose to use a cubist model because it produced more accurate predictions than did alternative techniques  in addition  tree models enable us to examine the classification rules learned from the data  in order to better understand the factors that are most correlated with hospital los  cubist recursively partitions the data set as it estimates linear regressions for each partition  and the error level differs for different partitions  so that it is possible to deduce what are the characteristics of patients whose los can be accurately predicted at admission  and what are the characteristics of patients for whom the los estimate at that point in time is more highly uncertain  for example  our model indicates that the prediction error is greater for patients who had more admissions in the recent past  and for those who had longer previous hospital stays  our approach suggests that mapping the cases into a higher dimensional space  using a radial basis function  rbf  kernel  helps to separate them by their level of cubist error  using a support vector machine  svm    c  2017 elsevier ltd  all rights reserved  
 integrating a cognitive computational model of planning and decision making considering affective information planning and decision making are two of the cognitive functions involved in the solution of problems  these functions  among others  have been studied from the point of view of a new field known as cognitive informatics focused on the development of cognitive architectures  autonomous agents  and human robots that are capable of showing human like behavior  we present an exhaustive study of current biological and computational models proposed in the fields of neuroscience  psychology  and cognitive informatics  also  we present a deep review of the brain areas involved in planning  decision making  and affection  however  the majority of the proposed computational models are seeking to mimic human external behavior  this paper aims to contribute to the cognitive informatics field with an innovative cognitive computational model of planning and decision making  the two main differences of our model with respect to the current models in the literature are   i  our model considers affective and motivational information as a basic and essential trigger in planning and decision making processes   ii  our model attempts to mimic both the internal human brain as well as the external human behavior  we developed a computational model capable of offering a direct mapping from human brain areas to computational modules of our model  thus  in this paper we present our model from a conceptual  formal  and computational approach in order to show how our proposal must be implemented  finally  a set of tests were conducted in order to validate our proposal  these tests show an interesting comparison between the behavior of our prototype and the behavior exhibited by some people involved in a case study   c  2017 elsevier b v  all rights reserved  
 integrating inverse data envelopment analysis and neural network to preserve relative efficiency values the present paper is an attempt to integrate inverse data envelopment analysis  dea  and artificial neural network  ann  for a large dataset with multiple decision making units  dmus   the purpose of this study is to determine the best possible values of inputs for a large number of dmus when their output levels are changed and their efficiency values remain unchanged  when the ann is used to develop inverse dea  it is not necessary to solve the inverse dea model for every single dmu  therefore  this approach can save the computer s memory and the cpu time especially for very large scale datasets  to illustrate the ability of the proposed methodology  a set of 600 iranian bank branches is used  
 integrating social power into the decision making of cognitive agents social power is a pervasive feature with acknowledged impact in a multitude of social processes  however  despite its importance  common approaches to social power interactions in multi agent systems are rather simplistic and lack a full comprehensive view of the processes involved  in this work  we integrated a comprehensive model of social power dynamics into a cognitive agent architecture based on an operationalization of different bases of social power inspired by theoretical background research in social psychology  the model was implemented in an agent framework that was subsequently used to generate the behavior of virtual characters in an interactive virtual environment  we performed a user study to assess users  perceptions of the agents and found evidence supporting both the social power capabilities provided by the model and their value for the creation of believable and interesting scenarios  we expect that these advances and the collected evidence can be used to support the development of agent systems with an enriched capacity for social agent simulation   c  2016 the authors  published by elsevier b v  
 integrating type theory and distributional semantics  a case study on adjective noun compositions in this article  we explore an integration of a formal semantic approach to lexical meaning and an approach based on distributional methods  first  we outline a formal semantic theory that aims to combine the virtues of both formal and distributional frameworks  we then proceed to develop an algebraic interpretation of that formal semantic theory and show how at least two kinds of distributional models make this interpretation concrete  focusing on the case of adjective noun composition  we compare several distributional models with respect to the semantic information that a formal semantic theory would need  and we show how to integrate the information provided by distributional models back into the formal semantic framework  
 integration of semantic and episodic memories this paper describes the integration of semantic and episodic memory  em  models and the benefits of such integration  semantic memory  sm  is used as a foundation of knowledge and concept learning  and is needed for the operation of any cognitive system  em retains personal experiences stored based on their significance it is supported by the sm  and in return  it supports sm operations  integrated declarative memories are critical for cognitive system development  yet very little research has been done to develop their computational models  we considered structural self organization of both semantic and episodic memories with a symbolic representation of input events  sequences of events are stored in em and are used to build associations in sm  we demonstrated that integration of semantic and episodic memories improves the native operation of both types of memories  experimental results are presented to illustrate how the two memories complement each other by improving recognition  prediction  and contextbased generalization of individual memories  
 intelligent agent supporting human multi robot team collaboration the number of multi robot systems deployed in field applications has risen dramatically over the years  nevertheless  supervising and operating multiple robots simultaneously is a difficult task for a single operator to execute  in this article we propose a novel approach for utilizing automated advising agents in assisting an operator to better manage a team of multiple robots in complex environments  we introduce an advice provision methodology and exemplify its implementation using automated advising agents in two real world human multi robot team collaboration tasks  the search and rescue  sar  and the warehouse operation tasks  our intelligent advising agents were evaluated through extensive field trials  with over 150 human operators using both simulated and physical mobile robots  and showed a significant improvement in the team s performance   c  2017 elsevier b v  all rights reserved  
 intelligent data analysis approaches to churn as a business problem  a survey globalization processes and market deregulation policies are rapidly changing the competitive environments of many economic sectors  the appearance of new competitors and technologies leads to an increase in competition and  with it  a growing preoccupation among service providing companies with creating stronger customer bonds  in this context  anticipating the customer s intention to abandon the provider  a phenomenon known as churn  becomes a competitive advantage  such anticipation can be the result of the correct application of information based knowledge extraction in the form of business analytics  in particular  the use of intelligent data analysis  or data mining  for the analysis of market surveyed information can be of great assistance to churn management  in this paper  we provide a detailed survey of recent applications of business analytics to churn  with a focus on computational intelligence methods  this is preceded by an in depth discussion of churn within the context of customer continuity management  the survey is structured according to the stages identified as basic for the building of the predictive models of churn  as well as according to the different types of predictive methods employed and the business areas of their application  
 intelligent wheelchair control strategies for older adults with cognitive impairment  user attitudes  needs  and preferences intelligent powered wheelchairs can increase mobility and independence for older adults with cognitive impairment by providing collision avoidance and navigation support  the level and or type of control desired by this target population during intelligent wheelchair use have not been previously explored  in this paper  we present user attitudes  needs  and preferences in a study conducted with a mock intelligent wheelchair offering three different modes of user control  users wanted to be in the loop during wheelchair operation and or high level decision making  and also provided specific contexts where an autonomous wheelchair would be helpful  participants identified benefits of and concerns with intelligent wheelchairs  along with desired features and functionality  the paper presents the implication of these findings and provides specific recommendations for future intelligent wheelchair development and deployment  
 intentionality and conflict in the best laid plans interactive narrative virtual environment in this paper  we present the best laid plans  an interactive narrative adventure game  and the planning technologies used to generate and adapt its story in real time  the game leverages computational models of intentionality and conflict when controlling the non player characters  npcs  to ensure they act believably and introduce challenge into the automatically generated narratives  we evaluate the game s ability to generate npc behaviors that human players recognize as intentional and as conflicting with their plans  we demonstrate that players recognize these phenomena significantly more than in a control with no npc actions and not significantly different from a control in which npc actions are defined by a human author  
 inter labeler and intra labeler variability of condition severity classification models using active and passive learning methods background and objectives  labeling instances by domain experts for classification is often time consuming and expensive  to reduce such labeling efforts  we had proposed the application of active learning  al  methods  introduced our caesar ale framework for classifying the severity of clinical conditions  and shown its significant reduction of labeling efforts  the use of any of three al methods  one well known  svm margin   and two that we introduced  exploitation and combination xa   significantly reduced  by 48  to 64   condition labeling efforts  compared to standard passive  random instance selection  svm learning  furthermore  our new al methods achieved maximal accuracy using 12  fewer labeled cases than the svm margin al method  however  because labelers have varying levels of expertise  a major issue associated with learning methods  and al methods in particular  is how to best to use the labeling provided by a committee of labelers  first  we wanted to know  based on the labelers  learning curves  whether using al methods  versus standard passive learning methods  has an effect on the intra labeler variability  within the learning curve of each labeler  and inter labeler variability  among the learning curves of different labelers   then  we wanted to examine the effect of learning  either passively or actively  from the labels created by the majority consensus of a group of labelers  methods  we used our caesar ale framework for classifying the severity of clinical conditions  the three al methods and the passive learning method  as mentioned above  to induce the classifications models  we used a dataset of 516 clinical conditions and their severity labeling  represented by features aggregated from the medical records of 1 9 million patients treated at columbia university medical center  we analyzed the variance of the classification performance within  intra labeler   and especially among  inter labeler  the classification models that were induced by using the labels provided by seven labelers  we also compared the performance of the passive and active learning models when using the consensus label  results  the al methods  produced  for the models induced from each labeler  smoother intra labeler learning curves during the training phase  compared to the models produced when using the passive learning method  the mean standard deviation of the learning curves of the three al methods over all labelers  mean  0 0379  range   0 0182 to 0 0496    was significantly lower  p   0 049  than the intra labeler standard deviation when using the passive learning method  mean  0 0484  range   0 0275 0 0724   using the al methods resulted in a lower mean inter labeler auc standard deviation among the auc values of the labelers  different models during the training phase  compared to the variance of the induced models  auc values when using passive learning  the inter labeler auc standard deviation  using the passive learning method  0 039   was almost twice as high as the inter labeler standard deviation using our two new al methods  0 02 and 0 019  respectively   the svm margin al method resulted in an inter labeler standard deviation  0 029  that was higher by almost 50  than that of our two al methods the difference in the inter labeler standard deviation between the passive learning method and the svm margin learning method was significant  p   0 042   the difference between the svm margin and exploitation method was insignificant  p   0 29   as was the difference between the combination xa and exploitation methods  p   0 67   finally  using the consensus label led to a learning curve that had a higher mean intra labeler variance  but resulted eventually in an auc that was at least as high as the auc achieved using the gold standard label and that was always higher than the expected mean auc of a randomly selected labeler  regardless of the choice of learning method  including a passive learning method   using a paired t test  the difference between the intra labeler auc standard deviation when using the consensus label  versus that value when using the other two labeling strategies  was significant only when using the passive learning method  p   0 014   but not when using any of the three al methods  conclusions  the use of al methods   a  reduces intra labeler variability in the performance of the induced models during the training phase  and thus reduces the risk of halting the process at a local minimum that is significantly different in performance from the rest of the learned models  and  b  reduces inter labeler performance variance  and thus reduces the dependence on the use of a particular labeler  in addition  the use of a consensus label  agreed upon by a rather uneven group of labelers  might be at least as good as using the gold standard labeler  who might not be available  and certainly better than randomly selecting one of the group s individual labelers  finally  using the al methods  when provided by the consensus label reduced the intra labeler auc variance during the learning phase  compared to using passive learning   c  2017 elsevier b v  all rights reserved  
 interaction of automation visibility and information quality in flight deck information automation an empirical study evaluated key human factors issues related to automation visibility and information quality  based on a refined definition of information automation  next generation air transportation system operational concepts will dramatically affect the types and amount of information available on flight decks  information automation systems collect  process  and present information to support pilot tasks and awareness  the definition of flight deck information automation was refined to differentiate it from other types of automation  pilots interacted with an example information automation system to investigate the premise that automation visibility will have an impact on the ability of pilots to detect problems resulting from poor information quality  poor information quality appeared to be difficult for pilots to detect  even when presented with high automation visibility  pilots tended to over trust automation  so when reporting high workload and information was missing  they chose the top plan suggested by the automation even though it was not the best  trust in automation was reduced by low information quality  but compensated for by increased automation visibility  added information to help pilots understand information automation state and outputs  given a level of information quality  should be balanced against potential increases in pilot workload due to the time and attention needed to process the extra information  
 international portfolio optimisation with integrated currency overlay costs and constraints international financial portfolios can be exposed to substantial risk from variations of the exchange rates between the countries in which they hold investments  nonetheless  foreign exchange can both generate extra return as well as loss to a portfolio  hence rather than just being avoided  there are potential advantages to well managed international portfolios  this paper introduces an optimisation model that manages currency exposure of a portfolio through a combination of foreign exchange forward contracts  thereby creating a  currency overlay  on top of asset allocation  crucially  the hedging and transaction costs associated with holding forward contracts are taken into account in the portfolio risk and return calculations  this novel extension of previous overlay models improves the accuracy of the risk and return calculations of portfolios  consequently  more accurate investment decisions are obtained through optimal asset allocation and hedging positions  our experimental results show that inclusion of such costs significantly changes the optimal decisions  furthermore  effects of constraints related to currency hedging are examined  it is shown that tighter constraints weaken the benefit of a currency overlay and that forward positions vary significantly across return targets  a larger currency overlay is advantageous at low and high return targets  whereas small overlay positions are observed at medium return targets  the resulting system can hence enhance intelligent expert decision support for financial managers   c  2017 published by elsevier ltd  
 interpretation of approximate numerical expressions  computational model and empirical study approximate numerical expressions  anes  are linguistic expressions involving numerical values and referring to imprecise ranges of values  illustrated by examples such as  about 100   in this paper  a general principle is proposed to interpret uncontextualised anes as intervals of denoted values  it is based on an empirically justified combination of characteristics of numerical values  both arithmetical and cognitive  and in particular  taking into account the cognitive salience of numbers  this general principle is instantiated in two computational models that can be extended so as to take into account the applicative context  an empirical study is conducted to assess the performances of the two models  comparing them to state of the art methods  on real interpretations collected through an on line questionnaire  results validate the proposed characteristics used to build the models and show that they offer the best performances in estimating the median interval chosen as representative of the collected intervals   c  2016 elsevier inc  all rights reserved  
 interval target based vikor method supported on interval distance and preference degree for machine selection by considering target values for attributes in addition to beneficial and non beneficial attributes  a traditional madm technique is converted to a comprehensive form  in many machine selection problems  some attributes have given target values  the target value regarding a machine attribute can be reported as a range of data  some target based decision making methods have recently been developed  however  a research gap exists in the area  for example  fuzzy axiomatic design approach presents a target based decision making supported on common area of membership functions of alternative ratings and target values of attributes  however  it has detects on finding a complete ranking because of probable infinite values of assessment index  two target based vikor models with interval data exist in the literature  however  the target values of attributes or ratings of alternatives on attributes are crisp numbers in the models and their formulations may have some limitations  the present paper tries to fill the gap by developing the vikor method with both interval target values of attributes and interval ratings of alternatives on attributes  moreover  we attempt to utilize the power of interval computations to minimize degeneration of uncertain information  in this regard  we employ interval arithmetic and introduce a new normalization technique based on interval distance of interval numbers  we use a preference matrix to determine extremum and rank interval numbers  two machine selection problems concerning punching equipment and continuous fluid bed tea dryer are solved employing the proposed method  preference degree based ranking lists are formed by calculating the relative degrees of preference for the arranged assessment values of the candidate machines  the resultant rankings for the problems are compared with the results of fuzzy axiomatic design approach and the interval target based multimoora method and its subordinate parts  
 interval valued intuitionistic fuzzy programming technique for multicriteria group decision making based on shapley values and incomplete preference information in the classical linear programming technique for multidimensional analysis of preference  linmap   the criteria are assumed to be independent  which may cause distorted results  the aim of this paper is to employ the linmap to develop an interval valued intuitionistic fuzzy  ivif  mathematical programming method for multicriteria group decision making considering not only the importance of criteria and that of their ordered positions but also the interactions among criteria and those among their ordered positions  in the spirit of linmap  new group consistency and group inconsistency indices based on cross entropy and shapley values are firstly defined by considering the interactions among criteria and among their ordered positions  to simultaneously determine the fuzzy measures on criteria set and on ordered set  a bi objective ivif programming model is then constructed by minimizing the inconsistency index and maximizing the consistency index  the proposed ivif programming model is subsequently solved by transforming it into a multiobjective programming model  furthermore  some generalizations of the proposed programming model are investigated  finally  two examples are given to illustrate the application of the proposed method  and its superiority is demonstrated by comparing the performance of the proposed method with that of the existing methods  
 introducing dynamism in emotional agent societies this paper presents the development of a dynamic emotional model to be employed in agent societies  the proposed model is based on the pad emotional model and allows the representation of the emotional contagion phenomena of a heterogeneous group of agents that are capable of express emotions  the model is mainly based on three elements  personality  empathy and affinity  these elements allow the characterisation of each individual  causing them susceptible to vary in some degree the emotions of other individuals  additionally  the model allows defining of the social emotion of this group of agents   c  2017 elsevier b v  all rights reserved  
 invention or incremental improvement  simulation modeling and empirical testing of firm patenting behavior under performance aspiration this paper explores how firms determine their patenting strategy when faced with different performance situations  patenting strategy in this study is defined in terms of either engaging more in inventions with more risk and higher profit or in more incremental improvements with less risk and lower profit  we develop two game theoretical models to analyze how different kinds of performance discrepancies encountered by a firm influence the evolution of the firm s propensity toward a patenting strategy  then  an empirical analysis of 1921 listed companies in china is conducted to test the propositions derived from the two game theoretical models  the results reveal the decision making pattern of a firm s patenting strategy  specifically  a firm with performance higher than its aspiration will prefer to engage more in invention type patents  while a firm with lower performance than its aspiration will invest more in incremental improvement patents  additionally  all else being equal  the patenting strategy more likely to succeed will be more appealing to firms  no matter what kinds of performance gaps they have   c  2017 elsevier b v  all rights reserved  
 inventory model for a multi echelon system with unidirectional lateral transshipment inventory management constitutes a fundamental decision making problem  especially in systems which must guarantee high availability levels  in complex multi echelon networks  in case of shortage at a location  resupplying  from a near location on the same echelon rather than from the original supplier at the upper echelon  would be a potential faster and then more profitable policy  a wide interest in literature shows the importance of this policy  i e  the lateral transhipment  ltr   as a means to reduce the inventory costs  this paper deals with unidirectional ltr  which often represent a reasonable policy in scenarios where backorders have different effects on the system  based on metric  this paper defines a system approach model for determining the stock levels of repairable items in a complex network  by a genetic algorithm optimization process  the model considers non zero maintenance time for each item and different skills of maintenance centres  in a multi echelon single indenture system  with unidirectional ltr allowed  the expert system developed in the paper assists the decision maker to define the inventory levels for the maintenance sites across the system  taking advantage of the ltr to reduce costs and enhance availability  a case study of a european airline shows the relevance of the developed expert system  also considering its reproducibility to face different industrial contexts   c  2016 elsevier ltd  all rights reserved  
 investigating the effects of visuospatial memory secondary tasks on lct driving performance memory demand is associated with increased mental workload  the objective of the present study was to examine the effects of visuospatial memory secondary tasks on driving performance  memory tasks for the unknown word figure pairs and recognition tasks for word figure pairs at two level difficulties were employed separately to represent working memory s process and long term memory s process  a simulator study was conducted based on the simulation of the standard environment of lane change test  lct   the performance of lane keeping  lane change  and secondary tasks was measured by statistical methods  the comprehensive appraisal model was constructed to quantify total driving performance  the results showed that the mean path deviation  steering angle  and lane excursion times increased  and the proportion of correct lane change decreased  with the perceived workload increasing and the total driving performance decreasing in dual task driving condition  compared with the simple working memory group  as the difficulty of tasks increased in difficult working memory group  lane change performance degraded and the perceived workload increased  in contrast to difficult working memory group  the performance of lane keeping and lane change increased  while the perceived workload decreased and the total performance increased by about 50  in difficult recognition group  there were few differences between the simple working memory group and simple recognition group  the difficult working memory group had the lowest total driving performance  the results indicate that as the secondary task s difficulty increases  driving performance will degrade  performance improves significantly when the working memory process is converted to the recognition process  this trend is more obvious when the memory task assumes to be more difficult  
 investment behavior prediction in heterogeneous information network the crowdfunding industry is growing rapidly worldwide and poses new challenges on how to understand investment behavior  indeed  a key challenge in this area is how to measure the similarity of an investor and a company  or the interest of an investor in a company  tremendous effort has been made in previous research regarding the single effective factor or homogeneous network model based on link prediction for investment behavior prediction  in this study  we build an investment behavior prediction model of meta path based heterogeneous network  which considers multiple entity and relation types associated with the investment behavior of a particular investor  our investment behavior prediction model provides an effective similarity measure function for meta path  to validate the proposed model  we perform experiments on real world data from crunchbase  experimental results reveal that our investment behavior prediction model is indeed a useful indicator   c  2016 elsevier b v  all rights reserved  
 iot based collaborative reputation system for associating visitors and artworks in a cultural scenario in this paper  starting from a comprehensive mathematical model of a collaborative reputation systems  crses   we present a research study within the cultural heritage domain  the main goal of this study has been the evaluation and classification of the visitors  behaviour during a cultural event  by means of mobile technological instruments  opportunely deployed within the environment  it is possible to collect data representing the knowledge to be inferred and give a reliable rate for both visitors and exposed artworks  discussed results  confirm the reliability and the usefulness of crses for deeply understand dynamics related to people visiting styles   c  2017 elsevier ltd  all rights reserved  
 is server virtualization implementation in business and public organizations a worthwhile investment  globalization is responsible for rapid development and release of information technologies  globalization drives the competitiveness of people and organizations  competitiveness makes organizations in particular to seek for solutions to enhance their competitive edge  recent survey results on server virtualization suggest that the need to enhance competitiveness by reducing costs and improving business productivity is a major reason for the adoption of server virtualization in organizations  thus  in taking adoption decisions on emerging technologies such as server virtualization  the provocative question we ask is  is server virtualization implementation in business and public organizations a worthwhile investment  the outcomes of this study clearly show the answer is yes  however  the pitfalls associated with the implementation of these technologies are capable of making future adopters skeptical  this study examined the benefits and drawbacks of server virtualization implementation in 83 south african organizations  server virtualization implementation has couple of drawbacks  which should be looked into  
 is the prosthetic homologue necessary for embodiment  embodiment is the process by which patients with limb loss come to accept their peripheral device as a natural extension of self  however  there is little guidance as to how exacting the prosthesis must be in order for embodiment to take place  is it necessary for the prosthetic hand to look just like the absent hand  here  we describe a protocol for testing whether an individual would select a hand that looks like their own from among a selection of five hands  and whether the hand selection  regardless of homology  is consistent across multiple exposures to the same  but reordered  set of candidate hands  pilot results using healthy volunteers reveals that hand selection is only modestly consistent  and that selection of the prosthetic homologue is atypical  61 of 192 total exposures   our protocol can be executed in minutes  and makes use of readily available equipment and softwares  we present both a face to face and a virtual protocol  for maximum flexibility of implementation  
 iso standard modeling of a large arabic dictionary in this paper  we address the problem of the large coverage dictionaries of arabic language usable both for direct human reading and automatic natural language processing  for these purposes  we propose a normalized and implemented modeling  based on lexical markup framework  lmf iso 24613  and data registry category  dcr iso 12620   which allows a stable and well defined interoperability of lexical resources through a unification of the linguistic concepts  starting from the features of the arabic language  and due to the fact that a large range of details and refinements need to be described specifically for arabic  we follow a finely structuring strategy  besides its richness in morphology  syntax and semantics knowledge  our model includes all the arabic morphological patterns to generate the inflected forms from a given lemma and highlights the syntactic semantic relations  in addition  an appropriate codification has been designed for the management of all types of relationships among lexical entries and their related knowledge  according to this model  a dictionary named el madar  1  has been built and is now publicly available on line  the data are managed by a user friendly web based lexicographical workstation  this work has not been done in isolation  but is the result of a collaborative effort by an international team mainly within the iso network during a period of eight years  
 isr aiwalker  robotic walker for intuitive and safe mobility assistance and gait analysis robotic walkers are assistive robotic devices that provide mobility assistance  in a domestic or clinical scenario  to individuals suffering from a gait disorder  being age related or due to injuries  surgery  or diseases  walkers also provide a significant potential for lower limb rehabilitation  in this paper  we present a novel multimodal robotic walker platform  the isr aiwalker  where innovative contributions were made both in the human machine interface  hmi  and in a gait analysis system placed on board the platform  taking into account the application potential of these devices  an effort was made to use low cost sensors without sacrificing the overall performance of the system  a change was made in the hmi paradigm  moving from a force sensing to a vision based approach  while maintaining a natural user interaction and adding complementary safety features like correct gripping enforcement  to cope with the close proximity of the user s body  a multimodal sensor setup was considered  using both rgb and depth map data  a kinematic model of the user s lower limbs is obtained  allowing the identification of a set of features that are used in a machine learning approach to discriminate gait asymmetries  experiments made with several subjects revealed that the proposed hmi is able to correctly estimate the user intention in a natural and intuitive way  the gait analysis system was also evaluated and evidenced a good discrimination capability to distinguish between different gait patterns  
 item recommendation using tag emotion in social cataloging services due to the overload of contents  the user suffers from difficulty in selecting items  the social cataloging services allow users to consume items and share their opinions  which influences in not only oneself but other users to choose new items  the recommendation system reduces the problem of the choice by recommending the items considering the behavior of the people and the characteristics of the items  in this study  we propose a tag based recommendation method considering the emotions reflected in the user s tags  since the user s estimation of the item is made after consuming the item  the feelings of the user obtained during consuming are directly reflected in ratings and tags  the rating has overall valence on the item  and the tag represents the detailed feelings  therefore  we assume that the user s rating for an item is the basic emotion of the tag attached to the item  and the emotion of tag is adjusted by the unique emotion value of the tag  we represent the relationships between users  items  and tags as a three order tensor and apply tensor factorization  the experimental results show that the proposed method achieves better recommendation performance than baselines   c  2017 elsevier ltd  all rights reserved  
 iterated local search for workforce scheduling and routing problems the integration of scheduling workers to perform tasks with the traditional vehicle routing problem gives rise to the workforce scheduling and routing problems  wsrp   in the wsrp  a number of service technicians with different skills  and tasks at different locations with pre defined time windows and skill requirements are given  it is required to find an assignment and ordering of technicians to tasks  where each task is performed within its time window by a technician with the required skill  for which the total cost of the routing is minimized  this paper describes an iterated local search  ils  algorithm for the wsrp  the performance of the proposed algorithm is evaluated on benchmark instances against an off the shelf optimizer and an existing adaptive large neighbourhood search algorithm  the proposed ils algorithm is also applied to solve the skill vehicle routing problem  which can be viewed as a special case of the wsrp  the computational results indicate that the proposed algorithm can produce high quality solutions in short computation times  
 iterated variable neighborhood search for the capacitated clustering problem the np hard capacitated clustering problem  ccp  is a general model with a number of relevant applications  this paper proposes a highly effective iterated variable neighborhood search  ivns  algorithm for solving the problem  ivns combines an extended variable neighborhood descent method and a randomized shake procedure to explore effectively the search space  the computational results obtained on three sets of 133 benchmarks reveal that the proposed algorithm competes favorably with the state of the art algorithms in the literature both in terms of solution quality and computational efficiency  in particular  ivns discovers an improved best known result  new lower bounds  for 28 out of 83 most popular instances  while matching the current best known results for the remaining 55 instances  several essential components of the proposed algorithm are investigated to understand their impacts on the performance of algorithin   c  2016 elsevier ltd  all rights reserved  
 iterative voting and acyclic games multi agent decision problems  in which independent agents have to agree on a joint plan of action or allocation of resources  are central to artificial intelligence  in such situations  agents  individual preferences over available alternatives may vary  and may try to reconcile these differences by voting  we consider scenarios where voters cannot coordinate their actions  but are allowed to change their vote after observing the current outcome  as is often the case both in offline committees and in online voting  specifically  we are interested in identifying conditions under which such iterative voting processes are guaranteed to converge to a nash equilibrium state that is  under which this process is acyclic  we classify convergence results based on the underlying assumptions about the agent scheduler  the order in which the agents take their actions  and the action scheduler  the actions available to the agents at each step   by so doing  we position iterative voting models within the general framework of acyclic games and game forms  in more detail  our main technical results provide a complete picture of conditions for acyclicity in several variations of plurality voting  in particular  we show that  a  under the traditional lexicographic tie breaking  the game converges from any state and for any order of agents  under a weak restriction on voters  actions  and that  b  plurality with randomized tie breaking is not guaranteed to converge under arbitrary agent schedulers  but there is always some path of better replies from any initial state of the game to a nash equilibrium  we thus show a first separation between order free acyclicity and weak acyclicity of game forms  thereby settling an open question from  7   in addition  we refute another conjecture of kukushkin regarding strongly acyclic voting rules  by proving the existence of strongly acyclic separable game forms   c  2017 elsevier b v  all rights reserved  
 joint learning of binocularly driven saccades and vergence by active efficient coding this paper investigates two types of eye movements  vergence and saccades  vergence eye movements are responsible for bringing the images of the two eyes into correspondence  whereas saccades drive gaze to interesting regions in the scene  control of both vergence and saccades develops during early infancy  to date  these two types of eye movements have been studied separately  here  we propose a computational model of an active vision system that integrates these two types of eye movements  we hypothesize that incorporating a saccade strategy driven by bottom up attention will benefit the development of vergence control  the integrated system is based on the active efficient coding framework  which describes the joint development of sensory processing and eye movement control to jointly optimize the coding efficiency of the sensory system  in the integrated system  we propose a binocular saliency model to drive saccades based on learned binocular feature extractors  which simultaneously encode both depth and texture information  saliency in our model also depends on the current fixation point  this extends prior work  which focused on monocular images and saliency measures that are independent of the current fixation  our results show that the proposed saliency driven saccades lead to better vergence performance and faster learning in the overall system than random saccades  faster learning is significant because it indicates that the system actively selects inputs for the most effective learning  this work suggests that saliency driven saccades provide a scaffold for the development of vergence control during infancy  
 joint occlusion boundary detection and figure ground assignment by extracting common fate fragments in a back projection scheme occlusion boundary detection and figure ground assignment are among the fundamental challenges for the real world visual pattern recognition applications  such as 3d spatial understanding  robotic navigation and object search  we attack these challenges by extracting an intermediate level image video representation  namely  common fate fragments  a common fate fragment is composed of both over segmented region and edge fragments  physically  it exists as a coupled edge region fragment bound with dynamic information  common fate fragment candidates are generated by an integrated line region growing process  which does not require complete object segmentation or closed object boundary extraction  to identify common fate fragments from these extracted candidates  we introduce a back projection verification scheme that can circumvent the notoriously difficult task of direct motion estimation on boundaries  this allows occlusion detection and figure ground labeling to be jointly conducted within a simple but effective hypothesize and test framework  we test the proposed method on youtube motion boundaries  ymb  data set and two benchmark data sets  the cmu and berkeley motion data sets  even though the idea of the proposed method is simple and transparent  promising experimental results are observed  
 judging crowds  size by ear and by eye in virtual reality judging the size of a group of people is an everyday task  on which many decisions are based  in the present study  we investigated whether judgment of size of different groups of people depended on whether they were presented through the auditory channel  through the visual channel  or through both auditory and visual channels  groups of humanoids of different sizes  from 8 to 128  were presented within a virtual environment to healthy participants  they had to judge whether there was a lot of people in each group and rate their discomfort in relation to the stimuli with subjective units of distress  our groups of 96 and 128 virtual humans were judged as crowds regardless of their sensory presentation  the sensory presentation influenced participants  judgment of virtual human group size ranging from 8 to 48  moreover  while the quantity judgments in the auditory condition increased linearly with the group size  participants judged the quantity of people in a logarithmic manner in the two other sensory conditions  these results suggest that quantity judgment based on auditory information in a realistic context may often involve implicit arithmetic  even though our participants were not phobic of crowds  our findings are of interest for the field of virtual reality based therapy for diverse disorders because they indicate that quantity judgment can potentially be altered in a sensory specific manner in patients with fear of crowds  
 kano integrated robust design approach for aesthetical product design  a case study of a car profile visual shape parameters and aesthetic aspects of a product are one of the crucial factors for the success of a product in the market  the type and the value of the shape parameters plays an important role in visual appearance of a product and designers tends to be critical while deciding these parameters  the aesthetic aspect of a product has been matter of concern for researchers with its electromechanical design  the kano model has been found to be a useful tool to establish the relationship between performance criteria and customer satisfaction  to achieve the desired customer satisfaction weight of each product criteria is determined by using kano model  this study presents an integrative design approach combining the kano model  taguchi method and grey relation analysis to obtain the optimal combination of shape parameters and aesthetic aspects  prioritized criteria of aesthetic attributes have been abstracted through proposed methodology  a case study has been presented to evolve a profile of a car  
 keeping it real  using real world problems to teach ai to diverse audiences in recent years  ai based applications have increasingly been used in real world domains  for example  game theory based decision aids have been successfully deployed in various security settings to protect ports  airports  and wildlife  this article describes our unique problem to project educational approach that used games rooted in real world issues to teach ai concepts to diverse audiences  specifically  our educational program began by presenting real world security issues  and progressively introduced complex ai concepts using lectures  interactive exercises  and ultimately hands on games to promote learning  we describe our experience in applying this approach to several audiences  including students of an urban public high school  university undergraduates  and security domain experts who protect wildlife  we evaluated our approach based on results from the games and participant surveys  
 kernel based features for predicting population health indices from geocoded social media data when using tweets to predict population health index  due to the large scale of data  an aggregation of tweets by population has been a popular practice in learning features to characterize the population  this would alleviate the computational cost for extracting features on each individual tweet  on the other hand  much information on the population could be lost as the distribution of textual features of a population could be important for identifying the health index of that population  in addition  there could be relationships between features and those relationships could also convey predictive information of the health index  in this paper  we propose mid level features namely kernel based features for prediction of health indices of populations from social media data  the kernel based features are extracted on the distributions of textual features over population tweets and encode the relationships between individual textual features in a kernel function  we implemented our features using three different kernel functions and applied them for two case studies of population health prediction  across year prediction and across county prediction  the kernel based features were evaluated and compared with existing features on a dataset collected from the behavioral risk factor surveillance system dataset  experimental results show that the kernel based features gained significantly higher prediction performance than existing techniques  by up to 16 3   suggesting the potential and applicability of the proposed features in a wide spectrum of applications on data analytics at population levels   c  2017 elsevier b v  all rights reserved  
 keyword extraction from emails emails constitute an important genre of online communication  many of us are often faced with the daunting task of sifting through increasingly large amounts of emails on a daily basis  keywords extracted from emails can help us combat such information overload by allowing a systematic exploration of the topics contained in emails  existing literature on keyword extraction has not covered the email genre  and no human annotated gold standard datasets are currently available  in this paper  we introduce a new dataset for keyword extraction from emails  and evaluate supervised and unsupervised methods for keyword extraction from emails  the results obtained with our supervised keyword extraction system  38 99  f score  improve over the results obtained with the best performing systems participating in the semeval 2010 keyword extraction task  
 knowledge network model with neurocognitive processing capabilities the prime focus of scientists and the researchers in the field of intelligent systems is oriented to understand and replicate information processing functionalities of the human brain network  there have been continuous efforts to develop an intelligent knowledge system that incorporates the neuronal processes involved in cognition  in this paper the authors give some details of their unique work on the development of a knowledge system with information processing functionalities moving toward cognitive processing of the human brain  using intelligent links and nodes with processing capabilities  the existing knowledge systems connect only the pieces of information represented by nodes of a network and connect nodes using connectors  referred as edges  the edge is an attribute defining a relationship  e g  isa  hasa  between the nodes  these edges lack cognitive properties  and the nodes lack functional processing to support efficient information transfer between nodes  the main objective of this paper is to provide an overview of the characteristics of a neurocognitive knowledge network model  nckm  developed by the authors  nckm is a knowledge network with nodes and links developed to provide methods that deal with the cognitive processes of the human brain  useful for efficient information processing  these cognitive processes provide self directivity and learning within the network for intelligent knowledge retrieval  this work opens up a pathway to ingrain cognitive and neuronal characteristics for information processing into knowledge networks   c  2016 elsevier b v  all rights reserved  
 knowledge based systems to enhance learning  a case study on formal languages and automata theory the internet expansion is not only impacting on our social behavior  but also on the way we learn and teach  thus  more and more online teaching approaches are becoming common and usual  this work focuses on the field of formal languages and automata theory  describing the process of building an online teaching tool able to help students learn by themselves the fundamentals of this subject  tools like that are useful to reinforce the knowledge acquired by the students during face to face lectures  to reach this goal  the suggested system takes the advantage of knowledge based systems  and incorporates concepts of declarative  procedural  and conditional knowledge of the subject   c  2017 elsevier b v  all rights reserved  
 lakatos style collaborative mathematics through dialectical  structured and abstract argumentation the simulation of mathematical reasoning has been a driving force throughout the history of artificial intelligence research  however  despite significant successes in computer mathematics  computers are not widely used by mathematicians apart from their quotidian applications  an oft cited reason for this is that current computational systems cannot do mathematics in the way that humans do  we draw on two areas in which automated theorem proving  atp  is currently unlike human mathematics  firstly in a focus on soundness  rather than understandability of proof  and secondly in social aspects  employing techniques and tools from argumentation to build a framework for mixed initiative collaboration  we develop three complementary arcs  in the first arc   our theoretical model   we interpret the informal logic of mathematical discovery proposed by lakatos  a philosopher of mathematics  through the lens of dialogue game theory and in particular as a dialogue game ranging over structures of argumentation  in our second arc   our abstraction level   we develop structured arguments  from which we induce abstract argumentation systems and compute the argumentation semantics to provide labelings of the acceptability status of each argument  the output from this stage corresponds to a final  or currently accepted proof artefact  which can be viewed alongside its historical development  finally  in the third arc   our computational model   we show how each of these formal steps is available in implementation  in an appendix  we demonstrate our approach with a formal  implemented example of real world mathematical collaboration  we conclude the paper with reflections on our mixed initiative collaborative approach   c  2017 published by elsevier b v  
 latent markov and growth mixture models for ordinal individual responses with covariates  a comparison objectivewe review two alternative ways of modeling stability and change of longitudinal data by using time fixed and time varying covariates for the observed individuals  both the methods build on the foundation of finite mixture models  and are commonly applied in many fields but they look at the data from different perspectives  our attempt is to make comparisons when the ordinal nature of the response variable is of interest  methodsthe latent markov model is based on time varying latent variables to explain the observable behavior of the individuals  it is proposed in a semiparametric formulation as the latent process has a discrete distribution and is characterized by a markov structure  the growth mixture model is based on a latent categorical variable that accounts for the unobserved heterogeneity in the observed trajectories and on a mixture of gaussian random variables to account for the variability in the growth factors  we refer to a real data example on self reported health status to illustrate their peculiarities and differences  
 learning a distance metric from relative comparisons between quadruplets of images this paper is concerned with the problem of learning a distance metric by considering meaningful and discriminative distance constraints in some contexts where rich information between data is provided  classic metric learning approaches focus on constraints that involve pairs or triplets of images  we propose a general mahalanobis like distance metric learning framework that exploits distance constraints over up to four different images  we show how the integration of such constraints can lead to unsupervised or semi supervised learning tasks in some applications  we also show the benefit on recognition performance of this type of constraints  in rich contexts such as relative attributes  class taxonomies and temporal webpage analysis  
 learning activity predictors from sensor data  algorithms  evaluation  and applications recent progress in internet of things  iot  platforms has allowed us to collect large amounts of sensing data  however  there are significant challenges in converting this large scale sensing data into decisions for real world applications  motivated by applications like health monitoring and intervention and home automation we consider a novel problem called activity prediction  where the goal is to predict future activity occurrence times from sensor data  in this paper  we make three main contributions  first  we formulate and solve the activity prediction problem in the framework of imitation learning and reduce it to a simple regression learning problem  this approach allows us to leverage powerful regression learners that can reason about the relational structure of the problem with negligible computational overhead  second  we present several metrics to evaluate activity predictors in the context of real world applications  third  we evaluate our approach using real sensor data collected from 24 smart home testbeds  we also embed the learned predictor into a mobile device based activity prompter and evaluate the app for nine participants living in smart homes  our results indicate that our activity predictor performs better than the baseline methods  and offers a simple approach for predicting activities from sensor data  
 learning bases of activity for facial expression recognition the extraction of descriptive features from the sequences of faces is a fundamental problem in facial expression analysis  facial expressions are represented by psychologists as a combination of elementary movements known as action units  each movement is localised and its intensity is specified with a score that is small when the movement is subtle and large when the movement is pronounced  inspired by this approach  we propose a novel data driven feature extraction framework that represents facial expression variations as a linear combination of localised basis functions  whose coefficients are proportional to movement intensity  we show that the linear basis functions of the proposed framework can be obtained by training a sparse linear model with gabor phase shifts computed from facial videos  the proposed framework addresses generalisation issues that are not tackled by existing learnt representations  and achieves  with the same learning parameters  state of the art results in recognising both posed expressions and spontaneous micro expressions  this performance is confirmed even when the data used to train the model differ from test data in terms of the intensity of facial movements and frame rate  
 learning from class imbalanced data  review of methods and applications rare events  especially those that could potentially negatively impact society  often require humans  decision making responses  detecting rare events can be viewed as a prediction task in data mining and machine learning communities  as these events are rarely observed in daily life  the prediction task suffers from a lack of balanced data  in this paper  we provide an in depth review of rare event detection from an imbalanced learning perspective  five hundred and seventeen related papers that have been published in the past decade were collected for the study  the initial statistics suggested that rare events detection and imbalanced learning are concerned across a wide range of research areas from management science to engineering  we reviewed all collected papers from both a technical and a practical point of view  modeling methods discussed include techniques such as data preprocessing  classification algorithms and model evaluation  for applications  we first provide a comprehensive taxonomy of the existing application domains of imbalanced learning  and then we detail the applications for each category  finally  some suggestions from  the reviewed papers are incorporated with our experiences and judgments to offer further research directions for the imbalanced learning and rare event detection fields   c  2016 elsevier ltd  all rights reserved  
 learning fuzzy semantic cell by principles of maximum coverage  maximum specificity  and maximum fuzzy entropy of vague concept concept modeling and learning have important significance in data mining  machine learning and knowledge discovery  in this paper a fuzzy semantic cell which is composed of a prototype p  a distance function d and a probability density function delta of granularity is considered as the smallest unit of vague concepts and the building brick of concept representation  for each fuzzy semantic cell we introduce three fundamental numeric characteristics  prototype p  expectation granularity r  and fuzzy entropy to characterize the underlying concept  then a novel learning strategy for the fuzzy semantic cell is proposed by using the principles of maximum coverage  maximum specificity  and maximum fuzzy entropy  furthermore a granularity control factor lambda is introduced into the learning strategy in order to make these principles coordinate with each other  the ultimate goal is to obtain a fuzzy semantic cell from a given data set which is the most appropriate to describe the data set  finally the fuzzy semantic cell learning algorithm as well as the crisp semantic cell learning algorithm is formulated  we test the proposed methods on synthetic data and real world data to demonstrate their feasibility and validity   c  2017 published by elsevier b v  
 learning in the visual association of novice and expert designers designers are adept at determining similarities between previously seen objects and new creations using visual association  however  extant research on the visual association of designers and the differences between expert and novice designers when they engage in the visual association task are scant  using electroencephalography  eeg   this study attempted to narrow this research gap  sixteen healthy designers eight experts and eight novices were recruited  and asked to perform visual association while eeg signals were acquired  subsequently analysed using independent component analysis  the results indicated that strong connectivity was observed among the prefrontal  frontal  and cingulate cortices  and the default mode network  the experts used both hemispheres and executive functions to support their association tasks  whereas the novices mainly used their right hemisphere and memory retrieval functions  the visual association of experts appeared to be more goal directed than that of the novices  accordingly  designing and implementing authentic and goal directed activities for improving the executive functions of the prefrontal cortex and default mode network are critical for design educators and creativity researchers   c  2017 elsevier b v  all rights reserved  
 learning modular and transferable forward models of the motions of push manipulated objects the ability to predict how objects behave during manipulation is an important problem  models informed by mechanics are powerful  but are hard to tune  an alternative is to learn a model of the object s motion from data  to learn to predict  we study this for push manipulation  the paper starts by formulating a quasi static prediction problem  we then pose the problem of learning to predict in two different frameworks   i  regression and  ii  density estimation  our architecture is modular  many simple  object specific  and context specific predictors are learned  we show empirically that such predictors outperform a rigid body dynamics engine tuned on the same data  we then extend the density estimation approach using a product of experts  this allows transfer of learned motion models to objects of novel shape  and to novel actions  with the right representation and learning method  these transferred models can match the prediction performance of a rigid body dynamics engine for novel objects or actions  
 learning preferences from paired opposite based semantics preference semantics examine the meaning of the preference predicate  according to the way that alternatives can be understood and organized for decision making purposes  through opposite based semantics  preference structures can be characterized by their paired decomposition of preference into opposite poles  and their respective valuation of binary preference relations  extending paired semantics by fuzzy sets  preference relations can be represented in a gradual functional form  under an enhanced representational frame for examining the meaning of preference  following a semantic argument on the character of opposition  the compound meaning of preference emerges from the fuzzy reinforcement of paired opposite concepts  searching for significant evidence for affirming dominance among the decision objects  here we propose a general model for the paired decomposition of preference  examining its characteristic semantics under a binary and fuzzy logical frame  and identifying solutions with different values of significance for preference learning   c  2017 elsevier inc  all rights reserved  
 learning simpler language models with the differential state framework learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks such as language modeling  existing architectures that address the issue are often complex and costly to train  the differential state framework  dsf  is a simple and high performing design that unifies previously introduced gated neural models  dsf models maintain longer term memory by learning to interpolate between a fast changing data driven representation and a slowly changing  implicitly stable state  within the dsf framework  a new architecture is presented  the delta rnn  this model requires hardly any more parameters than a classical  simple recurrent network  in language modeling at the word and character levels  the delta rnn outperforms popular complex architectures  such as the long short term memory  lstm  and the gated recurrent unit  gru   and  when regularized  performs comparably to several state of the art baselines  at the subword level  the delta rnn s performance is comparable to that of complex gated architectures  
 learning style identifier  improving the precision of learning style identification through computational intelligence algorithms identifying students  learning styles has several benefits such as making students aware of their strengths and weaknesses when it comes to learning and the possibility to personalize their learning environment to their learning styles  while there exist learning style questionnaires for identifying a student s learning style  such questionnaires have several disadvantages and therefore  research has been conducted on automatically identifying learning styles from students  behavior in a learning environment  current approaches to automatically identify learning styles have an average precision between 66  and 77   which shows the need for improvements in order to use such automatic approaches reliably in learning environments  in this paper  four computational intelligence algorithms  artificial neural network  genetic algorithm  ant colony system and particle swarm optimization  have been investigated with respect to their potential to improve the precision of automatic learning style identification  each algorithm was evaluated with data from 75 students  the artificial neural network shows the most promising results with an average precision of 80 7   followed by particle swarm optimization with an average precision of 79 1   improving the precision of automatic learning style identification allows more students to benefit from more accurate information about their learning styles as well as more accurate personalization towards accommodating their learning styles in a learning environment  furthermore  teachers can have a better understanding of their students and be able to provide more appropriate interventions   c  2017 elsevier ltd  all rights reserved  
 learning the personalized intransitive preferences of images most of the previous studies on the user preferences assume that there is a personal transitive preference ranking of the consumable media like images  for example  the transitivity of preferences is one of the most important assumptions in the recommender system research  however  the intransitive relations have also been widely observed  such as the win loss relations in online video games  in sport matches  and even in rock paper scissors games  it is also found that different subjects demonstrate the personalized intransitive preferences in the pairwise comparisons between the applicants for college admission  since the intransitivity of preferences on images has barely been studied before and has a large impact on the research of personalized image search and recommendation  it is necessary to propose a novel method to predict the personalized intransitive preferences of images  in this paper  we propose the novel multi criterion preference  mucri  models to predict the intransitive relations in the image preferences  the mucri models utilize different kinds of image content features as well as the latent features of users and images  meanwhile  a new data set is constructed in this paper  in order to evaluate the performance of the mucri models  the experimental evaluation shows that the mucri models outperform all the baselines  due to the interdisciplinary nature of this topic  we believe it would widely attract the attention of researchers in the image processing community as well as in other communities  such as machine learning  multimedia  and recommender system  
 learning to generate descriptions of visual data anchored in spatial relations the explosive growth of visual data both online and offline in private and public repositories has led to urgent requirements for better ways to index  search  retrieve  process and manage visual content  automatic methods for generating image descriptions can help with all these tasks  and also play an important role in assistive technology for the visually impaired  the task we address in this paper is the automatic generation of image descriptions that are anchored in spatial relations  we construe this as a three step task where the first step is to identify objects in an image  the second step detects spatial relations between object pairs on the basis of language and visual features  and in the third step  the spatial relations are mapped to natural language  nl  descriptions  we describe the data we have created  and compare a range of machine learning methods in terms of the success with which they learn the mapping from features to spatial relations  using automatic and human assessed evaluations  we find that a random forest model performs best by a substantial margin  we examine aspects of our approach in more detail  including data annotation and choice of features  we describe six alternative natural language generation  nlg  strategies  and evaluate the generated nl strings using measures of correctness  naturalness and completeness  finally  we discuss evaluation issues  including the importance of extrinsic context in data creation and evaluation design  
 learning to perceive the world as probabilistic or deterministic via interaction with others  a neuro robotics experiment we suggest that different behavior generation schemes  such as sensory reflex behavior and intentional proactive behavior  can be developed by a newly proposed dynamic neural network model  named stochastic multiple timescale recurrent neural network  s mtrnn   the model learns to predict subsequent sensory inputs  generating both their means and their uncertainty levels in terms of variance  or inverse precision  by utilizing its multiple timescale property  this model was employed in robotics learning experiments in which one robot controlled by the s mtrnn was required to interact with another robot under the condition of uncertainty about the other s behavior  the experimental results show that self organized and sensory reflex behavior based on probabilistic prediction emerges when learning proceeds without a precise specification of initial conditions  in contrast  intentional proactive behavior with deterministic predictions emerges when precise initial conditions are available  the results also showed that  in situations where unanticipated behavior of the other robot was perceived  the behavioral context was revised adequately by adaptation of the internal neural dynamics to respond to sensory inputs during sensory reflex behavior generation  on the other hand  during intentional proactive behavior generation  an error regression scheme by which the internal neural activity was modified in the direction of minimizing prediction errors was needed for adequately revising the behavioral context  these results indicate that two different ways of treating uncertainty about perceptual events in learning  namely  probabilistic modeling and deterministic modeling  contribute to the development of different dynamic neuronal structures governing the two types of behavior generation schemes  
 learning to predict characteristics for engineering service projects an engineering service project can be highly interactive  collaborative  and distributed  the implementation of such projects needs to generate  utilize  and share large amounts of data and heterogeneous digital objects  the information overload prevents the effective reuse of project data and knowledge  and makes the understanding of project characteristics difficult  toward solving these issues  this paper emphasized the using of data mining and machine learning techniques to improve the project characteristic understanding process  the work presented in this paper proposed an automatic model and some analytical approaches for learning and predicting the characteristics of engineering service projects  to evaluate the model and demonstrate its functionalities  an industrial data set from the aerospace sector is considered as a the case study  this work shows that the proposed model could enable the project members to gain comprehensive understanding of project characteristics from a multidimensional perspective  and it has the potential to support them in implementing evidence based design and decision making  
 legislative prediction with dual uncertainty minimization from heterogeneous information voting on legislative bills to form new laws serves as a key function of most of the legislatures  predicting the votes of such deliberative bodies leads better understanding of government policies and generate actionable strategies for social good  however  it is very difficult to predict legislative votes due to the myriad factors that affect the political decision making process  in this paper  we present a novel prediction model that maximizes the usage of publicly accessible heterogeneous data  i e   bill text and lawmakers  profile data  to carry out effective legislative prediction  in particular  we propose to design a probabilistic prediction model which archives high consistency with past vote recorders while ensuring the minimum uncertainty of the vote prediction reflecting the firm legal ground often hold by the lawmakers  in addition  the proposed legislative prediction model enjoys the following properties  inductive and analytical solution  abilities to deal with the prediction on new bills and new legislators  and the robustness to missing vote issue  we conduct extensive empirical study using the real legislative data from the joint sessions of the united states congress and compare with other representative methods in both quantitative political science and data mining communities  the experimental results clearly corroborate that the proposed method provides superior prediction accuracy with visible performance gain   c  2016 wiley periodicals  inc  
 lemaza  an arabic why question answering system  question answering systems retrieve information from documents in response to queries  most of the questions are who  and what type questions that deal with named entities  a less common and more challenging question to deal with is the why  question  in this paper  we introduce lemaza  arabic for why   a system for automatically answering why  questions for arabic texts  the system is composed of four main components that make use of the rhetorical structure theory  to evaluate lemaza  we prepared a set of why  question answer pairs whose answer can be found in a corpus that we compiled out of open source arabic corpora  lemaza performed best when the stop words were not removed  the performance measure was 72 7   79 2  and 78 7  for recall  precision and c 1  respectively  
 lens depth function and k relative neighborhood graph  versatile tools for ordinal data analysis in recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements  by ordinal distance information we refer to binary answers to distance comparisons such as d a  b    d c  d   for many problems in machine learning and statistics it is unclear how to solve them in such a scenario  up to now  the main approach is to explicitly construct an ordinal embedding of the data points in the euclidean space  an approach that has a number of drawbacks  in this paper  we propose algorithms for the problems of medoid estimation  outlier identification  classification  and clustering when given only ordinal data  they are based on estimating the lens depth function and the k relative neighborhood graph on a data set  our algorithms are simple  are much faster than an ordinal embedding approach and avoid some of its drawbacks  and can easily be parallelized  
 leveraging bilingual terminology to improve machine translation in a cat environment this work focuses on the extraction and integration of automatically aligned bilingual terminology into a statistical machine translation  smt  system in a computer aided translation scenario  we evaluate the proposed framework that  taking as input a small set of parallel documents  gathers domain specific bilingual terms and injects them into an smt system to enhance translation quality  therefore  we investigate several strategies to extract and align terminology across languages and to integrate it in an smt system  we compare two terminology injection methods that can be easily used at run time without altering the normal activity of an smt system  xml markup and cache based model  we test the cache based model on two different domains  information technology and medical  in english  italian and german  showing significant improvements ranging from 2 23 to 6 78 bleu points over a baseline smt system and from 0 05 to 3 03 compared to the widely used xml markup approach  
 lexicon based semantic detection of sentiments using expected likelihood estimate smoothed odds ratio sentiment analysis is an active research area in today s era due to the abundance of opinionated data present on online social networks  semantic detection is a sub category of sentiment analysis which deals with the identification of sentiment orientation in any text  many sentiment applications rely on lexicons to supply features to a model  various machine learning algorithms and sentiment lexicons have been proposed in research in order to improve sentiment categorization  supervised machine learning algorithms and domain specific sentiment lexicons generally perform better as compared to the unsupervised or semi supervised domain independent lexicon based approaches  the core hindrance in the application of supervised algorithms or domain specific sentiment lexicons is the unavailability of sentiment labeled training datasets for every domain  on the other hand  the performance of algorithms based on general purpose sentiment lexicons needs improvement  this research is focused on building a general purpose sentiment lexicon in a semi supervised manner  the proposed lexicon defines word semantics based on expected likelihood estimate smoothed odds ratio that are then incorporated with supervised machine learning based model selection approach  a comprehensive performance comparison verifies the superiority of our proposed approach  
 lifelong learning of human actions with deep neural network self organization lifelong learning is fundamental in autonomous robotics for the acquisition and fine tuning of knowledge through experience  however  conventional deep neural models for action recognition from videos do not account for lifelong learning but rather learn a batch of training data with a predefined number of action classes and samples  thus  there is the need to develop learning systems with the ability to incrementally process available perceptual cues and to adapt their responses over time  we propose a self organizing neural architecture for incrementally learning to classify human actions from video sequences  the architecture comprises growing self organizing networks equipped with recurrent neurons for processing time varying patterns  we use a set of hierarchically arranged recurrent networks for the unsupervised learning of action representations with increasingly large spatiotemporal receptive fields  lifelong learning is achieved in terms of prediction driven neural dynamics in which the growth and the adaptation of the recurrent networks are driven by their capability to reconstruct temporally ordered input sequences  experimental results on a classification task using two action benchmark datasets show that our model is competitive with state of the art methods for batch learning also when a significant number of sample labels are missing or corrupted during training sessions  additional experiments show the ability of our model to adapt to non stationary input avoiding catastrophic interference   c  2017 the author s   published by elsevier ltd  
 likert scales in group multiple criteria evaluation likert scales are a widely used tool for attitude expression in many fields of social science  in this paper we explore their use in multiple criteria multi expert evaluation  we propose a methodology that deals with the non uniformity of the distribution of linguistic labels along the evaluation universe and also with possible response bias  central tendency and extreme response tendency   the methodology represents the likert type evaluations of an alternative with respect to various criteria using histograms  histograms are used in the process of aggregation of information  since the underlying evaluation scale is ordinal  a transformation of the multi expert multiple criteria evaluation represented by a histogram into a 3 bin histogram to control for the response bias is performed and an ideal evaluation 3 bin histogram is defined  we propose a distance measure to assess the closeness of the overall evaluation to the ideal and suggest the use of its values in interpretation evaluation fuzzy rules  
 limited rationality and its quantification through the interval number judgments with permutations the relative importance of alternatives expressed in terms of interval numbers in the fuzzy analytic hierarchy process aims to capture the uncertainty experienced by decision makers  dms  when making a series of comparisons  under the assumption of full rationality  the judgements of dms in the typical analytic hierarchy process could be consistent  however  since the uncertainty in articulating the opinions of dms is unavoidable  the interval number judgements are associated with the limited rationality  in this paper  we investigate the concept of limited rationality by introducing interval multiplicative reciprocal comparison matrices  by analyzing the consistency of interval multiplicative reciprocal comparison matrices  it is observed that the interval number judgements are inconsistent  by considering the permutations of alternatives  the concepts of approximation consistency and acceptable approximation consistency of interval multiplicative reciprocal comparison matrices are proposed  the exchange method is designed to generate all the permutations  a novel method of determining the interval weight vector is proposed under the consideration of randomness in comparing alternatives  and a vector of interval weights is determined  a new algorithm of solving decision making problems with interval multiplicative reciprocal preference relations is provided  two numerical examples are carried out to illustrate the proposed approach and offer a comparison with the methods available in the literature  
 linguistic multi attribute decision making with considering decision makers  risk preferences for qualitative decision making problems based on decision makers   dms   risk preferences and unbalanced linguistic term sets  ltss   a new linguistic multi attribute decision making  madm  method is developed  firstly  a concept of the generalized linguistic term set  glts  with triangular fuzzy semantic information is introduced  in order to capture and measure the dm s risk preference  a triangular fuzzy membership function with risk preference parameters is constructed  then  based on the expected semantic information of linguistic terms given by the dm and the distance between two triangular fuzzy numbers  a nonlinear programming model is established to obtain an optimal glts  an approach to linguistic madm considering the dm s risk preference is developed and its detailed steps are given  finally  a numerical example and a sensitivity analysis of risk preference parameters are examined to illustrate the feasibility and effectiveness of the proposed models  
 liquefied natural gas importing security strategy considering multi factor  a multi objective programming approach lng importing strategies  in the literature  are primarily studied under a common single factor framework  however  lng importing strategies are affected by a variety of factors  to address this existing gap  this paper proposes a multi objective programming model  which takes into account the cost  the country risk  the shipping risk  and the impact of extreme events  a pure structural change model is used to determine the risk impact coefficient for extreme events  an enhanced simulated annealing algorithm is then used to solve the lng importing optimization problem  an experimental study is further conducted to verify the practicability of the proposed approach in the case of china s lng importing data  the software implementation of the proposed model is developed in python  the proposed model provides a decision support tool for lng importing companies to find an efficient portfolio strategy for lng importing  the optimization model can be used for analyzing similar scenarios involving such dimensions as economy  energy security  and especially energy diversification   c  2017 elsevier ltd  all rights reserved  
 local and global gestalt laws  a neurally based spectral approach this letter presents a mathematical model of figure ground articulation that takes into account both local and global gestalt laws and is compatible with the functional architecture of the primary visual cortex  v1   the local gestalt law of good continuation is described by means of suitable connectivity kernels that are derived from lie group theory and quantitatively compared with long range connectivity in v1  global gestalt constraints are then introduced in terms of spectral analysis of a connectivity matrix derived from these kernels  this analysis performs grouping of local features and individuates perceptual units with the highest salience  numerical simulations are performed  and results are obtained by applying the technique to a number of stimuli  
 logarithmic least squares approaches to deriving interval weights  rectifying inconsistency and estimating missing values for interval multiplicative preference relations the aim of this paper is to develop logarithmic least squares prioritization and completion methods for interval multiplicative preference relations  a parameterized transformation formula is proposed to convert a normalized interval weight vector into a consistent interval multiplicative preference relation  a logarithmic least squares model is established to derive a normalized interval weight vector from an interval multiplicative preference relation and construct the optimized consistent interval multiplicative preference relation  subsequently  a logarithmic least squares model is built to rectify inconsistency for a complete interval multiplicative preference relation without consistency  and a logarithmic least squares completion model is developed to estimate missing values for an incomplete interval multiplicative preference relation  several numerical examples are examined to illustrate the validity and applicability of the proposed methods  and comparisons with other existing methods are also made  
 logics of common ground according to clark s seminal work on common ground and grounding  participants collaborating in a joint  activity rely on their shared information  known as common ground  to perform that activity successfully  and continually align and augment this information during their collaboration  similarly  teams of human and artificial agents require common ground to successfully participate in joint activities  indeed  without appropriate information being shared  using agent autonomy to reduce the workload on humans may actually increase workload as the humans seek to understand why the stems are behaving as they are  while many researchers have identified the importance of common ground in artificial intelligence  there is no precise definition of common ground on which to build the foundational aspects of multi agent collaboration  in this paper  building previously defined model logics of belief  we present logic definitions for four different we define modal logics for three existing notions of common ground and introduce a new notion of common ground  called salient common  ground  salient common ground captures the common ground of a group participating in an activity and is based on the common ground that arises from that activity as well as on the common ground they shared prior to the activity  we show that the four definitions share some properties  and our analysis suggests possible refinements of the existing informal and semi formal definitions  
 logics with lower and upper probability operators we present a first order and a propositional logic with unary operators that speak about upper and lower probabilities  we describe the corresponding class of models  and we discuss decidability issues for the propositional logic  we provide infinitary axiomatizations for both logics and we prove that the axiomatizations are sound and strongly complete  for some restrictions of the logics we provide finitary axiomatic systems  1   c  2017 published by elsevier inc  
 long term knowledge acquisition using contextual information in a memory inspired robot architecture in this paper  we present a novel cognitive framework allowing a robot to form memories of relevant traits of its perceptions and to recall them when necessary  the framework is based on two main principles  on the one hand  we propose an architecture inspired by current knowledge in human memory organisation  on the other hand  we integrate such an architecture with the notion of context  which is used to modulate the knowledge acquisition process when consolidating memories and forming new ones  as well as with the notion of familiarity  which is employed to retrieve proper memories given relevant cues  although much research has been carried out  which exploits machine learning approaches to provide robots with internal models of their environment  including objects and occurring events therein   we argue that such approaches may not be the right direction to follow if a long term  continuous knowledge acquisition is to be achieved  as a case study scenario  we focus on both robot environment and human robot interaction processes  in case of robot environment interaction  a robot performs pick and place movements using the objects in the workspace  at the same time observing their displacement on a table in front of it  and progressively forms memories defined as relevant cues  e g  colour  shape or relative position  in a context aware fashion  as far as human robot interaction is concerned  the robot can recall specific snapshots representing past events using both sensory information and contextual cues upon request by humans  
 low carbon supplier selection under multi source and multi attribute procurement under the development mode of low carbon economy  selecting the best low carbon supplier is the basis and prerequisite for establishing low carbon supply chain  and is the inevitable choice to achieve sustainable development for enterprises  in this paper  we investigate the problem of low carbon supplier selection in the multi source and multi attribute procurement  concretely  we establish a new evaluation index system of low carbon supplier selection based on cost  low carbon  quality and service capacity  then we present a multi attribute decision making method for low carbon supplier selection based on a linguistic 2 tuple vikor method  in this proposed decision method  the hybrid attribute values  the real numbers and linguistic fuzzy variables coexist  are transformed into linguistic 2 tuples  and a ranking method based on an extended vikor method is then presented to rank all alternative suppliers  we also give an application example to highlight the implementation  availability  and feasibility of the proposed decision making method  
 low literates  support needs for societal participation learning  empirical grounding of theory  and model based design specialized learning support software can address the low societal participation of low literate dutch citizens  we use the situated cognitive engineering method to iteratively create a design specification for the envisioned system vessel  a virtual environment to support the societal participation education of low literates  an initial high level specification for this system is refined by incorporating the societal participation experiences of low literate citizens into the design  in two series of user studies  the participant workshop and cultural probe methods were used with 23 low literate participants  the grounded theory method was used to process the rich user data from these studies into the societal participation experience of low literates  spell  model  using this experience model  the existing vessel specification was refined  requirements were empirically situated in the daily practice of low literate societal participation  and new claims were written to explicate the learning effectiveness of the proposed vessel system  in conclusion  this study provides a comprehensive  theoretically and empirically grounded set of requirements and claims for the proposed vessel system  as well as the underlying spell model  which captures the societal participation experiences of low literates citizens  the research methods used in this study are shown to be effective for requirements engineering with low literate users   c  2017 elsevier b v  all rights reserved  
 machine learning and dynamic user interfaces in a context aware nurse application environment the increasing usage of smartphones in daily life has received considerable attention in academic and industry driven research to be utilized in the health sector  there has been development of a variety of health related smartphone applications  currently  however  there are few to none applications based on nurses  historical or behavioral preferences  mobile application development for the health care sector requires extensive attention to security  reliability  and accuracy  in nursing applications  the users are often required to navigate in hospital environments  select patients to support  read the patient history and set action points to assist the patient during their shift  finally  they have to report their performance on patient related activities and other relevant information before they leave for the day  in a working day  a nurse often visits different locations such as the patient s room  different laboratories  and offices for filling reports  there is still a limited capability to access context relevant information on a smartphone with minimal recourse such as wi fi triangulation  the wi fi triangulation signals fluctuate significantly for indoor location positioning  therefore  providing relevant location based services to a mobile subscriber has become challenging  this paper addresses this gap by applying machine learning and behavior analysis to anticipate the potential location of the nurse and provide the required services  the application concept was already presented at the imcom 2015 conference  this paper focuses on the process to ascertain a user s context  the process of analyzing and predicting user behavior  and finally  the process of displaying the information through a dynamically generated ui  
 machine learning models and bankruptcy prediction there has been intensive research from academics and practitioners regarding models for predicting bankruptcy and default events  for credit risk management  seminal academic research has evaluated bankruptcy using traditional statistics techniques  e g  discriminant analysis and logistic regression  and early artificial intelligence models  e g  artificial neural networks   in this study  we test machine learning models  support vector machines  bagging  boosting  and random forest  to predict bankruptcy one year prior to the event  and compare their performance with results from discriminant analysis  logistic regression  and neural networks  we use data from 1985 to 2013 on north american firms  integrating information from the salomon center database and compustat  analysing more than 10 000 firm year observations  the key insight of the study is a substantial improvement in prediction accuracy using machine learning techniques especially when  in addition to the original altman s z score variables  we include six complementary financial indicators  based on carton and hofer  2006   we use new variables  such as the operating margin  change in return on equity  change in price to book  and growth measures related to assets  sales  and number of employees  as predictive variables  machine learning models show  on average  approximately 10  more accuracy in relation to traditional models  comparing the best models  with all predictive variables  the machine learning technique related to random forest led to 87  accuracy  whereas logistic regression and linear discriminant analysis led to 69  and 50  accuracy  respectively  in the testing sample  we find that bagging  boosting  and random forest models outperform the others techniques  and that all prediction accuracy in the testing sample improves when the additional variables are included  our research adds to the discussion of the continuing debate about superiority of computational methods over statistical techniques such as in tsai  hsu  and yen  2014  and yeh  chi  and lin  2014   in particular  for machine learning mechanisms  we do not find svm to lead to higher accuracy rates than other models  this result contradicts outcomes from danenas and garsva  2015  and cleofas sanchez  garcia  marques  and senchez  2016   but corroborates  for instance  wang  ma  and yang  2014   liang  lu  tsai  and shih  2016   and cano et al   2017   our study supports the applicability of the expert systems by practitioners as in heo and yang  2014   kim  kang  and kim  2015  and xiao  xiao  and wang  2016    c  2017 elsevier ltd  all rights reserved  
 making sense of words  a robotic model for language abstraction building robots capable of acting independently in unstructured environments is still a challenging task for roboticists  the capability to comprehend and produce language in a  human like  manner represents a powerful tool for the autonomous interaction of robots with human beings  for better understanding situations and exchanging information during the execution of tasks that require cooperation  in this work  we present a robotic model for grounding abstract action words  i e  use  make  through the hierarchical organization of terms directly linked to perceptual and motor skills of a humanoid robot  experimental results have shown that the robot  in response to linguistic commands  is capable of performing the appropriate behaviors on objects  results obtained in case of inconsistency between the perceptual and linguistic inputs have shown that the robot executes the actions elicited by the seen object  
 managing consensus and weights in iterative multiple attribute group decision making this study put forwards a novel consensus framework to manage the consensus and weights  i e   weights of the experts and attributes  in iterative multiple attribute group decision making  magdm  problem  in this consensus framework  an optimization based consensus model is devised to support the process of preferences modifying  which seeks to minimize the adjustment amounts  in the sense of manhattan distance  between the original and adjusted preferences  then  the other two optimization based consensus models are constructed to support the weights updating  in which the consensus level among experts can be further improved  a numerical example is provided to show the application of the proposed consensus framework  and a detailed comparison analysis is presented to verify the effectiveness of the proposed consensus framework   c  2016 elsevier b v  all rights reserved  
 manifold matching using shortest path distance and joint neighborhood selection matching datasets of multiple modalities has become an important task in data analysis  existing methods often rely on the embedding and transformation of each single modality without utilizing any correspondence information  which often results in sub optimal matching performance  in this paper  we propose a nonlinear manifold matching algorithm using shortest path distance and joint neighborhood selection  specifically  a joint nearest neighbor graph is built for all modalities  then the shortest path distance within each modality is calculated from the joint neighborhood graph  followed by embedding into and matching in a common low dimensional euclidean  space  compared to existing algorithms  our approach exhibits superior performance for matching disparate datasets of multiple modalities   c  2017 elsevier b v  all rights reserved  
 manifold regularization in structured output space for semi supervised structured output prediction structured output prediction aims to learn a predictor to predict a structured output from a input data vector  the structured outputs include vector  tree  sequence  etc  we usually assume that we have a training set of input output pairs to train the predictor  however  in many real world applications  it is difficult to obtain the output for a input  and thus for many training input data points  the structured outputs are missing  in this paper  we discuss how to learn from a training set composed of some input output pairs and some input data points without outputs  this problem is called semi supervised structured output prediction  we propose a novel method for this problem by constructing a nearest neighbor graph from the input space to present the manifold structure and use it to regularize the structured output space directly  we define a slack structured output for each training data point and propose to predict it by learning a structured output predictor  the learning of both slack structured outputs and the predictor are unified within one single minimization problem  in this problem  we propose to minimize the structured loss between the slack structured outputs of neighboring data points and the prediction error measured by the structured loss  the problem is optimized by an iterative algorithm  experiment results over three benchmark data sets show its advantage  
 many objective stochastic path finding using reinforcement learning in this paper  we investigate solutions to path finding problems with many conflicting objectives  and introduce a new model free many objective reinforcement learning algorithm  called voting q learning  that is capable of finding a set of optimal policies in an initially unknown  stochastic environment with several conflicting objectives  current methods for solving this type of problem rely on pareto dominance to determine which actions are optimal  which decreases in effectiveness as the number of objectives increases  ultimately selecting actions at random in environments where all potential actions are pareto optimal  alternative methods for addressing this problem require interaction with a decision maker or a priori knowledge of the problem structure for guidance towards optimal solutions  making them insufficient for fully autonomous use or problems where preferred solutions are initially unknown  as an alternative  we propose the use of voting methods from social choice theory to determine a set of pareto optimal policies by aggregating preferences determined by the evaluation of environment conditions for each objective  we demonstrate the effectiveness of this method with multiple deterministic and stochastic many objective path finding problems that are solved optimally without any advance knowledge of the problem or interaction with a decision maker  showing that our approach is the first to provide optimal performance for an autonomous  intelligent system operating in a many objective environment   c  2016 elsevier ltd  all rights reserved  
 mapping arabic wordnet synsets to wikipedia articles using monolingual and bilingual features the alignment of wordnet and wikipedia has received wide attention from researchers of computational linguistics  who are building a new lexical knowledge source or enriching the semantic information of wordnet entities  the main challenge of this alignment is how to handle the synonymy and ambiguity issues in the contents of two units from different sources  therefore  this paper introduces mapping method that links an arabic wordnet synset to its corresponding article in wikipedia  this method uses monolingual and bilingual features to overcome the lack of semantic information in arabic wordnet  for evaluating this method  an arabic mapping data set  which contains 1 291 synset article pairs  is compiled  the experimental analysis shows that the proposed method achieves promising results and outperforms the state of the art methods that depend only on monolingual features  the mapped method has also been used to increase the coverage of arabic wordnet by inserting new synsets from wikipedia  
 mapping verbal ahp scale to numerical scale for cloud computing strategy selection analytic hierarchy process  ahp  is an established multi criteria decision making method based on pair wise comparisons  evaluations are given on a verbal scale and then converted into quantitative values for calculating the priorities of the criteria and alternatives  several conversion scales have been proposed  which confuses the decision maker  in order to select the best matching scale according to the mental representation of the verbal scale of each individual decision maker  verbal scales are first used to compare alternatives with known measures  e g  surface of figures  the best matching scale representing the real values is then selected  this ahp with individualised scales has been applied in a real case study to select cloud computing strategies   c  2016 elsevier b v  all rights reserved  
 markdown optimization for an apparel retailer under cross price and initial inventory effects apparel retailers have been using markdowns as a means of revenue maximization with an increased frequency  parallel to this increase  several authors have studied single product markdown optimization problem under various settings or assumed that the products are independent in case of multi  products  in this paper  we address the simultaneous determination of markdown prices and optimal initial inventory levels under the cross price effects in a random demand setting for multi product groups for an apparel retailer chain in turkey  first  we formulate the problem as a markov decision process that considers price based substitution and complementary effects among products and maximizes the expected total profit over a finite horizon  then  we find the approximate markdown policies of each product by using approximate dynamic programming algorithm  we investigate how cross price elasticity affects the markdown policies of each product by considering several relationships among them  such as the products are all substitute or all are complement or some are substitute and some are complement  in addition to this  we provide insights on how they affect the expected revenues when non optimal and optimal initial inventory levels are considered  when cross price effects are considered in case of non optimal initial inventory levels  average revenue increases about 32  while it increases to 50  when optimal initial inventory levels are in case   c  2017 elsevier b v  all rights reserved  
 market interfaces for electric vehicle charging we consider settings where owners of electric vehicles  evs  participate in a market mechanism to charge their vehicles  existing work on such mechanisms has typically assumed that participants are fully rational and can report their preferences accurately via some interface to the mechanism or to a software agent participating on their behalf  however  this may not be reasonable in settings with non expert human end users  thus  our overarching aim in this paper is to determine experimentally if a fully expressive market interface that enables accurate preference reports is suitable for the ev charging domain  or  alternatively  if a simpler  restricted interface that reduces the space of possible options is preferable  in doing this  we measure the performance of an interface both in terms of how it helps participants maximise their utility and how it affects deliberation time  our secondary objective is to contrast two different types of restricted interfaces that vary in how they restrict the space of preferences that can be reported  to enable this analysis  we develop a novel game that replicates key features of an abstract ev charging scenario  in two experiments with over 300 users  we show that restricting the users  preferences significantly reduces the time they spend deliberating  by up to half in some cases   an extensive usability survey confirms that this restriction is furthermore associated with a lower perceived cognitive burden on the users  more surprisingly  at the same time  using restricted interfaces leads to an increase in the users  performance compared to the fully expressive interface  by up to 70    we also show that some restricted interfaces have the desirable effect of reducing the energy consumption of their users by up to 20  while achieving the same utility as other interfaces  finally  we find that a reinforcement learning agent displays similar performance trends to human users  enabling a novel methodology for evaluating market interfaces  
 marketing impact on diffusion in social networks the article proposes a way to add marketing into the standard threshold model of social networks  within this framework  the article studies logical properties of the influence relation between sets of agents in social networks  two different forms of this relation are considered  one for promotional marketing and the other for preventive marketing  in each case a sound and complete logical system describing properties of the influence relation is proposed  both systems could be viewed as extensions of armstrong s axioms of functional dependency from the database theory   c  2016 elsevier b v  all rights reserved  
 matching parse thickets for open domain question answering boris galitsky traditional parse trees are combined together and enriched with anaphora and rhetoric information to form a unified representation for a paragraph of text  we refer to these representations as parse thickets  they are introduced to support answering complex questions  which include multiple sentences  to tackle as many constraints expressed in this question as possible  the question answering system is designed  so that an initial set of answers  which is obtained by a tf idf or other keyword search model  is re ranked  passage re ranking is performed using matching of the parse thickets of answers with the parse thicket of the question  to do that  a graph representation and matching technique for parse structures for paragraphs of text have been developed  we define the operation of generalization of two parse thickets as a measure of the distance between paragraphs of text to be the maximal common sub graph of these parse thickets  a partial case of parse thickets  a rhetoric map of an answer  allows leveraging discourse for relevance in a rule based manner  passage re ranking improvement via parse thickets is evaluated in a variety of search domains with long questions  using parse thickets improves search accuracy compared with the bag of words  the pairwise matching of parse trees for sentences  and the tree kernel approaches  as a baseline  we use a web search engine api  which provides much more accurate search results than the majority of search benchmarks  such as trec  a comparative analysis of the impact of various sources of discourse information on the search accuracy is conducted  an open source plug in for solr is developed so that the proposed technology can be easily integrated with industrial search engines  
 mathematical programming based heuristics for the 0 1 mip  a survey the 0 1 mixed integer programming problem is used for modeling many combinatorial problems  ranging from logical design to scheduling and routing as well as encompassing graph theory models for resource allocation and financial planning  this paper provides a survey of heuristics based on mathematical programming for solving 0 1 mixed integer programs  mip   more precisely  we focus on the stand alone heuristics for 0 1 mip as well as those heuristics that use linear programming techniques or solve a series of linear programming models or reduced problems  deduced from the initial one  in order to produce a high quality solution of a considered problem  our emphasis will be on how mathematical programming techniques can be used for approximate problem solving  rather than on comparing performances of heuristics  
 maximizing micro blog influence in online promotion the opportunities for dynamic and timely interaction afforded by micro blogs  accompanied by the increasingly widespread use of mobile devices  have drawn disparate groups of people together to form communities online  for businesses  marketers should not underestimate the importance of understanding and communicating effectively with customers  but it is difficult to use micro blogs to estimate bloggers  marketing influence due to their lack of structure  in this study  a framework is proposed for identifying opinion leaders and maximizing the dissemination of messages by analyzing existing micro blogs  more specifically  the framework enables companies to ascertain the subjects of marketing  select keywords  retrieve micro blog content and blogger information  form ontologies  estimate and analyze the indices of bloggers  influence  identify opinion leaders  and maximize message dissemination  we use weibo  the most popular micro blog platform in china to demonstrate the framework   c  2016 elsevier ltd  all rights reserved  
 mean semi entropy models of fuzzy portfolio selection in this paper  a concept of fuzzy semientropy is proposed to quantify the downside uncertainty  several properties of fuzzy semientropy are identified and interpreted  by quantifying the downside risk with the use of semientropy  two mean semi entropy portfolio selection models are formulated  and a fuzzy simulation based genetic algorithm is designed to solve the models to optimality  we carry out comparative analyses among the fuzzy mean entropy models and the fuzzy mean semi entropy models and demonstrate that the mean semi entropy models can significantly improve the dispersion of investment  several illustrative examples using stock dataset from the real world financial market  china shanghai stock exchange  also show the effectiveness of the models  
 measuring and moderating opinion polarization in social networks the polarization of society over controversial social issues has been the subject of study in social sciences for decades  isenberg in j personal soc psychol 50 6  1141 1151  1986  sunstein in j polit philos 10 2  175 195  2002   the widespread usage of online social networks and social media  and the tendency of people to connect and interact with like minded individuals has only intensified the phenomenon of polarization  bakshy et al  in science 348 6239  1130 1132  2015   in this paper  we consider the problem of measuring and reducing polarization of opinions in a social network  using a standard opinion formation model  friedkin and johnsen in j math soc 15 3 4  193 206  1990   we define the polarization index  which  given a network and the opinions of the individuals in the network  it quantifies the polarization observed in the network  our measure captures the tendency of opinions to concentrate in network communities  creating echo chambers  given this numeric measure of polarization  we then consider the problem of reducing polarization in the network by convincing individuals  e g   through education  exposure to diverse viewpoints  or incentives  to adopt a more neutral stand towards controversial issues  we formally define the moderateinternal and moderateexpressed problems  and we prove that both our problems are np hard  by exploiting the linear algebraic characteristics of the opinion formation model we design polynomial time algorithms for both problems  our experiments with real world datasets demonstrate the validity of our metric  and the efficiency and the effectiveness of our algorithms in practice  
 measuring decarbonated development of tourist attractions associated with ecological environment and tourism economy the more and more significant the global ecological environment problems highlight  putting forward new requirements on the development of tourism and the transformation way of economic growth  so it indicates that tourism economy development and environmental protection should coexist  which is the base for measuring decarbonated development in the system of tourist attractions  ddsta   due to the complicated system of a tourist attraction  it not only contains connection between tourist activities and tourism resources  but also includes the relationships between human activity and ecological economy as well as the interaction of various factors in tourist attractions  this paper concentrates on how to measure decarbonated development of tourist attractions using the organic combination of qualitative and quantitative approach  proposing  mdl  d 2 measurement mode associated with management entropy  measuring indicator reference set and application set for ddsta and taking jiuzhai valley as a case study to apply management entropy based measurement system for ddsta and extend previous research on low carbon tourist attraction and tourism  
 measuring the congruence of fuzzy partitions in fuzzy c means clustering we propose an internal cluster validity index for a fuzzy c means algorithm which combines a mathematical model for the fuzzy c partition and a heuristic search for the number of clusters in the data  our index resorts to information theoretic principles  and aims to assess the congruence between such a model and the data that have been observed  the optimal cluster solution represents a trade off between discrepancy and the complexity of the underlying fuzzy c partition  we begin by testing the effectiveness of the proposed index using two sets of synthetic data  one comprising a well defined cluster structure and the other containing only noise  then we use datasets arising from real life problems  our results are compared to those provided by several available indices and their goodness is judged by an external measure of similarity  we find substantial evidence supporting our index as a credible alternative to the cluster validation problem  especially when it concerns structureless data   c  2016 elsevier b v  all rights reserved  
 measuring the dependence among dimensions of welfare  a study based on spearman s footrule and gini s gamma welfare is multidimensional as it involves not only income  but also education  health or labour  the composite indicators of welfare are usually based on somehow aggregating the information across dimensions and individuals  however  this approach ignores the relationship between the dimensions being aggregated  to face this goal  in this paper  we analyse the multivariate dependence between the dimensions included in the human development index  hdi   namely income  health and schooling  through three copula based measures of multivariate association  spearman s footrule  gini s gamma and spearman s rho  we discuss their properties and prove new results on spearman s footrule  the copula approach focuses on the positions of the individuals across dimensions  rather than the values that the variables attain for such individuals  thus  it allows for more general types of dependence than the linear correlation  we base our study on data from 1980 till 2014 for the countries included in the 2015 human development report  we find out that though the overall hdi has increased over this period  the dependence between its dimensions remains high and nearly unchanged  so the richest countries also tend to be the best ranked in both health and education  
 meeting a deadline  shortest paths on stochastic directed acyclic graphs with information gathering we consider the problem of an agent traversing a directed graph with the objective of maximizing the probability of reaching a goal node before a given deadline  only the probability of the travel times of edges is known to the agent  the agent must balance between traversal actions towards the goal  and delays due to actions improving information about graph edge travel times  we describe the relationship of the problem to the more general partially observable markov decision process  further  we show that if edge travel times are independent and the underlying directed graph is acyclic  a closed loop solution can be computed  the solution specifies whether to execute a traversal or information gathering action as a function of the current node  the time remaining until the deadline  and the information about edge travel times  we present results from two case studies  quantifying the usefulness of information gathering as opposed to applying only traversal actions  
 memetic algorithm with route decomposing for periodic capacitated arc routing problem in this paper he periodic capacitated arc routing problem  pcarp  is investigated  pcarp is an extensionof the well known carp from a single period to a multi period horizon  in pcarp  two objectives are tobe minimized  one is the number of required vehicles  nv   and the other is the total cost  tc   due to the multi period nature  given the same graph or road network  pcarp can have a much larger solution space than the single period carp counterpart  furthermore  pcarp consists of an additional allocation sub problem  of the days to serve the arcs   which is interdependent with the routing sub problem  although some attempts have been made for solving pcarp  more investigations are yet to be done to further improve their performance especially on large scale problem instances  it has been shown that optimizing nv and tc separately  hierarchically  is a good way of dealing with the two objectives  in this paper  we further improve this strategy and propose a new route decomposition  rd  operatorthereby  then  the rd operator is integrated into a memetic algorithm  ma  framework for pcarp  in which novel crossover and local search operators are designed accordingly  in addition  to improve the search efficiency  a hybridized initialization is employed to generate an initial population consisting of both heuristic and random individuals  the ma with rd  mard  was evaluated and compared withthe state of the art approaches on two benchmark sets of pcarp instances and a large data set which is based on a real world road network  the experimental results suggest that mard outperforms the compared state of the art algorithms  and improves most of the best known solutions  the advantage of mard becomes more obvious when the problem size increases  thus  mard is particularly effective in solving large scale pcarp instances  moreover  the efficacy of the proposed rd operator in mard has been empirically verified   c  2016 elsevier b v  all rights reserved  
 metaheuristic design of feedforward neural networks  a review of two decades of research over the past two decades  the feedforward neural network  fnn  optimization has been a key interest among the researchers and practitioners of multiple disciplines  the fnn optimization is often viewed from the various perspectives  the optimization of weights  network architecture  activation nodes  learning parameters  learning environment  etc  researchers adopted such different viewpoints mainly to improve the fnn s generalization ability  the gradient descent algorithm such as backpropagation has been widely applied to optimite the fnns  its success is evident from the fnn s application to numerous real world problems  however  due to the limitations of the gradient based optimization methods  the metaheuristic algorithms including the evolutionary algorithms  swarm intelligence  etc   are still being widely explored by the researchers aiming to obtain generalized fnn for a given problem  this article attempts to summarize a broad spectrum of fnn optimization methodologies including conventional and metaheuristic approaches  this article also tries to connect various research directions emerged out of the fnn optimization practices  such as evolving neural network  nn   cooperative coevolution nn  complex valued nn  deep learning  extreme learning machine  quantum nn  etc  additionally  it provides interesting research challenges for future research to cope up with the present information processing era  
 methods for eliciting  annotating  and analyzing databases for child speech development methods from automatic speech recognition  asr   such as segmentation and forced alignment  have facilitated the rapid annotation and analysis of very large adult speech databases and databases of caregiver infant interaction  enabling advances in speech science that were unimaginable just a few decades ago  this paper centers on two main problems that must be addressed in order to have analogous resources for developing and exploiting databases of young children s speech  the first problem is to understand and appreciate the differences between adult and child speech that cause asr models developed for adult speech to fail when applied to child speech  these differences include the fact that children s vocal tracts are smaller than those of adult males and also changing rapidly in size and shape over the course of development  leading to between talker variability across age groups that dwarfs the between talker differences between adult men and women  moreover  children do not achieve fully adult like speech motor control until they are young adults  and their vocabularies and phonological proficiency are developing as well  leading to considerably more within talker variability as well as more between talker variability  the second problem then is to determine what annotation schemas and analysis techniques can most usefully capture relevant aspects of this variability  indeed  standard acoustic characterizations applied to child speech reveal that adult centered annotation schemas fail to capture phenomena such as the emergence of covert contrasts in children s developing phonological systems  while also revealing children s nonuniform progression toward community speech norms as they acquire the phonological systems of their native languages  both problems point to the need for more basic research into the growth and development of the articulatory system  as well as of the lexicon and phonological system  that is oriented explicitly toward the construction of age appropriate computational models   c  2017 elsevier ltd  all rights reserved  
 metrics and benchmarks in human robot interaction  recent advances in cognitive robotics robots are having an important growing role in human social life  which requires them to be able to behave appropriately to the context of interaction so as to create a successful long term human robot relationship  a major challenge in developing intelligent systems  which could enhance the interactive abilities of robots  is defining clear metrics and benchmarks for the different aspects of humanrobot interaction  like human and robot skills and performances  which could facilitate comparing between systems and avoid application biased evaluations based on particular measures  the point of evaluating robotic systems through metrics and benchmarks  in addition to some recent frameworks and technologies that could endow robots with advanced cognitive and communicative abilities  are discussed in this technical report that covers the outcome of our recent workshop on current advances in cognitive robotics  towards intelligent social robots   current advances in cognitive robotics  in conjunction with the 15th ieee ras humanoids conference   seoul   south korea   2015  https   intelligent robots ws ensta paristech fr    additionally  a summary of an interactive discussion session between the workshop participants and the invited speakers about different issues related to cognitive robotics research is reported   c  2016 elsevier b v  all rights reserved  
 metrintmeas a novel metric for measuring the intelligence of a swarm of cooperating agents we propose a novel metric called metrintmeas  metric for the intelligence measuring  for an accurate and robust measurement of the difficult problem solving intelligence of a swarm system  the metric allows the classification if a swarm system belongs to the same class with the systems which have a specific reference intelligence value  for proving the efficiency of the proposed metric we realized a case study on a swarm system specialized in solving a np hard problem  as an application of the proposed metric  we present the measurement of the swarm systems  evolution in intelligence  we gave a new definition to the intelligent evolving systems  the evolution of intelligent systems can be verified using the proposed metrintmeas metric   c  2017 elsevier b v  all rights reserved  
 minimally sufficient conditions for the evolution of social learning and the emergence of non genetic evolutionary systems social learning  defined as the imitation of behaviors performed by others  is recognized as a distinctive characteristic in humans and several other animal species  previous work has claimed that the evolutionary fixation of social learning requires decision making cognitive abilities that result in transmission bias  e g   discriminatory imitation  and or guided variation  e g   adaptive modification of behaviors through individual learning   here  we present and analyze a simple agent based model that demonstrates that the transition from instinctive actuators  i e   non learning agents whose behavior is hardcoded in their genes  to social learners  i e   agents that imitate behaviors  can occur without invoking such decision making abilities  the model shows that the social learning of a trait may evolve and fix in a population if there are many possible behavioral variants of the trait  if it is subject to strong selection pressure for survival  as distinct from reproduction   and if imitation errors occur at a higher rate than genetic mutation  these results demonstrate that the  sometimes implicit  assumption in prior work that decision making abilities are required is incorrect  thus allowing a more parsimonious explanation for the evolution of social learning that applies to a wider range of organisms  furthermore  we identify genotype phenotype disengagement as a signal for the imminent fixation of social learners  and explain the way in which this disengagement leads to the emergence of a basic form of cultural evolution  i e   a non genetic evolutionary system   
 minimizing cost and time through single objective function in multi choice interval valued transportation problem this paper explores the study of transportation problem  tp  under the light of multi choice environment with interval analysis  the parameters of tp follow multi choice interval valued type so this form of tp is called multi choice interval transportation problem  mcitp   introduction of time is an important notion in tp of this paper  transportation time and cost  both are minimized through single objective function of tp  which is the main aim of this paper  a procedure is shown for converting from mcitp to deterministic tp and then solve it  a case study is included to illustrate the usefulness of the paper  finally  concluding remarks and an outlook for future study are presented to this paper  
 minimizing makespan for the distributed hybrid flowshop scheduling problem with multiprocessor tasks the trend of globalization has recently seen the study of distributed scheduling problems  this study attempts to solve the distributed hybrid flowshop scheduling problem with multiprocessor tasks  and is the first attempt to address this problem  to solve this strongly np hard problem  a mixed integer linear programming formulation and self tuning iterated greedy  sig  algorithm that incorporates an adaptive cocktail decoding mechanism are presented to minimize the makespan  comprehensive computational results demonstrate that the proposed sig algorithm is extremely efficient and effective  this paper successfully expands the research area of distributed scheduling problems   c  2017 elsevier ltd  all rights reserved  
 minimum cost consensus models based on random opinions in some complex group decision making cases  the opinions of decision makers  dms  present random characteristic  however  it is difficult to determine the range of opinions by knowing only their probability distributions  in this paper  we construct cost consensus models with random opinions  the objective function is obtaining the minimum consensus budget under a certain confidence level  nonetheless  the constraints restrict the upper limit of the consensus cost  the lower limit of dms  compensations  and the opinions deviation between dms and the moderator  as such  probabilistic planning based on a genetic algorithm is designed to resolve the minimum cost consensus models based on china s urban demolition negotiation  which can better simulate the consensus decision making process and obtain a satisfactory solution for the random optimization consensus models  the proposed models generalize both ben arieh s minimum cost consensus model and gong s consensus model with uncertain opinions  considering that the opinions of dms and the moderator obey various distributions  the models simulate the opinion characteristics more effectively  in the case analysis  a sensitivity analysis method is adopted to obtain the minimum budget  and probabilistic planning based on genetic algorithm to obtain a satisfactory solution that is closer to reality   c  2017 elsevier ltd  all rights reserved  
 mining competitors from large unstructured datasets in any competitive business  success is based on the ability to make an item more appealing to customers than the competition  a number of questions arise in the context of this task  how do we formalize and quantify the competitiveness between two items  who are the main competitors of a given item  what are the features of an item that most affect its competitiveness  despite the impact and relevance of this problem to many domains  only a limited amount of work has been devoted toward an effective solution  in this paper  we present a formal definition of the competitiveness between two items  based on the market segments that they can both cover  our evaluation of competitiveness utilizes customer reviews  an abundant source of information that is available in a wide range of domains  we present efficient methods for evaluating competitiveness in large review datasets and address the natural problem of finding the top k competitors of a given item  finally  we evaluate the quality of our results and the scalability of our approach using multiple datasets from different domains  
 mining corporate annual reports for intelligent detection of financial statement fraud   a comparative study of machine learning methods financial statement fraud has been serious concern for investors  audit firms  government regulators  and other capital market stakeholders  intelligent financial statement fraud detection systems have therefore been developed to support decision making of the stakeholders  fraudulent misrepresentation of financial statements in managerial comments has been noticed in recent studies  as such  the purpose of this study was to examine whether an improved financial fraud detection system could be developed by combining specific features derived from financial information and managerial comments in corporate annual reports  to develop this system  we employed both intelligent feature selection and classification using a wide range of machine learning methods  we found that ensemble methods outperformed the remaining methods in terms of true positive rate  fraudulent firms correctly classified as fraudulent   in contrast  bayesian belief networks  bbn  performed best on non fraudulent firms  true negative rate   this finding is important because interpretable  green flag  values  for which fraud is likely absent  could be derived  providing potential decision support to auditors during client selection or audit planning  we also observe that both financial statements and text in annual reports can be utilised to detect non fraudulent firms  however  non annual report data  analysts  forecasts of revenues and earnings  are necessary to detect fraudulent firms  this finding has important implications for selecting variables when developing early warning systems of financial statement fraud   c  2017 elsevier b v  all rights reserved  
 misnis  an intelligent platform for twitter topic mining twitter has become a major tool for spreading news  for dissemination of positions and ideas  and for the commenting and analysis of current world events  however  with more than 500 million tweets flowing per day  it is necessary to find efficient ways of collecting  storing  managing  mining and visualizing all this information  this is especially relevant if one considers that twitter has no ways of indexing tweet contents  and that the only available categorization  mechanism  is the  hashtag  which is totally dependent of a user s will to use it  this paper presents an intelligent platform and framework  named misnis   intelligent mining of public social networks  influence in society   that facilitates these issues and allows a non technical user to easily mine a given topic from a very large tweet s corpus and obtain relevant contents and indicators such as user influence or sentiment analysis  when compared to other existent similar platforms  misnis is an expert system that includes specifically developed intelligent techniques that   1  circumvent the twitter api restrictions that limit access to 1  of all flowing tweets  the platform has been able to collect more than 80  of all flowing portuguese language tweets in portugal when online   2  intelligently retrieve most tweets related to a given topic even when the tweets do not contain the topic  hashtag or user indicated keywords  a 40  increase in the number of retrieved relevant tweets has been reported in real world case studies  the platform is currently focused on portuguese language tweets posted in portugal  however  most developed technologies are language independent  e g  intelligent retrieval  sentiment analysis  etc    and technically misnis can be easily expanded to cover other languages and locations   c  2017 elsevier ltd  all rights reserved  
 mixedtrails  bayesian hypothesis comparison on heterogeneous sequential data sequential traces of user data are frequently observed online and offline  e g   as sequences of visited websites or as sequences of locations captured by gps  however  understanding factors explaining the production of sequence data is a challenging task  especially since the data generation is often not homogeneous  for example  navigation behavior might change in different phases of browsing a website or movement behavior may vary between groups of users  in this work  we tackle this task and propose mixedtrails   a bayesian approach for comparing the plausibility of hypotheses regarding the generative processes of heterogeneous sequence data  each hypothesis is derived from existing literature  theory  or intuition and represents a belief about transition probabilities between a set of states that can vary between groups of observed transitions  for example  when trying to understand human movement in a city and given some data  a hypothesis assuming tourists to be more likely to move towards points of interests than locals can be shown to be more plausible than a hypothesis assuming the opposite  our approach incorporates such hypotheses as bayesian priors in a generative mixed transition markov chain model  and compares their plausibility utilizing bayes factors  we discuss analytical and approximate inference methods for calculating the marginal likelihoods for bayes factors  give guidance on interpreting the results  and illustrate our approach with several experiments on synthetic and empirical data from wikipedia and flickr  thus  this work enables a novel kind of analysis for studying sequential data in many application areas  
 mobile three dimensional maps for wayfinding in large and complex buildings  empirical comparison of first person versus third person perspective the computational capabilities of today s smart phones make it possible to take advantage of mobile three dimensional  3 d  maps to support navigation in the physical world  in particular  3 d maps might be useful to facilitate indoor wayfinding in large and complex buildings  where the typical orientation cues  e g   street names  and location tracking technologies that can be used outdoors are unavailable  the use of mobile 3 d maps for indoor wayfinding is still largely unexplored and research on how to best design such tools has been scarce to date  one overlooked but important design decision for 3 d maps concerns the perspective from which the map content should be displayed  with first person and third person perspectives being the two major options  this paper presents a user study involving wayfinding tasks in a large and complex building  comparing amobile 3 dmap with first person perspective  a mobile 3 d map with third person perspective  and a traditional mobile 2 d map  the first person perspective shows the mobile 3 d map of the building from a floor level egocentric point of view  whereas the third person perspective shows the surroundings of the user from a fixed distance behind and above her position  results of the study reveal that the mobile 3 dmap with third person perspective leads to shorter orientation time before walking  better clarity ratings  lower workload  mental demand and effort scores  and higher preference score compared to the mobile 3 d map with first person perspective  moreover  it leads to shorter orientation time before walking  better pleasantness ratings  lower mental demand scores  and higher preference score compared to the mobile 2 d map  
 model for evaluating the green supply chain performance under low carbon agricultural economy environment with 2 tuple linguistic information today  with the development of economic globalization  the quality competition between enterprises has extended to the supply chain  if the enterprises want to get sustainable development  the traditional closed vertical integration  mode needs to be changed   horizontal integration  has become the inevitable choice of enterprises  with the development of performance management and supply chain technology  the combination of the two and focusing on the green supply chain quality management under low carbon agricultural economy environment have become the hot topics in both academic area and practical area  in this paper  we study on the multiple attribute decision making problems to estimate the green supply chain performance under low carbon agricultural economy environment with 2 tuple linguistic information  then  we propose the 2 tuple power einstein weighted geometric  2tpewg  operatorfor aggregating 2 tuple linguistic information  and then apply 2tpewg operator for evaluating the green supply chain performance under low carbon agricultural economy environment with 2 tuple linguistic information  in the end  we propose an example to test the effectiveness of our proposed method  
 model of bias driven trend followers and interaction with manipulators stock investors are not fully rational in trading and many behavioral biases that affect them  however  most of the literature on behavioral finance has put efforts only to explain empirical phenomena observed in financial markets  little attention has been paid to how individual investors  trading performance is affected by behavioral biases  as against the common perception that behavioral biases are always detrimental to investment performance  we conjecture that these biases can sometimes yield better trading outcomes  focusing on representativeness bias  conservatism and disposition erect we construct a mathematical model in which the representative trend investor follows a bayesian trading strategy based on an underlying markov chain  switching beliefs between trending and mean reversion  by this model  scenario analysis is undertaken to track investor behavior and performance under different patterns of market movements  simulation results show the erect of biases on investor performance can sometimes be positive  further  we investigate how manipulators could take advantage of investor biases to profit  the model s potential for manipulation detection is demonstrated by real data of well known manipulation cases  
 model based space planning for temporary structures using simulation based multi objective programming construction trades need to share temporary structures to increase the output of direct work while controlling the labor input of indirect work  the purpose of this research is to develop a framework to determine the optimal location of temporary structures in a computerized practical manner for piping construction projects  based on the spatial relationship between work envelope and scaffolding placement requirements  this paper presents the optimization model in two phases  the simulation based optimization model and a multi attribute utility  mau  based alternative selection model  a multi objective optimization model is established to improve scaffolding availability among multiple activities while maximizing piping crew productivity  the multi attribute utility model is employed to handle the uncertainty of the assessment weights on the attributes to illustrate the preference of decision makers among different scaffolding placement alternatives obtained from the first phase  the approach was validated in a piping module  which provided superintendents and space planners with an effective decision making tool among possible scaffolding alternatives in piping construction  the proposed optimization technique is an alternative methodology for solving the productivity tasks scaffolding trade off problem  which further revolutionizes the spatial coordination process of workspace management and temporary structure planning   c  2017 elsevier ltd  all rights reserved  
 modeling 4d human object interactions for joint event segmentation  recognition  and object localization in this paper  we present a 4d human object interaction  4dhoi  model for solving three vision tasks jointly  i  event segmentation from a video sequence  ii  event recognition and parsing  and iii  contextual object localization  the 4dhoi model represents the geometric  temporal  and semantic relations in daily events involving human object interactions  in 3d space  the interactions of human poses and contextual objects are modeled by semantic co occurrence and geometric compatibility  on the time axis  the interactions are represented as a sequence of atomic event transitions with coherent objects  the 4dhoi model is a hierarchical spatial temporal graph representation which can be used for inferring scene functionality and object affordance  the graph structures and parameters are learned using an ordered expectation maximization algorithm which mines the spatial temporal structures of events from rgb d video samples  given an input rgb d video  the inference is performed by a dynamic programming beam search algorithm which simultaneously carries out event segmentation  recognition  and object localization  we collected a large multiview rgb d event dataset which contains 3 815 video sequences and 383 036 rgb d frames captured by three rgb d cameras  the experimental results on three challenging datasets demonstrate the strength of the proposed method  
 modeling and analysis of group dynamics in alcohol consumption environments high risk drinking is considered a major concern in public health  being the third leading preventable cause of death in the united states  several studies have been conducted to understand the etiology of high risk drinking and to design prevention strategies to reduce unhealthy alcohol consumption and related problems  but there are still major gaps in identifying and investigating the key components that affect the consumption patterns during the drinking event  there is a need to develop tools for the design of methodologies to not only identify such dangerous patterns but also to determine how their dynamics impact the event  in this paper  based on current empirical evidence and observations of drinking events  we model a human group that is in an alcohol consumption scenario as a dynamical system whose behavior is driven by the interplay between the environment  the network of interactions between the individuals  and their personal motivations and characteristics  we show how this mathematical model complements empirical research in this area by allowing us to analyze  simulate  and predict the drinking group behaviors  to improve the methodologies for field data collection  and to design interventions  through simulations and lyapunov stability theory  we provide a computational and mathematical analysis of the impact of the model parameters on the predicted dynamics of the drinking group at the drinking event level  also  we show how the dynamical model can be informed using data collected in situ and to generate information that can complement the analysis  
 modeling and minimizing information distortion in information diffusion through a social network it is very common in real life that information distorts during the process of transmission in a social network  which may lead to people s incorrect comprehension of the information and further poor decision making  in this paper  we study how to model and minimize the distortion of information when it diffuses through a social network  we propose the concept of information authenticity to measure distortion as well as a mathematical model to characterize how information distorts during its diffusion through a social network  and study the optimization problem of maximizing the information authenticity of a social network  in order to solve the problem  we employ a framework of greedy algorithms that was proposed by ni et al   inf sci 180 13  2514 2527  2010   which can trade off between optimality and complexity  finally  we perform experiments to show the greedy algorithms can effectively solve the problem we propose  
 modeling contagion in policy systems scholars of the policy process offer compelling explanations for patterns in the aggregate level attention of policymakers  yet  we have little systematic understanding of the day to day behavior of these individuals  why does a given policymaker  on a given day  decide to focus on one pressing issue while ignoring many others  i approach this question from a cognitive systems perspective and argue that policymakers are highly interdependent actors who are subject to cognitive limits and have incentives to closely monitor the political environment  these tendencies contribute to the emergence of widespread herd behavior in their individual attention to policy issues  a phenomenon i conceptualize as  issue contagion   i then utilize the methods of computational social science to build an agentbased simulation model of policymakers  issue attention over time  i also outline three empirical expectations regarding the density of communication ties between actors  the presence of segmented groups  e g  political parties and coalitions   and the rate at which actors take cues from one another  through a series of sensitivity tests  i document the internal validity of the model and show that incremental changes in network density  segmentation  and cue taking all generate clear and visible trends in the frequency of issue contagion events   c  2017 elsevier b v  all rights reserved  
 modeling dual channel supply chain based on fuzzy system and genetic algorithm in this paper  we have proposed a dual channel supply chain model in uncertain environment to analyze the demand of manufacturer and retailer demand in which the profit being maximized  linguistic terms are also utilized to establish two fuzzy systems for estimating the demand in direct and retail channels  in order to do that  a mathematical model is proposed based on decentralized situation of supply chain  to solve the model  we have developed a hybrid solution method of genetic algorithm  fuzzy system  and l p metric  finally  several test problems are first generated  then  the computational results are analyzed  
 modeling dynamics of expressive body gestures in dyadic interactions body gestures are an important non verbal expression channel during affective communication  they convey human attitudes and emotions as they dynamically unfold during an interpersonal interaction  hence  it is highly desirable to understand the dynamics of body gestures associated with emotion expression in human interactions  we present a statistical framework for robustly modeling the dynamics of body gestures in dyadic interactions  our framework is based on high level semantic gesture patterns and consists of three components  first  we construct a universal background model  ubm  using gaussian mixture modeling  gmm  to represent subject independent gesture variability  next  we describe each gesture sequence as a concatenation of semantic gesture patterns which are derived from a parallel hmm structure  then  we probabilistically compare the segments of each gesture sequence extracted from the second step with the ubm obtained from the first step  in order to select highly probabilistic gesture patterns for the sequence  the dynamics of each gesture sequence are represented by a statistical variation profile computed from the selected patterns  and are further described in a well defined kernel space  this framework is compared with three baseline models and is evaluated in emotion recognition experiments  i e   recognizing the overall emotional state of a participant in a dyadic interaction from the gesture dynamics  the recognition performance demonstrates the superiority of the proposed framework over the baseline models  the analysis of the relationship between the emotion recognition performance and the number of the selected segments also indicates that a few local salient events  rather than the whole gesture sequence  are sufficiently informative to trigger the human summarization of their overall global emotion perception  
 modeling of fuzzy based voice of customer for business decision analytics identification  interpretation and response to customer requirements are the key success factors for companies  regardless of their industry  failing to satisfy customer requirements can damage a company s reputation and cause heavy losses  in this study  we have developed a new approach for properly interpreting and analyzing the fuzzy voice of the customer using association rule learning and text mining  this unique methodology converts textual and qualitative data into a common quantitative format which is then used to develop a mapped integrated customer satisfaction index  icsi   icsi is a framework for measuring customer satisfaction  previous measures of customer satisfaction ratio failed to incorporate the cost implications of resolving customer complaints issues and the fuzzy impact of those complaints issues on the system  in addition to including these important and unique factors in the present study  we have also introduced a dynamic critical to quality  ctq  concept  a novel method that provides a real time system to monitor the ctq list through an updated ctq library  finally  a procedure for customer feedback mining and sentiment analysis is proposed that handles typographical errors  which are unavoidable in every real database  the results of this study suggest that incorporating the fuzzy level of negativity and positivity of comments into the model instead of treating negative and positive comments as binary variables  leads to more reasonable outcomes  in addition  this study provides a more structured framework for understanding customer requirements   c  2017 elsevier b v  all rights reserved  
 modeling rational  psychological  and social behavior toward diffusion of new technology using agent based simulation  the case of the public utility jeepney  puj  fleet in metro manila in most developing countries  over aged vehicles play a significant role in energy demand and air pollution  which make the transportation sector a suitable choice for investigating opportunities to mitigate climate change  apparently  people heterogeneity  social influence  and network configuration affect diffusion of innovation  this study presents an agent based model  abm  to simulate the rational decision making  psychological behavior  and social interaction of people to explore their reaction to policy scenarios toward adopting technological changes over time  the aim of model is to assist policymakers for energy and environmental policy design based on consumers  behavior  the jeepney owners in the old public utility jeepney  puj  fleet in metro manila are chosen as case study to prove the applicability of the model  the results show that rational  psychological  and social interaction of owners could not lead to diffusion of technology without intervention of policy instruments  however  by implementing incentive based policies  the entire jeepney fleet could be renovated before the end of simulation horizon and the government could launch a 5 year plan to combat pollution of the fleet  the model could be applied to evaluate and prioritize strategies for reducing the future energy requirements and emissions in other fleets and regions  
 modeling stock price dynamics with fuzzy opinion networks we propose a mathematical model for the word of mouth communications among stock investors through social networks and explore howthe changes of the investors  social networks influence the stock price dynamics and vice versa  an investor is modeled as a gaussian fuzzy set  a fuzzy opinion  with the center and standard deviation as inputs and the fuzzy set itself as output  investors are connected in the following fashion  the center input of an investor is taken as the average of the neighbors  outputs  where two investors are neighbors if their fuzzy opinions are close enough to each other  and the standard deviation  uncertainty  input is taken with local  global  or external reference schemes to model different scenarios of how investors define uncertainties  the centers and standard deviations of the fuzzy opinions are the expected prices and their uncertainties  respectively  that are used as inputs to the price dynamic equation  we prove that with the local reference scheme the investors converge to different groups in finite time  while with the global or external reference schemes all investors converge to a consensus within finite time and the consensus may change with time in the external reference case  we show how to model trend followers  contrarians  and manipulators within this mathematical framework and prove that the biggest enemy of a manipulator is the other manipulators  we perform monte carlo simulations to show how the model parameters influence the price dynamics  and we apply a modified version of the model to the daily closing prices of 15 top banking and real estate stocks in hong kong for the recent two years from december 5  2013 to december 4  2015 and discover that a sharp increase of the combined uncertainty is a reliable signal to predict the reversal of the current price trend  
 modeling the instinctive emotional thoughtful mind two process notions of mental function  based on the dichotomy between intuition  or emotion  and reason  or deliberation   have been popular both in the scientific community and the general educated public  yet the findings and insights of pioneering neuroscientists such as paul maclean  walle nauta  karl pribram  antonio damasio  and luiz pessoa argue against such a simple dichotomy  rather  results from the last half century yield a picture that is closer to a triune parcellation of the mind into instincts  emotions  and thoughts  with extensive overlap as well as connections among brain regions involved in all three  emotional processes are not always automatic and not in general opposition to reason  this article reviews the evidence from neuroscience and then discusses some computational models of interplay between instinctive  emotional  and deliberative brain processes in human decision making  the article concludes with comments on research and societal implications of this triune view of the mind   c  2017 elsevier b v  all rights reserved  
 modeling user interests from web browsing activities browsing sessions are rich in elements useful to build profiles of user interests  but at the same time html pages include noisy data such as advertisements  navigation menus and privacy notes  moreover  some pages cover several different topics making it difficult to identify the most relevant to the user  for these reasons  they are often ignored by personalized search and recommender systems  we propose a novel approach for recognizing valuable text descriptions of current user information needs namely cues based on the data mined from browsing interactions over the web  the approach combines page clustering techniques based on document object model based representations for acquiring evidence about relevant correlations between text contents  this evidence is exploited for better filtering out irrelevant information and facilitating the construction of interest profiles  a comparative framework proves the accuracy of the extracted cues in the personalize search task  where results are re ranked according to the last browsed resources  
 modelling a combined method based on anfis and neural network improved by de algorithm  a case study for short term electricity demand forecasting electricity demand forecasting  as a vital tool in the electricity market  plays a critical role in power utilities  which can not only reduce production costs but also save energy resources  thus making the forecasting techniques become an indispensable part of the energy system  a novel combined forecasting method based on back propagation  bp  neural network  adaptive network based fuzzy inference system  anfis  and difference seasonal autoregressive integrated moving average  diff sarima  are presented in this paper  firstly  the combined method uses all the three methods  bp  anfis  diff sarima  to forecast respectively  and the three forecasting results were obtained  by multiplying optimal weight coefficients of the three forecasting results respectively and then adding them up  in the end the final forecasting results can be obtained  among the three individual methods  bp and anfis had the ability to deal with the nonlinearity data  and diff sarima had the ability to deal with the linearity and seasonality data  so the combined method eliminates drawbacks and incorporates in the merits of the individual methods  it has the capability to deal with the linearity  nonlinearity and seasonality data  in order to optimize weight coefficients  differential evolution  de  optimization algorithm is brought into the combined method  to prove the superiority and accuracy  the capability of the combined method is verified by comparing it with the three individual methods  the forecasting results of the combined method proved to be better than all the three individual methods and the combined method was able to reduce errors and improve the accuracy between the actual values and forecasted values effectively  using the half hour electricity power data of the state of new south wales in australia  relevant experimental case studies showed that the proposed combined method performed better than the other three individual methods and had a higher accuracy   c  2016 published by elsevier b v  
 modelling diffusion for multi generational product planning strategies using bi level optimization when introducing  new products  companies seek maximal profits by determining an optimal production plan and marketing strategy  however  if they have limited production capacity  there is a conflict between sales and production due to the differences in their respective goals  when two generations of the same product co exist in the market  consumer behavior can have a significant influence on product diffusion  in this paper consumers are divided into four categories based on supply constraints and production strategies are classified into two distinct types  after modelling the demand diffusion and sorting the customer relationships into the different production strategies  two bi level models for two generation product diffusion are proposed both of which take profit as the upper level goal and stable production as the lower level goal  an interactive dynamic programming based genetic algorithm  idp ga  is then designed to solve the bi level models  the results found that scientific and practical production planning methods based on new product diffusion under supply constraints can achieve suitable results for both levels   c  2017 elsevier b v  all rights reserved  
 modelling propagation of public opinions on microblogging big data using sentiment analysis and compartmental models compartmental models have been used to model information diffusion on social media  however  there have been few studies on modelling positive and negative public opinions using compartmental models  this study aimed for using sentiment analysis and compartmental model to model the propagation of positive and negative opinions on microblogging big media  the authors studied the news propagation of seven popular social topics on china s sina weibo microblogging platform  natural language processing and sentiment analysis were used to identify public opinions from microblogging big data  then two existing  siz and seiz  models and a newly developed  se2iz  model were implemented to model the news propagation and evaluate the trends of public opinions on selected social topics  simulation study was used to check model fitting performance  the results show that the new se2iz model has a better model fitting performance than existing models  this study sheds some new light on using social media for public opinion estimation and prediction  
 modelling tonal attraction  tonal hierarchies  interval cycles  and quantum probabilities how well does a given pitch fit into a tonal scale or tonal key  let it be a major or minor key  a similar question can be asked regarding chords and tonal regions  structural and probabilistic approaches in computational music theory have tried to give systematic answers to the problem of tonal attraction  we will discuss two previous models of tonal attraction  one based on tonal hierarchies and the other based on interval cycles  to overcome the shortcomings of these models  both methodologically and empirically  i propose a new kind of models relying on insights of the new research field of quantum cognition  i will argue that the quantum approach integrates the insights from both group theory and quantum probability theory  in this way  it achieves a deeper understanding of the cognitive nature of tonal music  especially concerning the nature of musical expectations  leonhard meyer  and a better understanding of the affective meaning of music  
 models for evaluating the technological innovation capability of small and micro enterprises with hesitant fuzzy information the enhancement of small and micro enterprises  technological innovation capability not only helps enterprise increase its core competence to adapt to the changeful environment  but also helps our country to increase its national competence  in order to enhance small and micro enterprises  technological innovation capability  enterprises should build a well technological innovation system  increase the technology innovation input and be active in technological innovation activities  and at the same time  it is essential for the government to play a role of leading  make effective policies and build good environment  therefore  it is necessary to collect and study on small and micro enterprises  technological innovation capability related data  cases and policies in order to know the current situation and trend of small and micro enterprises  technological innovation capability  in order to make the above mentioned work more convenient and more efficient  in this paper  we investigate the multiple attribute decision making for evaluating the technological innovation capability of small and micro enterprises with hesitant fuzzy information  motivated by the ideal of dependent aggregation  we develop the dependent hesitant fuzzy hamacher weighted geometric  dhfhwg  operator  in which the associated weights only depend on the aggregated hesitant fuzzy arguments and can relieve the influence of unfair hesitant fuzzy arguments on the aggregated results by assigning low weights to those  false  and  biased  ones and then apply them to develop an approach for multiple attribute decision making with hesitant fuzzy information  finally  an illustrative example for evaluating the technological innovation capability of small and micro enterprises is given to verify the developed approach  
 models of mathematical programming for intuitionistic multiplicative preference relations inorder to capture model uncertainty associated with imprecision or vagueness  a decision maker may express her his judgments in terms of intuitionistic multiplicative preference relation  impr   two important research topics with this regard are studied in the paper  1  checking consistency of impr and 2  generating weights on the basis of this relation  a new definition of consistent impr is proposed  in light of this new definition  the properties of consistent impr are studied in detail  a transformation formula is proposed to construct a consistent impr from a given normalized intuitionistic fuzzy weight vector  by minimizing the differences between the constructed consistent impr and the given impr  some fractional programming models are developed to derive the intuitionistic fuzzy weight vector  moreover  the models are also extended to group decision making  finally  two numerical examples are provided to illustrate the effectiveness and practical relevance of the proposed models  
 modernising historical slovene words we propose a language independent word normalisation method and exemplify it on modernising historical slovene words  our method relies on character level statistical machine translation  csmt  and uses only shallow knowledge  we present relevant data on historical slovene  consisting of two  partially  manually annotated corpora and the lexicons derived from these corpora  containing historical word modern word pairs  the two lexicons are disjoint  with one serving as the training set containing 40 000 entries  and the other as a test set with 20 000 entries  the data spans the years 1750 1900  and the lexicons are split into fifty year slices  with all the experiments carried out separately on the three time periods  we perform two sets of experiments  in the first one   a supervised setting   we build a csmt system using the lexicon of word pairs as training data  in the second one   an unsupervised setting   we simulate a scenario in which word pairs are not available  we propose a two step method where we first extract a noisy list of word pairs by matching historical words with cognate modern words  and then train a csmt system on these pairs  in both sets of experiments  we also optionally make use of a lexicon of modern words to filter the modernisation hypotheses  while we show that both methods produce significantly better results than the baselines  their accuracy and which method works best strongly correlates with the age of the texts  meaning that the choice of the best method will depend on the properties of the historical language which is to be modernised  as an extrinsic evaluation  we also compare the quality of part of speech tagging and lemmatisation directly on historical text and on its modernised words  we show that  depending on the age of the text  annotation on modernised words also produces significantly better results than annotation on the original text  
 modification of the best worst and mabac methods  a novel approach based on interval valued fuzzy rough numbers this paper presents a new approach for the treatment of uncertainty which is based on interval valued fuzzy rough numbers  ivfrn   it is shown that by integrating the rough approach with the traditional fuzzy approach  the subjectivity that exists when defining the borders of fuzzy sets is eliminated  ivfrn make decision making possible using only the internal knowledge in the operative data available to the decision makers  in this way objective uncertainties are used and there is no need to rely on models of assumptions  instead of different external parameters in the application of ivfrn  the structure of the given data is used  on this basis an original multi criteria model was developed based on an ivfrn approach  in this multi criteria model the traditional steps of the bwm  best worst method  and mabac  multi attributive border approximation area comparison  methods are modified  the model was tested and validated on a study of the optimal selection of fire fighting helicopters  testing demonstrated that the model based on ivfrn enabled more objective expert evaluation of the criteria in comparison with traditional fuzzy and rough approaches  a sensitivity analysis of the ivfrn bwm mabac model was carried out by means of 57 scenarios  the results of which showed a high degree of stability  the results of the ivfrn model were validated by comparing them with the results of the fuzzy and rough extension of the mabac  copras and vikor models   c  2017 elsevier ltd  all rights reserved  
 modified emg based handgrip force prediction using extreme learning machine various myoelectric prostheses controlled by electromyography  emg  signals have been developed  however  there have been few studies that provide fast and accurate methods to predict handgrip force from emg signals  rapid and precise handgrip force prediction is required  especially for the real time control system of myoelectric prostheses  in this study  extreme learning machine  elm  is applied to predict handgrip force from surface emg signals of forearm muscles  furthermore  elm is compared with support vector machine  svm  and multiple nonlinear regression  mnlr   the below 10   of the surface emg and handgrip force signals were cut away  and then the root mean square feature extracted from the modified surface emg signals was taken as input vector for these three kinds of predicting mechanisms  for the testing dataset  elm achieved a slightly larger root mean squared error than svm did and a smaller one than mnlr did  meanwhile  all three methods showed high correlation coefficients  for the total processing time  elm and mnlr consumed much less time than svm did  experimental results demonstrate that elm possesses a relatively good accuracy and little consumed time  although svm is effective for handgrip force estimation in terms of accuracy  overall  elm has a promising potential for predicting handgrip force rapidly and precisely  
 modified genetic algorithm for simple straight and u shaped assembly line balancing with fuzzy processing times this paper aims at the straight and u shaped assembly line balancing  due to the uncertainty  variability and imprecision in actual production systems  the processing time of tasks are presented in triangular fuzzy numbers  in this case  it is intended to optimize the efficiency and idleness percentage of the assembly line as well as and concurrently with minimizing the number of workstations  to solve the problem  a modified genetic algorithm is proposed  one fifth success rule in selection operator to improve the genetic algorithm performance  this leads genetic algorithm being controlled in convergence and diversity simultaneously by the means of controlling the selective pressure  also a fuzzy controller in selective pressure employed for one fifth success rule better implementation in genetic algorithm  in addition  taguchi design of experiments used for parameter control and calibration  finally  numerical examples are presented to compare the performance of proposed method with existing ones  results show the high performance of the proposed algorithm  
 modularized design oriented systematic inventive thinking approach supporting collaborative service innovations the rapid evolution of new service systems raises crucial challenges for service design and requires effective methods  this study depicts a conceptual service design framework  called design oriented systematic inventive thinking  dsit  approach  which can be applied in different problem contexts  dsit is presented as a new systematic and collaborative intelligence approach for creating and evaluating complex service systems using multi criteria data analytics  dsit synthesizes the current field of triz service design knowledge system and the emerging area of non triz service design knowledge system  dsit enables integrated development of service offerings at four dimensions and provides the matching integrated service design approach for each dimension  four types of service design approaches are conceptualized as  human independent service engineering    problem clarified service engineering    solution converged service engineering   and  designing for service   a new service computer aided design system  service cad  named dsit explorer is developed consisting of customization  compatibility  and extensiveness of dsit modules  a pervasive and smart collaborative service system  i e   the smart mos burger service solution  designed using dsit explorer is illustrated  dsit is a holistic  interdisciplinary  and collaborative service design concept  which is incorporated into a collaborative and intelligent service cad framework to enable systematic inventive thinking throughout phases of service design lifecycle from problem definition  problem resolution  to solution evaluation   c  2016 elsevier ltd  all rights reserved  
 monitoring mechanisms in new product development with risk averse project manager it is necessary for one senior executive  she  to monitor her project manager  he  who conducts early research stage followed by a later development stage in new product development  in this paper  we analyze two monitoring mechanisms   1  the idea information based monitoring  im  mechanism wherein the senior executive engages one supervisor to monitor the project manager s idea information   2  the effort based monitoring  em  mechanism wherein the senior executive engages another supervisor to monitor the project manager s effort  within the framework of uncertainty theory  we first present two classes of bilevel uncertain principal agent monitoring models  and then derive their respective optimal incentive contracts  we find that the senior executive should set the incentive term as high as possible to motivate each supervisor to monitor the project manager s idea information and effort no matter how much the design idea value is  we also find that em mechanism can always dominate im mechanism when the monitoring costs are equal  moreover  comparing with a no monitoring scenario  we identify two values of monitoring  the value of monitoring idea information and the value of monitoring effort  our results show that adopting im and em mechanisms can improve the senior executive s profits obtained in the no monitoring scenario when the revenue uncertainty is sufficiently low  the results also indicate that the value of monitoring idea information decreases as the risk aversion level of the project manager improves  while the value of monitoring effort shows the opposite feature  
 monometrics and their role in the rationalisation of ranking rules the aggregation of rankings is a long standing problem that consists of  given a profile of rankings  obtaining the single ranking that best represents the nature of this given profile  under the name of metric rationalisation of ranking rules  it has been proven that most ranking rules can be characterized as minimizing the distance to a consensus state for some appropriate distance function  in this paper  we propose to consider monometrics instead of distance functions  although these concepts are closely related  monometrics better capture the nature of the problem  as the purpose of a monometric is to preserve a given betweenness relation  this is obviously only meaningful when an interesting betweenness relation is fixed  for instance  the one based on reversals in rankings proposed by kemeny  in this way  ranking rules can be characterized in terms of a consensus state and a monometric   c  2016 elsevier b v  all rights reserved  
 monotonicity based consensus states for the monometric rationalisation of ranking rules and how they are affected by ties aggregating the preferences of several voters on a set of candidates is a classical problem in several fields of application  in previous work  we have addressed this problem in the case where each voter expresses his her preferences in the form of a ranking on the set of candidates  by searching for monotonicity of three different types of representation of votes  the scorix  the votrix and the votex  however  this search was addressed differently for each of the three representations of votes  in this paper  we propose a kemeny like type of search that will equally deal with the three aforementioned representations of votes  moreover  in case some voters consider that two or more candidates are equally suitable  the proposed method needs to be adapted  in this setting  the decision is no longer between  candidate a is better than candidate b  and  candidate b is better than candidate a   but  candidates a and b are equally suitable  also needs to be considered  here  the scorix  the votrix and the votex are extended in order to deal with this new three way setting   c  2017 elsevier inc  all rights reserved  
 motor skill learning in an insect inspired neuro computational control system in nature  insects show impressive adaptation and learning capabilities  the proposed computational model takes inspiration from specific structures of the insect brain  after proposing key hypotheses on the direct involvement of the mushroom bodies  mbs  and on their neural organization  we developed a new architecture for motor learning to be applied in insect like walking robots  the proposed model is a nonlinear control system based on spiking neurons  mbs are modeled as a nonlinear recurrent spiking neural network  snn  with novel characteristics  able to memorize time evolutions of key parameters of the neural motor controller  so that existing motor primitives can be improved  the adopted control scheme enables the structure to efficiently cope with goal oriented behavioral motor tasks  here  a six legged structure  showing a steady state exponentially stable locomotion pattern  is exposed to the need of learning new motor skills  moving through the environment  the structure is able to modulate motor commands and implements an obstacle climbing procedure  experimental results on a simulated hexapod robot are reported  they are obtained in a dynamic simulation environment and the robot mimicks the structures of drosophila melanogaster  
 movie aspects  tweet metrics  and movie revenues  the influence of ios vs  android microblogging word of mouth  mwom  using twitter has been found to impact the success of experiential products such as movies  however  the influence of the type of device or platform used for tweeting  ios or android  on the relationship between well established tweet metrics   valence  volume  and time period of tweeting   and movie performance is not yet known  furthermore  it is not known if users of these platforms differ in the aspects of movies they discuss and how that may influence tweet metrics  in this study  we investigated these gaps by analyzing more than four million tweets for 29 movies from both ios and android users and conducted a robustness check on another 8 movies  results from mixed model estimations show that valence of tweets on android before a movie s release and volume of tweets on ios after the release significantly influence the revenues of a movie  results also show that mentions of director and script are more important in the case of android users whereas mentions of production and music are more important in the case of ios users  finally  results show that it may be more productive for movie studios and advertisers to reach the more prolific twitter users on android but relatively newer twitter users on ios  these findings have significant implications for movie studios as well as mobile advertisers to target their promotions to these platform users accordingly   c  2017 elsevier b v  all rights reserved  
 moving in time and space   location intelligence for carsharing decision support in this paper we develop a spatial decision support system that assists free floating carsharing providers in countering imbalances between vehicle supply and customer demand in existing business areas and reduces the risk of imbalance when expanding the carsharing business to a new city  for this purpose  we analyze rental data of a major carsharing provider in the city of amsterdam in combination with points of interest  pois   the spatio temporal demand variations are used to develop pricing zones for existing business areas  we then apply the influence of pois derived from carsharing usage in amsterdam in order to predict carsharing demand in the city of berlin  the results indicate that predicted and actual usage patterns are very similar  hence  our approach can be used to define new business areas when expanding to new cities to include high demand areas and exclude low demand areas  thereby reducing the risk of supply demand imbalance   c  2017 elsevier b v  all rights reserved  
 multi criteria assessment of partnership components partnership with other organizations could improve companies  performance  however  partnerships have a high failure rate according to the literature  therefore  monitoring the performance of a partnership and evaluating the components that affect its performance are essential  joint decision making  information sharing  risk reward sharing  relationship specific assets  trust and commitment are identified as the major components that affect the performance of an ongoing partnership  however  no previous study evaluated the components of an ongoing partnership over time  in this study  a multi criteria decision support model is proposed to assess the components that influence the performance of an ongoing partnership  multiple indicators are used to assess each component  the interdependency and importance of the components and their indicators are incorporated into the model using the warshall s algorithm and analytic network process  anp   respectively  the importance of each component and indicator  and a single number for the overall level of partnership components in each period  named as partnership component index  pci  here  are the outputs of the proposed model  pci is a quantitative multi dimensional index  a partnership between a forest company and a sawmill in british columbia  canada is used as a case study to test the model  the components of the partnership is assessed in three different periods using pci  the results are validated by the managers and sensitivity analysis is also performed   c  2016 elsevier ltd  all rights reserved  
 multi criteria assignment policies to improve global effectiveness of medico social service sector in this paper  we propose a multi criteria approach in order to reduce the waiting time on the assignment of users to medico social institutions  the main goal is to ascertain whether alternative assignment policies can improve global response to users  demands  and to assess the performances of each alternative  scenario  compared to the current practice  we propose a mathematical model of user assignment to medico social structures and professionals  under constraints of resource capacity  professionals  skills and user requirements  formulated in an individual support project   with alternative assignment policies  users  needs could be covered partially  they also authorize the involvement of several structures and several professionals in the response to the request  a mixed integer linear programming solver is used to solve this assignment problem  following a lexicographic approach in case of partial coverage policy  for some assignment policies that have long calculation times  we propose a simulated annealing approach in order to speed up the resolution  in order to validate simulated annealing approach  we compared it with a greedy randomized adaptive search procedure  the problem is solved for instances where we have real size data for a set of institutions for disabled children and teenagers  we present and compare computational results over several scenarios  highlighting the improvement provided by each alternative assignment policy  
 multi criteria decision making using interval valued hesitant fuzzy qualiflex methods based on a likelihood based comparison approach qualiflex is a very efficient outranking method to handle multi criteria decision making  mcdm  involving cardinal and ordinal preference information  based on a likelihood based comparison approach  this paper develops two interval valued hesitant fuzzy qualiflex outranking methods to handle mcdm problems within the interval valued hesitant fuzzy context  first  we define the likelihoods of interval valued hesitant fuzzy preference relations that compare two interval valued hesitant fuzzy elements  ivhfes   then  we propose the concepts of the concordance discordance index  the weighted concordance discordance index and the comprehensive concordance discordance index  moreover  an interval valued hesitant fuzzy qualiflex model is developed to solve mcdm problems where the evaluative ratings of the alternatives and the weights of the criteria take the form of ivhfes  additionally  this paper propounds another likelihood based interval valued hesitant fuzzy qualiflex method to accommodate the ivhfes  evaluative ratings of alternatives and non fuzzy criterion weights with incomplete information  finally  a numerical example concerning the selection of green suppliers is provided to demonstrate the practicability of the proposed methods  and a comparison analysis is given to illustrate the advantages of the proposed methods  
 multi criteria handwriting quality analysis with online fuzzy models intuiscript is an innovative project aiming at the development of a digital workbook providing feedback during the handwriting learning process for children from three to seven years old  in this context  the paper presents a method to analyse handwriting quality that responds to the expectations of the intuiscript educational scenario  on line and real time feedback for children  an automatic detection of children mistakes guiding the pedagogical progression  and a precise analysis of children writing saved to help teacher to understand children writing skills  the presented method introduces a multi criteria architecture to analyse handwriting quality based on three different aspects  shape  order and direction  the validation of the proposed approach is done on a realistic dataset collected in preschools and primary schools with 952 children  results show a positive feedback of children and teachers about the use of tactile digital devices  and a significant improvement of the performances of the multi criteria architecture compared to the previous analyser  the ground truth has been annotated by experts with different levels of confidence  specific evaluation metrics are introduced to deal with confidence annotations   c  2017 elsevier ltd  all rights reserved  
 multi dimension reviewer credibility quantification across diverse travel communities the rapid development of social media technologies enables travellers to share travel experiences and opinions online by posting reviews  which then serve as information source for other travellers  however  the explosive growth of reviews and the proliferation of uninformative  biased or even false information make it very challenging for travellers to find credible information  to help travellers seek credible information  most current work apply mainly qualitative approaches to investigate the credibility of reviews or reviewers  this paper adopts an impact index to quantify the credibility of reviewers by simultaneously evaluating the expertise and trustworthiness of reviewers based on the number of reviews posted by them and the number of helpful votes received by those reviews  furthermore  the impact index is enhanced into the exposure impact index by considering in addition reviewers  breadth of expertise in the form of the number of destinations on which reviewers posted reviews  to examine the effectiveness and applicability of impact index and exposure impact index  this paper evaluates them on several data sets collected from two rather different online travel communities  tripadvisor  the world s largest travel community  and qunar  one of the most popular travel communities in china  experimental results show that both impact index and exposure impact index lead to more consistent results with human judgments than the state of the art method in measuring the credibility of reviewers from diverse communities  manifesting their effectiveness and applicability  
 multi document summarization based on sentence cluster using non negative matrix factorization multi document summarization aims to produce a concise summary that contains salient information from a set of source documents  many approaches use statistics and machine learning techniques to extract sentences from documents  in this paper  we propose a new multi document summarization framework based on sentence cluster using nonnegative matrix tri factorization  nmtf   the proposed framework employs nmtf to cluster sentences using inter type relationships among documents  sentences and terms  and incorporate the intra type information through manifold regularization  the most informative sentences are selected from each sentence cluster to form the summary  when evaluated on the duc2004 and tac2008 datasets  the performance of the proposed framework is comparable with that of the top three systems  
 multi expert performance evaluation of healthcare institutions using an integrated intuitionistic fuzzy ahp dea methodology healthcare management and healthcare industry have been one of the popular and complex topics that many researchers and professionals have focused on  this paper proposes a new multi expert fuzzy approach integrating intuitionistic fuzzy data envelopment analysis  dea  and intuitionistic fuzzy analytic hierarchy process  if ahp  for solving the performance evaluation problem of healthcare institutions  in this paper  intuitionistic fuzzy sets  ifs  have been preferred since they simultaneously provide information on the membership  non membership  and hesitancy functions  a real life problem is demonstrated to validate the proposed methodology  a total number of 16 hospitals operating in istanbul have been analyzed based on a broad set of inputs and outputs  then  a comparison with crisp dea has been performed   c  2017 elsevier b v  all rights reserved  
 multi faceted assessment of trademark similarity trademarks are intellectual property assets with potentially high reputational value  their infringement may lead to lost revenue  lower profits and damages to brand reputation  a test normally conducted to check whether a trademark is highly likely to infringe other existing  already registered  trademarks is called a likelihood of confusion test  one of the most influential factors in this test is establishing similarity in appearance  meaning or sound  however  even though the trademark registration process suggests a multi faceted similarity assessment  relevant research in expert systems mainly focuses on computing individual aspects of similarity between trademarks  therefore  this paper contributes to the knowledge in this field by proposing a method  which  similar to the way people perceive trademarks  blends together the three fundamental aspects of trademark similarity and produces an aggregated score based on the individual visual  semantic and phonetic assessments  in particular  semantic similarity is a new aspect  which has not been considered by other researchers in approaches aimed at providing decision support in trademark similarity assessment  another specific scientific contribution of this paper is the innovative integration  using a fuzzy engine  of three independent assessments  which collectively provide a more balanced and human centered view on potential infringement problems  in addition  the paper introduces the concept of degree of similarity since the line between similar and dissimilar trademarks is not always easy to define especially when dealing with blending three very different assessments  the work described in the paper is evaluated using a database comprising 1400 trademarks compiled from a collection of real legal cases of trademark disputes  the evaluation involved two experiments  the first experiment employed information retrieval measures to test the classification accuracy of the proposed method while the second used human collective opinion to examine correlations between the trademark scoring rating and the ranking of the proposed method  and human judgment  in the first experiment  the proposed method improved the f score  precision and accuracy of classification by 12 5   35  and 8 3   respectively  against the best score computed using individual similarity  in the second experiment  the proposed method produced a perfect positive spearman rank correlation score of 1 00 in the ranking task and a pairwise pearson correlation score of 0 92 in the rating task  the test of significance conducted on both scores rejected the null hypotheses of the experiment and showed that both scores correlated well with collective human judgment  the combined overall assessment could add value to existing support systems and be beneficial for both trademark examiners and trademark applicants  the method could be further used in addressing recent cyberspace phenomena related to trademark infringement such as customer hijacking and cybersquatting   c  2016 elsevier ltd  all rights reserved  
 multi factors based sentence ordering for cross document fusion from multimodal content organizing a coherent structure of the sentences extracted from multiple documents  guarantees the fluency and readability of the fused document  in this paper  sentence ordering problem is treated as a combinatorial optimization problem and solved with continuous hopfield neural network  chnn   we unify the existing factors by considering the most frequent orders temporal information  and topical relevance between local themes during overall ordering process  specifically  ordering algorithm traverses all the local themes and locates a shortest path as the final sentence ordering  we show the results with data from document understanding conferences  duc  2002 2005  and demonstrate the effectiveness of the developed approach compared with random ordering  ro   chronological ordering  co   majority ordering  mo   and precedence relation ordering  pro    c  2017 elsevier b v  all rights reserved  
 multi layer architecture for adaptive fuzzy inference system with a large number of input features the sugeno adaptive fuzzy neural network using training data is a good approximation to model different systems  the large number of adaptive neuro fuzzy inference system  anfis  input features is a major challenge in using anfis and is not applicable with increased parameters  we present a solution for many input features solving modular problems  we created a multi layer architecture of sub anfis  mla anfis  for this purpose  different topologies were created with various combinations of multiple input features  and an error indicator was calculated for each combination of topologies  finally  the best topology was chosen among the states with the highest possible performance  we implemented a multi layered approach based on 365 day concrete compressive strength data with eight input features and the optimized mla anfis topology  5 3 1  for this purpose from different anfis topologies and neural networks  finally  the results from five other datasets prove the impact of the proposed mla anfis approach compared to the neural network method   c  2016 elsevier b v  all rights reserved  
 multi modal multiple kernel learning for accurate identification of tourette syndrome children tourette syndrome  ts  is a childhood onset neurobehavioral disorder characterized by the presence of multiple motor and vocal tics  to date  ts diagnosis remains somewhat limited and studies using advanced diagnostic methods are of great importance  in this paper  we introduce an automatic classification framework for accurate identification of ts children based on multi modal and multi type features  which is robust and easy to implement  we present in detail the feature extraction  feature selection  and classifier training methods  in addition  in order to exploit complementary information revealed by different feature modalities  we integrate multi modal image features using multiple kernel learning  mkl   the performance of our framework has been validated in classifying 44 ts children and 48 age and gender matched healthy children  when combining features using mkl  the classification accuracy reached 94 24  using nested cross validation  most discriminative brain regions were mostly located in the cortico basal ganglia  frontal cortico cortical circuits  which are thought to be highly related to ts pathology  these results show that our method is reliable for early ts diagnosis  and promising for prognosis and treatment outcome  
 multi objective colonial competitive algorithm for hybrid flowshop problem this study analyses the multi objective optimization in hybrid flowshop problem  in which two conflicting objectives  makespan and total weighted tardiness  are considered to be minimized simultaneously  the multi objective version of colonial competitive algorithm  cca  for real world optimization problem is introduced and investigated  in contrast to multi objective problems solved by cca  presented in the literature  which used the combination of the objectives as single objective  the proposed algorithm is established on pareto solutions concepts  another novelty of this paper is estimating the power of each imperialist by a probabilistic criterion for this multi objective algorithm  besides that  the variable neighborhood search is implemented as an assimilation strategy  performance of the algorithm is finally compared with a famous algorithm for scheduling problem  nsga ii  and the multi objective form of cca  28    c  2016 elsevier b v  all rights reserved  
 multi objective dynamic economic emission dispatch using particle swarm optimisation variants particle swarm optimisation  pso  is a bio inspired swarm based approach to solving optimisation problems  the algorithm functions as a result of particles traversing and evaluating the problem space  eventually converging on the optimum solution  this paper applies a number of pso variants to the dynamic economic emission dispatch  deed  problem  the deed problem is a multi objective optimisation problem in which the goal is to optimise two conflicting objectives  cost and emissions  the pso variants tested include  the standard pso  spso   the pso with avoidance of worst locations  pso awl   and also a selection of different topologies including the pso with a gradually increasing directed neighbourhood  pso gidn   the aim of the paper is to test the performance of different variants of the pso awl against variants of the spso on the deed problem  the results show that the pso awl outperforms the spso for every topology implemented  the results are also compared to state of the art genetic algorithm  nsga ii  and multi agent eeinforcement learning  marl   this paper then examines the performance of each pso algorithm when the power demand is modified to form a triangle wave  the purpose of this experiment was to analyse the performance of different pso variants on an increasingly constrained problem   c  2017 elsevier b v  all rights reserved  
 multi objective genetic algorithm with variable neighbourhood search for the electoral redistricting problem in a political redistricting problem  the aim is to partition a territory into electoral districts or clusters  subject to some constraints  the most common of these constraints include contiguity  population equality  and compactness  we propose an algorithm to address this problem based on multi objective optimization  the hybrid algorithm we propose combines the use of the well known pareto based nsga ii technique with a novel variable neighbourhood search strategy  a new ad hoc initialization method is also proposed  finally  new specific genetic operators that ensure the compliance of the contiguity constraint are introduced  the experimental results we present  which are performed considering five us states  clearly show the appropriateness of the proposed hybrid algorithm for the redistricting problem  we give evidence of the fact that our method produces better and more reliable solutions with respect to those returned by the state of the art methods  
 multi period efficiency and productivity changes in global automobile  a vrs vrm and sml productivity index approach this study tracked the static efficiency and dynamic productivity changes of global automakers from 2005 to 2012 using the variant of radial measure  vrm  model and sequential malmquist luenberger  sml  productivity index  this study attempted to highlight the operations management aspects of the automakers  overall competitive strategies using multi firm longitudinal data by looking at groups of automakers from different geographic regions  the analysis results provided general insights on the dynamic capabilities of global automotive industry as well as on the efficiency and productivity of individual automakers  the implications of this study are as follows  first  the annual efficiency of global automakers is sensitive to the internal capacity utilization and external macro  or microeconomic environment surrounding individual automakers  in particular  european automakers tended to show low efficiency caused by overcapacity and low capacity utilization  indicating a need for improving efficiency by adjusting production capacity  second  this study explores that dynamic capabilities can be used successfully for improving global automaker s efficiency  therefore  automakers need their specific and unique dynamic capabilities harmonizing internal competencies with external changes to achieving competitive advantage   c  2017 elsevier ltd  all rights reserved  
 multi period optimization with loss averse customer behavior  joint pricing and inventory decisions with stochastic demand to maximize a firm s profit over a finite planning horizon  we develop a dynamic optimization model by considering loss aversion when making pricing and inventory decisions  we estimate customer demand through a choice model  which incorporates reference price  utility function and customer loss aversion  our model forms the core of the expert system for decision support  through a sequence of bellman equations  we find that the firm s profit is a concave function of price and inventory  and we solve the model optimally  the profit is positively correlated with the reference price  and the price and inventory decisions are non monotonic functions of loss aversion intensity  our results shed new light on pricing and inventory management with customer behavior in a multi period system  through various theorem developments  we are able to identify the optimal inventory level and the corresponding price  numerical examples are provided to illustrate and validate the model and to derive managerial insights  to show the potential significance  we demonstrate how a dynamic programming model yields good results with customer loss aversion under realistic customer behavior assumptions  our system can improve the efficiency of decision making and provide better customer service   c  2016 elsevier ltd  all rights reserved  
 multi scale volatility feature analysis and prediction of gold price volatility of gold price is of great significance for avoiding the risk of gold investment  it is necessary to understand the effect of external events and intrinsic regularities to make accurate price predictions  this paper first compared emd with ceemd algorithm  and the results find that ceemd algorithm performance is better than that of emd in analysis gold price volatility  then this paper uses the complementary ensemble empirical mode decomposition  ceemd  to decompose the historical price of international gold into price components at different frequencies  and extracts a short term fluctuation  a shock from significant events and a long term price  in addition  this paper combines the iterative cumulative sum of squares  icss  with chow test to test the three event prices for structural breaks  and analyzes the effect of external events on volatility of gold price by comparing the external events with the test results for structural breaks  finally  this paper constructs support vector machine  svm  models and artificial neural network  ann  on three series for prediction  and finds that the svm performed better in gold price prediction in one step ahead and five step ahead  and when we combine the svms and anns with price components to make predictions  the error of the combined prediction is smaller than svms and anns with separate terms of series extracted  
 multi task mid level feature learning for micro expression recognition due to the short duration and low intensity of micro expressions  the recognition of micro expression is still a challenging problem  in this paper  we develop a novel multi task mid level feature learning method to enhance the discrimination ability of extracted low level features by learning a set of class specific feature mappings  which would be used for generating our mid level feature representation  moreover  two weighting schemes are employed to concatenate different mid level features  we also construct a new mobile micro expression set to evaluate the performance of the proposed mid level feature learning framework  the experimental results on two widely used non mobile micro expression datasets and one mobile micro expression set demonstrate that the proposed method can generally improve the performance of the low level features  and achieve comparable results with the state of the art methods  
 multi version ontology based personalization of clinical guidelines for patient centric healthcare when dealing with a specific patient case  physicians are often interested in retrieving a personalized version of a clinical guideline  that is a version tailored to their use needs  in a patient centric scenario  empowered patients make up another class of users interested in retrieving personalized care plans from a guideline repository  in their previous work  the authors proposed techniques to efficiently provide ontology based personalized access to very large collections of multi version clinical guidelines  in this paper  they address the problem of also dealing with a multi version ontology used to support personalized access to clinical guidelines  the authors  approach allows the semantic indexing of guideline contents with respect to multi version ontology classes and exploits the is a relationship among such classes for granting personalized access  efficiency is ensured by a newly introduced annotation scheme for guidelines and solutions to cope with the evolution of ontology structure  the tests performed on a prototype implementation confirm the goodness of the approach  
 multicriteria decision making with cognitive limitations  a ds ahp based approach in real life  sometimes multicriteria decision making  mcdm  problems are dealt with inevitably under cognitive limitations of human s minds  however  few existing models can directly solve mcdm problems of this kind  thus  to address the issue  this paper proposes a novel approach  which can   i  handle the cognitive limitations in mcdm problems by distinguishing the case of complete criteria  i e   there are no hidden cognitive factors that can deviate rational decisions  from the case of incomplete criteria  i e   there are some hidden cognitive factors that can deviate rational decisions    ii  differentiate incomplete and complete relative ranking of the groups of decision alternatives  das  over a criterion  and  iii  solve the imprecise and uncertain evaluation of criterion weight as well as the ambiguous evaluations of the groups of das regarding a given criterion  hence  we give a measure to consider the influence of cognitive limitations and give two methods to reduce the influence of cognitive limitations when a decision making needs more rational  moreover  we illustrate our approach by solving a real life problem of estate investment  finally  we give some experimental results about the reduction of the required number of knowledge judgments in our method compared with the previous methods  
 multigranulation fuzzy rough set over two universes and its application to decision making the original pawlak s rough set approach based on indiscernibility relation  single granularity  has been extended to multigranulation rough set structure in the recent years  multigranulation rough set approach has become a flouring research direction in rough set theory  this paper considers rough approximation of a fuzzy concept under the framework of multigranulation over two different universes of discourse  i e   multigranulation fuzzy rough set models over two universes  we present three types of multigranulation fuzzy rough set over two universes by the constructive approach  respectively  some interesting properties of the proposed models are discussed and also the interrelationships between the proposed models and the existing rough set models are given  we then propose a new approach to a kind of multiple criteria group decision making problem based on multigranulation fuzzy rough set model over two universes  the decision rules and algorithm of the proposed method are given and an example of handling multiple criteria group decision making problem of clothes ranking illustrates this approach  the main contribution of this paper is twofold  one is to establish the multigranulation fuzzy rough set theory over two universes  another is to try presenting a new approach to multiple criteria group decision making based on multigranulation fuzzy rough set over two universes  the proposed models not only enrich the theory of multigranulation rough set but also make a tentative to provide a new perspective for multiple criteria group decision making with uncertainty   c  2017 published by elsevier b v  
 multilevel behavioral synchronization in a joint tower building task human to human sensorimotor interaction can only be fully understood by modeling the patterns of bodily synchronization and reconstructing the underlying mechanisms of optimal cooperation  we designed a tower building task to address such a goal  we recorded upper body kinematics of dyads and focused on the velocity profiles of the head and wrist  we applied recurrence quantification analysis to examine the dynamics of synchronization within  and across the experimental trials  to compare the roles of leader and follower  our results show that the leader was more auto recurrent than the follower to make his her behavior more predictable  when looking at the cross recurrence of the dyad  we find different patterns of synchronization for head and wrist motion  on the wrist  dyads synchronized at short lags  and such a pattern was weakly modulated within trials  and invariant across them  head motion  instead  synchronized at longer lags and increased both within and between trials  a phenomenon mostly driven by the leader  our findings point at a multilevel nature of human to human sensorimotor synchronization  and may provide an experimentally solid benchmark to identify the basic primitives of motion  which maximize behavioral coupling between humans and artificial agents  
 multilingual extension and evaluation of a poetry generator  poetry generation is a specific kind of natural language generation where several sources of knowledge are typically exploited to handle features on different levels  such as syntax  semantics  form or aesthetics  but although this task has been addressed by several researchers  and targeted different languages  all known systems have focused on a limited purpose and a single language  this article describes the effort of adapting the same architecture to generate poetry in three different languages   portuguese  spanish and english  an existing architecture is first described and complemented with the adaptations required for each language  including the linguistic resources used for handling morphology  syntax  semantics and metric scansion  an automatic evaluation was designed in such a way that it would be applicable to the target languages  it covered three relevant aspects of the generated poems  namely  the presence of poetic features  the variation of the linguistic structure and the semantic connection to a given topic  the automatic measures applied for the second and third aspect can be seen as novel in the evaluation of poetry  overall  poems were successfully generated in the three languages addressed  despite minor differences in different languages or seed words  poems revealed to have a regular metre  frequent rhymes  to exhibit an interesting degree of variation  and to be semantically associated with the initially given seeds  
 multilingual metaphor processing  experiments with semi supervised and unsupervised learning highly frequent in language and communication  metaphor represents a significant challenge for natural language processing  nlp  applications  computational work on metaphor has traditionally evolved around the use of hand coded knowledge  making the systems hard to scale  recent years have witnessed a rise in statistical approaches to metaphor processing  however  these approaches often require extensive human annotation effort and are predominantly evaluated within a limited domain  in contrast  we experiment with weakly supervised and unsupervised techniqueswith little or no annotationto generalize higher level mechanisms of metaphor from distributional properties of concepts  we investigate different levels and types of supervision  learning from linguistic examples vs  learning from a given set of metaphorical mappings vs  learning without annotation  in flat and hierarchical  unconstrained and constrained clustering settings  our aim is to identify the optimal type of supervision for a learning algorithm that discovers patterns of metaphorical association from text  in order to investigate the scalability and adaptability of our models  we applied them to data in three languages from different language groupsenglish  spanish  and russianachieving state of the art results with little supervision  finally  we demonstrate that statistical methods can facilitate and scale up cross linguistic research on metaphor  
 multilingual native language identification we present the first comprehensive study of native language identification  nli  applied to text written in languages other than english  using data from six languages  nli is the task of predicting an author s first language using only their writings in a second language  with applications in second language acquisition and forensic linguistics  most research to date has focused on english but there is a need to apply nli to other languages  not only to gauge its applicability but also to aid in teaching research for other emerging languages  with this goal  we identify six typologically very different sources of non english second language data and conduct six experiments using a set of commonly used features  our first two experiments evaluate our features and corpora  showing that the features perform well and at similar rates across languages  the third experiment compares non native and native control data  showing that they can be discerned with 95 per cent accuracy  our fourth experiment provides a cross linguistic assessment of how the degree of syntactic data encoded in partof  speech tags affects their efficiency as classification features  finding that most differences between first language groups lie in the ordering of the most basic word categories  we also tackle two questions that have not previously been addressed for nli  other work in nli has shown that ensembles of classifiers over feature types work well and in our final experiment we use such an oracle classifier to derive an upper limit for classification accuracy with our feature set  we also present an analysis examining feature diversity  aiming to estimate the degree of overlap and complementarity between our chosen features employing an association measure for binary data  finally  we conclude with a general discussion and outline directions for future work  
 multimedia polysensory integration training system dedicated to children with educational difficulties this paper aims at presenting a multimedia system providing polysensory training for pupils with educational difficulties  the particularly interesting aspect of the system lies in the sonic interaction with image projection in which sounds generated lead to stimulation of a particular part of the human brain  the system architecture  video processing methods  therapeutic exercises and guidelines for children s interaction with the system are presented  results of pupils  improvements after several weeks of exercising with the system are provided  the outcome of this study suggests that learning and developing through the interactive method helped to improve children s spatial orientation skills  
 multimodal hci  exploratory studies on effects of first impression and single modality ratings in retrospective evaluation this paper deals with the relevance of the first impression of interactive systems  based on short passive visual auditory stimuli of system output  individual consistency between such impressions and retrospective user ratings  obtained directly after real interaction  is studied in four exploratory experiments  all systems allow for voice user input  two systems are considered to be multimodal as they support additional input other than speech  e g   gesture   whereas the other two systems  offering speech as the sole input modality  are multimedia systems  the first impression of the four systems is based on screen shots of typical display views and selected prompts of the systems  speech output  measures used here were pragmatic quality  i e   the functional aspects of a system such as efficiency and effectiveness that are closely related to the concept of usability  and hedonic qualities  i e   the systems non instrumental aspects such as its ability to provide stimulation and identification to evoke the psychological well being of the user  it was tested  whether consistency found for web sites can also be found for speech based systems  in our case  this consistency was assessed not between systems  but within systems  results indicate that users  first impression of system output does correlate with ratings collected after the interaction for each of the four systems  for the two truly multimodal systems  ratings after single input  e g   only voice  only touch screen  also correlates with ratings of a multimodal interaction with the same system  this result confirms data from literature  however  our assumption of lower correlations for the first impression of pragmatic quality  expected due to its experience based character  is not supported  instead  pragmatic quality seems to represent a construct with low consistency in general  reasons for this might be found in the benefit of pragmatic quality experienced during multimodal interaction that is neither covered by unimodal interaction  nor predictable from a first impression  additional multiple regression analysis for the two systems with multiple input modalities show that the first impression of the visual system output can complement predictors from the single modality interactions to model post usage multimodal ratings  however  which of the output channels has a relevant impact was found to be highly system dependent  
 multimodal learning for topic sentiment analysis in microblogging microblogging has become a widely spread platform of human communication  the massive amount of opinion rich data in microblogging is helpful to analyze and manage public opinion and social emotion  different from traditional texts  microblogging data are multimodal  containing multifarious data such as emoticons  image  etc  most existing sentiment and topic detection approaches treat the unique microblogging data as noise  however  this may lead to unsatisfactoriness in sentiment classification and topic identification  in order to address the issue  we propose a multimodal joint sentiment topic model  mjst  for weakly supervised sentiment analysis in microblogging  which applies latent dirichlet allocation  lda  to simultaneously analyze sentiment and topic hidden in messages based the introduction of emoticons and microbloggers personality  extensive experiments show that mjst outperforms state of the art unsupervised approaches jst  slda and dplda significantly in terms of sentiment classification accuracy and is promising   c  2017 elsevier b v  all rights reserved  
 multiobjective synthesis of robust vaccination policies this paper deals with the optimal planning of vaccination campaigns  using an evolutionary multiobjective optimization algorithm and a stochastic simulation of the epidemics dynamics in order to determine robust vaccination policies  a biobjective model is formulated  considering the minimization of control costs and number of infected individuals  the decision variables include number of campaigns  percentage of vaccination and time interval between each campaign  a sir  susceptible infected recovered  model and an ibm  individual based model  are employed for representing the epidemics  a two stage optimization process is proposed  a set of nondominated steady state regimes is obtained and one of them is selected to be concatenated to the transient regime vaccination policies  an evolutionary multiobjective optimization algorithm is proposed  with a local search procedure based on quadratic approximation supported by a hash table information storage  the resulting nondominated solutions are simulated in the ibm  in order to detect and discard the non robust solutions  final results show that optimal robust vaccination campaigns with different trade offs can be designed  allowing policymakers to choose the best strategy according to the monetary cost and the expected efficacy   c  2016 elsevier b v  all rights reserved  
 multiple attribute group decision making based on generalized power aggregation operators under interval valued dual hesitant fuzzy linguistic environment this paper aims to investigate the type of fuzzy multiple attribute group decision making  magdm  where arguments being aggregated are allowed to support each other  in order to enable decision makers to express their preferences more comprehensively  we firstly put forward a hybrid tool  an interval valued dual hesitant fuzzy linguistic set  ivdhfls   which employs interval valued hesitant membership and nonmembership degrees to assess linguistic terms  basic operational laws for ivdhfls are discussed  also a distance measure is designed to overcome irrationality in traditional methodology for hesitant fuzzy sets  i e   artificially adding values to mismatching membership or nonmembership degrees  we next develop fundamental generalized power average aggregation operators for ivdhfls  including power average operator  power geometric average operator  power ordered weighted average operator and power ordered weighted geometric average operator  desirable properties and special cases of these aggregation operators are further analyzed  furthermore  based on the generalized operators above  we construct two approaches for magdm with mutually supportive arguments being aggregated under intervalvalued dual hesitant fuzzy linguistic environments  finally  case studies are conducted to verify effectiveness and practicality of the developed approaches  
 multiple attribute group decision making  a generic conceptual framework and a classification scheme the research activities in group decision making have dramatically increased over the last decade  in particular  the application of multiple attribute decision making methods to group decision making problems occupies a vast area in the related literature  however  there is no systematic classification scheme for these researches  this paper presents a generic conceptual framework and a classification scheme for multiple attribute group decision making methods  the proposed framework consists of three main stages  the structuring and construction stage  the assessment stage  and the selection ranking stage  providing not only an outline for classification but also a road map for the researchers working on this topic  furthermore  top cited papers are classified based on this classification scheme in order to clarify the state of the art and to identify future research directions  as a result  eight significant suggestions for future research are identified   c  2017 elsevier b v  all rights reserved  
 multiple criteria inventory classiffication in an electronics firm efficient inventory classiffication is a vital activity for electronics firms that work with a large amount of inventory items  although one of the most widely used techniques in inventory classiffication is abc analysis  this technique considers only a single criterion as the annual sales volume of each item  in practice  inhomogeneity and the di r erences among the inventory items necessitate considering multiple criteria to obtain a reliable classiffication  in this study  an integrated process of the analytic hierarchy process  ahp  the technique for order preference by similarity to the ideal solution  topsis  abc approach is proposed and performed to solve the multiple criteria inventory classiffication problem in an electronics firm  the steps of the interactive approach are programmed using matlab  the results of the integrated interactive approach and of the traditional abc analysis are presented  the proposed approach gives e r ective and implementable results for the firm  
 multiple instance learning for behavioral coding we propose a computational methodology for automatically estimating human behavioral patterns using the multiple instance learning  mil  paradigm  we describe the incremental diverse density algorithm  a particular formulation of multiple instance learning  and discuss its suitability for behavioral coding  we use a rich multi modal corpus comprised of chronically distressed married couples having problem solving discussions as a case study to experimentally evaluate our approach  in the multiple instance learning framework  we treat each discussion as a collection of short term behavioral expressions which are manifested in the acoustic  lexical  and visual channels  we experimentally demonstrate that this approach successfully learns representations that carry relevant information about the behavioral coding task  furthermore  we employ this methodology to gain novel insights into human behavioral data  such as the local versus global nature of behavioral constructs as well as the level of ambiguity in the expression of behaviors through each respective modality  finally  we assess the success of each modality for behavioral classification and compare schemes for multimodal fusion within the proposed framework  
 multiple kernel learning based on three discriminant features for a p300 speller bci in this paper  we propose multiple kernel learning  mkl  based on three discriminant features to learn an efficient p300 classifier to improve the accuracy of character recognition in a p300 speller bci  three discriminant features are specified in raw samples and two morphological features  which can complement the mkl of a p300 classification  a linear kernel is established for each discriminant feature  a kernel weight differentiates the linear kernel to both explore complementary information among the three discriminant features and weigh a contribution of each discriminant feature for the mkl  here  the norm regularization of the kernel weight ultimately enforces an optimal discriminant feature set of the mkl of a p300 classification  the performance of the proposed method is then evaluated according to the size of the three discriminant feature sets that are generated from dataset ii of bci competition iii  compared to an existing svm based classification method  the proposed method consistently obtains better or similar accuracy in terms of character recognition  with a different execution time for the variable size of the three discriminant feature sets  furthermore  the kernel weight of the raw samples was found to consistently be more dominant than the kernel weight of the two morphological features on the variable size of the three discriminant feature sets  this finding means that the two morphological features supplement the lack of the raw samples for the mkl of a p300 classification  we ultimately could conclude that the proposed method is sufficiently robust to improve the accuracy of character recognition with a different time for the variable size of the three discriminant feature sets in a p300 speller bci  
 multiple objective solution approaches for aircraft rerouting under the disruption of multi aircraft this paper considers a multi objective aircraft recovery problem for airline disruption  an integer programming formulation is first established based on connection network with three conflicting objectives  where the first objective minimizes the total deviation from original flight schedules  the second objective minimizes the maximal flight delay time  and the third objective minimizes the number of aircraft actually attended in swapping  optimal analysis is provided for a small scale aircraft recovery problem with the last two objectives and then one polynomial time algorithm for aircraft recovery problem after the disruption to multi aircraft in a fleet at an airport  one heuristic combined epsilon constraints method and neighborhood search algorithm is designed for large scale disruption recovery problem  the results obtained from computational experiment illustrate effectiveness and efficiency of the heuristic  the outcome of this research could provide theoretical and practical supports for airlines to reduce flight delays   c  2017 elsevier ltd  all rights reserved  
 multiple cause discovery combined with structure learning for high dimensional discrete data and application to stock prediction causal discovery in observational data is crucial to a variety of scientific and business research  although many causal discovery algorithms have been proposed in recent decades  none of them is effective enough in dealing with high dimensional discrete data  the main challenge is the complex interactions among large volume of variables  leading to numerous spurious causalities found  in this work  we propose a novel multiple cause discovery method combined with structure learning  mcdsl  to eliminate the spurious causalities  the method is carried out in two phases  in the first phase  conditional independence test is used to distinguish direct causal candidates from the indirect ones  in the second phase  causal direction of multi cause structure is carefully determined with a hybrid causal discovery method  validation experiments on synthetic data showed that mcdsl is reliable in discovering multi cause structures and eliminating indirect causes  we then applied this algorithm in discovering multiple causes of stock return based on 13 year historical financial data of the shanghai stock exchanges of china  and established a stock prediction model  experimental results showed that the mcdsl discovered causes revealed changes of key risk factors of the stock market over 13 years  which indicated investors should change their investment strategy over time  moreover  the causes discovered by mcdsl have better performance in predicting stock return than that of other common filter based feature selection algorithms  
 multiscale strategies for computing optimal transport this paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets  it is particularly well suited for point sets that are in high dimensions  but are close to being intrinsically low dimensional  the approach is based on an adaptive multiscale decomposition of the point sets  the multiscale decomposition yields a sequence of optimal transport problems  that are solved in a top to bottom fashion from the coarsest to the finest scale  we provide numerical evidence that this multiscale approach scales approximately linearly  in time and memory  in the number of nodes  instead of quadratically or worse for a direct solution  empirically  the multiscale approach results in less than one percent relative error in the objective function  furthermore  the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets  an analysis of sets of brain mri based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set  the application demonstrates that multiscale optimal transport distances have the potential to improve on state of the art metrics currently used in computational anatomy  
 multisense context aware nonverbal behavior analysis framework  a psychological distress use case during face to face interactions  people naturally integrate nonverbal behaviors such as facial expressions and body postures as part of the conversation to infer the communicative intent or emotional state of their interlocutor  the interpretation of these nonverbal behaviors will often be contextualized by interactional cues such as the previous spoken question  the general discussion topic or the physical environment  a critical step in creating computers able to understand or participate in this type of social face to face interactions is to develop a computational platform to synchronously recognize nonverbal behaviors as part of the interactional context  in this platform  information for the acoustic and visual modalities should be carefully synchronized and rapidly processed  at the same time  contextual and interactional cues should be remembered and integrated to better interpret nonverbal  and verbal  behaviors  in this article  we introduce a real time computational framework  multisense  which offers flexible and efficient synchronization approaches for context based nonverbal behavior analysis  multisense is designed to utilize interactional cues from both interlocutors  e g   from the computer and the human participant  and integrate this contextual information when interpreting nonverbal behaviors  multisense can also assimilate behaviors over a full interaction and summarize the observed affective states of the user  we demonstrate the capabilities of the new framework with a concrete use case from the mental health domain where multisense is used as part of a decision support tool to assess indicators of psychological distress such as depression and post traumatic stress disorder  ptsd   in this scenario  multisense not only infers psychological distress indicators from nonverbal behaviors but also broadcasts the user state in real time to a virtual agent  i e   a digital interviewer  designed to conduct semi structured interviews with human participants  our experiments show the added value of our multimodal synchronization approaches and also demonstrate the importance of multisense contextual interpretation when inferring distress indicators  
 multiview physician specific attributes fusion for health seeking community based health services have risen as important online resources for resolving users health concerns  despite the value  the gap between what health seekers with specific health needs and what busy physicians with specific attitudes and expertise can offer is being widened  to bridge this gap  we present a question routing scheme that is able to connect health seekers to the right physicians  in this scheme  we first bridge the expertise matching gap via a probabilistic fusion of the physician expertise distribution and the expertise question distribution  the distributions are calculated by hypergraph based learning and kernel density estimation  we then measure physicians attitudes toward answering general questions from the perspectives of activity  responsibility  reputation  and willingness  at last  we adaptively fuse the expertise modeling and attitude modeling by considering the personal needs of the health seekers  extensive experiments have been conducted on a real world dataset to validate our proposed scheme  
 nasari  integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities owing to the need for a deep understanding of linguistic items  semantic representation is considered to be one of the fundamental components of several applications in natural language processing and artificial intelligence  as a result  semantic representation has been one of the prominent research areas in lexical semantics over the past decades  however  due mainly to the lack of large sense annotated corpora  most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses  in this paper we put forward a novel multilingual vector representation  called nasari  which not only enables accurate representation of word senses in different languages  but it also provides two main advantages over existing approaches   1  high coverage  including both concepts and named entities   2  comparability across languages and linguistic levels  i e   words  senses and concepts   thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space  respectively  moreover  our representations are flexible  can be applied to multiple applications and are freely available at http   1cl uniroma1 it nasari   as evaluation benchmark  we opted for four different tasks  namely  word similarity  sense clustering  domain labeling  and word sense disambiguation  for each of which we report state of the art performance on several standard datasets across different languages   c  2016 elsevier b v  all rights reserved  
 nash equilibria of co operative advertising programs with advertising threshold effects as an important advertising phenomenon which has been proven by previous empirical marketing research  the advertising threshold effect implies that advertising has little effect on sales when advertising investment is beneath a certain level  in this paper  we focus on a cooperative advertising program  in which a manufacturer shares part of its retailer s advertising cost  to illustrate the impact of the advertising threshold effect  dividing the advertisement efforts into national and local advertising  we propose a new advertising response model which can describe the advertising threshold effect well  based on the advertising response function  we derive the manufacturer s and the retailer s equilibrium advertising investments when they play a nash game  from the analysis we find that there are four possible nash equilibria when we take the advertising threshold effect into account  we derive the sufficient and necessary conditions under which each of the four possible equilibria is a nash equilibrium  and explain these conditions with examples from practice  another interesting result is that there are two different equilibria when certain conditions are satisfied  
 natural   normative dynamical coupling cognitive science has been dealt with the unique task of straddling and bridging the gaps between the mind and the body  one such gap that has not received as much attention within the literature is the gap between the natural and the normative  we propose that the theory of autopoiesis can be used for bridging this gap  and  so  we incorporate autopoiesis into the framework of dynamical systems theory in order to ground a physicalist theory of normativity  within this framework  the dynamical coupling between an autopoietic system and its environment can be either natural or normative  we then construct a full fledged theory of how cognition achieves normativity and find that we are confronted by the frame problem  we review vervaeke  lillicrap and richards   2012  theory of the mechanisms by which cognitive agents realize relevance and find that it promises to circumvent the frame problem  after augmenting their theory with our own theory of learning  we find that the frame problem has been circumvented in a way that meets the necessary conditions for normativity  we finally represent our theoretical findings in a dynamical systems framework and discuss some broad applications for social and psychological science  crown copyright  c  2016 published by elsevier b v  all rights reserved  
 natural language processing in mental health applications using non clinical texts natural language processing  nlp  techniques can be used to make inferences about peoples  mental states from what they write on facebook  twitter and other social media  these inferences can then be used to create online pathways to direct people to health information and assistance and also to generate personalized interventions  regrettably  the computational methods used to collect  process and utilize online writing data  as well as the evaluations of these techniques  are still dispersed in the literature  this paper provides a taxonomy of data sources and techniques that have been used for mental health support and intervention  specifically  we review how social media and other data sources have been used to detect emotions and identify people who may be in need of psychological assistance  the computational techniques used in labeling and diagnosis  and finally  we discuss ways to generate and personalize mental health interventions  the overarching aim of this scoping review is to highlight areas of research where nlp has been applied in the mental health literature and to help develop a common language that draws together the fields of mental health  human computer interaction and nlp  
 negotiating with other minds  the role of recursive theory of mind in negotiation with incomplete information theory of mind refers to the ability to reason explicitly about unobservable mental content of others  such as beliefs  goals  and intentions  people often use this ability to understand the behavior of others as well as to predict future behavior  people even take this ability a step further  and use higher order theory of mind by reasoning about the way others make use of theory of mind and in turn attribute mental states to different agents  one of the possible explanations for the emergence of the cognitively demanding ability of higher order theory of mind suggests that it is needed to deal with mixed motive situations  such mixed motive situations involve partially overlapping goals  so that both cooperation and competition play a role  in this paper  we consider a particular mixed motive situation known as colored trails  in which computational agents negotiate using alternating offers with incomplete information about the preferences of their trading partner  in this setting  we determine to what extent higher order theory of mind is beneficial to computational agents  our results show limited effectiveness of first order theory of mind  while second order theory of mind turns out to benefit agents greatly by allowing them to reason about the way they can communicate their interests  additionally  we let human participants negotiate with computational agents of different orders of theory of mind  these experiments show that people spontaneously make use of second order theory of mind in negotiations when their trading partner is capable of second order theory of mind as well  
 nera 2 0  improving coverage and performance of rule based named entity recognition for arabic named entity recognition  ner  is an essential task for many natural language processing systems  which makes use of various linguistic resources  ner becomes more complicated when the language in use is morphologically rich and structurally complex  such as arabic  this language has a set of characteristics that makes it particularly challenging to handle  in a previous work  we have proposed an arabic ner system that follows the hybrid approach  i e  integrates both rule based and machine learning based ner approaches  our hybrid ner system is the state of the art in arabic ner according to its performance on standard evaluation datasets  in this article  we discuss a novel methodology for overcoming the coverage drawback of rule based ner systems in order to improve their performance and allow for automated rule update  the presented mechanism utilizes the recognition decisions made by the hybrid ner system in order to identify the weaknesses of the rule based component and derive new linguistic rules aiming at enhancing the rule base  which will help in achieving more reliable and accurate results  we used ace 2004 newswire standard dataset as a resource for extracting and analyzing new linguistic rules for person  location and organization names recognition  we formulate each new rule based on two distinctive feature groups  i e  gazetteers of each type of named entities and part of speech tags  in particular noun and proper noun  fourteen new patterns are derived  formulated as grammar rules  and evaluated in terms of coverage  the conducted experiments exploit a pos tagged version of the ace 2004 nw dataset  the empirical results show that the performance of the enhanced rule based system  i e  nera 2 0  improves the coverage of the previously misclassified person  location and organization named entities types by 69 93 per cent  57 09 per cent and 54 28 per cent  respectively  
 network based analysis of requirement change in customized complex product development requirement change impact analysis has been acknowledged as one of the crucial steps in product design  in this paper  we propose a network based method to analyze change impact  by defining interconnections among parts  we build a directed weighted complex product network model to represent the product structure under given requirements  then  we discuss two requirement change cases and develop corresponding modification policies  to specify indirect impacts  we propose a change propagation searching model in light of matthew effect theory  to measure the degree of change impacts  we propose two criteria  network variation scale and extra network change cost   both of which can provide a systemic assessment of impacts  finally  a case of clutch is presented to illustrate the proposed approach  the results can provide way of measuring overall change impacts on the product  which can support decision makers to respond that the change request can be fulfilled or not  
 neural circuits trained with standard reinforcement learning can accumulate probabilistic information during decision making much experimental evidence suggests that during decision making  neural circuits accumulate evidence supporting alternative options  a computational model well describing this accumulation for choices between two options assumes that the brain integrates the log ratios of the likelihoods of the sensory inputs given the two options  several models have been proposed for how neural circuits can learn these log likelihood ratios from experience  but all of these models introduced novel and specially dedicated synaptic plasticity rules  here we show that for a certain wide class of tasks  the log likelihood ratios are approximately linearly proportional to the expected rewards for selecting actions  therefore  a simple model based on standard reinforcement learning rules is able to estimate the log likelihood ratios from experience and on each trial accumulate the log likelihood ratios associated with presented stimuli while selecting an action  the simulations of the model replicate experimental data on both behavior and neural activity in tasks requiring accumulation of probabilistic cues  our results suggest that there is no need for the brain to support dedicated plasticity rules  as the standard mechanisms proposed to describe reinforcement learning can enable the neural circuits to perform efficient probabilistic inference  
 neural implementation of probabilistic models of cognition bayesian models of cognition hypothesize that human brains make sense of data by representing probability distributions and applying bayes  rule to find the best explanation for available data  understanding the neural mechanisms underlying probabilistic models remains important because bayesian models provide a computational framework  rather than specifying mechanistic processes  here  we propose a deterministic neural network model which estimates and represents probability distributions from observable events a phenomenon related to the concept of probability matching  our model learns to represent probabilities without receiving any representation of them from the external world  but rather by experiencing the occurrence patterns of individual events  our neural implementation of probability matching is paired with a neural module applying bayes  rule  forming a comprehensive neural scheme to simulate human bayesian learning and inference  our model also provides novel explanations of base rate neglect  a notable deviation from bayes   c  2016 elsevier b v  all rights reserved  
 neural network models for group behavior prediction  a case of soccer match attendance soccer match attendance is an example of group behavior with noisy context that can only be approximated by a limited set of quantifiable factors  however  match attendance is representative of a wider spectrum of context based behaviors for which only the aggregate effect of otherwise individual decisions is observable  modeling of such behaviors is desirable from the perspective of economics  psychology  and other social studies with prospective use in simulators  games  product planning  and advertising  in this paper  we evaluate the efficiency of different neural network architectures as models of context in attendance behavior by comparing the achieved prediction accuracy of a multilayer perceptron  mlp   an elman recurrent neural network  rnn   a time lagged feedforward neural network  tlfn   and a radial basis function network  rbfn  against a multiple linear regression model  an autoregressive moving average model with exogenous inputs  and a naive cumulative mean model  we show that the mlp  tlfn  and rnn are superior to the rbfn and achieve comparable prediction accuracy on datasets of three teams from the english football league championship  which indicates weak importance of context transition modeled by the tlfn and the rnn  the experiments demonstrate that all neural network models outperform linear predictors by a significant margin  we show that neural models built on individual datasets achieve better performance than a generalized neural model constructed from pooled data  we analyze the input parameter influences extracted from trained networks and show that there is an agreement between nonlinear and linear measures about the most significant attributes  
 neural networks for computer aided diagnosis in medicine  a review this survey makes an overview of the most recent applications on the neural networks for the computer aided medical diagnosis  camd  over the past decade  camd can facilitate the automation of decision making  extraction and visualization of complex characteristics for clinical diagnosis purposes  over the past decade  neural networks have attained considerable research interest and are widely employed to complex camd systems in diverse clinical application domains  such as detecting diseases  classification of diseases  testing the compatibility of new drugs  etc  overall  this paper reviews the state of the art of neural networks for camd  it helps the readers understand the topic of neural networks for camd by summarizing the findings addressed in recent academic papers as well as presenting a few open issues of developing the research on this topic   c  2016 elsevier b v  all rights reserved  
 neural networks  an overview of early research  current frameworks and new challenges this paper presents a comprehensive overview of modelling  simulation and implementation of neural networks  taking into account that two aims have emerged in this area  the improvement of our understanding of the behaviour of the nervous system and the need to find inspiration from it to build systems with the advantages provided by nature to perform certain relevant tasks  the development and evolution of different topics related to neural networks is described  simulators  implementations  and real world applications  showing that the field has acquired maturity and consolidation  proven by its competitiveness in solving real world problems  the paper also shows how  over time  artificial neural networks have contributed to fundamental concepts at the birth and development of other disciplines such as computational neuroscience  neuro engineering  computational intelligence and machine learning  a better understanding of the human brain is considered one of the challenges of this century  and to achieve it as this paper goes on to describe  several important national and multinational projects and initiatives are marking the way to follow in neural network research   c  2016 elsevier b v  all rights reserved  
 neuroevolution in games  state of the art and open challenges this paper surveys research on applying neuroevolution  ne  to games  in neuroevolution  artificial neural networks are trained through evolutionary algorithms  taking inspiration from the way biological brains evolved  we analyze the application of ne in games along five different axes  which are the role ne is chosen to play in a game  the different types of neural networks used  the way these networks are evolved  how the fitness is determined and what type of input the network receives  the paper also highlights important open research challenges in the field  
 neuronal network and awareness measures of post decision wagering behavior in detecting masked emotional faces awareness can be measured by investigating the patterns of associations between discrimination performance  first order decisions  and confidence judgments  knowledge   in a typical post decision wagering  pdw  task  participants judge their performance by wagering on each decision made in a detection task  if participants are aware  they wager advantageously by betting high whenever decisions are correct and low for incorrect decisions  thus  pdw like other awareness measures with confidence ratings quantifies if the knowledge upon which they make their decisions is conscious  the present study proposes a new method of assessing the association between advantageous wagering and awareness in the pdw task with a combination of log linear  llm  modeling and neural network simulation to reveal the computational patterns that establish this association  we applied the post decision wagering measure to a backward masking experiment in which participants made first order decisions about whether or not a masked emotional face was present  and then used imaginary or real monetary stakes to judge the correctness of their initial decisions  the llm analysis was then used to examine whether advantageous wagering was aware by testing a hypothesis of partial associations between metacognitive judgments and accuracy of first order decisions  the llm outcomes were submitted into a feed forward neural network  the network served as a general approximator that was trained to learn relationships between input wagers and the output of the corresponding log linear function  the simulation resulted in a simple network architecture that successfully accounted for wagering behavior  this was a feed forward network unit consisting of one hidden neuron layer with four inputs and one output  in addition  the study indicated no effect of the monetary incentive cues on wagering strategies  although we observed that only low wager input weights of the neural network considerably contributed to advantageous wagering  
 new coral reefs based approaches for the model type selection problem  a novel method to predict a nation s future energy demand in this paper  we describe two new methods to address the model type selection problem  mtsp  based on modifications of the coral reefs optimisation algorithm  cro   the effectiveness of these novel approaches is subsequently illustrated in a problem of energy demand estimation in spain  first  we describe how coral species can be defined in the cro algorithm  so each specie defines a competing model for the mtsp  second  we propose another method to solve mtsps by modifying the original cro with a substrate layer  so that the different models considered can be encoded similarly  this second method to solve the mtsp simplifies the application of the cro operators  finally  we evaluate the performance of the two cro based algorithms by solving a mtsp consisting of the prediction of future energy demand from macro economic data in spain as a case study  
 new definitions of mean value and variance of fuzzy numbers  an application to the pricing of life insurance policies and real options we propose a new definition of mean value and variance for fuzzy numbers whose membership functions are upper semicontinuous but are not necessarily continuous  our proposal uses the total variation of bounded variation functions  the proposed concepts are used for the evaluation of insurance contracts and real options in a fuzzy framework   c  2017 elsevier inc  all rights reserved  
 new framework that uses patterns and relations to understand terrorist behaviors terrorism is a complex phenomenon with high uncertainties in user strategy  the uncertain nature of terrorism is a main challenge in the design of counter terrorism policy  government agencies  e g   cia  fbi  nsa  etc   cannot always use social media and telecommunications to capture the intentions of terrorists because terrorists are very careful in the use of these environments to plan and prepare attacks  to address this issue  this research aims to propose a new framework by defining the useful patterns of suicide attacks to analyze the terrorist activity patterns and relations  to understand behaviors and their future moves  and finally to prevent potential terrorist attacks  in the framework  a new network model is formed  and the structure of the relations is analyzed to infer knowledge about terrorist attacks  more specifically  an evolutionary simulating annealing lasso logistic regression  esallor  model is proposed to select key features for similarity function  subsequently  a new weighted heterogeneous similarity function is proposed to estimate the relationships among attacks  moreover  a graph based outbreak detection is proposed to define hazardous places for the outbreak of violence  experimental results demonstrate the effectiveness of our framework with high accuracy  more than 90  accuracy  for finding patterns when compared with that of actual terrorism events in 2014 and 2015  in conclusion  by using this intelligent framework  governments can understand automatically how terrorism will impact future events  and governments can control terrorists  behaviors and tactics to reduce the risk of future events   c  2017 elsevier ltd  all rights reserved  
 new online personalized recommendation approach based on the perceived value of consumer characteristics web 2 0 facilitates the bidirectional communication capabilities of online review  causing the personalization and asymmetry of online review  despite the problem of online personalized recommendation system  the influence of consumer characteristics on consumer repurchase intention is insufficiently examined in the extant literature  to address this issue  this study proposes a new online personalized recommendation approach based on the perceived value of consumer characteristics  two aspects of the proposed framework are addressed  the first aspect is the linguistic information transformation model  which converts online reviews to unbalanced linguistic label cloud  the second aspect is an online recommendation approach based on the linguistic information trans formation model  a series of experiments are conducted based on a set of hotel assessment data from four cities and the electronic consumer record of four consumers selected randomly  results show that the proposed cluster method is useful for identifying consumer characteristics and gives personalized recommendation  overall  this method reduces computation and provides a reference point based on consumer characteristics  
 new recommender framework  combining semantic similarity fusion and bicluster collaborative filtering collaborative filtering  cf  systems help address information overload  by using the preferences of users in a community to make personal recommendations for other users  the widespread use of these systems has exposed some well known limitations  such as sparsity  scalability  and cold start  which can lead to poor recommendations  during the last years  a great number of works have focused on the improvement of cf  but they do not solve all its problems efficiently  in this article  we present a new approach that applies semantic similarity fusion as well as biclustering to alleviate the aforementioned problems  the experimental results verify the effectiveness and efficiency of our approach over the benchmark cf methods  
 nlp driven citation analysis for scientometrics this paper summarizes ongoing research in natural language processing driven citation analysis and describes experiments and motivating examples of how this work can be used to enhance traditional scientometrics analysis that is based on simply treating citations as a   vote  from the citing paper to cited paper  in particular  we describe our dataset for citation polarity and citation purpose  present experimental results on the automatic detection of these indicators  and demonstrate the use of such annotations for studying research dynamics and scientific summarization  we also look at two complementary problems that show up in natural language processing driven citation analysis for a specific target paper  the first problem is extracting citation context  the implicit citation sentences that do not contain explicit anchors to the target paper  the second problem is extracting reference scope  the target relevant segment of a complicated citing sentence that cites multiple papers  we show how these tasks can be helpful in improving sentiment analysis and citation based summarization  
 no arbitrage theorem for multi factor uncertain stock model with floating interest rate in the stock models  the prices of the stocks are usually described via some differential equations  so far  uncertain stock model with constant interest rate has been proposed  and a sufficient and necessary condition for it being no arbitrage has also been derived  this paper considers the multiple risks in the interest rate market and stock market  and proposes a multi factor uncertain stock model with floating interest rate  a no arbitrage theorem is derived in the form of determinants  presenting a sufficient and necessary condition for the new stock model being no arbitrage  in addition  a strategy for the arbitrage is provided when the condition is not satisfied  
 non intrusive quantification of performance and its relationship to mood the number of jobs that takes place entirely or partially in a computer is nowadays very significant  these workplaces  as many others  often offer the key ingredients for the emergence of stress and the performance drop of its long term effects  long hours sitting  sustained cognitive effort  pressure from competitiveness  among others  this has a toll on productivity and work quality  with significant costs for both organizations and workers  moreover  a tired workforce is generally more susceptible to negative feelings and mood  which results in a negative environment  this paper contributes to the current need for the development of non intrusive methods for monitoring and managing worker performance in real time  we propose a framework that assesses worker performance and a case study in which this approach was validated  we also show the relationship between performance and mood  
 nonlinear dynamic classification of momentary mental workload using physiological features and narx model based least squares support vector machines this paper designs a pattern classifier based on a nonlinear autoregressive model with exogenous inputs  narx  to reveal intricate nonlinear dynamical correlation between mental workload  mwl  of a human operator and psychophysiological features  the salient electroencephalogram and electrocardiogram features were selected as inputs to the narx model  whose continuous output was discretized in terms of five mwl classes at each time instant  the orders of the narx model were determined using an objective function to achieve a good tradeoff between model accuracy and complexity via a least squares support vector machine  the physiological features from different measurement channels  electrodes  and frequency bands were compared in terms of multiclass mwl classification performance  the classification results showed that the locality projection preservation technique can maintain sufficiently high mwl classification accuracy  with the highest five class correct classification rate of 88   with a significantly reduced computational complexity  the comparative results of classification performance also demonstrated the superiority of the proposed dynamic model to a widely used static model  
 nonparametric discovery and analysis of learning patterns and autism subgroups from therapeutic data the spectrum nature and heterogeneity within autism spectrum disorders  asd  pose as a challenge for treatment  personalisation of syllabus for children with asd can improve the efficacy of learning by adjusting the number of opportunities and deciding the course of syllabus  we research the data motivated approach in an attempt to disentangle this heterogeneity for personalisation of syllabus  with the help of technology and a structured syllabus  collecting data while a child with asd masters the skills is made possible  the performance data collected are  however  growing and contain missing elements based on the pace and the course each child takes while navigating through the syllabus  bayesian nonparametric methods are known for automatically discovering the number of latent components and their parameters when the model involves higher complexity  we propose a nonparametric bayesian matrix factorisation model that discovers learning patterns and the way participants associate with them  our model is built upon the linear poisson gamma model  lpgm  with an indian buffet process prior and extended to incorporate data with missing elements  in this paper  for the first time we have presented learning patterns deduced automatically from data mining and machine learning methods using intervention data recorded for over 500 children with asd  we compare the results with non negative matrix factorisation and k means  which being parametric  not only require us to specify the number of learning patterns in advance  but also do not have a principle approach to deal with missing data  the f1 score observed over varying degree of similarity measure  jaccard index  suggests that lpgm yields the best outcome  by observing these patterns with additional knowledge regarding the syllabus it may be possible to observe the progress and dynamically modify the syllabus for improved learning  
 nonparametric risk bounds for time series forecasting we derive generalization error bounds for traditional time series forecasting models  our results hold for many standard forecasting tools including autoregressive models  moving average models  and  more generally  linear state space models  these non asymptotic bounds need only weak assumptions on the data generating process  yet allow forecasters to select among competing models and to guarantee  with high probability  that their chosen model will perform well  we motivate our techniques with and apply them to standard economic and financial forecasting tools a garch model for predicting equity volatility and a dynamic stochastic general equilibrium model  dsge   the standard tool in macroeconomic forecasting  we demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis specification  
 notes on the generalized backpropagation algorithm for contextual neural networks with conditional aggregation functions in this paper  we show that a contextual neural network with artificial neurons performing a conditional aggregation of signals can be trained by the generalized backpropagation algorithm  to allow this algorithm to be used for training contextual neural networks  we derive appropriate generalized delta rules  our approach is constructed on the basis of introduced generalized representation of the aggregation function in an ordered groups space and division of its attention function into binary scan path and contribution functions  the advantage of the proposed representation is that it clarifies the description of the aggregation process by using stark s scan path theory and allows us to achieve results independent from the actual form of the attention functions used during aggregation  as such  the proposed solution is valid for the whole presented family of conditional aggregation functions and is a considerable extension of the previously reported results  in particular  the obtained results are valid for the introduced exemplary attention functions which illustrate performed calculations  moreover  the presented solution can be further extended by considering real valued  non binary contribution functions inside ordered aggregation functions  especially promising are its possible applications in large deep neural networks and energy limited systems  
 novel intuitionistic fuzzy decision making models in the framework of decision field theory the intuitionistic fuzzy decision making problems have gained great popularity recently  most of the current methods depend on various aggregation operators that provide collective intuitionistic fuzzy values of alternatives to be ranked  such collective information only depicts the overall characteristics of the alternatives but ignores the detailed contrasts among them  most important of all  the current decision making procedure is not in accordance with the way that the decision makers  dms  think about the decision making problems  in this paper  we develop a novel intuitionistic fuzzy decision making model in the framework of decision field theory  the decision making model emphasizes the contrasts among alternatives with respect to each attribute that competes and influences each other  and thus  the preferences for alternatives can dynamically evolve and provide the final optimal result  after that  we develop an intuitionistic fuzzy group decision making model based on decision field theory  and then make a practical case study on the application of the developed models to the  one belt  one road  investment decision making problems  finally  we point out the characteristics and the limitations of our models in detail   c  2016 elsevier b v  all rights reserved  
 novelty driven cooperative coevolution cooperative coevolutionary algorithms  cceas  rely on multiple coevolving populations for the evolution of solutions composed of coadapted components  cceas enable  for instance  the evolution of cooperative multiagent systems composed of heterogeneous agents  where each agent is modelled as a component of the solution  previous works have  however  shown that cceas are biased toward stability  the evolutionary process tends to converge prematurely to stable states instead of  near  optimal solutions  in this study  we show how novelty search can be used to avoid the counterproductive attraction to stable states in coevolution  novelty search is an evolutionary technique that drives evolution toward behavioural novelty and diversity rather than exclusively pursuing a static objective  we evaluate three novelty based approaches that rely on  respectively  1  the novelty of the team as a whole   2  the novelty of the agents  individual behaviour  and  3  the combination of the two  we compare the proposed approaches with traditional fitness driven cooperative coevolution in three simulated multirobot tasks  our results show that team level novelty scoring is the most effective approach  significantly outperforming fitness driven coevolution at multiple levels  novelty driven cooperative coevolution can substantially increase the potential of cceas while maintaining a computational complexity that scales well with the number of populations  
 object category understanding via eye fixations on freehand sketches the study of eye gaze fixations on photographic images is an active research area  in contrast  the image sub category of freehand sketches has not received as much attention for such studies  in this paper  we analyze the results of a free viewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories  our analysis shows that fixation sequences exhibit marked consistency within a sketch  across sketches of a category and even across suitably grouped sets of categories  this multi level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand drawn object sketches  in this paper  we show that the multi level consistency in the fixation data can be exploited to 1  predict a test sketch s category given only its fixation sequence and 2  build a computational model which predicts part labels underlying fixations on objects  we hope that our findings motivate the community to deem sketch like representations worthy of gaze based studies vis a vis photographic images  
 observing choice of loan methods in not for profit microfinance using data envelopment analysis distributing loan using group lending method is one of the unique features in microfinance  as it utilises peer monitoring and dynamic incentive to lower credit risks in extending collateral free loan to the poor  however  many microfinance institutions  mfis  eventually perceive it to be costly and restricting loan growth thereby resorted to individual lending method to enhance profitability  on the other hand  village banking method was developed to boost outreach and to create self sustaining village microbanks  we thus seek to empirically observe the loan method  efficiency relationship and to examine the best loan method regionally  focusing on not for profit mfis that are widely regarded as best microfinance provider  non oriented data envelopment analysis with regional meta frontier approach is used for efficiency assessment of 628 mfis from 87 countries in 6 regions  followed by tobit regression  we also investigated factors affecting efficiencies such as borrowings  total donation  cost per borrower  cpb   portfolio at risk  par   interest rates  mfi age  regulation status  and legal format  the results support our argument that appropriate performance analysis should best be performed on regional basis separately as we find different results for different region  crown copyright  c  2017 published by elsevier ltd  
 offer acceptance prediction of academic placement the paper examines the validation of prediction models of acceptance of academic placement offers by students in the context of international applications at a large metropolitan australian university using data mining techniques  earlier works in enrolment management have examined various classification problems such as inquiry to enrol  persistence and graduation  the data and settings from different institutions are often different  which implies that in order to find out which models and techniques are applicable at a given university  the dataset from that university needs to be used in the validation effort  the whole dataset from the australian university comprised 24 283 offers made to international applicants from the year 2008 to 2013  every year around 2000 2500 new international students who accept offers of academic placement commence their studies  the important predictors for the acceptance of offers were as follows  the chosen course and faculty  whether the student was awarded any form of scholarship  and also the visa assessment level of the country by the immigration department  prediction models were developed using a number of classification methods such as logistic regression  naive bayes  decision trees  support vector machines  random forests  k nearest neighbour  neural networks and their performances compared  overall  the neural network prediction model with a single hidden layer produced the best result  
 ohman returns  new horizons in the collection and analysis of imaging data in speech production research there have been enormous technical advances in the use of imaging techniques in speech production research in terms of resolution and frame rates  however  a major bottleneck lies in the lack of appropriate data reduction and quantification methods which allow for a parsimonious representation of the high dimensional image data  particularly the rapid increase in frame rates seen in data acquisition makes traditional  error prone methods of contour tracking unwieldy due to the high amount of manual intervention required  we discuss recent developments in methods that obviate contour tracking but instead process the entire image  specifically  we focus on one such approach by demonstrating the application of principal component analysis to ultrasound images  this method not only exploits the information present in the entire image  but it also straightforwardly allows for the representation of the temporal evolution of an utterance by reducing a series of images to a time varying single value representation  in an illustrative example  inspired by the seminal work of ohman  1966   we show how characteristic patterns of coarticulation between vowels and consonants can be captured in this way   c  2017 elsevier ltd  all rights reserved  
 on branching heuristics for the bi objective 0 1 unidimensional knapsack problem this paper focuses on branching strategies that are involved in branch and bound algorithms when solving multi objective optimization problems  the choice of the branching variable at each node of the search tree constitutes indeed an important component of these algorithms  in this work we focus on multi objective knapsack problems  in the literature  branching heuristics used for these problems are static  i e   the order on the variables is determined prior to the execution  this study investigates the benefit of defining more sophisticated branching strategies  we first analyze and compare a representative set of classic branching heuristics and conclude that none can be identified as the best overall heuristic  using an oracle  we highlight that combining branching heuristics within the same branch and bound algorithm leads to considerably reduced search trees but induces high computational costs  based on learning adaptive techniques  we propose then dynamic adaptive branching strategies that are able to select the suitable heuristic to apply at each node of the search tree  experiments are conducted on the bi objective 0 1 unidimensional knapsack problem  
 on characterizing population commonalities and subject variations in brain networks brain networks based on resting state connectivity as well as inter regional anatomical pathways obtained using diffusion imaging have provided insight into pathology and development  such work has underscored the need for methods that can extract sub networks that can accurately capture the connectivity patterns of the underlying population while simultaneously describing the variation of sub networks at the subject level  we have designed a multi layer graph clustering method that extracts clusters of nodes  called  network hubs   which display higher levels of connectivity within the cluster than to the rest of the brain  the method determines an atlas of network hubs that describes the population  as well as weights that characterize subject wise variation in terms of within  and between hub connectivity  this lowers the dimensionality of brain networks  thereby providing a representation amenable to statistical analyses  the applicability of the proposed technique is demonstrated by extracting an atlas of network hubs for a population of typically developing controls  tdcs  as well as children with autism spectrum disorder  asd   and using the structural and functional networks of a population to determine the subject level variation of these hubs and their inter connectivity  these hubs are then used to compare asd and tdcs  our method is generalizable to any population whose connectivity  structural or functional  can be captured via non negative network graphs   c  2015 elsevier b v  all rights reserved  
 on computing probabilities of dismissal of 10b 5 securities class action cases the main goal of this paper is to propose a probability model for computing probabilities of dismissal of 10b 5 securities class action cases filed in united states federal district courts  by dismissal  we mean dismissal with prejudice in response to the motion to dismiss filed by the defendants  and not eventual dismissal after the discovery process  the proposed probability model is a hybrid of two widely used methods  logistic regression  and naive bayes  using a dataset of 925 10b 5 securities class action cases filed between 2002 and 2010  we show that the proposed hybrid model has the potential of computing better probabilities than either lr or nb models  by better  we mean lower root mean square errors of probabilities of dismissal  the proposed hybrid model uses the following features  allegations of generally accepted accounting principles violations  allegations of lack of internal control  bankruptcy filing during the class period  allegations of section 11 violations of securities act of 1933  and short term drop in stock price  our model is useful for those insurance companies which underwrite directors and officers liability policy   c  2016 elsevier b v  all rights reserved  
 on deterministic chaos in software reliability growth models software reliability growth models attempt to forecast the future reliability of a software system  based on observations of the historical occurrences of failures  this allows management to estimate the failure rate of the system in field use  and to set release criteria based on these forecasts  however  the current software reliability growth models have never proven to be accurate enough for widespread industry use  one possible reason is that the model forms themselves may not accurately capture the underlying process of fault injection in software  it has been suggested that fault injection is better modeled as a chaotic process rather than a random one  this possibility  while intriguing  has not yet been evaluated in large scale  modern software reliability growth datasets  we report on an analysis of four software reliability growth datasets  including ones drawn from the android and mozilla open source software communities  these are the four largest software reliability growth datasets we are aware of in the public domain  ranging from 1200 to over 86 000 observations  we employ the methods of nonlinear time series analysis to test for chaotic behavior in these time series  we find that three of the four do show evidence of such behavior  specifically  a multifractal attractor   finally  we compare a deterministic time series forecasting algorithm against a statistical one on both datasets  to evaluate whether exploiting the apparent chaotic behavior might lead to more accurate reliability forecasts   c  2016 elsevier b v  all rights reserved  
 on the dynamical interplay of positive and negative affects emotional disorders and psychological flourishing are the result of complex interactions between positive and negative affects that depend on external events and the subject s internal representations  based on psychological data  we mathematically model the dynamical balance between positive and negative affects as a function of the response to external positive and negative events  this modeling allows the investigation of the relative impact of two leading forms of therapy on affect balance  the model uses a delay differential equation to analytically study the bifurcation diagram of the system  we compare the results of the model to psychological data on a single  recurrently depressed patient who was administered the two types of therapies considered  coping focused versus affect focused   the model leads to the prediction that stabilization at a normal state may rely on evaluating one s emotional state through a historical ongoing emotional state rather than in a narrow present window  the simple mathematical model proposed here offers a theoretical framework for investigating the temporal process of change and parameters of resilience to relapse  
 on the hierarchical nature of partial preferences over lotteries in this work we consider preference relations that might not be total  partial preferences may be helpful to represent those situations where  due to lack of information or vacillating desires  the decision maker would like to maintain different options  alive  and defer the final decision  in particular  we show that  when totality is relaxed  different axiomatizations of classical decision theory are no longer equivalent but form a hierarchy where some of them are more restrictive than others  we compare such axiomatizations with respect to theoretical aspects such as their ability to propagate comparability incomparability over lotteries and the induced topology and to different preference elicitation methodologies that are applicable in concrete domains  we also provide a polynomial time procedure based on the bipartite matching problem to determine whether one lottery is preferred to another  
 on the implications of integrating linear tracing procedure with imprecise probabilities this paper explores the implications of integrating the so called linear tracing procedure with uncertainty modeling using sets of probabilities for equilibrium refinements under strategic uncertainty  we first reexamine the linear tracing procedure  10  by studying the relationship between priors and nash equilibria  a prior belongs to the source set of a nash equilibrium if the linear tracing procedure based on this prior leads to that equilibrium  we show that the source set of any nash equilibrium is always nonempty and closed  but not generally convex  we then motivate the idea of iteratively applying this procedure to the auxiliary games that are used to model hypothetical reasoning under the procedure  based on this idea  we propose a notion of robustness for equilibria that allows for modeling of players  initial expectations in the linear tracing procedure through sets of probability distributions  by considering c contaminated classes for modeling uncertainty  we illustrate this concept with two examples  and then discuss the role of strategic uncertainty in coordination failure   c  2016 elsevier inc  all rights reserved  
 on the mechanical  cognitive and sociable facets of human compliance and their robotic counterparts compliance has become a key requirement for robots meant to interact with humans  it is viewed as a necessary property to increase safety and efficiency in human robot cooperative actions  in humans  compliance takes three dimensions  mechanical  cognitive and social  while robotics has focused primarily on modeling the first two  we here discuss the importance to consider also the social dimension that compliance takes in human human interactions and how this can be extended to human robot interactions  we discuss situations in which requesting that the human complies to the machine may be advantageous  and not the converse  we conclude with a list of open ethical and legal issues that may arise from developing actively non compliant machines   c  2016 elsevier b v  all rights reserved  
 on the role of fairness and limited backward induction in sequential bargaining games new behavioral models and analyses experiments show that in sequential bargaining games     subjects usually deviate from game theoretic predictions  previous explanations have focused on considerations of fairness in the offers  and social utility functions have been formulated to model the data  however  a recent explanation by ho and su  manag  sci  59 2   452 469 2013  for observed deviations from game theoretic predictions in sequential games such as the centipede game is that players engage in limited backward induction  in this article  a suite of new and existing computational models that integrate different choice models with utility functions are comprehensively evaluated on data  these include debruyn and bolton s recursive quantal response with social utility functions  those based on ho and su s dynamic level k  and analogous extensions of the cognitive hierarchy with dynamic components  our comprehensive analysis reveals that in extended with 5 rounds  models that capture violations of backward induction perform better than those that model fairness  however  we did not observe this result for with less rounds  and fairness of the offer remains a key consideration in these games  these findings contribute to the broader observation that non social factors play a significant role in non equilibrium play of sequential games  
 on the statistical properties of operant settings and their contribution to the evaluation of sensitivity to reinforcement when using the matching law in applied settings  a recurring problem is to assess when subjects adjust their responses as a function of their associated reinforcers  specifically  the main concern is to determine whether subjects  behavior are sensitive to reinforcement or not  many researchers have followed  explicitly or implicitly  the criterion that 50  of explained variance is deemed acceptable to consider the subject sensitive  however  it is neither theoretically nor empirically grounded  this article presents a null hypothesis statistical test to assess whether an organism s behavior is sensitive to reinforcement as quantitatively expressed by the matching law  we first introduce the motivation as to why such a test is warranted  formally described the basis of the model used to compute the null hypothesis and then show some of its advantages  we conclude the article with a hypothetical example  
 on the syntax and semantics of virtual linguistic terms for information fusion in decision making the virtual linguistic model is a good technique for linguistic decision making and has been widely used in applications including linguistic information fusion  the main purpose of this paper is to define and specify the syntax and semantics of virtual linguistic terms  vlts  in detail  and then to serve as the theoretical foundation of the computational models based on vlts  the syntactical rule generates vlts by a symbolic transformation  and then the semantic rule presents the semantics of vlts by means of linguistic modifiers  based on the syntax and semantics  vlts could be a possible alternative for solving some current challenges of qualitative information fusion in decision making   c  2016 elsevier b v  all rights reserved  
 one  no one and one hundred thousand events  defining and processing events in an inter disciplinary perspective we present an overview of event definition and processing spanning 25 years of research in nlp  we first provide linguistic background to the notion of event  and then present past attempts to formalize this concept in annotation standards to foster the development of benchmarks for event extraction systems  this ranges from muc 3 in 1991 to the time and space track challenge at semeval 2015  besides  we shed light on other disciplines in which the notion of event plays a crucial role  with a focus on the historical domain  our goal is to provide a comprehensive study on event definitions and investigate which potential past efforts in the nlp community may have in a different research domain  we present the results of a questionnaire  where the notion of event for historians is put in relation to the nlp perspective  
 online algorithm for robots to learn object concepts and language model humans form concept of objects by classifying them into categories  and acquire language by simultaneously interacting with others  thus  the meaning of a word can be learned by connecting a recognized word to its corresponding concept  we consider this ability important for robots to flexibly develop knowledge of language and concepts  in this paper  we propose an online algorithm for robots to acquire knowledge of natural language and learn object concepts  a robot learns the language model from word sequences  which are obtained by the segmentation of phoneme sequences provided by a user  by using unsupervised word segmentation each time it is provided with a new object  moreover  the robot acquires object concepts using these word sequences as well as multimodal information obtained by observing objects  the crucial aspect of our model is the interdependence of words and concepts  there is a high probability that the same words will be uttered to describe objects in the same category  by taking this relationship into account  our proposed method enables robots to acquire a more accurate language model and object concepts online  experimental results verify this  
 online multimodal ensemble learning using self learned sensorimotor representations internal models play a key role in cognitive agents by providing on the one hand predictions of sensory consequences of motor commands  forward models   and on the other hand inverse mappings  inverse models  to realize tasks involving control loops  such as imitation tasks  the ability to predict and generate new actions in continuously evolving environments intrinsically requiring the use of different sensory modalities is particularly relevant for autonomous robots  which must also be able to adapt their models online  we present a learning architecture based on self learned multimodal sensorimotor representations  to attain accurate forward models  we propose an online heterogeneous ensemble learning method that allows us to improve the prediction accuracy by leveraging differences of multiple diverse predictors  we further propose a method to learn inverse models on the fly to equip a robot with multimodal learning skills to perform imitation tasks using multiple sensory modalities  we have evaluated the proposed methods on an icub humanoid robot  since no assumptions are made on the robot kinematic dynamic structure  the method can be applied to different robotic platforms  
 online review helpfulness  impact of reviewer profile image despite the growing number of studies on online reviews  the impact of visual cues on consumer s evaluation of review helpfulness has remained underexplored  it is not yet known whether and how images influence the way online reviews are perceived  this paper introduces and empirically examines the potential effects of reviewer profile image  a photo image displayed next to the reviewer name  on review helpfulness by drawing on the decorative and information functions of images  with a sample of 2178 reviews from mobile gaming applications  we report that reviewer profile image can significantly enhance consumer s evaluation of review helpfulness  whereas there is no differential effect among image types  i e  self  family  or random images   interestingly  the effect of reviewer profile image on review helpfulness is moderated by review length  but not review valence and equivocality  results suggest that reviewer profile image enhances the perception of review helpfulness by serving mainly as a visual decoration that creates affective responses rather than identity information   c  2017 elsevier b v  all rights reserved  
 online to offline  o2o  service recommendation method based on multi  dimensional similarity measurement with the rapid development of information technology  consumers are able to search for and buy services or products online  and then consume them in an offline store  this emerging ecommerce model is called online to offline  o2o  service  which has attracted business and academic attention  the large number of o2o services on the internet creates a scalability problem  creating massive but highly sparse matrices relating customers to items purchased  in this paper  we proposed a novel o2o service recommendation method based on multidimensional similarity measurements  this approach encompasses three similarity measures  collaborative similarity  preference similarity and trajectory similarity  experimental results show that a combination of multiple similarity measures performs better than any one single similarity measure  we also find that trajectory similarity performs better than the rating based similarity metrics  collaborative similarity and preference similarity  in sparse matrices   c  2017 elsevier b v  all rights reserved  
 online word of mouth  implications for the name your own price channel we investigate the impact of increased technology enabled consumer information sharing on the viability  optimal pricing  and profits of a name your own price  nyop  intermediary  the aim of our analysis is  a  to examine the nyop s strategy when faced with inforniation sharing about the threshold price above which offers are accepted   b  to assess how consumers  access to pricing information influences nyop s optimal pricing and profits  and  c  to examine conditions under which the nyop pricing format may be more profitable for an intermediary compared to a posted price format  we investigated these issues using an analytical model for an intermediary operating under two alternative business models  the merchant model and the agency model  our analysis identifies regions in the parameter space where each sales format might be superior to the other  we find that the optimality of the nyop format  relative to posted pricing  depends crucially on both the size of the informed consumers  market segment as well as the magnitude of bid shading  which limits the amount of surplus the nyop intermediary is able to extract from uninformed consumers  of particular note  we also find that  a  the nyop does not benefit from frequently changing the threshold price in an attempt to hinder consumer learning of the bid acceptance price level   b  increased  word of mouth  transmission of information about its undisclosed threshold price does not unambiguously erode the nyop retailer s profits  and  c  consumer valuations need not be positively correlated with haggling costs for the nyop selling strategy to dominate posted pricing format   c  2016 elsevier b v  all rights reserved  
 online offline evolutionary algorithms for dynamic urban green space allocation problems urban planning authorities continually face the problem of optimising the allocation of green space over time in developing urban environments  the problem is essentially a sequential decision making task involving several interconnected and non linear uncertainties  and requires time intensive computation to evaluate the potential consequences of individual decisions  we explore the application of two very distinct frameworks incorporating evolutionary algorithm approaches for this problem   i  an offline  approach  in which a candidate solution encodes a complete set of decisions  which is then evaluated by full simulation and  ii  an online  approach which involves a sequential series of optimisations  each making only a single decision  and starting its simulations from the endpoint of the previous run  we study the outcomes  in each case  in the context of a simulated urban development model  and compare their performance in terms of speed and quality  our results show that the online version is considerably faster than the offline counterpart  without significant loss in performance  
 ontological modelling and rule based reasoning for the provision of personalized patient education current approaches to patient education provide generic standardized materials to all patients regardless of their demographics such as age and cognitive abilities  thus  the effectiveness of this approach may suffer from a patient s motivation to fully engage with the material  to alleviate these concerns  this study proposes a personalized approach to patient education that is tailored to the individual characteristics and health objectives of the patient  personalized features will enhance the comprehensibility and usability of the process of medical education  taking this into consideration  this paper introduces a conceptual architecture to create a web based personalized patient education experience  a key component of this architecture comprises ontological models of the patient themselves  their medical conditions  physical activities and their educational attainments  furthermore  rule based reasoning is also proposed to achieve this personalization  a use case scenario is provided to highlight the effectiveness of personalized education provision  
 operationalizing engagement with multimedia as user coherence with context traditional approaches for assessing user engagement within multimedia environments rely on methods that are removed from the human computer interaction itself  such as surveys  interviews and baselined physiology  we propose a context coherence approach that operationalizes engagement as the amount of independent user variation that covaries in time with multimedia contextual events during unscripted interactions  this can address questions about the features of multimedia users are most engaged and how engaged users are without the need for prescribed interactions or baselining  we assessed the validity of this approach in a psychophysiological study  forty participants played interactive video games  intake and post stimulus questionnaires collected subjective engagement reports that provided convergent and divergent validity criteria to evaluate our approach  estimates of coherence between physiological variation and in game contextual events predicted subjective engagement and added information beyond physiological metrics computed from baselines taken outside of the multimedia context  our coherence metric accounted for task dependent engagement  independent of predispositions  this was not true of a baselined physiological approach that was used for comparison  our findings show compelling evidence that a context sensitive approach to measuring engagement overcomes shortcomings of traditional methods by making best use of contextual information sampled from multimedia in time series analyses  
 opinion summarization methods  comparing and extending extractive and abstractive approaches in the last years  the opinion summarization task has gained much importance because of the large amount of online information and the increasing interest in learning the user evaluation about products  services  companies  and people  although there are many works in this area  there is room for improvement  as the results are far from ideal  in this paper  we present our investigations to generate extractive and abstractive summaries of opinions  we study some well known methods in the area and compare them  besides using these methods  we also develop new methods that consider the main advantages of the ones before  we evaluate them according to three traditional summarization evaluation measures  informativeness  linguistic quality  and utility of the summary  we show that we produce interesting results and that our methods outperform some methods from literature   c  2017 elsevier ltd  all rights reserved  
 optimal bayesian equilibrium for n person credibilistic non cooperative game with risk aversion in this paper  a n person credibilistic non cooperative game with risk aversion is investigated in the fuzzy environment  firstly  optimistic value criterion is applied to modeling players  risk aversion  we define a alpha optimistic nash equilibrium for this games  the existence of alpha optimistic nash equilibrium strategies is proposed  then a bayesian alpha optimistic nash equilibrium is given by assuming alpha to be not common knowledge and its existence theorem is also proved  moreover  we present a sufficient and necessary condition to find the bayesian alpha optimistic nash equilibrium  finally  an example is given to illustrate the usefulness of the proposed game  
 optimal coordination strategy of dynamic supply chain based on cooperative stochastic differential game model under uncertain conditions the optimal coordination strategy is achieved when the dynamic supply chain profit is maximized  the profit of dynamical supply chain is decided by the corresponding effort level of each node enterprise  this paper presents a cooperative stochastic differential game model to research the optimal coordination strategy of a dynamic supply chain under uncertain conditions and studies how to coordinate the effort level of node enterprise to maximize supply chain profit  for this purpose  a cooperative stochastic differential game model of the enterprises  effort level is constructed  and the optimal solution of the model is solved by converting it to the solution of the equivalent stochastic partial differential equation  as a result  the optimal strategy of a local node enterprise is achieved  considering the gap between the independent decision of each node enterprise and the optimal decision of the supply chain system  the coordination mechanisms of the dynamic supply chain  including the temporary static allocation mechanism and the compensation mechanism  are given to narrow the gap to achieve the optimal coordination strategy under uncertain conditions   c  2016 elsevier b v  all rights reserved  
 optimal dealer pricing under transaction uncertainty dealers in securities markets are standing ready immediately to trade certain amounts of securities at stated bid and ask prices  this paper assumes that the amount of transactions follows an uncertain mean reverting process associated with the bid and ask prices  in order to maximize the dealer s total wealth  an optimal dealer pricing model under transaction uncertainty is established  and the optimal bid price and ask price over time are derived  finally  the variations of the optimal bid and ask prices with different parameters are presented  
 optimal distributed interconnectivity of multi robot systems by spatially constrained clustering a spatially constrained clustering algorithm is presented in this paper  this algorithm is a distributed clustering approach to fine tune the optimal distances between agents of the system to strengthen the data passing among them using a set of spatial constraints  in fact  this method will increase interconnectivity among agents and clusters  leading to improvement of the overall communicative functionality of the multi robot system  this strategy will lead to the establishment of loosely coupled connections among the clusters  these implicit interconnections will mobilize the clusters to receive and transmit information within the multi agent system  in other words  this algorithm classifies each agent into the clusters with the lowest cost of local communication with its peers  this research demonstrates that the presented decentralized method will actually boost the communicative agility of the swarm by probabilistic proof of the acquired optimality  hence  the common assumption regarding the full knowledge of the agents  primary locations has been fully relaxed compared to former methods  consequently  the algorithm s reliability and efficiency is confirmed  furthermore  the method s efficacy in passing information will improve the functionality of higher level swarm operations  such as task assignment and swarm flocking  analytical investigations and simulated accomplishments  corresponding to highly populated swarms  prove the claimed efficiency and coherence  
 optimal ordering strategy with service constraints under supply disruptions this paper investigates the impact of service constraints on the retailer s optimal ordering strategy under supply disruptions  considering a single period supply chain that consists of one supplier and one retailer  we first establish a nonbinding profit function model and prove the existence of an optimal order quantity at which the retailer s expected profit can achieve the optimal value  we then formulate the profit model with service constraints  i e  fill rate and service level constraints  respectively  through the analysis  we find that the expected value of the retailer s profit is a convex function of the order quantity and there exists a unique order quantity that makes the retailer s expected profit maximum under each service constraint  in numerical analysis we explain how the retailer can achieve the desired profit by changing the order quantity  
 optimisation of partial collaborative transportation scheduling in supply chain management with 3pl using aco in this paper  we study and analyze the characteristics of transportation vehicle scheduling problem in supply chain management with third party logistics enterprise  first  we subdivide all the transportation nodes into three distinct classifications  a novel partial collaborative transportation scheduling strategy is proposed based on two special kinds of transportation nodes which have integrated the self support vehicle and 3pl vehicle resource  then  depending on the transport mode of each kind of transportation nodes  a modified ant colony optimization with negative selection operation  aco nso  with varying dimension matrix encoding and modified transition probability operation method has been presented  finally  the simulative results demonstrate that the proposed approach is practical and efficient   c  2016 elsevier ltd  all rights reserved  
 optimization of makespan for the distributed no wait flow shop scheduling problem with iterated greedy algorithms the distributed production lines widely exist in modern supply chains and manufacturing systems  this paper aims to address the distributed no wait flow shop scheduling problem  dnwfsp  with the makespan criterion by using proposed iterated greedy  ig  algorithms  firstly  several speed up methods based on the problem properties of dnwfsp are investigated to reduce the evaluation time of neigh borhood with o 1  complexity  secondly  an improved neh heuristic is proposed to generate a promising initial solution  where the iteration step of the insertion step of neh is applied to the factory after inserting a new job  thirdly  four neighborhood structures  i e  critical swap single  critical insert single  critical swap multi  critical insert multi  based on factory assignment and job sequence adjustment are employed to escape from local optima  fourthly  four local search methods based on neighborhood moves are proposed to enhance local searching ability  which contains ls insert critical factory1  ls insert critical factory2  ls swap  and ls insert  finally  to organize neighborhood moves and local search methods efficiently  we incorporate them into the framework of variable neighborhood search  vns   variable neighborhood descent  vnd  and random neighborhood structure  rns   furthermore  three variants of ig algorithms are presented based on the designed vns  vnd and rns  the parameters of the proposed ig algorithms are tuned through a design of experiments on randomly generated benchmark instances  the effectiveness of the initialize phase and local search methods is shown by numerical comparison  and the comparisons with the recently published algorithms demonstrate the high effectiveness and searching ability of the proposed ig algorithms for solving the dnwfsp  ultimately  the best solutions of 720 instances from the well known benchmark set of naderi and ruiz for the dnwfsp are proposed   c  2017 elsevier b v  all rights reserved  
 optimization of water allocation decisions under uncertainty  the case of option contracts for decades  water managers need to allocate water efficiently to competing users due to growing water demand and shortage  water option contract is a flexible management tool for water managers  in this study  with consideration of the uncertainties of probability distributions and intervals  an inexact two stage mixed integer programming  itsmip  model is applied to support the decision making of water allocation when option contracts are proposed  a case study of water allocation with the option contract is presented  factors which influence the willingness of signing the option contract are analysed and the optimum water allocation plans are also provided  the results demonstrate that water option contracts can help water managers mitigate the impact of water shortage and increase economic benefits largely  
 optimizing influence diffusion in a social network with fuzzy costs for targeting nodes over the last decade  the problem of optimizing influence diffusion in a social network has drawn much attention  in this paper  we study the problem of minimizing the complete influence time in a social network where the cost for targeting each individual is with fuzzy uncertainty  by adopting three different decision criteria in the area of uncertain programming  we propose three decision models to characterize the problem we study  in view of the complexity of the problem  we design a hybrid intelligence algorithm to solve models  where fuzzy simulation technologies are integrated with a modified greedy algorithm  finally  numerical experiments are preformed to show the effectiveness of the models and algorithm we propose  
 option pricing with application of levy processes and the minimal variance equivalent martingale measure under uncertainty this paper is dedicated to european option pricing under assumption that the underlying asset follows a geometric levy process  the log price of a primary financial instrument has the form of a sum of a drift component  a brownian component  and a linear combination of time homogeneous poisson processes  modeling jumps in price  in our approach we apply stochastic analysis  especially the change of probability measure techniques  as well as fuzzy sets theory  to obtain the option valuation formulas we use the minimal variance equivalent martingale measure  which requires an advanced analysis of transformation of levy characteristic triplets  we obtain analytical option valuation expressions in crisp case  moreover  we assume that some model parameters are described in an imprecise way and therefore we use their fuzzy counterparts  applying fuzzy arithmetic  we take into account various types of uncertainty on the market  as a result  we obtain the analytical option pricing formulas with fuzzy parameters  we also propose a method of automatized decision making  which utilizes the fuzzy valuation formulas  apart from the general pricing expressions  we provide numerical examples to illustrate our theoretical results  
 oracle inequalities for ranking and u processes with lasso penalty we investigate properties of estimators obtained by minimization of u processes with the lasso penalty in the high dimensional setting  our attention is focused on the ranking problem that is popular in machine learning  it is related to guessing the ordering between objects on the basis of their observed predictors  we prove the oracle inequality for the excess risk of the considered estimator as well as the bound for the l 1  distance   theta  over cap theta   1  between the estimator and the oracle  besides  we study properties of estimators on simulated data sets   c  2017 elsevier b v  all rights reserved  
 oral health promotion program for fostering self management of the elderly living in communities objectives  a program fostering self management for the elderly was implemented and the effects of the program and their continuities were assessed  methods  subjects consisted of 19 males and 131 females  average age  73 1     7 4  range  60 94years   the intervention program consisted of the collective experience learning and private consultation  the collective experience learnings included   1  monitoring the oral condition and practicing oral self care   2  monitoring the oral function and practicing oral exercises  and  3  group discussion on continuing oral self care  outcomes were evaluated at the beginning and the end of the intervention program  and three months after the investigation by the scores in   1  oral self care  2  oral condition  i e   decayed teeth  community periodontal index  cpi   deposits of plaque and dental calculus   3  oral function such as rsst  oral diadochokinesis   4  qol  sf 8 v2  and gohai   and  5  cognitive function  mmse j   informed consent was obtained from all subjects  and this study was approved by the research ethics committee of the university of hyogo  results and discussion  at three months after intervention  124 subjects continued participating and 88 subjects  71   completed all data  on the oral self care  subjects cleaned their teeth more often and longer than before  p   0 001   the use of dental floss and interdental brushing significantly increased in number  p   0 001   and 67 participants  54   visited the dentist during the program  cpi and deposits of plaque were significantly reduced after intervention  p   0 001   the scores of oral function also significantly improved  p   0 001 0 05   the scores of qol  physical health   oral qol and cognitive function significantly improved  p   0 001 0 05   these results suggest that this program not only promotes oral self care  resulting in good oral health conditions  but also improves cognitive functions of the elderly  
 p2p lending survey  platforms  recent advances and prospects p2p lending is an emerging internet based application where individuals can directly borrow money from each other  the past decade has witnessed the rapid development and prevalence of online p2p lending platforms  examples of which include prosper  lendingclub  and kiva  meanwhile  extensive research has been done that mainly focuses on the studies of platform mechanisms and transaction data  in this article  we provide a comprehensive survey on the research about p2p lending  which  to the best of our knowledge  is the first focused effort in this field  specifically  we first provide a systematic taxonomy for p2p lending by summarizing different types of mainstream platforms and comparing their working mechanisms in detail  then  we review and organize the recent advances on p2p lending from various perspectives  e g   economics and sociology perspective  and data driven perspective   finally  we propose our opinions on the prospects of p2p lending and suggest some future research directions in this field  meanwhile  throughout this paper  some analysis on real world data collected from prosper and kiva are also conducted  
 pairs trading strategy optimization using the reinforcement learning method  a cointegration approach recent studies show that the popularity of the pairs trading strategy has been growing and it may pose a problem as the opportunities to trade become much smaller  therefore  the optimization of pairs trading strategy has gained widespread attention among high frequency traders  in this paper  using reinforcement learning  we examine the optimum level of pairs trading specifications over time  more specifically  the reinforcement learning agent chooses the optimum level of parameters of pairs trading to maximize the objective function  results are obtained by applying a combination of the reinforcement learning method and cointegration approach  we find that boosting pairs trading specifications by using the proposed approach significantly overperform the previous methods  empirical results based on the comprehensive intraday data which are obtained from s p500 constituent stocks confirm the efficiently of our proposed method  
 pairwise comparative classification for translator stylometric analysis in this article  we present a new type of classification problem  which we call comparative classification problem  ccp   where we use the term data record to refer to a block of instances  given a single data record with n instances for n classes  the ccp problem is to map each instance to a unique class  this problem occurs in a wide range of applications where the independent and identically distributed assumption is broken down  the primary difference between ccp and classical classification is that in the latter  the assignment of a translator to one record is independent of the assignment of a translator to a different record  in ccp  however  the assignment of a translator to one record within a block excludes this translator from further assignments to any other record in that block  the interdependency in the data poses challenges for techniques relying on the independent and identically distributed  iid  assumption  in the pairwise ccp  pwccp   a pair of records is grouped together  the key difference between pwccp and classical binary classification problems is that hidden patterns can only be unmasked by comparing the instances as pairs  in this article  we introduce a new algorithm  pwc4 5  which is based on c4 5  to manage pwccp  we first show that a simple transformation that we call gradient based transformation  gbt  can fix the problem of iid in c4 5  we then evaluate pwc4 5 using two real world corpora to distinguish between translators on arabic english and french english translations  while the traditional c4 5 failed to distinguish between different translators  gbt demonstrated better performance  meanwhile  pwc4 5 consistently provided the best results over c4 5 and gbt  
 panning for gold  enhancing the precision of sensitivity test data sensitivity tests apply a range of stimulus values to experimental subjects and record binary responses in order to estimate the distribution of threshold values in the subject population  where thresholds delineate responses from nonresponses  in many applications  such as explosives engineering  individual tests are expensive and are conducted in small runs  scarcity of data results in nonexistence of estimates  or estimates with low precision  we discuss various methods  such as combining test runs  covariate analysis  and penalized maximum likelihood  for enhancing precision and mining more gold from expensive test results  
 parameter identifiability in statistical machine learning  a review this review examines the relevance of parameter identifiability for statistical models used in machine learning  in addition to defining main concepts  we address several issues of identifiability closely related to machine learning  showing the advantages and disadvantages of state of  the art research and demonstrating recent progress  first  we review criteria for determining the parameter structure of models from the literature  this has three related issues  parameter identifiability  parameter redundancy  and reparameterization  second  we review the deep influence of identifiability on various aspects of machine learning from theoretical and application viewpoints  in addition to illustrating the utility and influence of identifiability  we emphasize the interplay among identifiability theory  machine learning  mathematical statistics  information theory  optimization theory  information geometry  riemann geometry  symbolic computation  bayesian inference  algebraic geometry  and others  finally  we present a new perspective together with the associated challenges  
 paraphrase focused learning to rank for domain specific frequently asked questions retrieval a frequently asked questions  faq  retrieval system improves the access to information by allowing users to pose natural language queries over an faq collection  from an information retrieval perspective  faq retrieval is a challenging task  mainly because of the lexical gap that exists between a query and an faq pair  both of which are typically very short  in this work  we explore the use of supervised learning to rank to improve the performance of domain specific faq retrieval  while supervised learning to rank models have been shown to yield effective retrieval performance  they require costly human labeled training data in the form of document relevance judgments or question paraphrases  we investigate how this labeling effort can be reduced using a labeling strategy geared toward the manual creation of query paraphrases rather than the more time consuming relevance judgments  in particular  we investigate two such strategies  and test them by applying supervised ranking models to two domain specific faq retrieval data sets  showcasing typical faq retrieval scenarios  our experiments show that supervised ranking models can yield significant improvements in the precision at rank 5 measure compared to unsupervised baselines  furthermore  we show that a supervised model trained using data labeled via a low effort paraphrase focused strategy has the same performance as that of the same model trained using fully labeled data  indicating that the strategy is effective at reducing the labeling effort while retaining the performance gains of the supervised approach  to encourage further research on faq retrieval we make our faq retrieval data set publicly available   c  2017 elsevier ltd  all rights reserved  
 pareto approach for dea cross efficiency evaluation based on interval programming efficiency assessment by using data envelopment analysis  dea  in interval environment is studied  two parameters with regard to the input and output are introduced to characterize the variability of the production possibility sets  the extended production facets with different production possibility sets are determined  the inclusion relation between different extended production facets is discussed  and self evaluation models are constructed to calculate the interval efficiency of the decisionmaking units  dmus  with the optimal production facet  by setting self evaluation as a target  the aggressive and benevolent cross efficiency models are established based on the likelihood between the values of self evaluation and peer evaluation  the analysis of the models yields the interval cross efficiency matrices and the weight allocation method that is more advantageous to the dmu for aggregating the interval cross efficiency matrices  an example is used to illustrate the applications of the models  
 parkinson s disease monitoring by biomechanical instability of phonation patients suffering from parkinson s disease  pd  may be successfully treated pharmacologically and surgically to preserve and even improve their life quality and health conditions  although the progress of the disease cannot be stopped  at least mitigation of the most handicapping symptoms can be achieved  but both pharmacological and surgical treatments require the adequate monitoring of the disease stage of progress and the effects of treatment  several techniques have been proposed for pd evolution monitoring  ranging from subjective auto evaluation by questionnaires  or from gait and handwriting examination by specialists  nevertheless  these techniques present certain difficulties  which make frequent evaluation impractical  on the other hand  it is known that speech acoustic analysis may estimate indicators of patient s conditions  and can be implemented for a frequent evaluation protocol  and under minimal help  it can be carried out at distance using communication technologies  the acoustic analysis  may be based on mel cepstral coefficients  distortion features as jitter  shimmer  harmonic to noise contents  or pitch perturbation estimates  among others  phonation biomechanical parameter and tremor estimates are also good markers of pd  the present work proposes a combination of biomechanical features to predict pd progress using bayesian likelihood estimation  this methodology proves to be very sensitive and allows a three band based comparison  pre treatment versus post treatment in reference to a control subject or a normative population  results from a study are presented  including eight patients recorded on a 4 week separation interval  meanwhile they were treated with medication  physical exercising and speech therapy  the conclusions show that certain distortion  biomechanical and tremor features are of special relevance to monitor pd phonation  and that they can be used as evolution markers   c  2017 elsevier b v  all rights reserved  
 parliamentary voting procedures  agenda control  manipulation  and uncertainty we study computational problems for two popular parliamentary voting procedures  the amendment procedure and the successive procedure  they work in multiple stages where the result of each stage may influence the result of the next stage  both procedures proceed according to a given linear order of the alternatives  an agenda  we obtain the following results for both voting procedures  on the one hand  deciding whether one can make a specific alternative win by reporting insincere preferences by the fewest number of voters  the coalitional manipulation problem  or whether there is a suitable ordering of the agenda  the agenda control problem  takes polynomial time  on the other hand  our experimental studies with real world data indicate that most preference profiles cannot be manipulated by only few voters and a successful agenda control is typically impossible  if the voters  preferences are incomplete  then deciding whether an alternative can possibly win is np hard for both procedures  whilst deciding whether an alternative necessarily wins is conp hard for the amendment procedure  it is polynomial time solvable for the successive procedure  
 parsing argumentation structures in persuasive essays in this article  we present a novel approach for parsing argumentation structures  we identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures  the proposed model globally optimizes argument component types and argumentative relations using integer linear programming  we show that our model significantly outperforms challenging heuristic baselines on two different types of discourse  moreover  we introduce a novel corpus of persuasive essays annotated with argumentation structures  we show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement  
 partial identification in statistical matching with misclassification the main target of statistical matching is to make inference on variables observed in different sources by using information on common variables  the partial information generally implies that the model is not identifiable  the aim of this paper is to study the case when common variables are affected by a misclassification  the partially identifiable region  that is the class of probabilities extending the conditional probabilities obtained by the information in different sources  is determined  these regions are determined in the general case and under some specific restrictions on the misclassification mechanism  an application to real data is used to show in practice the results achieved in the paper   c  2017 elsevier inc  all rights reserved  
 particle swarm optimization algorithm with environmental factors for clustering analysis in view of the model of bird flocking  particle swarm optimization  pso  is a promising method to tackle optimization  this study benefits from the fact that the living environment affects behaviors of the bird flocking  that is  a well defined environmental factor can improve the performance of pso  thus  the environment factor is taken into account to inspire the robust behaviors of bird flocking in depth  specifically  it not only can carry out effective searching in limited flying space  but also can strengthen the social behaviors of individual  in the field of clustering  it can be regarded as a search optimization issue  like the utilization of some useful information generated in its process  environment factor is considered and environment factor inspired pso  epso  is proposed in this study  to take full advantage of epso for solving issue of clustering  we divide its process into two stages  in the first stage  the environment factor is imported as a refined search technology to achieve the multi local optimums with high probability  in the second stage  the manifold information  i e   individual  swarm and environment factors  is considered to improve its global search capacity  such an approach can effectively overcome the defect of pso being prone to being trapped in local optima  to demonstrate the validity of our approach  epso  conventional pso  genetic algorithm  k means  artificial bee colony and hybrid abc are compared with benchmark document collections  the experiment results indicate that epso performs better than these state of the art clustering algorithms in most cases  
 patient flow prediction via discriminative learning of mutually correcting processes over the past decade  the rate of care unit  cu  use in the united states has been increasing  with an aging population and ever growing demand for medical care  effective management of patients  transitions among different care facilities will prove indispensible for shortening the length of hospital stays  improving patient outcomes  allocating critical care resources  and reducing preventable re admissions  in this paper  we focus on an important problem of predicting the so called  patient flow  from longitudinal electronic health records  ehrs   which has not been explored via existing machine learning techniques  by treating a sequence of transition events as a point process  we develop a novel framework for modeling patient flow through various cus and jointly predicting patients  destination cus and duration days  instead of learning a generative point process model via maximum likelihood estimation  we propose a novel discriminative learning algorithm aiming at improving the prediction of transition events in the case of sparse data  by parameterizing the proposed model as a mutually correcting process  we formulate the estimation problem via generalized linear models  which lends itself to efficient learning based on alternating direction method of multipliers  admm   furthermore  we achieve simultaneous feature selection and learning by adding a group lasso regularizer to the admm algorithm  additionally  for suppressing the negative influence of data imbalance on the learning of model  we synthesize auxiliary training data for the classes with extremely few samples  and improve the robustness of our learning method accordingly  testing on real world data  we show that our method obtains superior performance in terms of accuracy of predicting the destination cu transition and duration of each cu occupancy  
 patterns of business intelligence systems use in organizations business intelligence  bi  is often used as the umbrella term for large scale decision support systems  dss  in organizations  bi is currently the largest area of it investment in organizations and has been rated as the top technology priority by cios worldwide for many years  the most important use patterns in decision support are concerned with the type of decision to be supported and the type of manager that makes the decision  the seminal gorry and scott morton mis dss framework remains the most popular framework to describe these use patterns  it is widely believed that dss theory like this framework can be transferred to bi  this paper investigates bi systems use patterns using the gorry and scott morton framework and contemporary decision making theory from behavioral economics  the paper presents secondary case study research that analyzes eight bi systems and 86 decisions supported by these systems  based on the results of the case studies a framework to describe bi use patterns is developed  the framework provides both a theoretical and empirically based foundation for the development of high quality bi theory  it also provides a guide for developing organizational strategy for bi provision  the framework shows that enterprise and smaller functional bi systems exist together in an organization to support different decisions and different decision makers  the framework shows that personal dss theory cannot be applied to bi systems without specific empirical support   c  2017 elsevier b v  all rights reserved  
 perception of localized features during robotic sensorimotor development the understanding of concepts related to objects are developed over a long period of time in infancy  this paper investigates how physical constraints and changes in visual perception impact on both sensorimotor development for gaze control  as well as the perception of features of interesting regions in the scene  through a progressive series of developmental stages  simulating ten months of infant development  this paper examines feature perception toward recognition of localized regions in the environment  results of two experiments  conducted using the icub humanoid robot  indicate that by following the proposed approach a cognitive agent is capable of scaffolding sensorimotor experiences to allow gradual exploration of the surroundings and local region recognition  in terms of low level feature similarities  in addition  this paper reports the emergence of vision related phenomena that match human behaviors found in the developmental psychology literature  
 perceptual category learning and visual processing  an exercise in computational cognitive neuroscience the field of computational cognitive neuroscience  ccn  builds and tests neurobiologically detailed computational models that account for both behavioral and neuroscience data  this article leverages a key advantage of ccn   namely  that it should be possible to interface different ccn models in a plug and play fashion   to produce a new and biologically detailed model of perceptual category learning  the new model was created from two existing ccn models  the hmax model of visual object processing and the covis model of category learning  using bitmap images as inputs and by adjusting only a couple of learning rate parameters  the new hmax covis model provides impressively good fits to human category learning data from two qualitatively different experiments that used different types of category structures and different types of visual stimuli  overall  the model provides a comprehensive neural and behavioral account of basal ganglia mediated learning   c  2017 elsevier ltd  all rights reserved  
 performance optimization of integrated resilience engineering and lean production principles this paper conducts performance assessment from integrated resilience engineering  ire  and lean production points of view  this is the first study that evaluates the impact of integrated resilience engineering  ire  on lean production principles  second  this study considers integrated impact of lean production by a unique intelligent algorithm  the proposed algorithm is composed of radial basis function  rbf   multi layer perceptron  mlp  and adaptive neuro fuzzy inference system  anfis   moreover  the algorithm is capable of handling both crisp and fuzzy data due to the existence of intelligent approach  the proposed algorithm is equipped with verification and validation mechanism through conventional regression  statistical methods and data envelopment analysis  to demonstrate the applicability of the study  a real world pipe manufacturer is considered as our case study  the results showed that  pull system  and  fault tolerant  among lean and ire factors  respectively have been implemented inappropriately  while other factors are either suitably executed or ineffective   c  2017 elsevier ltd  all rights reserved  
 personal web revisitation by context and content keywords with relevance feedback getting back to previously viewed web pages is a common yet uneasy task for users due to the large volume of personally accessed information on the web  this paper leverages human s natural recall process of using episodic and semantic memory cues to facilitate recall  and presents a personal web revisitation technique called webpageprev through context and content keywords  underlying techniques for context and content memories  acquisition  storage  decay  and utilization for page re finding are discussed  a relevance feedback mechanism is also involved to tailor to individual s memory strength and revisitation habits  our 6 month user study shows that   1  compared with the existing web revisitation tool memento  history list searching method  and search engine method  the proposed webpageprev delivers the best re finding quality in finding rate  92 10 percent   average f1 measure  0 4318   and average rank error  0 3145    2  our dynamic management of context and content memories including decay and reinforcement strategy can mimic users  retrieval and recall mechanism  with relevance feedback  the finding rate of webpageprev increases by 9 82 percent  average f1 measure increases by 47 09 percent  and average rank error decreases by 19 44 percent compared to stable memory management strategy  among time  location  and activity context factors in webpageprev  activity is the best recall cue  and context content based re finding delivers the best performance  compared to context based re finding and content based re finding  
 personalising game difficulty to keep children motivated to play with a social robot  a bayesian approach for effective child education  playing games with a social robot should be motivating for a longer period of time  one aspect that can affect the motivation of a child is the difficulty of a game  the game should be perceived as challenging  while at the same time  the child should be confident to meet the challenge  we designed a user modelling module that adapts the difficulty of a game to the child s skill level  in order to provide children with the optimal challenge  this module applies a bayesian rating method that estimates the child s skill and game item s difficulty levels to personalise the game progress  in an experiment with 22 children  aged between 10 and 12 years old   we tested whether the personalisation leads to a higher motivation to play with the robot  although the personalised system did not challenge the participants optimally  this study shows that the bayesian rating system is in principle able to measure the skill and performance of children in playing a game with a robot  even without accurate estimates of the difficulty of items   we outline multiple ways in which the rating method and module can be used to further personalise and enhance the child robot interaction  other than adapting the difficulty of games  e g  by adapting the dialogue and feedback    c  2016 elsevier b v  all rights reserved  
 personality affected robotic emotional model with associative memory for human robot interaction the decision making process in communication is affected by internal and external factors from dynamic environments  humans can perform a variety of behaviors in a similar situation  unlike robots  this paper discusses human psychological phenomena during communication from the point of view of internal and external factors  such as perception  memory  and emotional information  based on these  we introduce the personality affected robotic emotional model and the emotion affected associative memory model for the robot  we organize an interactive robot system to provide suitable decisions for the robot  results from interactive communication experiments indicate that the robot is able to perform different actions based on internal and external factors   c  2017 elsevier b v  all rights reserved  
 personality based refinement for sentiment classification in microblog microblog has become one of the most widely used social media for people to share information and express opinions  as information propagates fast in social network  understanding and analyzing public sentiment implied in user generated content is beneficial for many fields and has been applied to applications such as social management  business and public security  most previous work on sentiment analysis makes no distinctions of the tweets by different users and ignores the diverse word use of people  as some sentiment expressions are used by specific groups of people  the corresponding textual sentiment features are often neglected in the analysis process  on the other hand  previous psychological findings have shown that personality influences the ways people write and talk  suggesting that people with same personality traits tend to choose similar sentiment expressions  inspired by this  in this paper we propose a method to facilitate sentiment classification in microblog based on personality traits  to this end  we first develop a rule based method to predict users  personality traits based on the most well studied personality model  the big five model  in order to leverage more effective but not widely used sentiment features  we then extract those features grouped by different personality traits and construct personality based sentiment classifiers  moreover  we adopt an ensemble learning strategy to integrate traditional textual feature based and our personality based sentiment classification  experimental studies on chinese microblog dataset show the effectiveness of our method in refining the performance of both the traditional and state of the art sentiment classifiers  our work is among the first to explicitly explore the role of user s personality in social media analytics and its application in sentiment classification   c  2017 elsevier b v  all rights reserved  
 philosophical foundations of partial belief models this paper is an attempt to put forward a new kind of partial model for representing belief states  i first introduce some philosophical motivations for working with partial models  then  i present the standard  total  model proposed by hintikka  and the partial models studied by humberstone and holliday  i then show how to reduce hintikka s semantics in order to obtain a partial model which  however  differs from humberstone s and holliday s  the nature of such differences is assessed  and i provide motivations for using the newly proposed semantics rather than the existing ones  finally  i review some promising philosophical applications of the ideas developed throughout the discussion   c  2016 elsevier b v  all rights reserved  
 phonetic realization of english lexical stress by native  l1  bengali speakers compared to native  l1  english speakers english lexical stress is acoustically related to combination of duration  intensity  fundamental frequency  f 0  and vowel quality  at phonetic level  the current study investigates l1 bengali speakers acoustic manipulation of english lexical stress to produce english lexical stress contrast  this study compares the use of these correlates in the production of english lexical stress contrasts by 10 l1 english and 20 l1 bengali speakers  the result showed that  although l1 bengali speakers did use all four acoustic correlates to distinguish stressed from unstressed syllables  they produced significantly less native like stress patterns  in particular  there was a significant difference in formant patterns across speaker groups  where l1 bengali speakers produced english like vowel reduction in some unstressed syllables  but in other cases  l1 bengali speakers had tendency to either not reduce or incorrectly reduce vowels in unstressed syllables  the results suggest that l1 bengali speakers production of english lexical stress contrast is influenced by l1 language experience and l1 phonology   c  2017 elsevier ltd  all rights reserved  
 phonetisaurus  exploring grapheme to phoneme conversion with joint n gram models in the wfst framework this paper provides an analysis of several practical issues related to the theory and implementation of grapheme to phoneme  g2p  conversion systems utilizing the weighted finite state transducer paradigm  the paper addresses issues related to system accuracy  training time and practical implementation  the focus is on joint n gram models which have proven to provide an excellent trade off between system accuracy and training complexity  the paper argues in favor of simple  productive approaches to g2p  which favor a balance between training time  accuracy and model complexity  the paper also introduces the first instance of using joint sequence rnnlms directly for g2p conversion  and achieves new state of the art performance via ensemble methods combining rnnlms and n gram based models  in addition to detailed descriptions of the approach  minor yet novel implementation solutions  and experimental results  the paper introduces phonetisaurus  a fully functional  flexible  open source  bsd licensed g2p conversion toolkit  which leverages the openfst library  the work is intended to be accessible to a broad range of readers  
 physical scaffolding accelerates the evolution of robot behavior in some evolutionary robotics experiments  evolved robots are transferred from simulation to reality  while sensor motor data flows back from reality to improve the next transferral  we envision a generalization of this approach  a simulation to reality pipeline  in this pipeline  increasingly embodied agents flow up through a sequence of increasingly physically realistic simulators  while data flows back down to improve the next transferral between neighboring simulators  physical reality is the last link in this chain  as a first proof of concept  we introduce a two link chain  a fast yet low fidelity  lo fi  simulator hosts minimally embodied agents  which gradually evolve controllers and morphologies to colonize a slow yet high fidelity  hi fi  simulator  the agents are thus physically scaffolded  we show here that  given the same computational budget  these physically scaffolded robots reach higher performance in the hi fi simulator than do robots that only evolve in the hi fi simulator  but only for a sufficiently difficult task  these results suggest that a simulation to reality pipeline may strike a good balance between accelerating evolution in simulation while anchoring the results in reality  free the investigator from having to prespecify the robot s morphology  and pave the way to scalable  automated  robot generating systems  
 physiological responses to affective tele touch during induced emotional stimuli the human touch has long been recognized to promote physical  emotional  social  and spiritual comfort  there are situations  however  when touch cannot be exchanged  although mobile phones and web based communication are ubiquitous  touch a communication modality that conveys powerful messages is inexistent in modern communications media  this paper describes a tele touch device that transfers affective touch to another person through the internet  commands for vibration  warmth  and tickle were sent over the internet to a haptic device at the subjects  forearm  with a heart rate  hr  monitor and a galvanic skin response  gsr  sensor  the physiological effect of the tele touch device was evaluated as the subjects watched an emotionally laden movie  we compared these to one group of subjects who were touched by their spouse or girlfriend and to subjects of a control group where no touch was provided  results show that the hr of the subjects with the tele touch device was not significantly different from those subjects who were touched by their loved ones  these results were in contrast to the subjects who were not provided with any form of touch  on the other hand  the gsr results revealed that all the three touch conditions were different from one another  
 pinning down polysemy  a formalisation for a brazilian portuguese preposition this paper presents a formal definition for a number of distinct cases where the preposition  em   in  can be applied in brazilian portuguese  the aim of this work is to establish a mathematical model using qualitative spatial reasoning formalisms within the idea of precisification from supervaluation semantics  our long term goal is to implement this model into artificial intelligent systems to allow for seamless communication with humans in a common speech   c  2016 elsevier b v  all rights reserved  
 plausibility validation of a decision making model using subjects  explanations of decisions the purpose of this work is to present a procedure to validate the cognitive plausibility of decision making models generated from a knowledge based computational modeling method  in order to probe the plausibility of the models  this study compared the explanations given by participants and models when they both make the same decision throughout the iowa gambling task  the procedure used in the comparison is based on the average of the positions of the concepts identified in the participant s explanation in an importance ordered list of concepts obtained from the model  the results demonstrate a close relation between the knowledge contained in both kinds of explanations   c  2017 elsevier b v  all rights reserved  
 plausible reasoning and plausibility monitoring in language comprehension in psychological research on language comprehension  so called epistemic stroop effects illustrate how implausible information can interfere with human action decisions  i e   actions with positive goals can be delayed after implausible information  and vice versa  the basic assumption here is that humans reason from suitable situation models that are built upon background beliefs  in this paper  we present formal models that are apt to simulate cognitive processes that are relevant for language comprehension and these epistemic stroop effects  since background knowledge is crucial for the situation model  we use the inductive methods of c representation and c revision that are capable of processing explicit  conditional  knowledge bases to make plausible reasoning in the experimental tasks transparent  we argue that the delays in response time are partially caused by belief revision processes which are necessary to overcome the mismatch between plausible context  or background resp  world  knowledge and implausible target words  we also present first tentative results that different types of knowledge may induce different processing patterns   c  2017 elsevier inc  all rights reserved  
 playing counter strike versus running  the impact of leisure time activities and cortisol on intermediate term memory in male students the everyday life of students is characterized by hours of learning in order to pass exams  after learning they tend to opt for an occupation that provides them with a great deal of entertainment  it is obvious that it would be advantageous if the chosen activity had a positive impact on memory consolidation  due to the circumstance that such activities can lead to stress and that memory is affected by stress we wanted to look at these coherences  we examined the effect of two different common leisure time activities on cortisol and memory to be able to formulate recommendations for society  for this purpose  a group was tested before and after playing a violent computer game while the second group was tested before and after running  in addition  a control group was set up  salivary cortisol was measured at the beginning  during  and at the end of the experiment  our data demonstrates that running increases cortisol levels and  performed immediately after a learning period  facilitates memorization of neutral information  in contrast  playing a violent computer game tends to impair memorization  the results of the present study have practical implications for the choice of recreational activities in the context of learning   c  2016 elsevier b v  all rights reserved  
 point sensitive aggregation operators  functional equations and applications to social choice in this paper we study aggregation operators that are point sensitive which means that their values depend on the point where the functions to be aggregated are defined  as well as on the values of those functions at that point  this analysis gives rise to consider several functional equations that appear in a natural way  further applications in mathematical social choice also appear as a by product  in particular  we characterize the representation of certain social choice rules by means of specific numerical functions  
 portfolio model for analyzing human resources  an approach based on neuro fuzzy modeling and the simulated annealing algorithm this paper presents a new model for developing a human resources portfolio based on a neuro fuzzy approach  the adaptive neural network is constructed based on the boston consulting group  bcg  portfolio matrix  the adaptive neural network was established by applying the simulated annealing algorithm  the model enables decision makers to evaluate and assess human resources potential in accordance with the environment and its circumstances  the purpose of creating this model is to enable insight into the existing potential and plan assets to improve and promote the employees  potential in a company  the model allows the priorities of the suggested strategies to be defined  which eliminates one of the flaws of the classic bcg portfolio matrix  in this neuro fuzzy model the input variables are described using fuzzy sets that are represented by gaussian functions  using expert reasoning a unique knowledge base is formed which enables employees to be scheduled by strategies  the portfolio model is tested in a realistic industrial environment   c  2017 elsevier ltd  all rights reserved  
 portfolio optimization using novel co variance guided artificial bee colony algorithm although the use of evolutionary algorithms and fuzzy logic for portfolio optimization is an established research area  this field remains fascinating because of its important financial aspects  the field is brisk and it trances as there always remain research issues which are yet to explore  the problem of portfolio optimization comprises of finding an optimal distribution of funds among various available securities so as to maximize the return and minimize the risk  artificial bee colony  abc  is one of the effectual and widely used optimization technique based on swarm intelligence  mixing co variance principles with abc algorithm assists in quick convergence with more precision  this paper presents a novel co variance guided artificial bee colony algorithm for portfolio optimization  as portfolio optimization consists of simultaneous optimization of multiple conflicting objectives  this algorithm is named as multi objective co variance based abc  m cabc   the efficacy of the proposed algorithm is tested on benchmark problems of portfolio optimization from the or library  the results validate the adept performance of the proposed algorithm in finding various optimal trade off solutions simultaneously handling realistic constraints  the article concludes with exhaustive post result analysis and observatory remarks to bring out some of the crucial properties of optimal portfolios  
 pos tagging arabic texts  a novel approach based on ant colony the specificities of the arabic language  mainly agglutination and vocalization make the task of pos tagging more difficult than for indo european languages  consequently  pos tagging texts with good accuracy remains a challenging problem for arabic language processing applications  in this work  we consider the task of pos tagging as an optimization problem modeled as a graph whose nodes correspond to all possible grammatical tags given by a morphological analyzer for words in a sentence and the goal is to find the best path  sequence of tags  in this graph  to resolve this problem  we propose a novel approach based on ant colony  ant colony based algorithms are among the most efficient methods to resolve optimization problems modeled as a graph  the collaboration of ants having various knowledge creates a collective intelligence and increases efficiency  we have performed experiments on both vocalized and non vocalized texts and tested two different tagsets containing fine and coarse grained composite tags  the obtained results showed good accuracy rates and hence  the benefits of swarm intelligence for the pos tagging problem  
 positional scoring based allocation of indivisible goods we define a family of rules for dividing m indivisible goods among agents  parameterized by a scoring vector and a social welfare aggregation function  we assume that agents  preferences over sets of goods are additive  but that the input is ordinal  each agent reports her preferences simply by ranking single goods  similarly to positional scoring rules in voting  a scoring vector consists of m nonincreasing  nonnegative weights  where is the score of a good assigned to an agent who ranks it in position i  the global score of an allocation for an agent is the sum of the scores of the goods assigned to her  the social welfare of an allocation is the aggregation of the scores of all agents  for some aggregation function such as  typically  or   the rule associated with s and maps a profile to  one of  the allocation s  maximizing social welfare  after defining this family of rules  and focusing on some key examples  we investigate some of the social choice theoretic properties of this family of rules  such as various kinds of monotonicity  and separability  finally  we focus on the computation of winning allocations  and on their approximation  we show that for commonly used scoring vectors and aggregation functions this problem is np hard and we exhibit some tractable particular cases  
 positional voting rules generated by aggregation functions and the role of duplication in this paper  we consider a typical voting situation where a group of agents show their preferences over a set of alternatives  under our approach  such preferences are codified into individual positional values  which can be aggregated in several ways through particular functions  yielding positional voting rules and providing a social result in each case  we show that scoring rules belong to such class of positional voting rules  but if we focus our interest on owa  ordered weighted averaging  operators as aggregation functions  other well known voting systems naturally appear  in particular  we determine those ones verifying duplication  i e   clone irrelevance  and present a proposal of an overall social result provided by them   c  2017 wiley periodicals  inc  
 positive and negative behavioral analysis in social networks use of online social networks has grown dramatically since the first web 2 0 technologies were deployed in the early 2000s  our ability to capture user data  in particular behavioral data has grown in concert with increased use of these social systems  in this study  we survey methods for modeling and analyzing online user behavior  we focus on negative behaviors  social spamming and cyberbullying  and mitigation techniques for these behaviors  we also provide information on the interplay between privacy and deception in social networks and conclude by looking at trending and cascading models in social media   c  2017 john wiley   sons  ltd 
 possibilistic risk aversion in group decisions  theory with application in the insurance of giga investments valued through the fuzzy pay off method giga investments are large industrial investments that have long construction times  long economic lives  and that are to a large degree irreversible  these characteristics make ex ante analysis of giga investment difficult and require methods that can consider estimation of imprecision and the value of managerial flexibility  fuzzy pay off method is a recently introduced profitability analysis tool  based on using managerial cash flow scenarios estimated by a group of experts  to form a fuzzy  possibilistic  pay off distribution for an investment that is compatible with the requirements set by the circumstances surrounding giga investments  due to their large size  most giga investments require external financing and most often lead investors must acquire insurance coverage to bring down the idiosyncratic risk of these projects to attract funding  defining an insurance strategy requires an estimation of the risk as perceived by the group of experts  managers   who are supposed to be risk averse  in this vein  this paper analyzes the effect of the risk aversion in the possibilistic setting  relevant to giga investments  and in a multi expert decision making context  insurance pricing for a giga investment risk is reached through finding an optimal coinsurance rate in the group possibilistic framework  the presented models are new and contribute to the project insurance pricing literature  
 predicting beijing s tertiary industry with an improved grey model in the context of the growth slowdown in china  it is important to accurately forecast the future economic trend to guide policy makers the direction of adjusting their current economic policies  in this paper  we intend to predict beijing s tertiary industry  whose datasets are small  irregular and non stationary  leading to a difficulty of building an accurate prediction model  to this end  we present an improved grey model  named prgm 1 1   which extends the grey prediction model by integrating two techniques  i e   the particle swarm optimization algorithm for parameter optimization and the exponential preprocessing method for data cleaning  the experimental results show that prgm 1 1  outperforms other variants of the grey prediction model in predicting beijing s tertiary industry  and is viable to do reasonable prediction over short and fluctuated economic data sequences  in addition  we employ prgm 1 1  in the economic prediction of beijing s tertiary industry in the next five years  and conclude that the growth rate will decelerate  our prediction result seems to be in line with the economic slowdown in china this year   c  2017 elsevier b v  all rights reserved  
 predicting the helpfulness of online reviews using a scripts enriched text regression model in this paper  we examine the utility of script analysis for predicting the helpfulness of online customer reviews  we employ the lens of cognitive scripts and posit that people share a cognitive script for what constitutes a helpful review in a given domain  conceptually  a script includes the salient elements that readers look for before determining whether a review is helpful  to operationalize the construct of cognitive script  we seek the help of human annotators and ask them to highlight phrases that they believe are important for determining review helpfulness  the words in the annotated phrases are collected and become part of the script lexicon for a given domain  the lexicon entries represent the shared conception of essential elements  which are key to the evaluation of review helpfulness  we employ the words in the script lexicon as features in a text regression model to predict review helpfulness  furthermore  we develop and empirically validate a new approach for combining script analysis and dimension reduction  the purpose of the study is to propose a new method to predict review helpfulness and to evaluate the effectiveness and efficiency of the scripts enriched model  to demonstrate the efficacy of the scripts enriched model  we compare it with benchmark models   a baseline model and a bag of words  bow  model  the results show that the scripts enriched text regression model not only produces the highest accuracy  but also the lowest training  testing  and feature selection times   c  2016 elsevier ltd  all rights reserved  
 predicting the occurrence of adverse events using an adaptive neuro fuzzy inference system  anfis  approach with the help of anfis input selection this study presents an adaptive neuro fuzzy inference system  anfis  approach performed to estimate the number of adverse events where the dependent variables are adverse events leading to four types of variables  number of people killed  wounded  hijacked and total number of adverse events  fourteen infrastructure development projects were selected based on allocated budgets values at different time periods  population density  and previous month adverse event numbers selected as independent variables  firstly  number of independent variables was reduced by using anfis input selection approach  then  several anfis models were performed and investigated for afghanistan and the whole country divided into seven regions for analysis purposes  performances of models were assessed and compared based on the mean absolute errors  the difference between observed and estimated value was also calculated within range with values around 90    we included multiple linear regression  mlr  model results to assess the predictive power of the anfis approach  in comparison to a traditional statistical approach  when the model accuracy was calculated according to the performance metrics  anfis showed greater predictive accuracy than mlr analysis  as indicated by experimental results  as a result of this study  we conclude that anfis is able to estimate the occurrence of adverse events according to economical infrastructure development project data  
 predicting the relationships between virtual enterprises and agility in supply chains in the recent advanced information communications and technology  ict  era  collaborating virtually and temporarily in supply chains  scs  to receive mutual benefits such as agility while sharing resources and information becomes an important strategy for enterprises that seek to increase their competitiveness and to optimise their processes and resource usage  as a dynamic and temporary form of alliance from the resource perspective  virtual enterprises  ves  may contribute network resource heterogeneity and sustain competitive advantage  in addition  agility is suggested as a rare  valuable  network resource that is difficult to imitate and that cannot easily be substituted by other attributes  although many researchers have investigated ves and their agility  the research pays less attention to the relationship between ves and agility in complex sc situations  this paper therefore investigates the relationship between ve and agility in scs  ascs  and explores drivers and enablers of agility and outcomes  to clarify the relationships between factors a structural equation model  sem  is adopted to examine the model fit according to the measurement variables and supporting hypotheses  the results provide rich empirical evidence of the beneficial impact of ves on ascs  and theoretical and managerial insights that can be used to strengthen the drivers  enablers and capabilities to enhance the effectiveness of ve collaboration in ascs in a global and dynamic context  also  the analysis results can aid a decision maker which ones of the factors are the important ones that he or she should devote more resources and efforts on   c  2017 elsevier ltd  all rights reserved  
 prediction from regional angst   a study of nfl sentiment in twitter using technical stock market charting to predict nfl game outcomes  we examine the application of technical stock market techniques to sentiment gathered from social media  from our analysis we found a  14 84 average return per sentiment based wager compared to a  12 21 average return loss on the entire 256 games of the 2015 2016 regular season if using an odds only approach  we further noted that wagers on underdogs  i e   the less favored teams  that exhibit a  golden cross  pattern in sentiment  e g   the most recent sentiment signal crosses the longer baseline sentiment   netted a  48 18 return per wager on 41 wagers  these results show promise of cross domain research and we believe that applying stock market techniques to sports wagering may open an entire new research area   c  2017 elsevier b v  all rights reserved  
 prediction of technical efficiency and financial crisis of taiwan s information and communication technology industry with decision tree and dea this study aims to analyze the business performance and technical efficiency of taiwan s ict industry with the malmquist productivity index of data envelopment analysis  the regression method is used to verify the influence of ict industry labor input and research input on yield  the tested units include taiwan companies among the top 250 companies of the ict industry in the world  as well as companies with great contributions to the ict industry in taiwan  for a total of 16 objects  using the data mining classification method  the usage variables in the financial crisis model are utilized to predict whether the technology of the tested units is efficient  the research results are as follows  first  during the test period  3  3 and 6 companies have  respectively  increasing returns  constant returns  and decreasing returns to scale  this suggests that in taiwan s ict industry  only 3 companies can continuously grow to seek the maximum benefit  while 6 are in a period with stable efficiency  and 6 are in a period with declined operating efficiency and increasing costs  second  labor input has a significant positive correlation with yield  and the influence of labor input on yield is relative to other inputs  the influence of r d on yield has a positive but insignificant correlation  which is different from previous research  this is because the effect of research input is not relatively obvious  thus requiring to increase the items to be input  third  by combining two different variables  this study uses the financial crisis precaution model and data envelopment model to predict technical inefficiency before and after the financial tsunami of 2008  no matter whether before or after the financial tsunami of 2008  the financial crisis precaution model is more accurate than the data envelopment model in terms of prediction of technical inefficiency  
 predictive biometrics  a review and analysis of predicting personal characteristics from biometric data interest in the exploitation of soft biometrics information has continued to develop over the last decade or so  in comparison with traditional biometrics  which focuses principally on person identification  the idea of soft biometrics processing is to study the utilisation of more general information regarding a system user  which is not necessarily unique  there are increasing indications that this type of data will have great value in providing complementary information for user authentication  however  the authors have also seen a growing interest in broadening the predictive capabilities of biometric data  encompassing both easily definable characteristics such as subject age and  most recently  higher level  characteristics such as emotional or mental states  this study will present a selective review of the predictive capabilities  in the widest sense  of biometric data processing  providing an analysis of the key issues still adequately to be addressed if this concept of predictive biometrics is to be fully exploited in the future  
 predictive mechanisms are not involved the same way during human human vs  human machine interactions  a review nowadays  interactions with others do not only involve human peers but also automated systems  many studies suggest that the motor predictive systems that are engaged during action execution are also involved during joint actions with peers and during other human generated action observation  indeed  the comparator model hypothesis suggests that the comparison between a predicted state and an estimated real state enables motor control  and by a similar functioning  understanding and anticipating observed actions  such a mechanism allows making predictions about an ongoing action  and is essential to action regulation  especially during joint actions with peers  interestingly  the same comparison process has been shown to be involved in the construction of an individual s sense of agency  both for self generated and observed other human generated actions  however  the implication of such predictive mechanisms during interactions with machines is not consensual  probably due to the high heterogeneousness of the automata used in the experimentations  from very simplistic devices to full humanoid robots  the discrepancies that are observed during human machine interactions could arise from the absence of action observation matching abilities when interacting with traditional low level automata  consistently  the difficulties to build a joint agency with this kind of machines could stem from the same problem  in this context  we aim to review the studies investigating predictive mechanisms during social interactions with humans and with automated artificial systems  we will start by presenting human data that show the involvement of predictions in action control and in the sense of agency during social interactions  thereafter  we will confront this literature with data from the robotic field  finally  we will address the upcoming issues in the field of robotics related to automated systems aimed at acting as collaborative agents  
 preserving differential privacy in convolutional deep belief networks the remarkable development of deep learning in medicine and healthcare domain presents obvious privacy issues  when deep neural networks are built on users  personal and highly sensitive data  e g   clinical records  user profiles  biomedical images  etc  however  only a few scientific studies on preserving privacy in deep learning have been conducted  in this paper  we focus on developing a private convolutional deep belief network  pcdbn   which essentially is a convolutional deep belief network  cdbn  under differential privacy  our main idea of enforcing  differential privacy is to leverage the functional mechanism to perturb the energy based objective functions of traditional cdbns  rather than their results  one key contribution of this work is that we propose the use of chebyshev expansion to derive the approximate polynomial representation of objective functions  our theoretical analysis shows that we can further derive the sensitivity and error bounds of the approximate polynomial representation  as a result  preserving differential privacy in cdbns is feasible  we applied our model in a health social network  i e   yesiwell data  and in a handwriting digit dataset  i e   mnist data  for human behavior prediction  human behavior classification  and handwriting digit recognition tasks  theoretical analysis and rigorous experimental evaluations show that the pcdbn is highly effective  it significantly outperforms existing solutions  
 press accept to update now  individual differences in susceptibility to malevolent interruptions increasingly  connected communication technologies have resulted in people being exposed to fraudulent communications by scammers and hackers attempting to gain access to computer systems for malicious purposes  common influence techniques  such as mimicking authority figures or instilling a sense of urgency  are used to persuade people to respond to malevolent messages by  for example  accepting urgent updates  an  accept  response to a malevolent influence message can result in severe negative consequences for the user and for others  including the organisations they work for  this paper undertakes exploratory research to examine individual differences in susceptibility to fraudulent computer messages when they masquerade as interruptions during a demanding memory recall primary task compared to when they are presented in a post task phase  a mixed methods approach was adopted to examine when and why people choose to accept or decline three types of interrupting computer update message  genuine  mimicked  and low authority  and the relative impact of such interruptions on performance of a serial recall memory primary task  results suggest that fraudulent communications are more likely to be accepted by users when they interrupt a demanding memory based primary task  that this relationship is impacted by the content of the fraudulent message  and that influence techniques used in fraudulent communications can over ride authenticity cues when individuals decide to accept an update message  implications for theories  such as the recently proposed suspicion  cognition and automaticity model and the integrated information processing model of phishing susceptibility  are discussed   c  2017 the authors  published by elsevier b v  
 presuppositions about the role of consciousness in the agent causation conception of agents and the problem of the disappearing agent well known theories of agent causation rely on a conception of agency that expects that agents play a role in the production of their action  a conscious role  according to this conception of agents  the requirement of consciousness provides ground for these theories to pose the disappearing agent objection to the causal theory of action  in a similar way  wegner  2002  holds that without the conscious will playing a role in the production of actions we are not agents  in this sense  the elements that ground the disappearing agent objection resemble wegner s conclusion that it is an illusion that we are agents  i will argue that the objection raised by agent causation theories equates lack of consciousness with lack of control and  consequently  of agential role in the production of action  this will show that the issue is grounded on a specific conception of what an agent is  and what her role in producing actions should be  i  however  defend the claim that this conception of agency should be revised  as well as the objection that springs from it  because if we accept that consciousness does not always play a relevant role in the production of actions  then human agents cannot fulfill the requirements in question   c  2017 elsevier b v  all rights reserved  
 preventing traffic accidents with in vehicle decision support systems   the impact of accident hotspot warnings on driver behaviour despite continuous investment in road and vehicle safety  as well as improvements in technology standards  the total amount of road traffic accidents has been increasing over the last decades  consequently  identifying ways of effectively reducing the frequency and severity of traffic accidents is of utmost importance  in light of the depicted challenge  latest studies provide promising evidence that in vehicle decision support systems  dsss  can have significant positive effects on driving behaviour and collision avoidance  going beyond existing research  we developed a comprehensive in vehicle dss  which provides accident hotspot warnings to drivers based on location analytics applied to a national historical accident dataset  composed of over 266 000 accidents  as such  we depict the design and field evaluation of an in vehicle dss  bridging the gap between real world location analytics and in vehicle warnings  the system was tested in a country wide field test of 57 professional drivers  with over 170 000km driven during a four week period  where vehicle data were gathered via a connected car prototype system  ultimately  we demonstrate that in vehicle warnings of accident hotspots have a significant improvement on driver behaviour over time  in addition  we provide first evidence that an individual s personality plays a key role in the effectiveness of in vehicle dsss  however  in contrast to existing lab experiments with very promising results  we were unable to find an immediate effect on driver behaviour  hence  we see a strong need for further field experiments with high resolution car data to confirm that in vehicle dsss can deliver in diverse field situations   c  2017 elsevier b v  all rights reserved  
 pricing ad slots with consecutive multi unit demand we consider the optimal pricing problem for a model of the rich media advertisement market  that has other related applications  our model differs from traditional position auctions in that we consider buyers whose demand might be multiple consecutive slots  which is motivated by modeling buyers who may require these to display a large size ad  we study three major pricing mechanisms  the bayesian pricing model  the maximum revenue market equilibrium model and an envy free solution model  under the bayesian model  we design a polynomial time computable truthful mechanism that optimizes the revenue  for the market equilibrium paradigm  we find a polynomial time algorithm to obtain the maximum revenue market equilibrium solution  in the envy free setting  an optimal solution is presented for the case where the buyers have the same demand for the number of consecutive slots  we present results of a simulation that compares the revenues from the above schemes  
 pricing decision problem for substitutable products based on uncertainty theory increasing studies in marketing and distribution channels have shown that the power of manufacturers and retailers is reversing  in this paper  we consider a pricing decision problem in which two different manufacturers compete to distribute differentiated but substitutable products through a common retailer under different power structures  the manufacturing costs  sales costs and demands are characterized by uncertain variables  meanwhile  uncertainty theory and game theory based modeling approaches are employed to formulate the pricing decision problem with three different power structures under uncertain environment  how to make the optimal pricing decisions on wholesale prices and retailer markups under three possible scenarios is derived  numerical experiments are also given to examine the effects of power structures on the equilibrium prices and profits in uncertain environment  it is found that if the sales cost is high  consumers can enjoy lower prices when facing a powerful retailer and the super retailer can also make the supply chain more efficient  
 prism  profession identification in social media profession is an important social attribute of people  it plays a crucial role in commercial services such as personalized recommendation and targeted advertising  in practice  profession information is usually unavailable due to privacy and other reasons  in this article  we explore the task of identifying user professions according to their behaviors in social media  the task confronts the following challenges that make it non trivial  how to incorporate heterogeneous information of user behaviors  how to effectively utilize both labeled and unlabeled data  and how to exploit community structure  to address these challenges  we present a framework called profession identification in social media  it takes advantage of both personal information and community structure of users in the following aspects   1  we present a cascaded two level classifier with heterogeneous personal features to measure the confidence of users belonging to different professions   2  we present a multi training process to take advantages of both labeled and unlabeled data to enhance classification performance   3  we design a profession identification method synthetically considering the confidences from personal features and community structure  we collect a real world dataset to conduct experiments  and experimental results demonstrate the significant effectiveness of our method compared with other baseline methods  by applying prediction on large scale users  we also analyze characteristics of microblog users  finding that there are significant diversities among users of different professions in demographics  social network structures  and linguistic styles  
 privacy concerns for mobile app download  an elaboration likelihood model perspective in the mobile age  protecting users  information from privacy invasive apps becomes increasingly critical  to precaution users against possible privacy risks  a few android app stores prominently disclose app permission requests on app download pages  focusing on this emerging practice  this study investigates the effects of contextual cues  perceived permission sensitivity  permission justification and perceived app popularity  on android users  privacy concerns  download intention  and their contingent effects dependent on users  mobile privacy victim experience  drawing on elaboration likelihood model  our empirical results suggest that perceived permission sensitivity makes users more concerned about privacy  while permission justification and perceived app popularity make them less concerned  interestingly  users  mobile privacy victim experience negatively moderates the effect of permission justification  in particular  the provision of permission justification makes users less concerned about their privacy only for those with less mobile privacy victim experience  results also reveal a positive effect of perceived app popularity and a negative effect of privacy concerns on download intention  this study provides a better understanding of android users  information processing and the formation of their privacy concerns in the app download stage  and proposes and tests emerging privacy protection mechanisms including the prominent disclosure of app permission requests and the provision of permission justifications   c  2016 elsevier b v  all rights reserved  
 probabilistic fuzzy regression approach for preference modeling two types of uncertainty  namely  randomness and fuzziness  exist in preference modeling  fuzziness is mainly caused by human subjective judgment and incomplete knowledge  and randomness often originates from the variability of influences on the inputs and outputs of a preference model  various techniques have been utilized to develop preference models  however  only few previous studies have addressed both fuzziness and randomness in preference modeling  among these limited studies  none have considered the randomness caused by particular independent variables  to fill this research gap  this study proposes probabilistic fuzzy regression  pfr   a new approach for preference modeling  pfr considers both the fuzziness of data sets and the randomness caused by independent variables  in the proposed approach  probability density functions  pdfs  are adopted to model randomness  the parameter settings of the pdfs are determined using a chaos optimization algorithm  the probabilistic terms of the pfr models are generated according to the expected value functions of the random variables  fuzzy regression analysis is employed to determine the fuzzy coefficients for all the terms of the pfr models  an industrial case study of a tea maker design is used to illustrate the applicability of pfr and evaluate its effectiveness  modeling results obtained from pfr are compared with those obtained from statistical regression  fuzzy regression  and fuzzy least squares regression  results of the training and validation tests show that pfr outperforms the other approaches in terms of training and validation errors   c  2017 elsevier ltd  all rights reserved  
 probabilistic modeling and visualization for bankruptcy prediction in accounting and finance domains  bankruptcy prediction is of great utility for all of the economic stakeholders  the challenge of accurate assessment of business failure prediction  specially under scenarios of financial crisis  is known to be complicated  although there have been many successful studies on bankruptcy detection  seldom probabilistic approaches were carried out  in this paper we assume a probabilistic point of view by applying gaussian processes  gp  in the context of bankruptcy prediction  comparing it against the support vector machines  svm  and the logistic regression  lr   using real world bankruptcy data  an in depth analysis is conducted showing that  in addition to a probabilistic interpretation  the gp can effectively improve the bankruptcy prediction performance with high accuracy when compared to the other approaches  we additionally generate a complete graphical visualization to improve our understanding of the different attained performances  effectively compiling all the conducted experiments in a meaningful way  we complete our study with an entropy based analysis that highlights the uncertainty handling properties provided by the gp  crucial for prediction tasks under extremely competitive and volatile business environments   c  2017 elsevier b v  all rights reserved  
 probabilistic squares and hexagons of opposition under coherence various semantics for studying the square of opposition and the hexagon of opposition have been proposed recently  we interpret sentences by imprecise  set valued  probability assessments on a finite sequence of conditional events  we introduce the acceptability of a sentence within coherence based probability theory  we analyze the relations of the square and of the hexagon in terms of acceptability  then  we show how to construct probabilistic versions of the square and of the hexagon of opposition by forming suitable tripartitions of the set of all coherent assessments on a finite sequence of conditional events  finally  as an application  we present new versions of the square and of the hexagon involving generalized quantifiers   c  2017 elsevier inc  all rights reserved  
 problem definition and information provision by federal bureaucrats federal bureaucrats are important sources of information about policy problems  however  federal officials compete for this influence with organized interests plying their own problems and solutions  we attribute the differential agenda influence of the federal bureaucracy to efforts in congress to construct workable problem definitions in a context of uncertainty about issues  from both behavioral and rational models of congressional decision making  we develop a theory of congressional search for information during problem definition under conditions of uncertainty  the theory presages the prominence of federal bureaucrats in this search  and especially under uncertainty  using new data sets capturing the appearance of federal bureaucrats at congressional hearings  we find that the mobilization  prominence  and types of federal bureaucrats providing information is explainable in terms of congressional uncertainty about problem definitions   c  2016 elsevier b v  all rights reserved  
 processprofiler3d  a visualisation framework for log based process performance comparison an organisation can significantly improve its performance by observing how their business operations are currently being carried out  a great way to derive evidence based process improvement insights is to compare the behaviour and performance of processes for different process cohorts by utilising the information recorded in event logs  a process cohort is a coherent group of process instances that has one or more shared characteristics  such process performance comparisons can highlight positive or negative variations that can be evident in a particular cohort  thus enabling a tailored approach to process improvement  although existing process mining techniques can be used to calculate various statistics from event logs for performance analysis  most techniques calculate and display the statistics for each cohort separately  furthermore  the numerical statistics and simple visualisations may not be intuitive enough to allow users to compare the performance of various cohorts efficiently and effectively  we developed a novel visualisation framework for log based process performance comparison to address these issues  it enables analysts to quickly identify the performance differences between cohorts  the framework supports the selection of cohorts and a three dimensional visualisation to compare the cohorts using a variety of performance metrics  the approach has been implemented as a set of plug ins within the open source process mining framework prom and has been evaluated using two real life datasets from the insurance domain to assess the usefulness of such a tool  this paper also derives a set of design principles from our approach which provide guidance for the development of new approaches to process cohort performance comparison   c  2017 elsevier b v  all rights reserved  
 product attributes and user experience design  how to convey product information through user centered service the user experience  ux  is a critical issue in both the product design and service sector  ensuring that customers can easily understand a product or service and thus ensuring higher satisfaction with the customer product  service  interaction process is essential for survival among competitive industries  however  few studies have attempted to integrate all relevant factors into a comprehensive model of ux  the present study addresses this need by proposing a conceptual ux interaction model that incorporates the characteristics of usability and user interaction level based on quality of experience  a method to apply the model and the steps to implement the data collection and analysis are also proposed  the ux interaction model requires a systematic approach to   1  decompose the product or service characteristics during a ux interaction process   2  determine typology of ux items of each characteristic  and  3  select appropriate and feasible strategies to improve these ux items  a quantitative survey for mobile phone users was developed to investigate differences among types of ux and product characteristics  this study provides valuable empirical evidence on the ux interaction model for particular industries where superior quality service experiences are to be achieved  
 profarima  a profit driven order identification algorithm for arima models in sales forecasting in forecasting  evolutionary algorithms are often linked to existing forecasting methods to optimize their input parameters  traditionally  the fitness function of these search heuristics is based on an accuracy measure  in this paper  however  we combine forecasting accuracy with business expertise by defining a flexible and easily interpretable profit function for sales forecasting  which is based on the profit margin of a given product  the volume of its sales and the accuracy of the forecast  profarima is a new procedure that selects the lags of a seasonal arima model according to the profit of a model s forecasts by taking advantage of search heuristics  this procedure is tested on both publicly available datasets and a real life application with datasets of the coca cola company in order to assess its performance  both in profit and accuracy  three different evolutionary algorithms were implemented during this testing process  i e  genetic algorithms  particle swarm optimization and simulated annealing  the results indicate that profarima always performs at least equally to the boxjenkins methodology and often outperforms this traditional procedure  for the coca cola company  our new algorithm in combination with genetic algorithms even leads to a significantly larger profit for out of sample forecasts   c  2017 elsevier b v  all rights reserved  
 promoting interactions between humans and robots using robotic emotional behavior the objective of a socially assistive robot is to create a close and effective interaction with a human user for the purpose of giving assistance  in particular  the social interaction  guidance  and support that a socially assistive robot can provide a person can be very beneficial to patient centered care  however  there are a number of research issues that need to be addressed in order to design such robots  this paper focuses on developing effective emotion based assistive behavior for a socially assistive robot intended for natural human robot interaction  hri  scenarios with explicit social and assistive task functionalities  in particular  in this paper  a unique emotional behavior module is presented and implemented in a learning based control architecture for assistive hri  the module is utilized to determine the appropriate emotions of the robot to display  as motivated by the well being of the person  during assistive task driven interactions in order to elicit suitable actions from users to accomplish a given person centered assistive task  a novel online updating technique is used in order to allow the emotional model to adapt to new people and scenarios  experiments presented show the effectiveness of utilizing robotic emotional assistive behavior during hri scenarios  
 prototyping strategies for multisensory product experience engineering the paper deals with prototyping strategies aimed at supporting engineers in the design of the multisensory experience of products  it is widely recognised that the most effective strategy to design it is to create working prototypes and analyse user s reactions when interacting with them  starting from this consciousness  we will discuss of how virtual reality  vr  technologies can support engineers to build prototypes suitable to this aim  furthermore we will demonstrate how vr based prototypes do not only represent a valid alternative to physical prototypes  but also a step forward thanks to the possibility of simulating and rendering multisensory and real time modifiable interactions between the user and the prototype  these characteristics of vr based prototypes enable engineers to rapidly test with users different variants and to optimise the multisensory experience perceived by them during the interaction  the discussion is supported both by examples available in literature and by case studies we have developed over the years on this topic  specifically  in our research we have concentrated on what happens in the physical contact between the user and the product  such contact strongly influences the user s impression about the product  
 psychologically inspired visual information storage and retrieval modeling for multiclass image classification the computer vision research aims to enable the computers to recognize visual images as easily as human  studies have shown that human could segregate target from its surrounding environment  which is intimately associated with the brain memory mechanism  however  it is not quite clear about how the visual images are stored and retrieved in the human brain  in this paper  we propose a psychologically visual information storage and retrieval model  pvisrm  based on sparse coding and probabilistic decision theory  first  the dense scale invariant feature transform  sift  algorithm is applied to extract the features of visual images and then the extracted features are represented by sparse coding  in the storage procedure  each component of the feature vector is correctly copied with certain probability generated by an exponential distribution  for retrieval  the likelihood ratio between the probe image feature vector and that of each studied image is calculated based on probabilistic theory  then the category likelihood ratio between the probe image and each category is obtained by adding the ratio values of all images belonging to the same category  finally  the bayesian decision rule for image classification is presented  experimental results show that the proposed pvisrm model can obtain good classification performance and outperforms the svm approach   c  2017 published by elsevier b v  
 psychophysiologically based real time adaptive general type 2 fuzzy modeling and self organizing control of operator s performance undertaking a cognitive task this paper presents a new modeling and control fuzzy based framework validated with real time experiments on human participants experiencing stress via mental arithmetic cognitive tasks identified through psychophysiological markers  the ultimate aim of the modeling control framework is to prevent performance breakdown in human computer interactive systems with a special focus on human performance  two designed modeling control experiments which consist of carrying out arithmetic operations of varying difficulty levels were performed by ten participants  operators  in the study  with this new technique  modeling is achieved through a new adaptive  self organizing  and interpretable modeling framework based on general type 2 fuzzy sets  this framework is able to learn in real time through the implementation of a restructured performance learning algorithm that identifies important features in the data without the need for prior training  the information learnt by the model is later exploited via an energy model based controller that infers adequate control actions by changing the difficulty level of the arithmetic operations in the human computer interaction system  these actions being based on the most current psychophysiological state of the subject under study  the real time implementation of the proposed modeling and control configurations for the human machine interaction under study shows superior performance as compared to other forms of modeling and control  with minimal intervention in terms of model retraining or parameter retuning to deal with uncertainties  disturbances  and inter intrasubject parameter variability  
 public entities driven robotic innovation in urban areas cities present new challenges and needs to satisfy and improve lifestyle for their citizens under the concept  smart city   in order to achieve this goal in a global manner  new technologies are required as the robotic one  but public entities unknown the possibilities offered by this technology to get solutions to their needs  in this paper the development of the innovative public procurement instruments is explained  specifically the process pdti  public end users driven technological innovation  as a driving force of robotic research and development and offering a list of robotic urban challenges proposed by european cities that have participated in such a process  in the next phases of the procedure  this fact will provide novel robotic solutions addressed to public demand that are an example to be followed by other smart cities   c  2017 elsevier b v  all rights reserved  
 public policy and the wisdom of crowds collective intelligence  or the wisdom of crowds  refers to a phenomenon by which  under the right conditions  groups of individuals can render highly accurate judgments  this phenomenon has long played an important role in economics  where understanding the behavior of groups is often essential to explaining economic outcomes  more recently  political scientists have shown that trends in public opinion show evidence of collective intelligence  this article further explores how the wisdom of crowds affects politics  i look at two types of decision making processes  those governed by group dynamics versus those rendered by organizations  distributional analysis of financial markets and foreign exchange rates shows that when policies are determined by groups they are less prone to instabilities  evidence that in certain issue areas decision making by groups is more readily adaptive to shifting environmental cues than decisions made through organizational deliberation   c  2017 elsevier b v  all rights reserved  
 pure exchange competitive equilibrium under uncertainty we investigate in this paper a version of pure exchange competitive equilibrium under uncertain circumstances  those uncertain factors are embedded in each agent s preference  which is characterized by the uncertain utility function  by maximizing the expected utility of each agent  we formulate this kind of pure exchange competitive equilibrium problem into a quasi variational inequality problem  this idea is applied in a pure exchange economy which consists of two agents and two goods  and we find the competitive equilibrium of this economy with each agent s preference being an uncertain variable  
 pythagorean fuzzy multiattribute group decision making with probabilistic information and owa approach the aim of this paper is to develop a pythagorean fuzzy multiattribute group decision making  magdm  method based on probabilistic information and the ordered weighted averaging  owa  approach  the pythagorean fuzzy probabilistic ordered weighted averaging  pfpowa  operator is presented  it is a new aggregation operator that considers the probabilities and the owa in the same formulation  therefore  it is able to take into account the degree of importance that each concept has in the particular problem considered  some main properties and different particular cases of the pfpowa operators are studied  moreover  a method based on the proposed operator for multiattribute group decision making is put forward  finally  an example showing analysis of a supplier selection is given to verify the effectiveness and practicability of the proposed method  
 qplan  decision support for evaluating planning quality in software development projects decisions about whether or not to approve a project plan for execution are critical  a decision to continue with a bad plan may lead to a failed project  whereas requesting unnecessary additional planning for an already high quality plan may be counterproductive  however  these decisions can be influenced by psychological biases  such as the endowment effect  optimism bias and ambiguity effect  which are enhanced when uncertainty is substantial and information incomplete  as a result  a non biased model for evaluating the quality of project planning is important to improve planning approval decisions and resource allocation  this paper introduces a novel artifact  qplan  that evaluates and improves planning quality  and a case study to demonstrate its effectiveness within a business environment   c  2017 elsevier b v  all rights reserved  
 quality of life and poverty  measurement and comparability in this paper  we analyze the correspondence among the rankings of the spanish regions according to different measures of monetary poverty and quality of life  in 2012  to do that  the spearman s rank correlation coefficient is used  different aggregation methods are applied to calculate the selected measures of poverty and quality of life  the monetary poverty measures aggregate the income gaps  while the quality of life measures aggregate a set of indicators dealing with ten different domains  in both cases  among other traditional aggregation procedures  the exponential mean is used because its properties are especially adequate in these contexts  
 quantification of human confidence in functional relations what makes people infer that two continuous valued entities are functionally related  involving factors influencing human confidence in the existence of a functional link between two supposed variables has not so far been discussed in function learning literature  by examining this problem and based on relevant results from cognitive psychology  i propose a hypothesis according to which human confidence in a link between cue and criterion is affected by three factors  the difficulty of functions  the level of noise in observed data  and the sample size  here  the formalization of this hypothesis forms a novel mathematical model of function learning which can also be used for predictions  so the resulting model receives cue criterion pairs of a supposed relation and produces two outputs  confidence and predicting function  in an experiment  the performance of a computational implementation of the model is compared with human data  the results show that the model is successful in tracking changes in human confidence  a close correspondence between the predictions of the model and humans was also achieved   c  2016 elsevier b v  all rights reserved  
 quantile autoregression neural network model with applications to evaluating value at risk we develop a new quantile autoregression neural network  qarnn  model based on an artificial neural network architecture  the proposed qarnn model is flexible and can be used to explore potential non linear relationships among quantiles in time series data  by optimizing an approximate error function and standard gradient based optimization algorithms  qarnn outputs conditional quantile functions recursively the utility of our new model is illustrated by monte carlo simulation studies and empirical analyses of three real stock indices from the hong kong hang seng index  hsi   the us s p500 index  s p500  and the financial times stock exchange 100 index  ftse100    c  2016 elsevier b v  all rights reserved  
 quanto european option pricing with ambiguous return rates and volatilities this paper presents a model of quanto  quantity adjusting option  european option pricing when returns and volatilities are ambiguity  first  we use set valued stochastic differential inclusions to describe the black scholes quanto model with ambiguous return rates and volatilities  the risk neutral martingale measures are not unique but a set in this model  so we consider the upper and lower bounds of contingent claim by using the maximal and minimal conditional expectations  respectively  since the maximal  minimal  conditional expectations are nonlinear  we provide a computational method for calculating maximal  minimal  conditional expectations by backward stochastic differential equations in the second part of this paper  third  we give the exact upper bound and lower bound formulas of quanto european options and provide a numerical example to illustrate our model  finally  we show some conclusions and further work  
 query based multi documents summarization using linguistic knowledge and content word expansion in this paper  a query based summarization method  which uses a combination of semantic relations between words and their syntactic composition  to extract meaningful sentences from document sets is introduced  the problem with current statistical methods is that they fail to capture the meaning when comparing a sentence and a user query  hence there is often a conflict between the extracted sentences and users  requirements  however  this particular method can improve the quality of document summaries because it is able to avoid extracting a sentence whose similarity with the query is high but whose meaning is different  the method is executed by computing the semantic and syntactic similarity of the sentence to sentence and sentence to query  to reduce redundancy in summary  this method uses the greedy algorithm to impose diversity penalty on the sentences  in addition  the proposed method expands the words in both the query and the sentences to tackle the problem of information limit  it bridges the lexical gaps for semantically similar contexts that are expressed using different wording  the experimental results display that the proposed method is able to improve performance compared with the participating systems in duc 2006  the experimental results also showed that the proposed method demonstrates better performance as compared to other existing techniques on duc 2005 and duc 2006 datasets  
 r d project evaluation and project portfolio selection by a new interval type 2 fuzzy optimization approach in today s ever changing and highly uncertain environment  organizations depend on research and development  r d  activities to adapt intensive growth of technology  one of the most important and trickiest tasks of any developing firms is to define new projects  this process gets difficult when it comes to choosing an appropriate portfolio from a set of candidate projects  since organizations are faced with limited resources of r d and budget constraints  they have to choose a project portfolio that mitigates the corresponding risk and enhances the overall value of portfolio  therefore  the purpose of this study is to introduce a practical model to select the best and the most proper project portfolio while considering project investment capital  return rate  and risk  the ever changing and highly uncertain environment of projects is addressed by utilizing interval type 2 fuzzy sets  it2fss   in this paper  a new model of r d project evaluation is first introduced  this model includes a new risk return index  this model is then extended in project portfolio selection  and as a result  a new model of r d project portfolio selection is proposed under uncertainty  constraints and limitations of r d project portfolio selection are comprehensively addressed  in this model  lower semi variance is applied to consider risk of proposed projects  therefore  this paper offers a new model that applies it2fss to handle uncertainty  uses semi variance to assess risk  synchronously considers risk and return in its selection process  and addresses the considerations and limits of real world problems  eventually  to verify the proposed model  a numerical example of the existing literature is solved with the model  and the results are compared  the first proposed model is used to prioritize proposed r d projects of a gas and oil development holding firm as a real case study  to illustrate further  a practical example is also provided to demonstrate the applicability of the proposed project portfolio selection model  
 randomized social choice functions under metric preferences we determine the quality of randomized social choice algorithms in a setting in which the agents have metric preferences  every agent has a cost for each alternative  and these costs form a metric  we assume that these costs are unknown to the algorithms  and possibly even to the agents themselves   which means we cannot simply select the optimal alternative  i e  the alternative that minimizes the total agent cost  or median agent cost   however  we do assume that the agents know their ordinal preferences that are induced by the metric space  we examine randomized social choice functions that require only this ordinal information and select an alternative that is good in expectation with respect to the costs from the metric  to quantify how good a randomized social choice function is  we bound the distortion  which is the worst case ratio between the expected cost of the alternative selected and the cost of the optimal alternative  we provide new distortion bounds for a variety of randomized algorithms  for both general metrics and for important special cases  our results show a sizable improvement in distortion over deterministic algorithms  
 ranking efficient dmus using cooperative game theory the problem of ranking decision making units  dmus  in data envelopment analysis  dea  has been widely studied in the literature  some of the proposed approaches use cooperative game theory as a tool to perform the ranking  in this paper  we use the shapley value of two different cooperative games in which the players are the efficient dmus and the characteristic function represents the increase in the discriminant power of dea contributed by each efficient dmu  the idea is that if the efficient dmus are not included in the modified reference sample then the efficiency score of some inefficient dmus would be higher  the characteristic function represents  therefore  the change in the efficiency scores of the inefficient dmus that occurs when a given coalition of efficient units is dropped from the sample  alternatively  the characteristic function of the cooperative game can be defined as the change in the efficiency scores of the inefficient dmus that occurs when a given coalition of efficient dmus are the only efficient dmus that are included in the sample  since the two cooperative games proposed are dual games  their corresponding shapley value coincide and thus lead to the same ranking  the more an efficient dmu impacts the shape of the efficient frontier  the higher the increase in the efficiency scores of the inefficient dmus its removal brings about and  hence  the higher its contribution to the overall discriminant power of the method  the proposed approach is illustrated on a number of datasets from the literature and compared with existing methods   c  2017 published by elsevier ltd  
 ranking of z numbers and its application in decision making real world decision problems in decision analysis  system analysis  economics  ecology  and other fields are characterized by fuzziness and partial reliability of relevant information  in order to deal with such information  prof  zadeh suggested the concept of a z number as an ordered pair z    a  b  of fuzzy numbers a and b  the first of which is a linguistic value of a variable of interest  and the second one is a linguistic value of probability measure of the first one  playing a role of reliability of information  decision making under z number based information requires ranking of z numbers  in this paper we suggest a human like fundamental approach for ranking of z numbers which is based on two main ideas  one idea is to compute optimality degrees of z numbers and the other one is to adjust the obtained degrees by using a human being s opinion formalized by a degree of pessimism  two examples and a real world application are provided to show validity of the suggested research  a comparison of the proposed approach with the existing methods is conducted  
 ranking product aspects through sentiment analysis of online reviews the electronic word of mouth  e wom  is one of the most important among all the factors affecting consumers  behaviours  opinions towards a product through online reviews will influence purchase decisions of other online consumers by changing their perceptions on the product quality  furthermore  each product aspect may impact consumers  intentions differently  thus  sentiment analysis and econometric models are incorporated to examine the relationship between purchase intentions and aspect opinion pairs  which enable the weight estimation for each product aspect  we first identify product aspects and reduce dimensions to extract aspect opinion pairs  next the information gain is calculated for each aspect through entropy theory  based on sentiment polarity and sentiment strength  we formulate an econometric model by integrating the information gain to measure the aspect s weight  in the experiment  we track 386 digital cameras on amazon for 39 months  and results show that the aspect weight for digital cameras is detected more precisely than tf id and hac algorithms  the results will bridge product aspects and consumption intention to facilitate e wom based marketing  
 ranking products through online reviews  a method based on sentiment analysis technique and intuitionistic fuzzy set theory online product reviews have significant impacts on consumers  purchase decisions  to support consumers  purchase decisions  how to rank the products through online reviews is a valuable research topic  while research concerning this issue is still relatively scarce  this paper proposes a method based on the sentiment analysis technique and the intuitionistic fuzzy set theory to rank the products through online reviews  an algorithm based on sentiment dictionaries is developed to identify the positive  neutral or negative sentiment orientation on the alternative product concerning the product feature in each review  according to the identified positive  neutral and negative sentiment orientations  an intuitionistic fuzzy number is constructed for representing the performance of an alternative product concerning a product feature  the ranking of alternative products is determined by intuitionistic fuzzy weighted averaging  ifwa  operator and preference ranking organization methods for enrichment evaluations ii  promethee ii   a case study is given to illustrate the use of the proposed method  the comparisons and experiments are further conducted to illustrate the characteristics and advantages of the proposed method  converting the identified positive  neutral and negative sentiment orientations into intuitionistic fuzzy numbers is a new idea for processing and fusing a large number of sentiment orientations of online reviews  based on the proposed method  decision support system can be developed to support the consumers  purchase decisions more conveniently   c  2016 elsevier b v  all rights reserved  
 rapport with virtual agents  what do human social cues and personality explain  rapport has been recognized as an important aspect of relationship building  while rapport in the context of human human interaction has been widely studied  how it can be established and maintained in human agent interaction has been studied only recently  our study investigates how social cues and personality of a human interacting with an agent can be used for automatic prediction of rapport in this context  we conduct experiments with two emotional virtual agents  alongside the audio visual data  we also collect human personality measures and two measures of rapport  self reported rapport and rapport judged by observers  the social cues  such as turn taking patterns and facial expressions are extracted from audio visual data  our results show that the most significant cues that infer the rapport judgments are the number of turn taking cues and pauses  we also find that some of the significant social cues related to rapport are similar to those reported in previous psychology literature  we also confirm previous findings on how human personality plays an important role in perceiving the interaction with agents people who score high in extraversion and agreeableness report higher rapport with both agents  finally  the rapport prediction results suggest that automatic analysis of social phenomena in human agent interaction could be a feasible method for agent evaluation  
 rating effects on social news posts and comments at a time when information seekers first turn to digital sources for news and opinion  it is critical that we understand the role that social media plays in human behavior  this is especially true when information consumers also act as information producers and editors through their online activity  in order to better understand the effects that editorial ratings have on online human behavior  we report the results of a two large scale in vivo experiments in social media  we find that small  random rating manipulations on social media posts and comments created significant changes in downstream ratings  resulting in significantly different final outcomes  we found positive herding effects for positive treatments on posts  increasing the final rating by 11 02  on average  but not for positive treatments on comments  contrary to the results of related work  we found negative herding effects for negative treatments on posts and comments  decreasing the final ratings  on average  of posts by 5 15  and of comments by 37 4   compared to the control group  the probability of reaching a high rating     2 000  for posts is increased by 24 6  when posts receive the positive treatment and for comments it is decreased by 46 6  when comments receive the negative treatment  
 rational imitation for robots  the cost difference model infants imitate behaviour flexibly  depending on the circumstances  they copy both actions and their effects or only reproduce the demonstrator s intended goals  in view of this selective imitation  infants have been called rational imitators  the ability to selectively and adaptively imitate behaviour would be a beneficial capacity for robots  indeed  selecting what to imitate is an outstanding unsolved problem in the field of robotic imitation  in this paper  we first present a formalized model of rational imitation suited for robotic applications  next  we test and demonstrate it using two humanoid robots  
 re framing the characteristics of concepts and their relation to learning and cognition in artificial agents in this work  the problems of knowledge acquisition and information processing are explored in relation to the definitions of concepts and conceptual processing  and their implications for artificial agents  the discussion focuses on views of cognition as a dynamic property in which the world is actively represented in grounded mental states which only have meaning in the action context  reasoning is understood as an emerging property consequence of actions environment couplings achieved through experience  and concepts as situated and dynamic phenomena enabling behaviours  re framing the characteristics of concepts is considered crucial to overcoming settled beliefs and reinterpreting new understandings in artificial systems  the first part presents a review of concepts from cognitive sciences  support is found for views on grounded and embodied cognition  describing concepts as dynamic  flexible  context dependent  and distributedly coded  that is argued to contrast with many technical implementations assuming concepts as categories  whilst explains limitations when grounding amodal symbols  or in unifying learning  perception and reasoning  the characteristics of concepts are linked to methods of active inference  self organization  and deep learning to address challenges posed and to reinterpret emerging techniques  in a second part  an architecture based on deep generative models is presented to illustrate arguments elaborated  it is evaluated in a navigation task  showing that sufficient representations are created regarding situated behaviours with no semantics imposed on data  moreover  adequate behaviours are achieved through a dynamic integration of perception and action in a single representational domain and process   c  2017 elsevier b v  all rights reserved  
 reacog  a minimal cognitive controller based on recruitment of reactive systems it has often been stated that for a neuronal system to become a cognitive one  it has to be large enough  in contrast  we argue that a basic property of a cognitive system  namely the ability to plan ahead  can already be fulfilled by small neuronal systems  as a proof of concept  we propose an artificial neural network  termed reacog  that  first  is able to deal with a specific domain of behavior  six legged walking   second  we show how a minor expansion of this system enables the system to plan ahead and deploy existing behavioral elements in novel contexts in order to solve current problems  to this end  the system invents new solutions that are not possible for the reactive network  rather these solutions result from new combinations of given memory elements  this faculty does not rely on a dedicated system being more or less independent of the reactive basis  but results from exploitation of the reactive basis by recruiting the lower level control structures in a way that motor planning becomes possible as an internal simulation relying on internal representation being grounded in embodied experiences  
 reaction times in visual search can be explained by a simple model of neural synchronization we present an oscillatory neural network model that can account for reaction times in visual search experiments  the model consists of a central oscillator that represents the central executive of the attention system and a number of peripheral oscillators that represent objects in the display  the oscillators are described as generalized kuramoto type oscillators with adapted parameters  an object is considered as being included in the focus of attention if the oscillator associated with this object is inphase with the central oscillator  the probability for an object to be included in the focus of attention is determined by its saliency that is described in formal terms as the strength of the connection from the peripheral oscillator to the central oscillator  by computer simulations it is shown that the model can reproduce reaction times in visual search tasks of various complexities  the dependence of the reaction time on the number of items in the display is represented by linear functions of different steepness which is in agreement with biological evidence   c  2016 elsevier ltd  all rights reserved  
 real time biologically inspired action recognition from key poses using a neuromorphic architecture intelligent agents  such as robots  have to serve a multitude of autonomous functions  examples are  e g   collision avoidance  navigation and route planning  active sensing of its environment  or the interaction and non verbal communication with people in the extended reach space  here  we focus on the recognition of the action of a human agent based on a biologically inspired visual architecture of analyzing articulated movements  the proposed processing architecture builds upon coarsely segregated streams of sensory processing along different pathways which separately process form and motion information  layher et al   2014   action recognition is performed in an event based scheme by identifying representations of characteristic pose configurations  key poses  in an image sequence  in line with perceptual studies  key poses are selected unsupervised utilizing a feature driven criterion which combines extrema in the motion energy with the horizontal and the vertical extendedness of a body shape  per class representations of key pose frames are learned using a deep convolutional neural network consisting of 15 convolutional layers  the network is trained using the energy efficient deep neuromorphic networks  eedn  framework  esser et al   2016   which realizes the mapping of the trained synaptic weights onto the ibm neurosynaptic system platform  merolla et al   2014   after the mapping  the trained network achieves real time capabilities for processing input streams and classify input images at about 1 000 frames per second while the computational stages only consume about 70 mw of energy  without spike transduction   particularly regarding mobile robotic systems  a low energy profile might be crucial in a variety of application scenarios  cross validation results are reported for two different datasets and compared to state of the art action recognition approaches  the results demonstrate  that  i  the presented approach is on par with other key pose based methods described in the literature  which select key pose frames by optimizing classification accuracy   ii  compared to the training on the full set of frames  representations trained on key pose frames result in a higher confidence in class assignments  and  iii  key pose representations show promising generalization capabilities in a cross dataset evaluation  
 real time coordination in human robot interaction using face and voice when humans interact and collaborate with each other  they coordinate their turn taking behaviors using verbal and nonverbal signals  expressed in the face and voice  if robots of the future are supposed to engage in social interaction with humans  it is essential that they can generate and understand these behaviors  in this article  i give an overview of several studies that show how humans in interaction with a humanlike robot make use of the same coordination signals typically found in studies on human human interaction  and that it is possible to automatically detect and combine these cues to facilitate real time coordination  the studies also show that humans react naturally to such signals when used by a robot  without being given any special instructions  they follow the gaze of the robot to disambiguate referring expressions  they conform when the robot selects the next speaker using gaze  and they respond naturally to subtle cues  such as gaze aversion  breathing  facial gestures  and hesitation sounds  
 real time imaging based assessment model for improving teaching performance and student experience in e learning multimedia is an essential and integral part of electronic learning  e learning   in this study  teaching performance and student learning experience are measured using real time multimedia processing tools and techniques for the e learning paradigm  visual attention and visual engagement analysis are performed using two developed algorithms  video lectures are recorded and delivered to students in e learning pedagogical setup  which are examined for the visual attention and visual engagement of the student and teacher  respectively  proposed methodology integrates the assessment on both student and teacher ends  multimedia processing of video lectures for teaching performance produces scoring dataset  the same methodology on student end for visual attention is used to investigate student experience  these types of datasets then reduced to time based datasets from the image based dataset  correlation and association of both datasets provide the opportunity to relate both student experience and teaching performance as well as to move forward to create content that is more useful  computational performance of the developed algorithms is compared using different video lectures with their processed frames per second  which is analyzed as per their corresponding bins  mean  max  and median of the processed frames of all the processed videos are also compared  
 reasoning from imperfect knowledge one reason why there is a lack of cross references between articles on knowledge representation in the cognitive and the information sciences is that cognitive scientists are interested in descriptive models of how people reason whereas information scientists are interested in prescriptive models to help people reason  formal ontologies such as the suggested upper merged ontology aid human reasoning by providing  1  an accurate knowledge base   2  a formalization of the knowledge base as axioms  and  3  a logic to derive new information through deductive reasoning  however  all systems confront obstacles when reasoning from imperfect knowledge consisting of ambiguous  conditional  contradictory  fragmented  inert  misclassified  or uncertain knowledge  we use work from the cognitive and the information sciences to analyze obstacles for both computers and people when confronted with ambiguous  contradictory  misclassified  and uncertain knowledge  a concluding section considers both practical and theoretical applications of ontologies in the cognitive sciences   c  2016 elsevier b v  all rights reserved  
 reasoning in non probabilistic uncertainty  logic programming and neural symbolic computing as examples this article aims to achieve two goals  to show that probability is not the only way of dealing with uncertainty  and even more  that there are kinds of uncertainty which are for principled reasons not addressable with probabilistic means   and to provide evidence that logic based methods can well support reasoning with uncertainty  for the latter claim  two paradigmatic examples are presented  logic programming with kleene semantics for modelling reasoning from information in a discourse  to an interpretation of the state of affairs of the intended model  and a neural symbolic implementation of input output logic for dealing with uncertainty in dynamic normative contexts  
 recent advances in document summarization the task of automatic document summarization aims at generating short summaries for originally long documents  a good summary should cover the most important information of the original document or a cluster of documents  while being coherent  non redundant and grammatically readable  numerous approaches for automatic summarization have been developed to date  in this paper we give a self contained  broad overview of recent progress made for document summarization within the last 5 years  specifically  we emphasize on significant contributions made in recent years that represent the state of the art of document summarization  including progress on modern sentence extraction approaches that improve concept coverage  information diversity and content coherence  as well as attempts from summarization frameworks that integrate sentence compression  and more abstractive systems that are able to produce completely new sentences  in addition  we review progress made for document summarization in domains  genres and applications that are different from traditional settings  we also point out some of the latest trends and highlight a few possible future directions  
 recent automatic text summarization techniques  a survey as information is available in abundance for every topic on internet  condensing the important information in the form of summary would benefit a number of users  hence  there is growing interest among the research community for developing new approaches to automatically summarize the text  automatic text summarization system generates a summary  i e  short length text that includes all the important information of the document  since the advent of text summarization in 1950s  researchers have been trying to improve techniques for generating summaries so that machine generated summary matches with the human made summary  summary can be generated through extractive as well as abstractive methods  abstractive methods are highly complex as they need extensive natural language processing  therefore  research community is focusing more on extractive summaries  trying to achieve more coherent and meaningful summaries  during a decade  several extractive approaches have been developed for automatic summary generation that implements a number of machine learning and optimization techniques  this paper presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade  their needs are identified and their advantages and disadvantages are listed in a comparative manner  a few abstractive and multilingual text summarization approaches are also covered  summary evaluation is another challenging issue in this research field  therefore  intrinsic as well as extrinsic both the methods of summary evaluation are described in detail along with text summarization evaluation conferences and workshops  furthermore  evaluation results of extractive summarization approaches are presented on some shared duc datasets  finally this paper concludes with the discussion of useful future directions that can help researchers to identify areas where further research is needed  
 recent development in big data analytics for business operations and risk management big data is an emerging topic and has attracted the attention of many researchers and practitioners in industrial systems engineering and cybernetics  big data analytics would definitely lead to valuable knowledge for many organizations  business operations and risk management can be a beneficiary as there are many data collection channels in the related industrial systems  e g   wireless sensor networks  internet based systems  etc    big data research  however  is still in its infancy  its focus is rather unclear and related studies are not well amalgamated  this paper aims to present the challenges and opportunities of big data analytics in this unique application domain  technological development and advances for industrial based business systems  reliability and security of industrial systems  and their operational risk management are examined  important areas for future research are also discussed and revealed  
 recognition of college students from weibo with deep neural networks classification of college students is a key to conduct further research on students  in this paper  we collect a set of samples and build deep neural network classifiers to recognize them  we also analyze the experiences and behaviors of the college students on weibo  firstly  we manually label 1502 student users and 1498 non college students  then  the data about their posts are crawled from weibo to be transformed into input vectors by feature engineering techniques  finally  classifiers are built based on two deep learning algorithms  including stacked autoencoders and deep belief network  experimental results show that deep neural networks performs better than other machine learning algorithms and the classification of the college students can achieve a very high accuracy  
 recognizing intentions of e commerce consumers based on ant colony optimization simulation identification of a consumer s intent has a vital impact on commodity recommendation  selection of hot drainage commodity  website layout  and link settings  most of the present studies on user intent are considered static  specific intent is accompanied by a specific environment  thus  intent is static when the environment does not change  however  the uncertainty of user access and purchase in e commerce activities indicates that user intent can assume multiple forms and has multiple developmental stages  therefore  this study draws support from the core ideas of an ant colony algorithm  ants represent users  and pheromones represent user intent  user intents of browsing  collection  cart shopping  and purchasing behavior are obtained from ant responses to pheromones  pheromone is expressed as the inner product of the objective attribute of commodity and user perception ability  because user intent is the matching result of objective attributes of commodity and subjective feelings of users  and its value is the concentration of user intent pheromone  thus  the dynamics and uncertainty of user intention development can be presented by the ant colony algorithm  in this study  data were obtained from a netlogo simulation experiment  we used neural networks to identify and verify user intentions of browsing  collection  cart shopping  and purchasing  the experimental results showed that the accuracy of intention prediction increased from 48  to 67   and a level of the 11 20  accuracy improvement shows good  realistic predictions  
 recognizing spontaneous micro expression from eye region micro expression is a kind of spontaneous facial expression  which is with short duration and low intensity  because of its involuntary feature  it is helpful to reveal one s true emotion when someone tries to conceal  therefore  it has attracted a great of attentions from the field of affective computing  previous methods focus on recognizing micro expression on the whole face  in fact  it is worthy to note that micro expression often appears in the eye area  in this paper  we present a framework to recognize micro expressions within the eye region  namely eyeme  specifically  the lbp top feature is extracted from the eye region  and multiple classifiers are trained to recognize the expressions  we test the proposed framework on the widely used casme2 database  the experimental results showed that the proposed eyeme framework performs better than the methods using the whole face and mouth region when identifying happy and disgust expressions  it confirmed that the information on eye region is critical to the recognition of these kinds of micro expressions   c  2016 elsevier b v  all rights reserved  
 recognizing users feedback from non verbal communicative acts in conversational recommender systems conversational recommender systems produce personalized recommendations of potentially useful items by utilizing natural language dialogues for detecting user preferences  as well as for providing recommendations  in this work we investigate the role of affective factors such as attitudes  emotions  likes and dislikes in conversational recommender systems and how they can be used as implicit feedback to improve the information filtering process  we thus developed a multimodal framework for recognizing the attitude of the user during their conversation with diva  a dress shopping interactive assistant aimed at recommending fashion apparel  wee took into account speech prosody  body poses and facial expressions for providing implicit feedback to the system and for refining the recommendation accordingly  the shopping assistant has been embodied in the social robot nao and has been tested in the dress shopping scenario  our experimental results show that the proposed method is a promising way to implicitly profile the user and improve the performance of recommendations when explicit feedback is not available  thus demonstrating its effectiveness and viability   c  2017 elsevier b v  all rights reserved  
 recommender systems for product bundling recommender systems  rs  are a class of information filter applications whose main goal is to provide personalized recommendations  content  and services to users  recommendation services may support a firm s marketing strategy and contribute to increase revenues  most rs methods were designed to provide recommendations of single items  generating bundle recommendations  i e   recommendations of two or more items together  can satisfy consumer needs  while at the same time increase customers  buying scope and the firm s income  thus  finding and recommending an optimal and personal bundle becomes very important  recommendation of bundles of products should also involve personalized pricing to predict which price should be offered to a user in order for the bundle to maximize purchase probability  however  most recommendation methods do not involve such personal price adjustment  this paper introduces a novel model of bundle recommendations that integrates collaborative filtering  cf  techniques  demand functions  and price modeling  this model maximizes the expected revenue of a recommendation list by finding pairs of products and pricing them in a way that maximizes both the probability of its purchase by the user and the revenue received by selling the bundle  experiments with several real world datasets have been conducted in order to evaluate the accuracy of the bundling model predictions  this paper compares the proposed method with several state of theart methods  collaborative filtering and svd   it has been found that using bundle recommendation can improve the accuracy of results  furthermore  the suggested price recommendation model provides a good estimate of the actual price paid by the user and at the same time can increase the firm s income   c  2016 elsevier b v  all rights reserved  
 recurrent cartesian genetic programming of artificial neural networks cartesian genetic programming of artificial neural networks is a neuroevolutionary method based on cartesian genetic programming  cartesian genetic programming has recently been extended to allow recurrent connections  this work investigates applying the same recurrent extension to cartesian genetic programming of artificial neural networks in order to allow the evolution of recurrent neural networks  the new recurrent cartesian genetic programming of artificial neural networks method is applied to the domain of series forecasting where it is shown to significantly outperform all standard forecasting techniques used for comparison including autoregressive integrated moving average and multilayer perceptrons  an ablation study is also performed isolating which specific aspects of recurrent cartesian genetic programming of artificial neural networks contribute to it s effectiveness for series forecasting  
 red  a new method for performance ranking of large decision making units data envelopment analysis  dea  method has been widely used in many economic and industrial applications to measure efficiency and rank performances of decision making units  dmus   improving the accuracy and computation time in measuring the efficiency of dmus have been two main challenges for the dea  specifically  with large dmus  the dea based methods are argued to require large amount of memory space and cpu time to measure dmus efficiencies  and suffer from inability to obtain complete performance ranking  to address these issues  in this paper  a new alternative method that is based on input oriented model  iom  and efficiency ratio  er   called ratio efficiency dominance  red   is proposed  the proposed method seeks to minimize the inputs while maximizing the outputs to obtain efficiency or performance scores  which is independent of dea method and the use of linear programming  lp   it is also to overcome the drawbacks of uncontrolled convergence  non generalization and instability induced from integrating prediction techniques such as neural networks  nns  with dea  to evaluate the proposed method  experiments were performed on small  large and very large dmus data sets to show the effectiveness of proposed method  the experimental results demonstrated that  in all cases  the proposed method is able to produce a complete and more accurate ranking compared to the conventional dea methods or its hybrids  
 relevant applications of monte carlo simulation in solvency ii the definition of solvency for insurance companies  within the european union  is currently being revised as part of solvency ii directive  the new definition induces revolutionary changes in the logic of control and expands the responsibilities in business management  the rationale of the fundamental measures of the directive cannot be understood without reference to probability distribution functions  many insurers are struggling with the realisation of a so called  internal model  to assess risks and determine the overall solvency needs  as requested by the directive  the quantitative assessment of the solvency position of an insurer relies on monte carlo simulation  in particular on nested monte carlo simulation that produces very hard computational and technological problems to deal with  in this paper  we address methodological and computational issues of an  internal model  designing a tractable formulation of the very complex expectations resulting from the  market consistent  valuation of fundamental measures  such as technical provisions  solvency capital requirement and probability distribution forecast  in the solvency assessment of life insurance companies  we illustrate the software and technological solutions adopted to integrate the disar system an asset liability computational system for monitoring life insurance policies in advanced computing environments  thus meeting the demand for high computing performance that makes feasible the calculation process of the solvency measures covered by the directive  
 reliable flow in stochastic supply networks servicing pre  and after sales markets in this paper we consider a company including both a forward supply chain producing and supplying products to markets and an after sales supply chain providing spare parts to fulfill its after sales commitments  uncertainty in estimating product demand in the premarkets and spare parts demands in the after sales markets and qualified outputs of production facilities throughout these two chains are considered in this problem  stochastic qualified outputs of these facilities are due to their imperfect productions systems  we show that uncertainties propagate and qualified flow depreciates by moving material  components  and products from upstream to the downstream of these supply chains  and these phenomena should be quantified to determine the service levels of this company in the pre  and after sales markets  at the end  we propose a mathematical model determining the best marketing strategies for this company  i e   price  warranty length  and service levels  in the markets and their preserving reliable flow dynamics throughout the chains  networks  finally  the model is tested on a test problem defined in engine industry and some managerial insights are provided by analyzing the results  
 reliable intervals method in decision based support models for group decision making in this paper  a methodology to derive reliable intervals for multiplicative preference relations  or pairwise comparison matrices  satisfying consistency and consensus indexes is introduced  our approach is proposed via a combination of numerical algorithms and a nonlinear optimization algorithm  a synthesis of reliable intervals is achieved  where group decision makers show evidence of these intervals to express flexibility in the manner of their preferences  while accomplishing some a priori decision targets  rules and advice given by their current framework  the algorithms are applied to some examples in order to illustrate our results and compare them with other methodologies  
 relpron  a relative clause evaluation data set for compositional distributional semantics this article introduces relpron  a large data set of subject and object relative clauses  for the evaluation of methods in compositional distributional semantics  relpron targets an intermediate level of grammatical complexity between content word pairs and full sentences  the task involves matching terms  such as wisdom  with representative properties  such as quality that experience teaches  a unique feature of relpron is that it is built from attested properties  but without the need for them to appear in relative clause format in the source corpus  the article also presents some initial experiments on relpron  using a variety of composition methods including simple baselines  arithmetic operators on vectors  and finally  more complex methods in which argument taking words are represented as tensors  the latter methods are based on the categorial framework  which is described in detail  the results show that vector addition is difficult to beatin line with the existing literaturebut that an implementation of the categorial framework based on the practical lexical function model is able to match the performance of vector addition  the article finishes with an in depth analysis of relpron  showing how results vary across subject and object relative clauses  across different head nouns  and how the methods perform on the subtasks necessary for capturing relative clause semantics  as well as providing a qualitative analysis highlighting some of the more common errors  our hope is that the competitive results presented here  in which the best systems are on average ranking one out of every two properties correctly for a given term  will inspire new approaches to the relpron ranking task and other tasks based on linguistically interesting constructions  
 representability of fuzzy biorders and fuzzy weak orders in this paper  we have studied representability of both fuzzy biorders and fuzzy weak orders  it has also been shown that union of a finite family of fuzzy weak orders with respect to a t norm t is a fuzzy quasi transitive relation with respect to t  in the last theorem  we have obtained a characterization for a t l representable fuzzy weak order  
 representing meaning with a combination of logical and distributional models nlp tasks differ in the semantic information they require  and at this time no single semantic representation fulfills all requirements  logic based representations characterize sentence structure  but do not capture the graded aspect of meaning  distributional models give graded similarity ratings for words and phrases  but do not capture sentence structure in the same detail as logic based approaches  it has therefore been argued that the two are complementary  we adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic  specifically markov logic networks  in this article  we focus on the three components of a practical system  1  1  logical representation focuses on representing the input problems in probabilistic logic  2  knowledge base construction creates weighted inference rules by integrating distributional information with other sources  and 3  probabilistic inference involves solving the resulting mln inference problems efficiently  to evaluate our approach  we use the task of textual entailment  which can utilize the strengths of both logic based and distributional representations  in particular we focus on the sick data set  where we achieve state of the art results  we also release a lexical entailment data set of 10 213 rules extracted from the sick data set  which is a valuable resource for evaluating lexical entailment systems  2  
 reprint of  multimodal adaptive interfaces for 3d robot mediated upper limb neuro rehabilitation  an overview of bio cooperative systems  robot mediated neuro rehabilitation has been proved to be an effective therapeutic approach for upper limb motor recovery after stroke  though its actual potential when compared to other conventional approaches has still to be fully demonstrated  most of the proposed solutions use a planar workspace  one key aspect for influencing motor recovery mechanisms  such as neuroplasticity and the level of motivation and involvement of the patient in the exercise  is the design of patient tailored protocols and on line adaptation of the assistance provided by the robotic agent to the patient performance  also  when abilities for performing activities of daily living shall be targeted  exercises in 3d workspace are highly preferable  this paper wants to provide a complete overview on bio cooperative systems on neuro rehabilitation  with a special focus on 3d multimodal adaptive interfaces  by partly in depth reviewing the literature and partly proposing an illustrative case of how to build such a bio cooperative based on the authors  current research  it consists of an operational robotic platform for 3d upper limb robot aided rehabilitation  directly derived from the maat system previously developed by the same research group  the system features on line adaptation of therapy characteristics to specific patient needs and to the measured level of performance  by including the patient in the control loop  the system is composed of a 7 dof robot arm  an adaptive interaction control system  a motorized arm weight support system and a module for on line evaluation of patient performance  such module records patient biomechanical data through an unobtrusive  wearable sensory system  evaluates patient biomechanical state and updates robot control parameters for modifying level of assistance and task complexity in the 3d workspace  in addition  a multimodal interface provides information needed to control the motorized arm weight support by means of a dedicated cable pulley system   c  2016 elsevier b v  all rights reserved  
 reputation in an open source software community  antecedents and impacts a developer s reputation in the oss community is determined by all the evaluations received from his or her peers  while a large body of studies focuses on the importance of developers  reputations in their participation motivations  there is still lack of understanding for two issues  first  which factors can lead to a high developer s reputation  second  how does the overall reputation of the developers  in a project impact project success  in this study  we develop a theoretical model and conduct an empirical analysis in a large online open source community  the results show that a developer s reputation level is determined by his or her 1  coding quality  2  the deviation of the commitment behavior  3  community experience  and 4  collaboration experience  in addition  we find that the group with an overall higher level of reputation would achieve a better performance  while the individual reputation level deviation within the group would impair its technical success  the implications of our findings and the future research directions are then discussed   c  2016 elsevier b v  all rights reserved  
 research on closed loop supply chain with reference price effect this paper considers a closed loop supply chain with the manufacturer as the stackelberg leader  the manufacturer faces three different reverse channels  i e    1  manufacturer managed   2  retailer managed  or  3  third party managed channels  the reference price affects the purchase decision of consumers  based on game theory  we discuss the reference price effect on the performances across three decentralized reverse channels  and examine the impact of reference price parameter  i e   reference price coefficient in this paper  on optimal strategies  we conclude that higher reference price coefficient results in lower manufacturer and retailer profits  however  the profit of the third party increases in the reference price coefficient  in addition  some meaningful insights can be derived by comparison without the reference price effect in our models  we found that the scenario without reference price effect is generally superior to that with reference price effect  
 research on the evaluation of enterprise s organization innovative climate based on intuitionistic fuzzy number innovation is the effective weapon for enterprises to face complicated dynamic environment  and to obtain long term competitive advantages  organization innovative climate can promote enterprises  innovative capability and performance through affecting individual staff attitude  motivation and innovative activities  therefore  by evaluating organization innovative climate of enterprises with scientific methods  weak points can be discovered and adjusted  so promotion of organization innovative climate is the essence of maintaining enterprises  innovative energy  based on domestic and overseas scholars  research  this paper illustrates new quantitative evaluation method  enterprises  organization innovative climate on the basis of intuitionistic fuzzy number  indicators are scored through newly constructed evaluation indicator system  scores of enterprises  organization innovative climate are obtained through the calculation of intuitionistic fuzzy number  this model which is more scientific and completed  avoids the awareness of much evaluation information  and it enriched enterprises  organization innovative climate evaluation theory and methods  at last  the feasibility and practicability of the approach introduced are proved through empirical analysis  
 research on visual cognition about sharp turn sign based on driver s eye movement characteristic as a traffic language  sharp turn sign is a kind of important road infrastructure that indicates a sharp turning will show up at the coming road  warning drivers to slow down to ensure the driving safety  in this paper  real vehicle test was carried out on mountain road with an eye tracker equipment  at differerent driving speeds  parameters of eye movement characteristics in the visual cognition process of sharp turn signs were collected  including distribution of gaze points  fixation and saccade  simultaneously  driver s scan paths of recognizing sharp turn signs with diffrerent supporting forms were gathered  the results of the analysis of testing data showed that the dispersion of distribution of gaze points would increase with driving velocity  saccade was the main method for driver to capture information of sharp turn signs  while driving speed was lower than 60 km h  fixation was also one of the methods  for the visual cognition process of sharp turn sign with cantilever  compared to post  the searching scope was wider both in horizontal and vertical directions  this study is beneficial to evaluate the rationality of sharp turn signs  promoting the using efficiency of signs and improving the driving safety on mountain road  
 resistance and power in a security certification scheme  the case of c cure using the lens of clegg s circuits of power  cop  framework  this study examines the resistance to a uk information security certification scheme through three episodes of power that led to its withdrawal in 2000  the uk authorities sought to generate market competition between a generic certificate scheme with lower costs and international recognition and one based on technical rigor  but they failed in their objectives because of resistance from organizational players  this paper makes contributions to the understanding of the discursive nature of resistance to change in the research of standards and certification  and contributes to the literature by formulating the concept of discourse resilience  the property of discourses to resist change  it identifies the non agentic nature of resistance in the absence of coercive power and presents a reflection on legitimacy as a required attribute for the acceptance of a certificate scheme  the research finds that what organizations deem to be legitimate is the result of power   c  2016 elsevier b v  all rights reserved  
 resting state effective connectivity allows auditory hallucination discrimination hallucinations are elusive phenomena that have been associated with psychotic behavior  but that have a high prevalence in healthy population  some generative mechanisms of auditory hallucinations  ah  have been proposed in the literature  but so far empirical evidence is scarce  the most widely accepted generative mechanism hypothesis nowadays consists in the faulty workings of a network of brain areas including the emotional control  the audio and language processing  and the inhibition and self attribution of the signals in the auditive cortex  in this paper  we consider two methods to analyze resting state fmri  rs fmri  data  in order to measure effective connections between the brain regions involved in the ah generation process  these measures are the dynamic causal modeling  dcm  cross covariance function  ccf  coefficients  and the partially directed coherence  pdc  coefficients derived from granger causality  gc  analysis  effective connectivity measures are treated as input classifier features to assess their significance by means of cross validation classification accuracy results in a wrapper feature selection approach  experimental results using support vector machine  svm  classifiers on an rs fmri dataset of schizophrenia patients with and without a history of ah confirm that the main regions identified in the ah generative mechanism hypothesis have significant effective connection values  under both dcm and pdc evaluation  
 restricted non projectivity  coverage vs  efficiency in the last decade  various restricted classes of non projective dependency trees have been proposed with the goal of achieving a good tradeoff between parsing efficiency and coverage of the syntactic structures found in natural languages  we perform an extensive study measuring the coverage of a wide range of such classes on corpora of 30 languages under two different syntactic annotation criteria  the results show that  among the currently known relaxations of projectivity  the best tradeoff between coverage and computational complexity of exact parsing is achieved by either 1 endpoint crossing trees or mhk trees  depending on the level of coverage desired  we also present some properties of the relation of mhk trees to other relevant classes of trees  
 review of the use of ai techniques in serious games  decision making and machine learning the video game market has become an established and ever growing global industry  the health of the video and computer games industry  together with the variety of genres and technologies available  means that video game concepts and programmes are being applied in numerous different disciplines  one of these is the field known as serious games  the main goal of this paper is to collect all the relevant articles published during the last decade and create a trend analysis about the use of certain artificial intelligence algorithms related to decision making and learning in the field of serious games  a categorization framework was designed and outlined to classify the 129 papers that met the inclusion criteria  the authors made use of this categorization framework for drawing some conclusions regarding the actual use of intelligent serious games  the authors consider that over recent years enough knowledge has been gathered to create new intelligent serious games to consider not only the final aim but also the technologies and techniques used to provide players with a nearly real experience  however  researchers may need to improve their testing methodology for developed serious games  so as to ensure they meet their final purposes  
 review popularity and review helpfulness  a model for user review effectiveness the wide adoption and perceived helpfulness of online user reviews on consumers  decision making have energized academic research on the assessment of review effectiveness  although the literature probed the impacts of user reviews on various elements of review effectiveness independently  little research has done to examine them jointly  inspired by communication theories  we conceptualize a framework for user review effectiveness in which we focus on the joint assessment of its first two elements  review popularity and review helpfulness  we develop our hypotheses regarding the effects of the user review determinants on both review popularity and review helpfulness  and further develop an operational model to empirically test our hypotheses using data collected from amazon  our study suggests that disentangling review popularity and review helpfulness in assessing review effectiveness is not only conceptually sounding  but also managerially beneficial  we find that review popularity is as important as review helpfulness in review effectiveness evaluations  review determinants may play opposite roles on review popularity and review helpfulness  e g   valence   and can drive review effectiveness via review popularity or review helpfulness or both  these findings offer new insights for various decision makers to harvest user review effectiveness in online markets   c  2017 elsevier b v  all rights reserved  
 revisiting the ontologising of semantic relation arguments in wordnet synsets ontologising is the task of associating terms  in text  with an ontological representation of their meaning  in an ontology  in this article  we revisit algorithms that have previously been used to ontologise the arguments of semantic relations in a relationless thesaurus  resulting in a wordnet  for increased flexibility  the algorithms do not use the extraction context when selecting the most adequate synsets for each term argument  instead  they exploit a term based lexical network which can be established by knowledge extracted automatically  or obtained from the resource the relations are being ontologised to  on the latter idea  we made several experiments to conclude that the algorithms can be used both for wordnet creation and for their enrichment  besides describing the algorithms with some detail  the aforementioned experiments  which target both english and portuguese  and their results are reported and discussed  
 rfid enabled flexible warehousing we propose a smart warehouse environment where not only inventory items but also the shelves are tracked by an rfid based system  both operational activities and warehouse configurations are continually monitored to facilitate real time response  we study the dynamics of a flexible warehouse scenario where items of any type can be dropped off anywhere within the premises  unlike existing models  we relax both the location constraint and local  e g   item type level  capacity constraints with a periodically renewable fixed global capacity  dynamic decisions on location and local capacity are made based on the stochastic markovian demand states  we optimize processing and routing constraints and compare the performance of this flexible storage setup with classical models through multiple levels of real time decision support  our results provide corroborating evidence to support the following observations   1   free pick n drop  combined with fluid warehousing mechanism greatly reduces trip costs and lead time for single trip demand   2  there exists a lower bound on the performance in such a setup with fixed local capacities  and  3  the lower bound can be further improved when inventory capacity and location are dynamically adjusted according to actual demand patterns   c  2017 elsevier b v  all rights reserved  
 risk factor assessment improvement for china s cloud computing auditing using a new hybrid madm model with the rapid growth of modern technology in all facets of trade and industry  traditional auditing systems can no longer meet today s ever increasing technological requirements  contemporary auditing is in urgent need of straightforward and quick cloud computing services to meet the needs of auditors  in keeping with current trends  auditors must be provided with the means of using data and auditing information stored in cloud systems for more effcient and elastic auditing  however  the risks involved in cloud computing auditing are widespread and complex  dimensions should be established  a mutually influential relationship used to delineate the influence weights of the dimensions and criteria should be determined  to this end  a multiple attribute decision making   madm  model can precisely solve multi criteria problems simultaneously  therefore  the main focus of this study is to determine how to assess and establish the best improvement strategies to achieve the aspiration level for cloud auditing risk factors  by using the opinions and practical experience of china s accounting experts  applied with a decision making trial and evaluation laboratory  dematel  technique  dematel based anp  danp  and modified vikor method  the results provide cloud auditing risks with a knowledge based understanding of the problem sources in order to establish the best improvement strategies for reducing risk auditing performance gaps and attaining the aspiration levels  based on the degree of impact of the dimensions criteria on an influence network relation map  inrm   improvements should be prioritized as follows  system operations  technology risks  identity and access management and data protection  
 risk based material selection process supported on information theory  a case study on industrial gas turbine turbine blades are produced using high temperature high strength materials because they are oftenexposed to severe environments  material selection for such cases should be performed with sensitivity and using a systematic method  moreover  various risks may exist about the values of materials properties  for example  the difference between the designed and unexpected conditions may pose some risksto materials properties  although considering risks for gas turbine blade material selection problem or similar high tech practical cases is important  a research gap exists in such fields  this paper presents ariskbased material selection algorithm using the principles of suh and shannon entropies supported on information theory  in this regard  we develop the risk based fuzzy axiomatic design approach with theintegrated shannon significance coefficients of attributes  a real  world example about material selection of industrial gas turbine blade is examined using four techniques including the fuzzy axiomatic design weighted fuzzy axiomatic design  risk based fuzzy axiomatic design  and weighted risk basedfuzzy axiomatic design approaches  in the example  risk factors are determined to consider the effect oftemperature variation on materials properties  finally  the resultant rankings are compared by calculatingspearman rank correlation coefficients  the comparisons show that considering risk factors in the problem affects the resultant ranking  we validate the results of the proposed methods by the unweighted and weighted fuzzy multimoora approaches   c  2016 elsevier b v  all rights reserved  
 robotic wrist training after stroke  adaptive modulation of assistance in pediatric rehabilitation in this paper we present a case study in which a 14 year old  right handed stroke patient with severe weakness  spasticity  and motor dysfunction of the left upper extremity participated in a three month distal robotic training program  the robotic device was compliant to the patient s movements and was able to modulate the level of assistance continuously throughout the trial  i e   online adaptive modulation   standard clinical and robotic evaluations of upper extremity motor performance were conducted before and after robotic training  there were improvements in upper extremity spasticity and motor functions  in addition  robotic training lead to positive changes in wrist active range of motion and kinematics  movements were smoother and there was a noticeable decrease in the level of robotic intervention required to complete each trial  in sum  results of the present case study demonstrate that distal upper extremity robotic rehabilitation that features the proposed adaptive control algorithm promoted positive changes in upper limb motor coordination and function after pediatric stroke   c  2017 published by elsevier b v  
 robust dea efficiency scores  a probabilistic combinatorial approach in this paper we propose robust efficiency scores for the scenario in which the specification of the inputs outputs to be included in the dea model is modelled with a probability distribution  this probabilistic approach allows us to obtain three different robust efficiency scores  the conditional expected score  the unconditional expected score and the expected score under the assumption of maximum entropy principle  the calculation of the three efficiency scores involves the resolution of an exponential number of linear problems  the algorithm presented in this paper allows to solve over 200 millions of linear problems in an affordable time when considering up 20 inputs outputs and 200 dmus  the approach proposed is illustrated with an application to the assessment of professional tennis players   c  2017 elsevier ltd  all rights reserved  
 robust identification of highly persistent interest rate regimes parametric specifications in state space models  ssms  are a source of bias in case of mismatch between modeling assumptions and reality  we propose a bayesian semiparametric ssm that is robust to misspecified emission distributions  the markovian nature of the latent stochastic process creates a temporal dependence and links the random probability distributions of the observations in a mixture of products of dirichlet processes  mpdp   the model is shown to be adequate and it is applied to simulated data and to the motivating empirical problem of regime shifts in interest rates with latent state persistence   c  2017 elsevier inc  all rights reserved  
 robust semi supervised clustering with polyhedral and circular uncertainty we consider a semi supervised clustering problem where the locations of the data objects are subject to uncertainty  each uncertainty set is assumed to be either a closed convex bounded polyhedron or a closed disk  the final clustering is expected to be in accordance with a given number of instance level constraints  the objective function considered minimizes the total of the sum of the violation costs of the unsatisfied instance level constraints and a weighted sum of squared maximum euclidean distances between the locations of the data objects and the centroids of the clusters they are assigned to  given a cluster  we first consider the problem of computing its centroid  namely the centroid computation problem under uncertainty  ccpu    we show that the ccpu can be modeled as a second order cone programing problem and hence can be efficiently solved to optimality  as the ccpu is one of the key ingredients of the several algorithms considered in this paper  a subgradient algorithm is also adopted for its faster solution  we then propose a mixed integer second order cone programing formulation for the considered clustering problem which is only able to solve small size instances to optimality  for larger instances  approaches from the semi supervised clustering literature are modified and compared in terms of computational time and quality   c  2017 elsevier b v  all rights reserved  
 roc curves and nonrandom data this paper shows that when a classifier is evaluated with nonrandom test data  roc curves differ from the roc curves that would be obtained with a random sample  to address this bias  this paper introduces a procedure for plotting roc curves that are inferred from nonrandom test data  i provide simulations to illustrate the procedure as well as the magnitude of bias that is found in empirical roc curves constructed with nonrandom test data  the paper also includes a demonstration of the procedure on  non simulated  data used to model wine preferences in the wine industry   c  2016 elsevier b v  all rights reserved  
 rule based back propagation neural networks for various precision rough set presented kansei knowledge prediction  a case study on shoe product form features extraction nonlinear operators for kansei evaluation dataset were significantly developed such as uncertainty reason techniques including rough set  fuzzy set and neural networks  in order to extract more accurate kansei knowledge  rule based presentation was concluded a promising way in kansei engineering research  in the present work  variable precision rough set was applied in rule based system to reduce the complexity of the knowledge database from normal item dataset to high frequent rule set  in addition  evidence theory s reliability indices  namely the support and confidence for rule based knowledge presentation  were proposed by using back propagation neural network with bayesian regularization algorithm  the proposed method was applied in shoes kansei evaluation system  for a certain kansei adjective  the key form features of products were predicted  some similar algorithms such as levenberg marquardt and scaled conjugate gradient were also discussed and compared to establish the effectiveness of the proposed approach  the experimental results established the effectiveness and feasibility of the proposed algorithms customized for shoe industry  where the proposed back propagation neural network bayesian regularization approach achieved superior performance compared to the other algorithms in terms of the performance  gradient  mu  effective number of parameter  and the sum square parameter in kansei support and confidence time series prediction  
 safety management of waterway congestions under dynamic risk conditions a case study of the yangtze river with the continuous increase of traffic volume in recent years  inland waterway transportation suffers more and more from congestion problems  which form a major impediment to its development  thus  it is of great significance for the stakeholders and decision makers to address these congestion issues properly  fuzzy techniques for order preference by similarity to an ideal solution  topsis  is widely used for solving multiple criteria decision making  mcdm  problems with ambiguity  when taking into account fuzzy topsis  decisions are made in a static scenario with fixed weights assigned to the criteria  however  risk conditions usually vary in real life cases  which will inevitably affect the preference ranking of the alternatives  to make flexible decisions according to the dynamics of congestion risks and to achieve a rational risk analysis for prioritising congestion risk control options  rcos   the cost benefit ratio  cbr  is used in this paper to reflect the change of risk conditions  the hybrid of cbr and fuzzy topsis is illustrated by investigating the congestion risks of the yangtze river  the ranking of rcos varies depending on the scenarios with different congestion risk conditions  the research findings indicate that some rcos  e g   channel dredging and maintenance   and  prohibition of navigation   are more cost effective in the situation of a high level of congestion risk  while the other rcos  e g   loading restriction   and  crew management and training   are more beneficial in a relatively low congestion risk condition  the proposed methods and the evaluation results provide useful insights for effective safety management of the inland waterway congestions under dynamic risk conditions   c  2017 elsevier b v  all rights reserved  
 salient object subitizing we study the problem of salient object subitizing  i e  predicting the existence and the number of salient objects in an image using holistic cues  this task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range  1 4   to this end  we present a salient object subitizing image dataset of about 14k everyday images which are annotated using an online crowdsourcing marketplace  we show that using an end to end trained convolutional neural network  cnn  model  we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object  for images with multiple salient objects  our model also provides significantly better than chance performance without requiring any localization process  moreover  we propose a method to improve the training of the cnn subitizing model by leveraging synthetic images  in experiments  wedemonstrate the accuracy and generalizability of our cnn subitizing model and its applications in salient object detection and image retrieval  
 sameness difference spiking neural circuit as a relational concept precursor model  a bio inspired robotic implementation this paper presents a bio inspired virtual and physical robotic model of sameness difference  sd  abstract relations investigated through associative learning tasks  considering that the invertebrate bees presumably possess this higher cognitive ability with a relatively tiny brain  giurfa  zhang  jenett  menzel    srinivasan  2001   an hypothesis of a neural correlate may hold as a minimalist cellular circuit  in order to simulate it  we implemented the model using an artificial spiking neural network  snn  acting as a robot s brain controller and a reward modulated spike timing dependent plasticity as the learning rule  the model is tested with different operant conditioning procedures of delayed matching to sample and delayed non matching to sample tasks from dual choices of colors  apart sd  the color and side simpler associative rules also co exist  this allows the robot to learn in different scenarios  not knowing the rule to come  therefore  we showed that depending on its action and the applied reinforcing rules  sd  color or side   the robot dynamically learns any of it  independently and one after the other  all supported by a single snn  we believe that this sd snn model could be used as a precursor base to understand and build a generic form of a relational concept mechanism   c  2017 elsevier b v  all rights reserved  
 sampling algorithms for stochastic graphs  a learning automata approach recently  there has been growing interest in social network analysis  graph models for social network analysis are usually assumed to be a deterministic graph with fixed weights for its edges or nodes  as activities of users in online social networks are changed with time  however  this assumption is too restrictive because of uncertainty  unpredictability and the time varying nature of such real networks  the existing network measures and network sampling algorithms for complex social networks are designed basically for deterministic binary graphs with fixed weights  this results in loss of much of the information about the behavior of the network contained in its time varying edge weights of network  such that is not an appropriate measure or sample for unveiling the important natural properties of the original network embedded in the varying edge weights  in this paper  we suggest that using stochastic graphs  in which weights associated with the edges are random variables  can be a suitable model for complex social network  once the network model is chosen to be stochastic graphs  every aspect of the network such as path  clique  spanning tree  network measures and sampling algorithms should be treated stochastically  in particular  the network measures should be reformulated and new network sampling algorithms must be designed to reflect the stochastic nature of the network  in this paper  we first define some network measures for stochastic graphs  and then we propose four sampling algorithms based on learning automata for stochastic graphs  in order to study the performance of the proposed sampling algorithms  several experiments are conducted on real and synthetic stochastic graphs  the performances of these algorithms are studied in terms of kolmogorov smirnov d statistics  relative error  kendall s rank correlation coefficient and relative cost   c  2017 elsevier b v  all rights reserved  
 sao semantic information identification for text mining a subject action object  sao  is a triple structure which can be used to both describe topics in detail and explore the relationship between them  sao analysis has become popular in bibliometrics  however there are two challenges in the identification of sao structures  low relevance of saos to domain topics  and synonyms in sao  these problems make the identification of sao greatly dependent upon domain experts  limiting the further usage of sao and influencing further the mining of sao characteristics  this paper proposes a parse tree based sao identification method that includes  1  a model to identify the core components  candidate terms for subject   object  of sao structures  where term clumping processes and co word analysis are involved   2  a parse tree based hierarchical sao extraction model to divide entire sao structures into a collection of simpler sub tasks for separate subject  action  and object identification  and  3  an sao weighting model to rank sao structures for result selection  the proposed method is applied to publications in the journal of scientometrics  scim   to identify and rank significant sao structures  our experiment results demonstrate the validity and feasibility of the proposed method  
 scalable computational techniques for centrality metrics on temporally detailed social network increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale  a collection of all such social interactions among a group of individuals  or agents  observed over an interval of time is referred to as a temporally detailed  td  social network  a td social network opens up the opportunity to explore td questions on the underlying social system  e g    how is the betweenness centrality of an individual changing with time   to this end  related work has proposed temporal extensions of centrality metrics  e g   betweenness and closeness   however  scalable computation of these metrics for long time intervals is challenging  this is due to the non stationary ranking of shortest paths  the underlying structure of betweenness and closeness  between a pair of nodes which violates the assumptions of classical dynamic programming based techniques  to this end  we propose a novel computational paradigm called epoch point based techniques for addressing the non stationarity challenge of td social networks  using the concept of epoch points  we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a td social network  we prove the correctness and completeness of our algorithm  our experimental analysis shows that the proposed algorithm out performs the alternatives by a wide margin  
 scalable iterative classification for sanitizing large scale datasets cheap ubiquitous computing enables the collection of massive amounts of personal data in a wide variety of domains  many organizations aim to share such data while obscuring features that could disclose personally identifiable information  much of this data exhibits weak structure  e g   text   such that machine learning approaches have been developed to detect and remove identifiers from it  while learning is never perfect  and relying on such approaches to sanitize data can leak sensitive information  a small risk is often acceptable  our goal is to balance the value of published data and the risk of an adversary discovering leaked identifiers  we model data sanitization as a game between 1  a publisher who chooses a set of classifiers to apply to data and publishes only instances predicted as non sensitive and 2  an attacker who combines machine learning and manual inspection to uncover leaked identifying information  we introduce a fast iterative greedy algorithm for the publisher that ensures a low utility for a resource limited adversary  moreover  using five text data sets we illustrate that our algorithm leaves virtually no automatically identifiable sensitive instances for a state of the art learning algorithm  while sharing over 93 percent of the original data  and completes after at most five iterations  
 scatter search for minimizing weighted tardiness in a single machine scheduling with setups single machine scheduling problems have many real life applications and may be hard to solve due to the particular characteristics of some production environments  in this paper  we tackle the single machine scheduling problem with sequence dependent setup times with the objective of minimizing the weighted tardiness  to solve this problem  we propose a scatter search algorithm which uses path relinking in its core  this algorithm is enhanced with some procedures to speed up the neighbors  evaluation and with some diversification and intensification techniques  the latter taking some elements from iterated local search  we conducted an experimental study across a well known set of instances to analyze the contribution of each component to the overall performance of the algorithm  as well as to compare our proposal with the state of the art metaheuristics  obtaining competitive results  we also propose a new benchmark with larger and more challenging instances and provide the first results for them  
 scatter search for mixed blocking flowshop scheduling empty or limited storage capacities between machines introduce various types of blocking constraint in the industries with flowshop environment  while large applications demand flowshop scheduling with a mix of different types of blocking  research in this area mainly focuses on using only one kind of blocking in a given problem instance  in this paper  using makespan as a criterion  we study permutation flowshops with zero capacity buffers operating under mixed blocking conditions  we present a very effective scatter search  ss  algorithm for this  at the initialisation phase of ss  we use a modified version of the well known nawaz  enscore and ham  neh  heuristic  for the improvement method in ss  we use an iterated local search  ils  algorithm that adopts a greedy job selection and a powerful neh based perturbation procedure  moreover  in the reference set update phase of ss  with small probabilities  we accept worse solutions so as to increase the search diversity  on standard benchmark problems of varying sizes  our algorithm very significantly outperforms well known existing algorithms in terms of both the solution quality and the computing time  moreover  our algorithm has found new upper bounds for 314 out of 360 benchmark problem instances   c  2017 elsevier ltd  all rights reserved  
 script passive orthosis  design of interactive hand and wrist exoskeleton for rehabilitation at home after stroke recovery of functional hand movements after stroke is directly linked to rehabilitation duration and intensity  continued therapy at home has the potential to increase both  for many patients this requires a device that helps them overcome the hyperflexion of wrist and fingers that is limiting their ability to open and use their hand  we developed an interactive hand and wrist orthosis for post stroke rehabilitation that provides compliant and adaptable extension assistance at the wrist and fingers  interfaces with motivational games based on activities of daily living  is integrated with an off the shelf mobile arm support and includes novel wrist and finger actuation mechanisms  during the iterative development  multiple prototypes have been evaluated by therapists in clinical settings and used intensively and independently by 33 patients at home  this paper details the final design of the script passive orthosis resulting from these efforts  
 season aware attraction recommendation method with dual trust enhancement attraction recommendation plays an essential role in tourism  for example  it can relieve information overload for tourists and increase sales for tourism operators  when making travel decisions  tourists depend heavily on the personal preferences and suggestions from people they trust  however  most existing attraction recommendation methods focus on the tourist preferences for topics of attractions  yet overlook the seasonality in topic preferences  additionally  extant studies are generally based on a single type of trust  which may represent trust relations inaccurately  in order to overcome these issues  we propose a novel season aware attraction recommendation method based on the seasonal topic preferences and dual trust relations  firstly  we capture tourists  seasonal topic preferences by analyzing their travel histories along two dimensions  time and attraction  secondly  we develop a dual trust relationship  dtr  model based on familiarity based trust and similarity based trust  in contrast to existing studies that purely focus on a single type of trust  thirdly  we propose a novel season aware attraction recommendation method named sar dtr  in a specific season  it predicts ratings based on both topic preferences in the given season and suggestions from tourists they trust  to demonstrate the superiority of the proposed method to other approaches  an empirical study with real world data was conducted  the experimental results regarding both prediction and recommendation performance are reported  
 selective opportunity disclosure at the service of strategic information platforms this paper studies the strategic behavior of platforms that provide agents easier access to the type of opportunities in which they are interested  e g   ecommerce platforms  used cars bulletins and dating web sites   we show that under four common service schemes  a platform can benefit from not necessarily listing all the opportunities with which it is familiar  even if there is no marginal cost for listing any additional opportunity  the main implication of this result is that platforms should take the subset of opportunities to be included in their listings as a decision variable  alongside the fees set for the service in their expected profit  maximizing  optimization problem  we show that none of the four schemes generally dominates any of the others or is dominated by any  for the case of homogeneous preferences  however  several dominance relationships can be proved  furthermore  the analysis provides a game theoretic search based explanation for a possible preference of buyers to pay for the service rather than receive it for free  e g   when the service is sponsored by ads   the paper shows that this preference can hold both for the users and the platform simultaneously in a given setting  even if both sides are fully strategic  finally  the paper analyzes the potential improvement in the platform s expected profit that can be achieved by considering hybrid service schemes that combine the basic ones  in particular  we focus in the two part tariff scheme that combines the two commonly used subscription and pay per click schemes  
 semi automatic terminology ontology learning based on topic modeling ontologies provide features like a common vocabulary  reusability  machine readable content  and also allows for semantic search  facilitate agent interaction and ordering   structuring of knowledge for the semantic web  web 3 0  application  however  the challenge in ontology engineering is automatic learning  i e   the there is still a lack of fully automatic approach from a text corpus or dataset of various topics to form ontology using machine learning techniques  in this paper  two topic modeling algorithms are explored  namely lsi   svd and mr lda for learning topic ontology  the objective is to determine the statistical relationship between document and terms to build a topic ontology and ontology graph with minimum human intervention  experimental analysis on building a topic ontology and semantic retrieving corresponding topic ontology for the user s query demonstrating the effectiveness of the proposed approach   c  2017 elsevier ltd  all rights reserved  
 semi supervised learning for affective common sense reasoning big social data analysis is the area of research focusing on collecting  examining  and processing large multi modal and multi source datasets in order to discover patterns correlations and extract information from the social web  this is usually accomplished through the use of supervised and unsupervised machine learning algorithms that learn from the available data  however  these are usually highly computationally expensive  either in the training or in the prediction phase  as they are often not able to handle current data volumes  parallel approaches have been proposed in order to boost processing speeds  but this clearly requires technologies that support distributed computations  extreme learning machines  elms  are an emerging learning paradigm  presenting an efficient unified solution to generalized feed forward neural networks  elm offers significant advantages such as fast learning speed  ease of implementation  and minimal human intervention  however  elm cannot be easily parallelized  due to the presence of a pseudo inverse calculation  therefore  this paper aims to find a reliable method to realize a parallel implementation of elm that can be applied to large datasets typical of big data problems with the employment of the most recent technology for parallel in memory computation  i e   spark  designed to efficiently deal with iterative procedures that recursively perform operations over the same data  moreover  this paper shows how to take advantage of the most recent advances in statistical learning theory  slt  in order to address the issue of selecting elm hyperparameters that give the best generalization performance  this involves assessing the performance of such algorithms  i e   resampling methods and in sample methods  by exploiting the most recent results in slt and adapting them to the big data framework  the proposed approach has been tested on two affective analogical reasoning datasets  affective analogical reasoning can be defined as the intrinsically human capacity to interpret the cognitive and affective information associated with natural language  in particular  we employed two benchmarks  each one composed by 21 743 common sense concepts  each concept is represented according to two models of a semantic network in which common sense concepts are linked to a hierarchy of affective domain labels  the labeled data have been split into two sets  the first 20 000 samples have been used for building the model with the elm with the different slt strategies  while the rest of the labeled samples  numbering 1743  have been kept apart as reference set in order to test the performance of the learned model  the splitting process has been repeated 30 times in order to obtain statistically relevant results  we ran the experiments through the use of the google cloud platform  in particular  the google compute engine  we employed the google compute engine platform with nm   4 machines with two cores and 1 8 gb of ram  machine type n1 highcpu 2  and an hdd of 30 gb equipped with spark  results on the affective dataset both show the effectiveness of the proposed parallel approach and underline the most suitable slt strategies for the specific big data problem  in this paper we showed how to build an elm model with a novel scalable approach and to carefully assess the performance  with the use of the most recent results from slt  for a sentiment analysis problem  thanks to recent technologies and methods  the computational requirements of these methods have been improved to allow for the scaling to large datasets  which are typical of big data applications  
 sensory metrics of neuromechanical trust today digital sources supply a historically unprecedented component of human sensorimotor data  the consumption of which is correlated with poorly understood maladies such as internet addiction disorder and internet gaming disorder  because both natural and digital sensorimotor data share common mathematical descriptions  one can quantify our informational sensorimotor needs using the signal processing metrics of entropy  noise  dimensionality  continuity  latency  and bandwidth  such metrics describe in neutral terms the informational diet human brains require to self calibrate  allowing individuals to maintain trusting relationships  with these metrics  we define the trust humans experience using the mathematical language of computational models  that is  as a primitive statistical algorithm processing finely grained sensorimotor data from neuromechanical interaction  this definition of neuromechanical trust implies that artificial sensorimotor inputs and interactions that attract low level attention through frequent discontinuities and enhanced coherence will decalibrate a brain s representation of its world over the long term by violating the implicit statistical contract for which self calibration evolved  our hypersimplified mathematical understanding of human sensorimotor processing as multiscale  continuous time vibratory interaction allows equally broad brush descriptions of failure modes and solutions for example  we model addiction in general as the result of homeostatic regulation gone awry in novel environments  sign reversal  and digital dependency as a sub case in which the decalibration caused by digital sensorimotor data spurs yet more consumption of them  we predict that institutions can use these sensorimotor metrics to quantify media richness to improve employee well being  that dyads and family size groups will bond and heal best through low latency  high resolution multisensory interaction such as shared meals and reciprocated touch  and that individuals can improve sensory and sociosensory resolution through deliberate sensory reintegration practices  we conclude that we humans are the victims of our own success  our hands so skilled they fill the world with captivating things  our eyes so innocent they follow eagerly  
 sentence level emotion mining based on combination of adaptive meta level features and sentence syntactic features nowadays  because of increasing of text data  recognizing the emotions of text can help to get a better comprehension of context  however  finding emotional information from text is a very complex task because it is needed to automatically understand of human sentences that usually are vague and dependent on context which should be interpreted and represented in different ways  the sense of a word can be inferred by investigating the frequency of occurrence of the word in a large corpus of annotated text  this paper has presented a method for learning adaptive lexicon from existing lexicon resources  static lexicons  to improve performance of emotion detection task for two data sets  the learning of adaptive lexicon would allow us to distinguish between the initial emotion of words in the static lexicons and adaptive emotion of words that is mentioned in context  and we get a better understanding of the emotional orientation of words  furthermore  this study proposes a novel approach for emotion detection based on the combination of meta level features derived from static and adaptive lexicons and sentences syntactic features  to the best of our knowledge  this is the first study that provides a comprehensive analysis of the relative importance of a very diverse feature set for automatic emotion detection  extensive experiments on isear and aman data sets show that learning adaptive lexicon enables emotion mining algorithms to be more accurate   c  2017 elsevier ltd  all rights reserved  
 sentiment analysis in financial texts the growth of financial  texts in the wake of big data has challenged most organizations and brought escalating demands for analysis tools  in general  text streams are more challenging to handle than numeric data streams  text streams are unstructured by nature  but they represent collective expressions that are of value in any financial decision  it can be both daunting and necessary to make sense of unstructured textual data  in this study  we address key questions related to the explosion of interest in how to extract insight from unstructured data and howto determine if such insight provides any hints concerning the trends of financial markets  a sentiment analysis engine  sae  is proposed which takes advantage of linguistic analyses based on grammars  this engine extends sentiment analysis not only at the word token level  but also at the phrase level within each sentence  an assessment heuristic is applied to extract the collective expressions shown in the texts  also  three evaluations are presented to assess the performance of the engine  first  several standard parsing evaluation metrics are applied on two treebanks  second  a benchmark evaluation using a dataset of english movie review is conducted  results show our sae outperforms the traditional bag of words approach  third  a financial text stream with twelve million words that aligns with a stock market index is examined  the evaluation results and their statistical significance provide strong evidence of a long persistence in the mood time series generated by the engine  in addition  our approach establishes grounds for belief that the sentiments expressed through text streams are helpful for analyzing the trends in a stock market index  although such sentiments and market indices are normally considered to be completely uncorrelated   c  2016 elsevier b v  all rights reserved  
 sentiment analysis in turkish at different granularity levels sentiment analysis has attracted a lot of research interest in recent years  especially in the context of social media  while most of this research has focused on english  there is ample data and interest in the topic for many other languages  as well  in this article  we propose a comprehensive sentiment analysis system for turkish  we cover different levels of sentiment analysis such as aspect  sentence  and document levels as well as some linguistic issues such as conjunction and intensification in turkish sentiment analysis  our system is evaluated on turkish movie reviews and the obtained accuracies range from sixty per cent to seventy nine per cent in ternary and binary classification tasks at different levels of analysis  
 sentiment analysis leveraging emotions and word embeddings sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents  these tasks have become of great importance  especially for business and marketing professionals  since online posted products and services reviews impact markets and consumers shifts  this work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges  due to the textual phenomena and the language specific expression variations  this paper proposes a fast  flexible  generic methodology for sentiment detection out of textual snippets which express people s opinions in different languages  the proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model  several documents  vector representation approaches have been studied  including lexicon based  word embedding based and hybrid vectorizations  the competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both greek and english languages  in order to represent high and weak inflection language groups  the proposed methodology requires minimal computational resources  thus  it might have impact in real world scenarios where limited resources is the case   c  2016 elsevier ltd  all rights reserved  
 sentiment analysis of player chat messaging in the video game starcraft 2  extending a lexicon based model there is a growing need for automated tools which make predictions about the positivity or negativity of sentiment conveyed by text  such tools have a number of important applications in game user research  they are useful for understanding users generally  as they may give big data researchers access to a new source of information about player learning environments  sentiment analysis methods are also applicable to the detection of toxicity  and the identification of players or player messages that are a potential threat to the player experience  a major challenge in sentiment analysis  however  is developing portable models that can be applied to new domains with relatively little effort  in the present study we extend a lexicon based sentiment extractor  so cal  to the analysis of instant messages across 1000 games of starcraft 2  we show that  with updates to dictionary entries that are tailored to the classification task at hand  so cal constitutes a respectable classifier of sentiment and toxicity that is robust across differences in player region and league  we verify the performance of our toxicity detector against a sample of 2025 additional games  our results support the proposal that lexicon based sentiment extraction is a useful and portable method of sentiment analysis  and that it can be deployed to identify toxicity   c  2017 elsevier b v  all rights reserved  
 sequential geophysical and flow inversion to characterize fracture networks in subsurface systems subsurface applications  including geothermal  geological carbon sequestration  and oil and gas  typically involve maximizing either the extraction of energy or the storage of fluids  fractures form the main pathways for flow in these systems  and locating these fractures is critical for predicting flow  however  fracture characterization is a highly uncertain process  and data from multiple sources  such as flow and geophysical are needed to reduce this uncertainty  we present a nonintrusive  sequential inversion framework for integrating data from geophysical and flow sources to constrain fracture networks in the subsurface  in this framework  we first estimate bounds on the statistics for the fracture orientations using microseismic data  these bounds are estimated through a combination of a focal mechanism  physics based approach  and clustering analysis  statistical approach  of seismic data  then  the fracture lengths are constrained using flow data  the efficacy of this inversion is demonstrated through a representative example  
 service robotics and human labor  a first technology assessment of substitution and cooperation since the beginning of robotics  the substitution of human labor has been one of the crucial issues  the focus is on the economic perspective  asking how robotics affects the labor market  and on changes in the work processes of human workers  while there are already some lessons learnt from industrial robotics  the area of service robots hag been analyzed to a much lesser extent  first insights into these aspects are of utmost relevance to technology assessment providing policy advice  as conclusions for service robots in general cannot be drawn  we identify criteria for the ex ante evaluation of service robots in concrete application areas   c  2016 the authors  published by elsevier b v  
 setting accessibility preferences about learning objects within adaptive elearning systems  user experience and organizational aspects higher and further education providers are facing the challenge of supporting the interaction needs of an increasing number of students who feature accessibility preferences to use both elearning contents and services  in the next future  we can expect that  within adaptive elearning systems  both automatic and manual procedures will interoperate to elicit users  interaction needs for ensuring accessibility  in this paper  we report findings from the user experience with the self assessment of interaction needs  as part of a content personalization system  which tackles possible mismatches in the interaction between the user and the learning objects  all stakeholders involved in providing this service along with intended user groups  students with visual  auditory or mobility impairments  and without impairments  participated in the evaluation with over 100 users described in this paper  from the evaluation  results follow that our approach allows students to self assess and report adequately their interaction preferences  furthermore  the paper describes findings of interest and open issues about how massive online courses may address the accessibility needs of an increasing number of elearning users  
 sign prediction in social networks based on tendency rate of equivalent micro structures online social networks have significantly changed the way people shape their everyday communications  signed networks are a class of social networks in which relations can be positive or negative  these networks emerge in areas where there is interplay between opposite attitudes such as trust and distrust  recent studies have shown that sign of relationships is predictable using data already present in the network  in this work  we study the sign prediction problem in networks with both positive and negative links and investigate the application of network tendency in the prediction task  accordingly  we develop a simple algorithm that can infer unknown relation types with high performance  we conduct experiments on three real world signed networks  epinions  slashdot and wikipedia  experimental results indicate that the proposed approach outperforms the state of the art methods in terms of both overall accuracy and true negative rate  furthermore  significantly low computational complexity of the proposed algorithm allows applying it to large scale datasets   c  2017 elsevier b v  all rights reserved  
 signage visibility analysis and optimization system using bim enabled virtual reality  vr  environments the proper placement of signage greatly influences pathfinding and information provision in public spaces  clear visibility  easy comprehension  and efficient placement are all important for successful signage  we propose a signage visibility analysis and optimization system  utilizing an updated building information model  bim  and a game engine software application to simulate the movement of pedestrians  bim can provide an up to date digital representation of a building and its assets  while computer simulation environments have the potential to simulate the movement of pedestrians  combining these two technologies provides an opportunity to create a tool that analyzes the efficiency of installed signage and visualizes them in vr environments  the proposed tool contains algorithms  functions and predefined scenarios to calculate the coverage and the visibility of a building s signage system  this system assists building managers to analyze  visually or by using statistics  the visibility of signboards  to assess their proper placement  and to optimize their placement  the applicability of the method has been validated in case studies performed in subway stations in japan   c  2017 elsevier ltd  all rights reserved  
 similarity based approach for group decision making with multi granularity linguistic information the aim of this article is to investigate the approach for multi attribute group decision making  in which the attribute values take the form of multi granularity multiplicative linguistic information  firstly  to process multiple sources of decision information assessed in different multiplicative linguistic label sets  a method for transforming multi granularity multiplicative linguistic information into multiplicative trapezoidal fuzzy numbers is proposed  then  a formula for ranking multiplicative trapezoidal fuzzy numbers is given based on geometric mean  furthermore  the concept of similarity degree between two multiplicative trapezoidal fuzzy numbers is defined  the attribute weights are obtained by solving some optimization models  an effective approach for group decision making with multi granularity multiplicative linguistic information is developed based on the ordered weighted geometric mean operator and proposed formulae  finally  a practical example is provided to illustrate the practicality and validity of the proposed method  
 simulated annealing based grasp for pareto optimal dissimilar paths problem this paper investigates a meta heuristic  mh  for the pareto optimal dissimilar path problem  dpp   pdpp  whose solution is a set composed of at least two different paths  the objective vector of a pdpp includes some conflicting objectives  on the one hand  the average path measures such as the length and risk of paths in a solution must be kept low and  on the other hand  the dissimilarity among these paths should be kept high  the dissimilarity of the dpp is a measure of a paths set with cardinality no less than two  however  just one path can be extracted from a chromosome in the existing mhs for various path problems  this results in a great difficulty to evaluate the chromosome in the existing mhs when we apply them to solve dpp and  consequently  there exists no mh for solving the dpp so far  in this paper  a new decoding approach of a chromosome is first explored and  with this approach  a set of paths can be extracted from a chromosome  by combining the simulated annealing  sa   in which the new decoding approach is adopted  with the well known greedy randomized adaptive search procedure  grasp   a sa based grasp for the pdpp is proposed  the proposed algorithm is compared against a most recent heuristic  whose performance is better than all of the early approaches  for the pdpp and the experimental results show that the proposed algorithm is able to quickly create superior approximation of the efficient set of the pdpp than the existing solution approaches for the pdpp  
 simulation based scheduling of multiple change propagations in multistage product development processes multiple engineering changes can simultaneously occur in different product development domains or stages  which can cause product delivery delays or development failure  to carefully manage multiple change propagations in multistage product development processes  a digraph based model  combined with input and output logics  is adopted to represent entities obtained in different product development stages  dependencies and mapping relationships are used to model inter  and intra domain entity connections  along which engineering changes can propagate  mathematical models are developed to compute change impacts and required efforts for resolving changes  simulation algorithms are present to explore possible change evolution paths across different stages  genetic algorithm  ga  is employed to find the optimal change propagation paths  a multistage development model representing functional design  structural design and manufacturing process of an air conditioner is used to test the change scheduling method  change durations and propagation traces for one  two and three emergent changes are compared to find the best strategy for coping with multiple change requests in multistage product development processes   c  2017 elsevier ltd  all rights reserved  
 simulation modeling and ergonomic assessment of complex multiworker physical processes discrete simulation of complex multiworker physical processes  for ergonomic and or performance analysis  is still relatively undeveloped  existing approaches typically have taken either a classical simulation view of the problem  focus on activities  resources  and queues  workers treated as machines  or an analytical ergonomics perspective  detailed human modeling and ergonomic assessment for well defined limited tasks   this paper presents a new way to tackle such problems  based upon elements of both of these approaches  novel methods are described for modeling complex multiworker physical processes within a traditional discrete event simulation environment  these methods result in a high level language for generating activity based process models quickly and easily  based upon simplified activity representations  laboratory experiments are used to derive equations that can then be used to generate ergonomic assessments  i e   relative injury risks  for the most influential activities  by implementing these equations using data generated from a simulation run  estimates of ergonomic consequences can be obtained  to illustrate the approach  it is applied to the panelized residential housing construction process  where multiple workers build houses by moving and installing large heavy prefabricated wall panels  the example illustrates the steps presented and feasibility of the approach  
 simulation within simulation for agent decision making  theoretical foundations from cognitive science to operational computer model this article deals with artificial intelligence models inspired from cognitive science  the scope of this paper is the simulation of the decision making process for virtual entities  the theoretical framework consists of concepts from the use of internal behavioral simulation for human decision making  inspired from such cognitive concepts  the contribution consists in a computational framework that enables a virtual entity to possess an autonomous world of simulation within the simulation  it can simulate itself  using its own model of behavior  and simulate its environment  using its representation of other entities   the entity has the ability to anticipate using internal simulations  in complex environments where it would be extremely difficult to use formal proof methods  comparing the prediction and the original simulation  its predictive models are improved through a learning process  illustrations of this model are provided through two implementations  first illustration is an example showing a shepherd  his herd and dogs  the dog simulates the sheep s behavior in order to make predictions testing different strategies  second  an artificial 3d juggler plays in interaction with virtual jugglers  humans and robots  for this application  the juggler predicts the behavior of balls in the air and uses prediction to coordinate its behavior in order to juggle   c  2016 elsevier b v  all rights reserved  
 simulation based evaluation of adaptive automation revoking strategies on cognitive workload and situation awareness adaptive systems have used a variety of approaches to return the task responsibility back to the user  including restoration of performance or physiological thresholds  leveling of task loads  and user initiated deactivation  little attention has been paid to evaluating the relative effectiveness of these various handoff mechanisms  or the impact that these mechanisms may have on operator workload and situation awareness  sa   this study uses an improved performance research integration tool simulation  based upon a human in the loop study  to examine two different revoking strategies  minimum duration and workload threshold  across numerous levels in order to identify impacts on operator workload and sa  this study finds that operator workload and sa are affected by the type of revoking strategy as well as the particular level used for that strategy  for the scenario examined  automation revoking that relied upon a minimum duration of at least 5 s was found to be more effective in reducing workload and increasing sa than using a revoking strategy based upon workload thresholds  
 situating machine intelligence within the cognitive ecology of the internet the internet is an important focus of attention for the philosophy of mind and cognitive science communities  this is partly because the internet serves as an important part of the material environment in which a broad array of human cognitive and epistemic activities are situated  the internet can thus be seen as an important part of the  cognitive ecology  that helps to shape  support and  on occasion  realize aspects of human cognizing  much of the previous philosophical work in this area has sought to analyze the cognitive significance of the internet from the perspective of human cognition  there has  as such  been little effort to assess the cognitive significance of the internet from the perspective of  machine cognition   this is unfortunate  because the internet is likely to exert a significant influence on the shape of machine intelligence  the present paper attempts to evaluate the extent to which the internet serves as a form of cognitive ecology for synthetic  machine based  forms of intelligence  in particular  the phenomenon of internet situated machine intelligence is analyzed from the perspective of a number of approaches that are typically subsumed under the heading of situated cognition  these include extended  embedded  scaffolded and embodied approaches to cognition  for each of these approaches  the internet is shown to be of potential relevance to the development and operation of machine based cognitive capabilities  such insights help us to appreciate the role of the internet in advancing the current state of the art in machine intelligence  
 situation awareness in human machine interactive systems this special issue brings together six papers on situation awareness in human machine interactive systems  in particular in teams of collaborating humans and artificial agents  the editorial provides a brief introduction and overviews the contributions  addressing issues such as team and shared situation awareness  trust  transparency  timing  engagement  and ethical aspects   c  2017 published by elsevier b v  
 skepticism revisited  chalmers on the matrix and brains in vats thought experiments involving the matrix  brains in vats  or cartesian demons have traditionally thought to describe skeptical possibilities  chalmers has denied this  claiming that the simulations involved are real enough to at least sometimes defeat the skeptic  through an examination of the meaning of kind terms in natural language i argue that  though the chalmers view may be otherwise attractive  it is not an antidote to skepticism   c  2016 published by elsevier b v  
 slacks based efficiency measurements with undesirable outputs in data envelopment analysis data envelopment analysis  dea  has recently gained great popularity in modeling environmental performance because it provides condensed information to decision makers when the production process includes undesirable outputs  in this paper  we develop a new slacks based efficiency measurement for modeling environmental performance using the environmental dea technology  the proposed index has more theoretical justification  and distinguishes among different decision making units  dmus  better in practice  then we further extend it to the nonoriented index with double aim of increasing desirable outputs and reducing undesirable outputs  finally  we calculate the index for each of 25 oecd european countries in a model of co2 emission performance from 2007 to 2009 and the results obtained are presented  
 smart health  big data enabled health paradigm within smart cities in the era of  big data   recent developments in the area of information and communication technologies  ict  are facilitating organizations to innovate and grow  these technological developments and wide adaptation of ubiquitous computing enable numerous opportunities for government and companies to reconsider healthcare prospects  therefore  big data and smart healthcare systems are independently attracting extensive attention from both academia and industry  the combination of both big data and smart systems can expedite the prospects of the healthcare industry  however  a thorough study of big data and smart systems together in the healthcare context is still absent from the existing literature  the key contributions of this article include an organized evaluation of various big data and smart system technologies and a critical analysis of the state of the art advanced healthcare systems  we describe the three dimensional structure of a paradigm shift  we also extract three broad technical branches  3t  contributing to the promotion of healthcare systems  more specifically  we propose a big data enabled smart healthcare system framework  bshsf  that offers theoretical representations of an infra and inter organizational business model in the healthcare context  we also mention some examples reported in the literature  and then we contribute to pinpointing the potential opportunities and challenges of applying bshsf to healthcare business environments  we also make five recommendations for effectively applying  bshsf to the healthcare industry  to the best of our knowledge  this is the first in depth study about state of the art big data and smart healthcare systems in parallel  the managerial implication of this article is that organizations can use the findings of our critical analysis to reinforce their strategic arrangement of smart systems and big data in the healthcare context  and hence better leverage them for sustainable organizational invention   c  2017 elsevier ltd  all rights reserved  
 smart maintenance decision support systems  smdss  based on corporate big data analytics the purpose of this article is to outline the architectural design and the conceptual framework for a smart maintenance decision support system  smdss  based on corporate data from a fortune 500 company  motivated by the rapidly transforming landscape for big data analytics and predictive maintenance decision making  we have created a system capable of providing end users with recommendations to improve asset lifecycles  methodologically  a cost minimization algorithm is used to analyze a large industry service and warranty data sets and two analytical decision models were developed and applied to a case study for an electrical circuit breaker maintenance problem  some of these techniques can be applied to other industries  such as jet engine maintenance  and can be expanded to others with implications for robust decision analysis  the smdss provides a predictive analytical model that can be applied in manufacturing and service based industries  our findings and results show that existing solution algorithms and optimization models can be applied to large data sets to lay out executable decisions for managers  published by elsevier ltd  
 social incentives in paid collaborative crowdsourcing paid microtask crowdsourcing has traditionally been approached as an individual activity  with units of work created and completed independently by the members of the crowd  other forms of crowdsourcing have  however  embraced more varied models  which allow for a greater level of participant interaction and collaboration  this article studies the feasibility and uptake of such an approach in the context of paid microtasks  specifically  we compare engagement  task output  and task accuracy in a paired worker model with the traditional  single worker version  our experiments indicate that collaboration leads to better accuracy and more output  which  in turn  translates into lower costs  we then explore the role of the social flow and social pressure generated by collaborating partners as sources of incentives for improved performance  we utilise a bayesian method in conjunction with interface interaction behaviours to detect when one of the workers in a pair tries to exit the task  upon this realisation  the other worker is presented with the opportunity to contact the exiting partner to stay  either for personal financial reasons  i e   they have not completed enough tasks to qualify for a payment  or for fun  i e   they are enjoying the task   the findings reveal that   1  these socially motivated incentives can act as furtherance mechanisms to help workers attain and exceed their task requirements and produce better results than baseline collaborations   2  microtask crowd workers are empathic  as opposed to selfish  agents  willing to go the extra mile to help their partners get paid  and   3  social furtherance incentives create a win win scenario for the requester and for the workers by helping more workers get paid by re engaging them before they drop out  
 social media text normalization for turkish text normalization is an indispensable stage in processing noncanonical language from natural sources  such as speech  social media or short text messages  research in this field is very recent and mostly on english  as is known from different areas of natural language processing  morphologically rich languages  mrls  pose many different challenges when compared to english  turkish is a strong representative of mrls and has particular normalization problems that may not be easily solved by a single stage pure statistical model  this article introduces the first work on the social media text normalization of an mrl and presents the first complete social media text normalization system for turkish  the article conducts an in depth analysis of the error types encountered in web 2 0 turkish texts  categorizes them into seven groups and provides solutions for each of them by dividing the candidate generation task into separate modules working in a cascaded architecture  for the first time in the literature  two manually normalized web 2 0 datasets are introduced for turkish normalization studies  the exact match scores of the overall system on the provided datasets are 70 40 per cent and 67 37 per cent  77 07 per cent with a case insensitive evaluation   
 social network analytics for churn prediction in telco  model building  evaluation and network architecture social network analytics methods are being used in the telecommunication industry to predict customer churn with great success  in particular it has been shown that relational learners adapted to this specific problem enhance the performance of predictive models  in the current study we benchmark different strategies for constructing a relational learner by applying them to a total of eight distinct call detail record datasets  originating from telecommunication organizations across the world  we statistically evaluate the effect of relational classifiers and collective inference methods on the predictive power of relational learners  as well as the performance of models where relational learners are combined with traditional methods of predicting customer churn in the telecommunication industry  finally we investigate the effect of network construction on model performance  our findings imply that the definition of edges and weights in the network does have an impact on the results of the predictive models  as a result of the study  the best configuration is a non relational learner enriched with network variables  without collective inference  using binary weights and undirected networks  in addition  we provide guidelines on how to apply social networks analytics for churn prediction in the telecommunication industry in an optimal way  ranging from network architecture to model building and evaluation   c  2017 elsevier ltd  all rights reserved  
 social network pruning for building optimal social network  a user perspective social networks with millions of nodes and edges are difficult to visualize and understand  therefore  approaches to simplify social networks are needed  this paper addresses the problem of pruning social network while not only retaining but also improving its information propagation properties  the paper presents an approach which examines the nodal attribute of a node and develops a criterion to retain a subset of nodes to form a pruned graph of the original social network  to authenticate feasibility of the proposed approach to information propagation process  it is evaluated on small world properties such as average clustering coefficient  diameter  path length  connected components and modularity  the pruned graph  when compared to original social network  shows improvement in small world properties which are essential for information propagation  results also give a significantly more refined picture of social network  than has been previously highlighted  the efficacy of the pruned graph is demonstrated in the information diffusion process under independent cascade  ic  and linear threshold  lt  models on various seeding strategies  in all size ranges and across various seeding strategies  the proposed approach performs consistently well in ic model and outperforms other approaches in lt model  although  the paper discusses the problem with the context of information propagation for viral marketing  the pruned graph generated from the proposed approach is also suitable for any application  where information propagation has to take place reasonably fast and effectively   c  2016 elsevier b v  all rights reserved  
 solvency prediction for small and medium enterprises in banking this paper describes novel approaches to predict default for smes  multivariate outlier detection techniques based on local outlier factor are proposed to improve the out of sample performance of parametric and non parametric models for credit risk estimation  the models are tested on a real data set provided by unicredit bank  the results at hand confirm that our proposal improves the results in terms of predictive capability and support financial institutions to make decision  single and ensemble models are compared and in particular  inside parametric models  the generalized extreme value regression model is proposed as a suitable competitor of the logistic regression   c  2017 elsevier b v  all rights reserved  
 solving the p median bilevel problem with order through a hybrid heuristic a variant of the p median problem is considered and presented in this paper  this variant is based on the assumption that customers are free to choose the located facility that will serve them  the latter decision is made by considering the customers preferences towards the facilities  to study this problem  a mathematical bilevel programming formulation is proposed  given the difficulty in solving such bilevel programs  two reformulations are used for solving the problem  the first reformulation adds constraints and variables to the mathematical model  while the second one adds only constraints  yet  both reformulations avoid the need to solve an optimization problem parameterized by the upper level variables to find the value of the lower level variables  the results of numerical experiments show that the required time for both reformulations is significant increased as the size of the instance increases  moreover  the reformulations are unable to solve the large size instances  this led us to develop a hybrid heuristic algorithm based on scatter search  which obtains high quality solutions for all tested instances in less time than is required by the abovementioned reformulations  furthermore  the proposed heuristic was able to solve larger size instances obtaining the optimal or currently best known solution  the registered results from the computational experimentation show that the proposed algorithm performs steadily  a comparison against a scatter search with random construction  a scatter search with greedy construction  a grasp and a genetic algorithm shows that the proposed hybrid heuristic outperforms the other algorithms   c  2017 elsevier b v  all rights reserved  
 some clarifications of remedies for candecomp parafac degeneracy by means of an svd penalized approach the candecomp parafac model is a widely used tool for synthetizing three way arrays through a limited number of components  unfortunately  its applicability may be prevented by the risk of obtaining degenerate solutions  characterized by diverging  highly collinear and uninterpretable components  a recognized remedy is to impose  soft  orthogonality constraints to the components  in this context  a new strategy for solving the degeneracy problem where the orthogonality constraints are suitably relaxed depending on the data is proposed  this allows for a comparative assessment of some existing remedies against degeneracy from both the theoretical and practical point of views  
 some intuitionistic linguistic dependent bonferroni mean operators and application in group decision making the dependent ordered weighted averaging  dowa  operator can relieve the influence of unfair data from the aggregated arguments  and bonferroni mean  bm  operator can capture the interrelationship of the aggregated arguments  in order to making fully use of the advantages of these two types of operators  we combine the dowa with the bm operator in intuitionistic linguistic setting  and propose the intuitionistic linguistic dependent bonferroni mean  ildbm  operator and the intuitionistic linguistic dependent geometric bonferroni mean  ildgbm  operator  simultaneously  several properties of these novel operators are discussed  moreover  a method based on these operators is developed to solve the multi attribute group decision making magdm  problems with intuitionistic linguistic information  the advantages of the proposed method are  1  it can consider the interrelationship between any two attribute values   2  it can relieve the influence of unfair attribute values given by some biased decision makers  finally  an application example is represented to illustrate the practicality and validity of the developed method by comparing with the existing methods  
 some weighted geometric operators with svtrn numbers and their application to multi criteria decision making problems since the single valued triangular neutrosophic number  svtrn number  is a generalization of triangular fuzzy numbers and triangular intuitionistic fuzzy numbers  it may express more abundant and flexible information as compared with the triangular fuzzy numbers and triangular intuitionistic fuzzy numbers  this article introduces an approach to handle multi criteria decision making  mcdm  problems under the svtrn numbers  therefore  we first proposed some new geometric operators are called svtrn weighted geometric operator  svtrn ordered weighted geometric operator and svtrn ordered hybrid weighted geometric operator  also we studied some desirable properties of the geometric operators  and then  an approach based on the svtrn ordered hybrid weighted geometric operator is developed to solve multi criteria decision making problems with svtrn number  finally  a numerical example is used to demonstrate how to apply the proposed approach  
 sparks will fly  engineering creative script conflicts scripts are often dismissed as the stuff of good movies and bad politics  they codify cultural experience so rigidly that they remove our freedom of choice and become the very antithesis of creativity  yet  mental scripts have an important role to play in our understanding of creative behaviour  since a deliberate departure from an established script can produce results that are simultaneously novel and familiar  especially when others stick to the conventional script  indeed  creative opportunities often arise at the overlapping boundaries of two scripts that antagonistically compete to mentally organise the same situation  this work explores the computational integration of competing scripts to generate creative friction in short texts that are surprising but meaningful  our exploration considers conventional macro scripts   ordered sequences of actions   and the less obvious micro scripts that operate at even the lowest levels of language  for the former  we generate plots that squeeze two scripts into a single mini narrative  for the latter  we generate ironic descriptions that use conflicting scripts to highlight the speaker s pragmatic insincerity  we show experimentally that verbal irony requires both kinds of scripts   macro and micro   to work together to reliably generate creative sparks from a speaker s subversive intent  
 sparse covariance matrix estimation by dca based algorithms this letter proposes a novel approach using the l 0  norm regularization for the sparse covariance matrix estimation  scme  problem  the objective function of scme problem is composed of a nonconvex part and the l 0  term  which is discontinuous and difficult to tackle  appropriate dc  difference of convex functions  approximations of l 0  norm are used that result in approximation scme problems that are still nonconvex  dc programming and dca  dc algorithm   powerful tools in nonconvex programming framework  are investigated  two dc formulations are proposed and corresponding dcaschemes developed  two applications of the scme problem that are considered are classification via sparse quadratic discriminant analysis and portfolio optimization  acareful empirical experiment is performed through simulated and real data sets to study the performance of the proposed algorithms  numerical results showed their efficiency and their superiority compared with seven stateof  the art methods  
 sparse tensor canonical correlation analysis for micro expression recognition a micro expression is considered a fast facial movement that indicates genuine emotions and thus provides a cue for deception detection  due to its promising applications in various fields  psychologists and computer scientists  particularly those focus on computer vision and pattern recognition  have shown interest and conducted research on this topic  however  micro expression recognition accuracy is still low  to improve the accuracy of such recognition  in this study  micro expression data and their corresponding local binary pattern  lbp   ojala et al   2002   1  code data are fused by correlation analysis  here  we propose sparse tensor canonical correlation analysis  stcca  for micro expression characteristics  a sparse solution is obtained by the regularized low rank matrix approximation  experiments are conducted on two micro expression databases  casme and casme 2  and the results show that stcca performs better than the three dimensional canonical correlation analysis  3d cca  without sparse resolution  the experimental results also show that stcca performs better than three order discriminant tensor subspace analysis  dtsa3  with discriminant information  smaller projected dimensions and a larger training set sample size  the experiments also showed that multi linear principal component analysis  mpca  is not suitable for micro expression recognition because the eigenvectors corresponding to smaller eigenvectors are discarded  and those eigenvectors include brief and subtle motion information   c  2016 elsevier b v  all rights reserved  
 spatial concept acquisition for a mobile robot that integrates self localization and unsupervised word discovery from spoken sentences in this paper  we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots  from human continuous speech signals  we address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model  furthermore  we propose a method that allows a robot to effectively use the learned words and their meanings for self localization tasks  the proposed method is non parametric bayesian spatial concept acquisition method  spcoa  that integrates the generative model for self localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept  we implemented the proposed method spcoa on sigverse  which is a simulation environment  and turtlebot 2  which is a mobile robot in a real environment  further  we conducted experiments for evaluating the performance of spcoa  the experimental results showed that spcoa enabled the robot to acquire the names of places from speech sentences  they also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self localization  
 specifying and testing the design rationale of social robots for behavior change in children we are developing a social robot that helps children with diabetes type 1 to acquire self management skills and routines  there is a diversity of behavior change techniques  bcts  and guidelines that seem to be useful for the development of such support  but it is not yet clear how to work out the techniques into concrete robot support functions and behaviors  the situated cognitive engineering  sce  methodology provides guidance for the design and evaluation of such functions and behaviors  but doesn t provide a univocal specification method of the theoretical and empirical justification  this paper presents an extension of sce  a formal template that describes the relations between support objectives  behavior change theory  design specifications and evaluation outcomes  called situated design rationale  sdr  and the method to get this  as test case  the european aliz e project is used to instantiate this design rationale and to evaluate the usage  this case study showed that sdr provides concrete guidance  1  to derive robot functions and behaviors from the theory and  2  to designate the corresponding effects with evaluation instruments  furthermore  it helps to establish an effective  incremental and iterative  design and evaluation process  by relating positive and negative evaluation outcomes to robot behaviors at the task and communication level  the proposed solution for explicating the design rationale makes it possible for others to understand the decisions made and thereby supports replicating experiments or reusing parts of the design rationale   c  2016 elsevier b v  all rights reserved  
 spreading semantic information by word sense disambiguation this paper presents an unsupervised approach to solve semantic ambiguity based on the integration of the personalized pagerank algorithm with word sense frequency information  natural language tasks such as machine translation or recommender systems are likely to be enriched by our approach  which includes semantic information that obtains the appropriate word sense via support from two sources  a multidimensional network that includes a set of different resources  i e  wordnet  wordnet domains  wordnet affect  sumo and semantic classes   and the information provided by word sense frequencies and word sense collocation from the semcor corpus  our series of results were analyzed and compared against the results of several renowned studies using senseval 2  senseval 3 and semeval 2013 datasets  after conducting several experiments  our procedure produced the best results in the unsupervised procedure category taking senseval campaigns rankings as reference   c  2017 elsevier b v  all rights reserved  
 sprinkled semantic diffusion kernel for word sense disambiguation word sense disambiguation  wsd   the task of identifying the intended meanings  senses  of words in context  has been a long standing research objective for natural language processing  nlp   in this paper  we are concerned with kernel methods for automatic wsd  under this framework  the main difficulty is to design an appropriate kernel function to represent the sense distinction knowledge  semantic diffusion kernel  which models semantic similarity by means of a diffusion process on a graph defined by lexicon and co occurrence information to smooth the typical  bag of words   bow  representation  has been successfully applied to wsd  however  the diffusion is an unsupervised process  which fails to exploit the class information in a supervised classification scenario  to address the limitation  we present a sprinkled semantic diffusion kernel to make use of the class knowledge of training documents in addition to the co occurrence knowledge  the basic idea is to construct an augmented term document matrix by encoding class information as additional terms and appending them to training documents  diffusion is then performed on the augmented term document matrix  in this way  the words belonging to the same class are indirectly drawn closer to each other  hence the class specific word correlations are strengthened  we evaluate our method on several senseval semeval benchmark examples with support vector machine  svm   and show that the proposed kernel can significantly improve the disambiguation performance over semantic diffusion kernel in terms of different measures and yield a competitive result with the state of the art kernel methods for wsd   c  2017 elsevier ltd  all rights reserved  
 stanislavsky s system as an enactive guide to embodiedcognition  this paper presents a model of the structure of subjective experience derived from the work of konstantin stanislavsky  and demonstrates its usefulness as a functional framework of enacted cognitive embodiment by using it to articulate his approach to the process of acting  research into stanislavsky s training exercises reveals that they evoke a spatial adpositional conceptualisation of experience  when reflected back onto the practice from which it emerges  this situates the choices made by actors as contributing towards the construction of a stable attention field with which they enter into relationship during performance  it is suggested that the resulting template might clarify conceptual distinctions between practices at the unconscious level  and a brief illustrative comparison between stanislavsky s and meisner s practices is essayed  a parallel is drawn throughout with the basic principles of embodied cognition  and correlations found with aspects of dynamic field theory and wilson s notions of on  and off line processing  
 statistical comparison of classifiers through bayesian hierarchical modelling usually one compares the accuracy of two competing classifiers using null hypothesis significance tests  yet such tests suffer from important shortcomings  which can be overcome by switching to bayesian hypothesis testing  we propose a bayesian hierarchical model that jointly analyzes the cross validation results obtained by two classifiers on multiple data sets  the model estimates more accurately the difference between classifiers on the individual data sets than the traditional approach of averaging  independently on each data set  the cross validation results  it does so by jointly analyzing the results obtained on all data sets  and applying shrinkage to the estimates  the model eventually returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different  
 statistical models for unsupervised  semi supervised  and supervised transliteration mining we present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings  unsupervised  semi supervised  and supervised transliteration mining  the model interpolates two sub models  one for the generation of transliteration pairs and one for the generation of non transliteration pairs  i e   noise   the model is trained on noisy unlabeled data using the em algorithm  during training the transliteration sub model learns to generate transliteration pairs and the fixed non transliteration model generates the noise pairs  after training  the unlabeled data is disambiguated based on the posterior probabilities of the two sub models  we evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora  for three out of four language pairs  our system outperforms all semi supervised and supervised systems that participated in the news 2010 shared task  on word pairs extracted from parallel corpora with fewer than 2  transliteration pairs  our system achieves up to 86 7  f measure with 77 9  precision and 97 8  recall  
 statistical relational learning for game theory in this paper  we motivate the use of models and algorithms from the area of statistical relational learning  srl  as a framework for the description and the analysis of games  srl combines the powerful formalism of first order logic with the capability of probabilistic graphical models in handling uncertainty in data and representing dependencies between random variables  for this reason  srl models can be effectively used to represent several categories of games  including games with partial information  graphical games and stochastic games  inference algorithms can be used to approach the opponent modeling problem  as well as to find nash equilibria or pareto optimal solutions  structure learning algorithms can be applied  in order to automatically extract probabilistic logic clauses describing the strategies of an opponent with a high level  human interpretable formalism  experiments conducted using markov logic networks  one of the most used srl frameworks  show the potential of the approach  
 step to step ankle inversion eversion torque modulation can reduce effort associated with balance below knee amputation is associated with higher energy expenditure during walking  partially due to difficulty maintaining balance  we previously found that once per step push off work control can reduce balance related effort  both in simulation and in experiments with human participants  simulations also suggested that changing ankle inversion eversion torque on each step  in response to changes in body state  could assist with balance  in this study  we investigated the effects of ankle inversion eversion torque modulation on balance related effort among amputees  n   5  using a multi actuated ankle foot prosthesis emulator  in stabilizing conditions  changes in ankle inversion eversion torque were applied so as to counteract deviations in side to side center of mass acceleration at the moment of intact limb toe off  higher acceleration toward the prosthetic limb resulted in a corrective ankle inversion torque during the ensuing stance phase  destabilizing controllers had the opposite effect  and a zero gain controller made no changes to the nominal inversion eversion torque  to separate the balance related effects of step to step control from the potential effects of changes in average mechanics  average ankle inversion eversion torque and prosthesis work were held constant across conditions  high gain stabilizing control lowered metabolic cost by 13  compared to the zero gain controller  p   0 05   we then investigated individual responses to subject specific stabilizing controllers following an enforced exploration period  four of five participants experienced reduced metabolic rate compared to the zero gain controller   15   14   11   6  and  4   an average reduction of 9   p   0 05   average prosthesis mechanics were unchanged across all conditions  suggesting that improvements in energy economy might have come from changes in step to step corrections related to balance  step to step modulation of inversion eversion torque could be used in new  active ankle foot prostheses to reduce walking effort associated with maintaining balance  
 stochastic blockmodeling and variational bayes learning for signed network analysis signed networks with positive and negative links attract considerable interest in their studying since they contain more information than unsigned networks  community detection and sign  or attitude  prediction are still primary challenges  as the fundamental problems of signed network analysis  for this  a generative bayesian approach is presented wherein 1  a signed stochastic blockmodel is proposed to characterize the community structure in the context of signed networks  by explicit formulating the distributions of the density and frustration of signed links from a stochastic perspective  and 2  a model learning algorithm is advanced by theoretical deriving a variational bayes em for the parameter estimation and variation based approximate evidence for the model selection  the comparison of the above approach with the state of the art methods on synthetic and real world networks  shows its advantage in the community detection and sign prediction for the exploratory networks  
 stochastic fuzzy multi objective backbone selection and capacity allocation problem under tax band pricing policy with different fuzzy operators in this paper  we investigate a multi objective optimization problem that a telecom bandwidth broker  bb  faces when acquiring and selling bandwidth in an uncertain market environment in which there exists several backbone providers  bps  and end users  the proposed model incorporates two important goals  maximizing expected profit and minimizing expected loss capacity within realistic constraints such as bps  capacity  meeting the end users  bandwidth requests and satisfying the quality of service requests of end users   considering stochastic capacity loss rates of bps  the fuzzy set theory and stochastic programming techniques are employed to handle the non deterministic nature of telecommunication network setting due to the presence of vagueness and randomness of information  the model is formulated in such a way that it simultaneously considers the randomness in demand and determines the allocation of end users  bandwidth requests into purchased capacity based on tax band pricing scheme  as solution strategies  two different fuzzy operators  namely max min and weighted additive max min  are integrated into a resulting two stage multi objective stochastic linear programming model  then  algorithms are provided to solve and to compare methodologies with deterministic approaches  finally  the proposed algorithms are tested on several randomly generated test scenarios to provide managerial insight to decision makers of bb companies  
 stock market one day ahead movement prediction using disparate data sources there are several commercial financial expert systems that can be used for trading on the stock exchange  however  their predictions are somewhat limited since they primarily rely on time series analysis of the market  with the rise of the internet  new forms of collective intelligence  e g  google and wikipedia  have emerged  representing a new generation of  crowd sourced  knowledge bases  they collate information on publicly traded companies  while capturing web traffic statistics that reflect the public s collective interest  google and wikipedia have become important  knowledge bases  for investors  in this research  we hypothesize that combining disparate online data sources with traditional time series and technical indicators for a stock can provide a more effective and intelligent daily trading expert system  three machine learning models  decision trees  neural networks and support vector machines  serve as the basis for our  inference engine   to evaluate the performance of our expert system  we present a case study based on the aapl  apple nasdaq  stock  our expert system had an 85  accuracy in predicting the next day aapl stock movement  which outperforms the reported rates in the literature  our results suggest that   a  the knowledge base of financial expert systems can benefit from data captured from nontraditional  experts  like google and wikipedia   b  diversifying the knowledge base by combining data from disparate sources can help improve the performance of financial expert systems  and  c  the use of simple machine learning models for inference and rule generation is appropriate with our rich knowledge database  finally  an intelligent decision making tool is provided to assist investors in making trading decisions on any stock  commodity or index   c  2017 elsevier ltd  all rights reserved  
 stock picking by probability possibility approaches this paper presents a performance evaluation of stock picking by merging several technical indicators  several fusion operators have been proposed either in the probabilistic or in the possibilistic framework  the latter fuzzy framework has been introduced to manage the uncertain information embedded in financial time series due to human biases as studied by behavioral finance  performances of portfolio resulting from the proposed systems are evaluated according to cumulative returns but also through a risk analysis point of view  sharpe ratio   two fusion mechanisms  one probabilistic  one possibilistic  aiming at discriminating common information from merged technical indicators  produce the higher portfolio performances  it also appears that selecting specific technical indicators affects the overall performances of the proposed stock picking systems  indeed  studying the technical indicators selection through a shared nonshared information point of view reveals that possibilistic framework is more robust to redundant sources than probabilistic framework  effects of some parameters used in the fusion algorithms  amount of assets  window length analysis       are also investigated  results from all these tests clearly show the high potentiality of technical indicators fusion to improve portfolio performances  however  these first promising results have to be further inspected within wider contexts  as discussed at the end of the paper  
 stock portfolio selection using learning to rank algorithms with news sentiment in this study  we apply learning to rank algorithms to design trading strategies using relative perforrnance of a group of stocks based on investors  sentiment toward these stocks  we show that learning to rank algorithms are effective in producing reliable rankings of the best and the worst performing stocks based on investors  sentiment  more specifically  we use the sentiment shock and trend indicators introduced in the previous studies  and we design stock selection rules of holding long positions of the top 25  stocks and short positions of the bottom 25  stocks according to rankings produced by learning to rank algorithms  we then apply two learning to rank algorithms  listnet and ranknet  in stock selection processes and test long only and long short portfolio selection strategies using 10 years of market and news sentiment data  through backtesting of these strategies from 2006 to 2014  we demonstrate that our portfolio strategies produce risk adjusted returns superior to the s p 500 index return  the hedge fund industry average performance   hfriemn  and some sentiment based approaches without learning to rank algorithm during the same period   c  2017 elsevier b v  all rights reserved  
 strategic style change using grammar transformations new styles can be created by modifying existing ones  in order to formalize style change using grammars  style has to be formally defined in the design language of a grammar  previous studies in the use of grammars for style change do not give explicit rationale for transformation  how would designers decide which rules to modify in a grammar to generate necessary changes in style s  of designs  this paper addresses the aforementioned issues by presenting a framework for strategic style change using goal driven grammar transformations  the framework employs a style description scheme constructed by describing the aesthetic qualities of grammar elements using adjectival descriptors  we present techniques for the formal definition of style in the designs generated by grammars  the utility of the grammar transformation framework and the style description scheme is tested with an example of mobile phone design  analyses reveal that constraining rules in grammars is a valid technique for generating designs with a dominance of desired adjectival descriptors  thus aiding in strategic style change  
 strategies for utility maximization in social groups with preferential exploration social group is group of interconnected nodes interested in obtaining common content  scott  in social network analysis  2012   social groups are observed in many networks for example  cellular network assisted device to device network  fodor et al   in ieee commun mag 50 170 177  2012  lei et al   in wirel commun 19 96 104  2012   hybrid peer to peer content distribution  christos gkantsidis and miller  in 5th international workshop on peer to peer systems  2006  vakali and pallis  in ieee internet comput 7 68 74  2003  etc  in this paper  we consider a  social group  of networked nodes  seeking a  universe  of data segments for maximizing their individual utilities  each node in social group has a subset of the universe  and access to an expensive link for downloading data  nodes can also acquire the universe by exchanging copies of data segments among themselves  at low cost  using inter node links  while exchanges over inter node links ensure minimum or negligible cost  some nodes in the group try to exploit the system by indulging in collusion  identity fraud etc  we term such nodes as  non reciprocating nodes  and prohibit such behavior by proposing the  give and take  criterion  where exchange is allowed iff each participating node provides at least one segment to the node which is unavailable with the node  while complying with this criterion  each node wants to maximize its utility  which depends on the node s segment set available with the node  link activation between pair of nodes requires mutual consent of the participating nodes  each node tries to find a pairing partner by preferentially exploring nodes for link formation  unpaired nodes download data segments using the expensive link with pre defined probability  defined as segment aggressiveness probability   we present various linear complexity decentralized algorithms based on the stable roommates problem that can be used by nodes for choosing the best strategy based on available information  we present a decentralized randomized algorithm that is asymptotically optimal in the number of nodes  we define price of choice for benchmarking the performance of social groups consisting of non aggressive nodes  i e  nodes not downloading data segments from the expensive link  only  we evaluate performances of various algorithms and characterize the behavioral regime that will yield best results for nodes and social groups  spending the least on the expensive link  the proposed algorithms are compared with the optimal  we find that the link for sure algorithm performs nearly optimally  
 strategy proof school choice mechanisms with minimum quotas and initial endowments we consider a school choice program where minimum quotas are imposed for each school  i e   a school must be assigned at least a certain number of students to operate  we require that the obtained matching must respect the initial endowments  i e   each student must be assigned to a school that is at least as good as her initial endowment school  although minimum quotas are relevant in school choice programs and strategy proofness is important to many policymakers  few existing mechanisms simultaneously achieve both  one difficulty is that no strategy proof mechanism exists that is both efficient and fair under the presence of minimum quotas  furthermore  existing mechanisms require that all students consider all schools acceptable to obtain a feasible matching that respects minimum quotas  this assumption is unrealistic in a school choice program  we consider the environment where a student considers her initial endowment school acceptable and the initial endowments satisfy all the minimum quotas  we develop two strategy proof mechanisms  one mechanism  which we call the top trading cycles among representatives with supplementary seats  ttcr ss   is based on the top trading cycles  ttc  mechanism and is significantly extended to handle the supplementary seats of schools while respecting minimum quotas  ttcr ss is pareto efficient  the other mechanism  which we call priority list based deferred acceptance with minimum quotas  plda mq   is based on the deferred acceptance  da  mechanism  plda mq is fair  satisfies a concept called priority list based  pl   stability  and obtains the student optimal matching within all pl stable matchings  our simulation results show that our new mechanisms are significantly better than simple extensions of the existing mechanisms   c  2017 the authors  published by elsevier b v  
 stress detection using wearable physiological and sociometric sensors stress remains a significant social problem for individuals in modern societies  this paper presents a machine learning approach for the automatic detection of stress of people in a social situation by combining two sensor systems that capture physiological and social responses  we compare the performance using different classifiers including support vector machine  adaboost  and k nearest neighbor  our experimental results show that by combining the measurements from both sensor systems  we could accurately discriminate between stressful and neutral situations during a controlled trier social stress test  tsst   moreover  this paper assesses the discriminative ability of each sensor modality individually and considers their suitability for real time stress detection  finally  we present an study of the most discriminative features for stress detection  
 study on an improved adaptive pso algorithm for solving multi objective gate assignment gate is a key resource in the airport  which can realize rapid and safe docking  ensure the effective connection between flights and improve the capacity and service efficiency of airport  the minimum walking distances of passengers  the minimum idle time variance of each gate  the minimum number of flights at parking apron and the most reasonable utilization of large gates are selected as the optimization objectives  then an efficient multi objective optimization model of gate assignment problem is proposed in this paper  then an improved adaptive particle swarm optimization doadapo  algorithm based on making full use of the advantages of alpha stable distribution and dynamic fractional calculus is deeply studied  the dynamic fractional calculus with memory characteristic is used to reflect the trajectory information of particle updating in order to improve the convergence speed  the alpha stable distribution theory is used to replace the uniform distribution in order to escape from the local minima in a certain probability and improve the global search ability  next  the doadapo algorithm is used to solve the constructed multi objective optimization model of gate assignment in order to fast and effectively assign the gates to different flights in different time  finally  the actual flight data in one domestic airport is used to verify the effectiveness of the proposed method  the experiment results show that the doadapo algorithm can improve the convergence speed and enhance the local search ability and global search ability  and the multi objective optimization model of gate assignment can improve the comprehensive service of gate assignment  it can effectively provide a valuable reference for assigning the gates in hub airport   c  2017 elsevier b v  all rights reserved  
 subset selection via implicit utilitarian voting how should one aggregate ordinal preferences expressed by voters into a measurably superior social choice  a well established approach   which we refer to as implicit utilitarian voting   assumes that voters have latent utility functions that induce the reported rankings  and seeks voting rules that approximately maximize utilitarian social welfare  we extend this approach to the design of rules that select a subset of alternatives  we derive analytical bounds on the performance of optimal  deterministic as well as randomized  rules in terms of two measures  distortion and regret  empirical results show that regret based rules are more compelling than distortion based rules  leading us to focus on developing a scalable implementation for the optimal  deterministic  regret based rule  our methods underlie the design and implementation of robovote org  a not for profit website that helps users make group decisions via ai driven voting methods  
 supervised approach to recognise polish temporal expressions and rule based interpretation of timexes a key challenge of the information extraction in natural language processing is the ability to recognise and classify temporal expressions  timexes   it is a crucial source of information about when something happens  how often something occurs or how long something lasts  timexes extracted automatically from text  play a major role in many information extraction systems  such as question answering or event recognition  we prepared a broad specification of polish timexes   plimex  it is based on the state of the art annotation guidelines for english  mainly timex2 and timex3  a part of timeml   markup language for temporal and event expressions   we have expanded our specification for a description of the local meaning of timexes  based on ltimex annotation guidelines for english  temporal description supports further event identification and extends event description model  focussing on anchoring events in time  events ordering and reasoning about the persistence of events  we prepared the specification  which is designed to address these issues  and we annotated all documents in polish corpus of wroclaw university of technology  kpwr  using our annotation guidelines  we also adapted our liner2 machine learning system to recognise polish timexes and we propose two phase method to select a subset of features for conditional random fields sequence labelling method  this article presents the whole process of corpus annotation  evaluation of inter annotator agreement  extending liner2 system with new features and evaluation of the recognition models before and after feature selection with the analysis of statistical significance of differences  liner2 with presented models is available as open source software under the gnu general public license  
 supplier collaboration and speed to market of new products  the mediating and moderating effects in a turbulent market economy  the role of suppliers in manufacturer s new product development has received great attention from both practitioners and researchers  substantial empirical evidence on the contribution of suppliers in addressing challenges in terms of shorter product life  more immediate response  and faster information flows has been presented  this study aims to investigate which of the supplier collaboration  sc  practices are directly or indirectly related to the speed to market  stm  of new products across different firm sizes  the results confirm the direct and positive effect of information sharing on stm  furthermore  information sharing may partially mediate the effect of strategic purchasing on stm  and completely mediate the effect of supplier involvement on stm  it is also shown that firm size significantly affects the relationship between strategic purchasing and information sharing and that between information sharing and stm  the implications on improving stm via sc for future research and managerial practices are also discussed  
 supplier selection in nuclear power industry with extended vikor method under linguistic information with chinese nuclear power restarting  supplier selection in quality sensitive nuclear power industry has become increasingly urgent and necessary  however  the current research on supplier selection in nuclear power industry is rather few  moreover  there is still one great problem in the present methods  the description of the information uncertainty is inadequate  this paper proposes an extended vlsekriterijumska optimizacija i kompromisno resenje  vikor  under linguistic information to evaluate the uncertainty of potential supplier quantitatively and scientifically  the cloud model is used to handle imprecise numerical quantities  which can give consideration to both fuzziness and randomness of uncertain information  an empirical example of a nuclear power plant in china illustrates an application to supplier selection in nuclear power industry  which proves the effectiveness of the proposed method  finally  a comparative analysis with fuzzy vikor and sensitivity analysis of results are presented to verify the correctness and robustness of the extended method respectively   c  2016 elsevier b v  all rights reserved  
 supporting continuous changes to business intents software supporting an enterprise s business  also known as a business support system  needs to support the correlation of activities between actors as well as influence the activities based on knowledge about the value networks in which the enterprise acts  this requires the use of policies and rules to guide or enforce the execution of strategies or tactics within an enterprise as well as in collaborations between enterprises  with the help of policies and rules  an enterprise is able to capture an actor s intent in its business support system  and act according to this intent on behalf of the actor  since the value networks an enterprise is part of will change over time the business intents  life cycle states might change  achieving the changes in an effective and efficient way requires knowledge about the affected intents and the correlation between intents  the aim of the study is to identify how a business support system can support continuous changes to business intents  the first step is to find a theoretical model which serves as a foundation for intent driven systems  we conducted a case study using a focus group approach with employees from ericsson  this case study was influenced by the spiral case study process  the study resulted in a model supporting continuous definition and execution of an enterprise  the model is divided into three layers  define  execute  and a common governance view layer  this makes it possible to support continuous definition and execution of business intents and to identify the actors needed to support the business intents  life cycles  this model is supported by a meta model for capturing information into viewpoints  the research question is addressed by suggesting a solution supporting continuous definition and execution of an enterprise as a model of value architecture components and business functions  the results will affect how ericsson will build the business studio for their next generation business support systems  
 supporting operators in process control tasks benefits of interactive 3 d visualization in today s automated systems  the plant operator is confronted with a growing amount of diverse and distributed data about the plant process  for process control  the operator has to observe  interpret  and integrate the process data to form a basis of decision making for input parameter settings  this difficult task is prone to errors and can quickly result in insufficient product quality  an effective display design can support the operator and mitigate this effect  two experiments investigated whether the integration of process data in 3 d visualizations could increase the operators  performance in this environment  the first experiment examined benefits of improving reaction times and error rates for problem detection and corrective inputs  the possible reduction of the operators  workload was examined simultaneously  additionally  experiment 1 offered insights on how interaction with the 3 d visualization could further improve the appropriateness of selected process settings by the operator  results of this experiment showed 3 d and interaction as beneficial factors for the detection of problems in process control tasks and participants showed a low mental workload compared to 2 d presentations  in the second experiment  the scenario was extended by the investigation of a 3 d input design  in comparison to regular 2 d input  results showed that a combination of 3 d input and interaction exhibited higher accuracy in problem solving  
 supporting the social dimension of shopping for personalized products through online sales configurators mass customizers often sell personalized products through online sales configurators  also known as mass customization toolkits  recently  a number of mass customizers have connected their sales configurators with social software applications  this is not surprising  as social software enables an interactive and socially rich shopping experience  which makes shopping with a mass customization toolkit more similar to retail shopping  however  research on the use of social software by mass customizers is very limited  almost all previous studies on mass customization toolkits have focused on the dyadic interaction between a sales configurator and an isolated  potential customer  based on an analysis of 277 real online sales configurators  the present paper identifies eight different ways in which online sales configurators can connect with social software  these different connection modalities are compared both in terms of enabled social interactions and in terms of support provided for the sales configuration task  the paper also shows that  in the analyzed sample  the level of adoption varies substantially across the different modalities and  for the same modality  across industries  a number of opportunities for future research on these sales configurator social software connection modalities are finally outlined  
 supporting theoretically grounded model building in the social sciences through interactive visualisation the primary purpose for which statistical models are employed in the social sciences is to understand and explain phenomena occurring in the world around us  in order to be scientifically valid and actionable  the construction of such models need to be strongly informed by theory  to accomplish this  there is a need for methodologies that can enable scientists to utilise their domain knowledge effectively even in the absence of strong a priori hypotheses or whilst dealing with complex datasets containing hundreds of variables and leading to large numbers of potential models  in this paper  we describe enhanced model building processes in which we use interactive visualisations as the underlying mechanism to facilitate the construction and documentation of theory driven models  we report our observations from a collaborative project involving social and computer scientists  and identify key roles for visualisation to support model building within the context of social science  we describe a suite of techniques to facilitate the exploration of statistical summaries of input variables  to compare the quality of alternative models  and to keep track of the model building process  we demonstrate how these techniques operate in coordination to allow social scientists to efficiently generate models that are tightly underpinned by domain specific theory   c  2017 elsevier b v  all rights reserved  
 sustainable maritime inventory routing problem with time window constraints maritime inventory routing problem is addressed in this paper to satisfy the demand at different ports during the planning horizon  it explores the possibilities of integrating slow steaming policy as mentioned in kontovas et al   2011  and norstad et al   2011  within ship routing  a mixed integer non linear programming model is presented considering various scheduling and routing constraints  loading unloading constraints and vessel capacity constraints  non linear equation between fuel consumption and vessel speed has been incorporated to capture the sustainability aspects  several time window constraints are inculcated in the mathematical model to enhance the service level at each port  penalty costs are incurred if the ship arrives early before the starting of the time window or if it finishes its operation after the ending of the time window  costs associated with the violation of time window helps in maintaining a proper port discipline  now  owing to the inherent complexity of the aforementioned problem  an effective search heuristics named particle swarm optimization for composite particle  pso cp  is employed  particle swann optimization differential evolution  pso de   basic pso and genetic algorithm  ga  are used to validate the result obtained from pso cp  computational results provided for different problem instances shows the superiority of pso cp over the other algorithms in terms of the solution obtained  
 sustainable market valuation of buildings by the single valued neutrosophic mamva method traditionally  the real estate asset assessment is performed by experienced valuators  who take into account its economic  social  physical and locational aspects  nowadays  the construction industry is becoming more and more influenced by the sustainability requirements  therefore  the inclusion of the sustainability evaluation into real estate asset valuation is of utmost importance  the neutrosophic multi attribute market value assessment  mamva  method developed by the authors of this article handles market value calculations by solving multiple criteria assessment problems  and the initial information vagueness is modelled explicitly  the supplementary novelty of the present paper is the inclusion of the sustainability aspects into the real estate market valuation  the sustainable market valuation of croydon university hospital  emergency department  is performed as the case study to present numerical capabilities of the proposed approach  our research findings suggest that neutrosophic mamva is a rational approach for calculations of property market valuation and might be suitable for application worldwide   c  2017 elsevier b v  all rights reserved  
 sustainable production  using simulation modeling to identify the benefits of green information systems researchers and practitioners highlight the potential for information systems to promote sustainability in agricultural production  but little is known about the private and social benefits of specific agricultural decision support tools  in this study  we utilize the resource based view to assess a specific green technology using an agricultural economics simulation to estimate the quantitative benefits of this technology expressed as dollars saved and reduced greenhouse gas emissions  in particular  we employ a five step simulation modeling approach within a micro economic model of crop production to assess the ability of yield monitors to promote liquefied petroleum  lp  gas savings and subsequently reduce production costs  reduce greenhouse gas  ghg  emissions associated with lp gas burning  and generate additional revenue at a market for ghg mitigation credits  we estimate that the total benefits of using the green is to improve the harvesting decision would have been  82 million in post harvest cost savings and a significant reduction in greenhouse gas emissions  we present this simulation modeling approach  a common methodology in environmental sciences and economics  as a viable methodology for is researchers interested in modeling intricate decision making processes that are impacted by technology   c  2017 elsevier b v  all rights reserved  
 swarmic approach for symmetry detection of cellular automata behaviour since the introduction of cellular automata in the late 1940s they have been used to address various types of problems in computer science and other multidisciplinary fields  their generative capabilities have been used for simulating and modelling various natural  physical and chemical phenomena  besides these applications  the lattice grid of cellular automata has been providing a by product interface to generate graphical contents for digital art creation  one important aspect of cellular automata is symmetry  detecting of which is often a difficult task and computationally expensive  in this paper a swarm intelligence algorithm stochastic diffusion search is proposed as a tool to identify points of symmetry in the cellular automata generated patterns  
 symbiotic organisms search and two solution representations for solving the capacitated vehicle routing problem this paper presents the symbiotic organisms search  sos  heuristic for solving the capacitated vehicle routing problem  cvrp   which is a well known discrete optimization problem  the objective of cvrp is to decide the routes for a set of vehicles to serve a set of demand points while minimizing the total routing cost  sos is a simple and powerful metaheuristic that simulates the symbiotic interaction strategies adopted by an organism for surviving in an ecosystem  as sos is originally developed for solving continuous optimization problems  we therefore apply two solution representations  sr 1 and sr 2  to transform sos into an applicable solution approach for cvrp and then apply a local search strategy to improve the solution quality of sos  the original sos uses three interaction strategies  mutualism  commensalism  and parasitism  to improve a candidate solution  in this improved version  we propose two new interaction strategies  namely competition and amensalism  we develop six versions of sos for solving cvrp  the first version  soscanonical  utilizes a commonly used continuous to discrete solution representation transformation procedure  the second version is an improvement of canonical sos with a local search strategy  denoted as sosbasic  the third and fourth versions use sr 1 and sr 2 with a local search strategy  denoted as sossr 1 and sossr 2  the fifth and sixth versions  denoted as isossr 1 and isossr 2  improve the implementation of sossr 1 and sossr 2 by adding the newly proposed competition and amensalism interaction strategies  the performances of soscanonical  sosbasic  sossr 1  and sossr 2 are evaluated on two sets of benchmark problems  first  the results of the four versions of sos are compared  showing that the preferable result was obtained from sossr 1 and sossr 2   the performances of sossr 1  sossr 2  isossr 1  and isossr 2 are then compared  presenting that isossr 1 and isossr 2 offer a better performance  next  the isossr 1 and isossr 2 results are compared to the best known solutions  the results show that isossr 1 and isossr 2 produce good vrp solutions under a reasonable computational time  indicating that each of them is a good alternative algorithm for solving the capacitated vehicle routing problem   c  2016 elsevier b v  all rights reserved  
 synergistic effects on the elderly people s motor control by wearable skin stretch device combined with haptic joystick cutaneous sensory feedback can be used to provide additional sensory cues to a person performing a motor task where vision is a dominant feedback signal  a haptic joystick has been widely used to guide a user by providing force feedback  however  the benefit of providing force feedback is still debatable due to performance dependency on factors such as the user s skill level  task difficulty  meanwhile  recent studies have shown the feasibility of improving a motor task performance by providing skin stretch feedback  therefore  a combination of two aforementioned feedback types is deemed to be promising to promote synergistic effects to consistently improve the person s motor performance  in this study  we aimed at identifying the effect of the combined haptic and skin stretch feedbacks on the aged person s driving motor performance  for the experiment  15 healthy elderly subjects  age 72 8     6 6 years  were recruited and were instructed to drive a virtual power wheelchair through four different courses with obstacles  four augmented sensory feedback conditions were tested  no feedback  force feedback  skin stretch feedback  and a combination of both force and skin stretch feedbacks  while the haptic force was provided to the hand by the joystick  the skin stretch was provided to the steering forearm by a custom designed wearable skin stretch device  we tested two hypotheses   i  an elderly individual s motor control would benefit from receiving information about a desired trajectory from multiple sensory feedback sources  and  ii  the benefit does not depend on task difficulty  various metrics related to skills and safety were used to evaluate the control performance  repeated measure anova was performed for those metrics with two factors  task scenario and the type of the augmented sensory feedback  the results revealed that elderly subjects  control performance significantly improved when the combined feedback of both haptic force and skin stretch feedback was applied  the proposed approach suggest the feasibility to improve people s task performance by the synergistic effects of multiple augmented sensory feedback modalities  
 syntactic methods for topic independent authorship attribution the efficacy of syntactic features for topic independent authorship attribution is evaluated  taking a feature set of frequencies of words and punctuation marks as baseline  the features are  deep  in the sense that they are derived by parsing the subject texts  in contrast to  shallow  syntactic features for which a part of speech analysis is enough  the experiments are made on two corpora of online texts and one corpus of novels written around the year 1900  the classification tasks include classical closed world authorship attribution  identification of separate texts among the works of one author  and cross topic authorship attribution  in the first tasks  the feature sets were fairly evenly matched  but for the last task  the syntax based feature set outperformed the baseline feature set  these results suggest that  compared to lexical features  syntactic features are more robust to changes in topic  
 synthetic learning agents in game playing social environments this paper investigates the performance of synthetic agents in playing and learning scenarios in a turn based zero sum game and highlights the ability of opponent based learning models to demonstrate competitive playing performances in social environments  synthetic agents are generated based on a variety of combinations of some key parameters  such as exploitation vs exploration trade off  learning back up and discount rates  and speed of learning  and interact over a very large number of games on a grid infrastructure  experimental data is then analysed to generate clusters of agents that demonstrate interesting associations between eventual performance ranking and learning parameters  set up  the evolution of these clusters indicates that agents with a predisposition to knowledge exploration and slower learning tend to perform better than exploiters  which tend to prefer fast learning  observing these clusters vis a vis the playing behaviours of the agents makes it also possible to investigate how to select opponents best from a group  initial results suggest that good progress and stable evolution arise when an agent faces opponents of increasing capacity  and that an agent with a good learning mechanism set up progresses better when it faces less favourably set up agents  
 systemic banking crisis early warning systems using dynamic bayesian networks for decades  the literature on banking crisis early warning systems has been dominated by two methods  namely  the signal extraction and the logit model methods  however  these methods  do not model the dynamics of the systemic banking system  in this study  dynamic bayesian networks are applied as systemic banking crisis early warning systems  in particular  the hidden markov model  the switching linear dynamic system and the naive bayes switching linear dynamic system models are considered  these dynamic bayesian networks provide the means to model system dynamics using the markovian framework  given the dynamics  the probability of an impending crisis can be calculated  a unique approach to measuring the ability of a model to predict a crisis is utilised  the results indicate that the dynamic bayesian network models can provide precise early warnings compared with the signal extraction and the logit methods   c  2016 elsevier ltd  all rights reserved  
 tailored platform for the development of nfc tourist services in recent years near field communication  nfc  technology has undergone a rapid evolution  in the tourism sector this technology enables any user equipped with a smart phone to interact with the surrounding objects and have access to information and related services  in this paper  we present a fully configurable system for tourist services  the proposal contains a database and multimedia services that can be presented and personalized on mobile devices using dynamic websites  users may use nfc technology to interact with smart objects  augmented with nfc tags to store information and services that will be customized according to user interaction based on content and services defined in the metamodel  associating metamodels to objects  solutions can be applied to any tourism sectors  
 takeover requests in simulated partially autonomous vehicles considering human factors in the development of autonomous vehicles  the main focus of sensor research has been in relation to environmental perception  and only minimal work has focused on the human vehicle interaction perspective  however  human factors need to be considered to ensure the safe operation of partially autonomous vehicles  this study briefly introduces a design methodology for the takeover request  tor  time in national highway traffic safety administration level 3 vehicles and compares four different tors in a simulated environment based on human in the loop experiments with various driving scenarios  a total of 30 drivers participated in the study  and the quantitative qualitative data obtained show statistically significant differences between the four tor thresholds  this study shows that the timing involved in the takeover can be obtained by using a performance based approach considering human factors  
 task oriented control of a humanoid robot through the implementation of a cognitive architecture this work presents a novel approach on task oriented control of a humanoid robot through the implementation of a cognitive architecture  the architecture developed here provides humanoid robots with systems that allow them to continuously learn new skills  adapt these skills to new contexts and robustly reproduce new behaviours in dynamical environments  this architecture can be thought of as a first stepping stone upon which to incrementally build more complex cognitive processes  providing this way a minimum degree of intelligence for the humanoid robot  several experiments are conducted to prove the validity of the system and to test the operation of the architecture  
 tax payment default prediction using genetic algorithm based variable selection according to the statistics from the finnish tax authorities  about 12  of all active firms in finland had unpaid taxes at the end of year 2015  in monetary terms  this translates to over 3 billion euros in unpaid taxes  this is a highly significant amount as the total amount of taxes collected during 2015 was 49 billion euros  considering the economic significance of the unpaid taxes  relatively little research has been done on identifying tax defaulting firms  the objective of this study is to develop a genetic algorithm based decision support tool for predicting tax payment defaults  more closely  a genetic algorithm is used for determining an optimal or near optimal subset of variables for a linear discriminant analysis  lda  model that classifies the examined firms as either defaulting or non defaulting  the tool also provides information about the importance of various variables in predicting a tax default  the dataset consists of finnish limited liability firms that have defaulted on employer contribution taxes or on value added taxes and the total number of available variables is 72  the results show that variables measuring solvency  liquidity and payment period of trade payables are important variables in predicting tax defaults  the best performing model comprises three non linearly transformed variables and has a predictive accuracy of 73 8    c  2017 elsevier ltd  all rights reserved  
 team situation awareness within the context of human autonomy teaming effective team communication  a fundamental part of team coordination  is crucial for both effective team situation awareness  tsa  and team performance  in this study  we looked at the role that team interaction  i e   more specifically team verbal behaviors  played in tsa and team performance in order to better understand human autonomy teaming  hat   we first analyzed team verbal behaviors  i e   pushing and pulling information  across conditions of human  autonomy teams and human  human teams  and then analyzed their relationship with tsa and team performance via growth curve modelling  gcm   good teamwork involves anticipating the needs of teammates and that means pushing information before it is requested  therefore  if things are going well  there should be little need for pulling information  in this study s task  participants were instructed to push information to others  and over time master the specific timing of information sharing to the intended recipient  findings indicate that pushing information was positively associated with tsa and team performance  and human  autonomy teams had lower levels of both pushing and pulling information than all human teams  through this study  we have learned that anticipation of other team member behaviors and information requirements in human  autonomy teams are important for effective tsa and team performance  in order to make hat more effective in terms of teamwork  we need to develop mechanisms to enhance pushing information within hat   c  2016 elsevier b v  all rights reserved  
 tensors for data mining and data fusion  models  applications  and scalable algorithms tensors and tensor decompositions are very powerful and versatile tools that can model a wide variety of heterogeneous  multiaspect data  as a result  tensor decompositions  which extract useful latent information out of multiaspect data tensors  have witnessed increasing popularity and adoption by the data mining community  in this survey  we present some of the most widely used tensor decompositions  providing the key insights behind them  and summarizing them from a practitioner s point of view  we then provide an overview of a very broad spectrum of applications where tensors have been instrumental in achieving state of the art performance  ranging from social network analysis to brain data analysis  and from web mining to healthcare  subsequently  we present recent algorithmic advances in scaling tensor decompositions up to today s big data  outlining the existing systems and summarizing the key ideas behind them  finally  we conclude with a list of challenges and open problems that outline exciting future research directions  
 the agreement measure  cat  a complement to focused on categorization of a continuum agreement on unitizing  where several annotators freely put units of various sizes and categories on a continuum  is difficult to assess because of the simultaneaous discrepancies in positioning and categorizing  the recent agreement measure offers an overall solution that simultaneously takes into account positions and categories  in this article  i propose the additional coefficient  cat   which complements by assessing the agreement on categorization of a continuum  putting aside positional discrepancies  when applied to pure categorization  with predefined units    cat  behaves the same way as the famous dedicated krippendorff s   even with missing values  which proves its consistency  a variation of  cat  is also proposed that provides an in depth assessment of categorizing for each individual category  the entire family of coefficients is implemented in free software  
 the analysis for the cargo volume with hybrid discrete wavelet modeling many efforts have been made to the development of models that able to analyze and predict marine cargo volume  however  improving forecasting especially marine cargo throughput time series forecasting accuracy is an important yet often diffcult issue facing managers  in this study  a tei i methodology based hybrid forecasting model is proposed  the original time series are decomposed different scale components using discrete wavelet technique based on seasonality analysis of components  all decomposed components are predicted by radial basis function networks due to its flexible nonlinear modeling capability  empirical results suggest that the use of discrete wavelet technique enhances the ability of monthly volatility mining and demonstrate consistent better performance of the proposed approach  
 the application of 3d fruit fly optimization algorithm to the keywords analysis of macau s international relations in order to comprehend the new policy directions regarding macau s participation in international organizations  this paper made the depth interviews and applied the text mining analysis to the interviewer s responses  through applying the 3d fruit fly optimization algorithm  3d foa  and our modified 3d foa model to make the classification analysis of interviewees  keywords  the study concluded the modified 3d foa model has the better performance in the optimization process  and the key points for our interviewees regarding macau s participation in international organizations include the basic law s regularization and macau s participation in international economic organization  
 the authorship dilemma  alphabetical or contribution  scientific communities have adopted different conventions for ordering authors on publications  are these choices inconsequential  or do they have significant influence on individual authors  the quality of the projects completed  and research communities at large  what are the trade offs of using one convention over another  in order to investigate these questions  we formulate a basic two player game theoretic model  which already illustrates interesting phenomena that can occur in more realistic settings  we find that contribution based ordering leads to a denser collaboration network and a greater number of publications  while alphabetical ordering can improve research quality  contrary to the assumption that free riding is a weakness of the alphabetical ordering scheme  when there are only two authors  this phenomenon can occur under any contribution scheme  and the worst case occurs under contribution based ordering  finally  we show how authors working on multiple projects can cooperate to attain optimal research quality and eliminate free riding given either contribution scheme  
 the bayesian stance  equations for  as if  sensorimotor agency the verb to do  plays a vital part in our understanding of the world  and it goes hand in hand with words such as active  action and agent  but the physical sciences describe only mechanical happenings  not acts  their theoretical language is  in essence  a strict mathematical formalism applied to the description of variables  usually quantitative ones  that can   at least in principle   be measured by mechanical instruments  in such a language  what is the definition of an agent  of an act  in contrast to previous approaches  which attempt to discriminate between agent and non agent systems  we pursue a more dennettian approach that attempts only to characterise the explanatory logic of intentional  agentive  interpretations of a physical system  we wish to do so purely in terms of the formal relations that hold between variables in a dynamical system or stochastic process  our approach is straightforward  we use pearl s causal formalism to identify physical variables at the causal boundary between agent  and environment   and identify these with variables in bayesian decision theory  this provides a rigorous bridge between mathematical models of physics and mathematical models of rational decision making  
 the behavioral topsis this paper updates and expands the topsis that was introduced four decades ago  first  we re present the logic of topsis according to the traditional decision theory  it shows that topsis also has a built in multi attribute value function that is not revealed explicitly  so far  this has been a hidden aspect of topsis  second  we incorporate the decision maker s  dm  behavioral tendency into topsis  the behavioral topsis accommodates the loss aversion concept in behavioral economics  a dm can set the loss aversion ratio to reflect his her choice inclination  two cases are illustrated to show the efficacy of the behavioral topsis  c  2017 elsevier ltd  all rights reserved  
 the best of two worlds  balancing model strength and comprehensibility in business failure prediction using spline rule ensembles numerous organizations and companies rely upon business failure prediction to assess and minimize the risk of initiating business relationships with partners  clients  debtors or suppliers  advances in research on business failure prediction have been largely dominated by algorithmic development and comparisons led by a focus on improvements in model accuracy  in this context  ensemble learning has recently emerged as a class of particularly well performing methods  albeit often at the expense of increased model complexity  however  in practice  model choice is rarely based on predictive performance alone  models should be comprehensible and justifiable to assess their compliance with common sense and business logic  and guarantee their acceptance throughout the organization  a promising ensemble classification algorithm that has been shown to reconcile performance and comprehensibility are rule ensembles  in this study  an extension entitled spline rule ensembles is introduced and validated in the domain of business failure prediction  spline rule ensemble complement rules and linear terms found in conventional rule ensembles with smooth functions with the aim of better accommodating nonlinear simple effects of individual features on business failure  experiments on a large selection of 21 datasets of european companies in various sectors and countries  i  demonstrate superior predictive performance of spline rule ensembles over a set of well established yet powerful benchmark methods   ii  show the superiority of spline rule ensembles over conventional rule ensembles and thus demonstrate the value of the incorporation of smoothing splines   iii  investigate the impact of alternative term regularization procedures and  iv  illustrate the comprehensibility of the resulting models through a case study  in particular  the ability of the technique to reveal the extent and the way in which predictors impact business failure  and if and how variables interact  are exemplified   c  2017 elsevier ltd  all rights reserved  
 the big chase  a decision support system for client acquisition applied to financial networks bank agencies daily store a huge volume of data regarding clients and their operations  this information  in turn  can be used for marketing purposes to acquire new clients or sell products to existing clients  a decision support system  dss  can help a manager to decide the sequence of clients to contact to reach a designed target  in this paper we present the big chase  a dss that translates bank data into a reliability graph  this graph models relationships based on a probability of traversal function that includes social measures  the proposed dss  developed in close collaboration with banco santander  s a   fits the parameters of the probability function to explicit solution evaluations given by experts by means of a specifically designed projected gradient descent algorithm  the fitted probability function determines the reliabilities associated to the edges of the graph  an optimization procedure tailored to be efficient on very large sparse graphs with millions of nodes and edges identifies the most reliable sequence of clients that a manager should contact to reach a specific target  the big chase has been tested with a case study on real data that includes banco santander  s a  2015 spain bank records  experimental results show that the proposed dss is capable of modeling the experts  evaluations into probability function with a small error   c  2017 elsevier b v  all rights reserved  
 the binomial decomposition of owa functions  the 2 additive and 3 additive cases in n dimensions in the context of the binomial decomposition of ordered weighted averaging  owa  functions  we investigate the constraints associated with the 2 additive and 3 additive cases in n dimensions  the 2 additive case depends on one coefficient whose feasible region does not depend on the dimension n  on the other hand  the feasible region of the 3 additive case depends on two coefficients and is explicitly dependent on the dimension n  this feasible region is a convex polygon with n vertices and n edges  which is strictly expanding in the dimension n  the orness of the owa functions within the feasible region is linear in the two coefficients  and the vertices associated with maximum and minimum orness are identified  finally  we discuss the 3 additive binomial decomposition in the asymptotic infinite dimensional limit  
 the cognitive underpinnings of policy process studies  introduction to a special issue of cognitive systems research this article introduces the special issue of cognitive systems research on public policy processes  we begin with a discussion of the cognitive foundations of public policy that stem from the complexity of human cognition and emotion  next  we provide an overview of the articles in the special issue  which occur at the edge of a public policy cognitive systems boundary  we then turn to a discussion of promising new work in the study of public policy that explores or may benefit from the cognitive systems perspective   c  2017 elsevier b v  all rights reserved  
 the commercial nlp landscape in 2017 the commercialisation of natural language processing began over 35 years ago  but it s only in the last year or two that it s become substantially more visible  largely because of the intense popular interest in artificial intelligence  so what s the state of commercial nlp today  we survey the main industry categories of relevance  and offer comment on where the action is today  
 the competence management to improve the learning engagement the organizational structures of public and local administration depend on many bureaucratic aspects  emergencies and political factors instead of rational motivations related to knowledge  competences and profiles often influence the allocation of people in offices  in most cases  such situations generate dissatisfactions  low productivity and become gangrenous  the objective of this work is applying competence management  skill gap analysis and a study of the existing organizational structures to point out the functional unit with the most critical situation in terms of allocated employees in order to suggest the people to involve in learning programmes according to the complex laws  internal regulation for the staff management and trade union influences  in fact  the proposed approach identifies the real gaps that create inefficiencies and suggests the employees to engage in learning activities by focusing exactly on what the organizations need with respect to what the employees have  it happens by elaborating a priority scale on the base of existing hierarchies  relationships  logistic constraints and other aspects with the aim  above all  of enhancing the identified unit and the local administration itself  
 the cost of dichotomizing continuous labels for binary classification problems  deriving a bayesian optimal classifier many pattern recognition problems involve characterizing samples with continuous labels instead of discrete categories  while regression models are suitable for these learning tasks  these labels are often discretized into binary classes to formulate the problem as a conventional classification task  e g   classes with low versus high values   this methodology brings intrinsic limitations on the classification performance  the continuous labels are typically normally distributed  with many samples close to the boundary threshold  resulting in poor classification rates  previous studies only use the discretized labels to train binary classifiers  neglecting the original  continuous labels  this study demonstrates that  even in binary classification problems  exploiting the original labels before splitting the classes can lead to better classification performance  this work proposes an optimal classifier based on the bayesian maximum a posterior  map  criterion for these problems  which effectively utilizes the real valued labels  we derive the theoretical average performance of this classifier  which can be considered as the expected upper bound performance for the task  experimental evaluations on synthetic and real data sets show the improvement achieved by the proposed classifier  in contrast to conventional classifiers trained with binary labels  these evaluations clearly demonstrate the optimality of the proposed classifier  and the precision of the expected upper bound obtained by our derivation  
 the dynamics of group cognition the aim of this paper is to demonstrate that the postulation of irreducible  distributed cognitive systems  or group minds as they are also known in the literature  is necessary for the successful explanatory practice of cognitive science and sociology  towards this end  and with an eye specifically on the phenomenon of distributed cognition  the debate over reductionism versus emergence is examined from the perspective of dynamical systems theory  dst   the motivation for this novel approach is threefold  firstly  dst is particularly popular amongst cognitive scientists who work on modelling collective behaviors  secondly  dst can deliver two distinct arguments in support of the claim that the presence of mutual interactions between group members necessitates the postulation of the corresponding group entity  thirdly  dst can also provide a succinct understanding of the way group entities exert downward causation on their individual members  the outcome is a naturalist account of the emergent  and thereby irreducible  nature of distributed cognitive systems that avoids the reductionists  threat of epiphenomenalism  while being well in line with materialism  
 the dynamics of pedestrians  evacuation during emergency situations emergencies like natural disasters and terrorist attacks  which are characterized by sudden onset situations  have always been part of the world s reality  in these situations  collective human behavior such as crowd stampedes may be triggered  sometimes stampedes lead to fatalities  as people are crushed or trampled  in this paper  a dynamic model for the evacuation of pedestrians during emergency situations is proposed with the consideration of information transmission in the crowd  in this model  based on a complex adaptive system  it is assumed that the information  part of which is psychology and behavior  has an influence on the interactions of pedestrians  we investigated two factors   1  the systematic condition of information transmission and  2  the intensity of beneficial information that affects the interaction  parameters of the factors are discussed to capture the dynamic features of the pedestrian crowd through proved theorems  and the critical values for effective evacuation are obtained  finally  a numerical example for the evacuation of pedestrians is designed to validate the practicality of this model and mathematical demonstration  
 the effect of a haptic guidance steering system on fatigue related driver behavior prolonged driving on monotonous roads often leads to a reduction in task load that causes drivers passive fatigue  passive fatigue results in loss of driver alertness and is detrimental to driver safety  this paper focuses on the effect of a haptic guidance steering system on improving behaviors of passively fatigued drivers  by continuously exerting active torque on a steering wheel  the haptic system guides drivers to follow the centerline of a lane  meanwhile  the drivers sense the torque and interact with it while operating the steering wheel  an experiment was conducted with 12 healthy participants in a high fidelity driving simulator  a monotonous driving course was designed  and vehicle speed was fixed in order to induce drivers  passive fatigue  a treatment session was arranged with the haptic guidance steering system  and a control session was conducted as a comparison  driving performance  assessed by standard deviation of lane position  mean absolute lateral error  and time to lane crossing  was significantly improved when haptic guidance was activated  results of physiological measures  including heart rate variability and percentage of eye closure  revealed that passively fatigued drivers were aroused when they were aware of the active torque on the steering wheel  in conclusion  the activation of haptic guidance can be regarded as an effective countermeasure for the passively fatigued drivers who have performed a prolonged monotonous driving task  
 the effect of genetic algorithm learning with a classifier system in limit order markets by introducing a genetic algorithm with a classifier system as a learning mechanism for uninformed traders into a dynamic limit order market with asymmetric information  this paper examines the effect of the learning on traders  trading behavior  market liquidity and efficiency  we show that the learning is effective and valuable with respect to information acquisition  forecasting  buy sell order choice accuracies  and profit opportunity for uninformed traders  it improves information dissemination efficiency and reduces the information advantage of informed traders and hence the value of the private information  in particular  the learning and information become more valuable with higher volatility  less informed traders  and longer information lag  furthermore  the learning makes not only uninformed but also informed traders submit more limit orders and hence increases market liquidity supply   c  2017 elsevier ltd  all rights reserved  
 the effect of robotic wheelchair control paradigm and interface on user performance  effort and preference  an experimental assessment the exact manner in which control is shared between a human and an autonomous system is a crucial factor for assistive robots that provide physical support to people with severe motor impairments  there has however been little comparative study between different control sharing paradigms within the field of assistive robotics  we present a control architecture for a robotic  smart  wheelchair that allows for the seamless interchange and evaluation of any number of control sharing paradigms  and so facilitates comparative study between them  we present an implementation of four control sharing paradigms  and results from a study that compares all four to each other and teleoperation  and moreover using multiple control interfaces and across multiple sessions  experimental results suggest that  i  task performance metrics differ with each control interface   ii  performance increases with increasing autonomy assistance however it is not statistically significantly different between higher levels of autonomy   iii  metrics related to user effort show a decrease with increasing autonomy  which is more emphasized with more limited control interface and  iv  how much the autonomy is utilized differs greatly between control paradigms  but not control interfaces  moreover   v  for almost all performance metrics  there is a consistent performance increase in session 2 compared to session 1  and for both control interfaces  lastly  subjective questionnaires  control paradigm preference and perceived utility  reveal both similarities and differences between sci and uninjured subjects  no single control paradigm is the clear winner in performance or preference  suggesting that it will be important to offer end users multiple control options to accommodate their individual needs and preferences   c  2017 elsevier b v  all rights reserved  
 the effectiveness of word of mouth in offline and online social networks social networks connect users to share thoughts and build friendships  the high degree of intimacy among users has made it a good venue for word of mouth  wom  marketing  admittedly there are some basic differences  but this study focuses on the effectiveness of wom marketing in offline and online social networks  a system was developed to simulate offline and online networks using small world  sw  and scale free  sf  networks  respectively  an offline network was found to be more effective in promoting a product with a fixed advertising budget and in selling higher margin products than an online network  however  if customers have diversified backgrounds and are strongly opinionated  an online network is a better venue  these findings can be used as guidelines to determine the appropriateness of moving wom marketing from offline to online networks   c  2017 elsevier ltd  all rights reserved  
 the effects of cognitive biases and imperfectness in long term robot human interactions  case studies using five cognitive biases on three robots the research presented in this paper demonstrates a model for aiding human robot companionship based on the principle of  human  cognitive biases applied to a robot  the aim of this work was to study how cognitive biases can affect human robot companionship in long time  in the current paper  we show comparative results of the experiments using five biased algorithms in three different robots such as erwin  mykeepon and marc  the results were analysed to determine what difference if any of biased vs unbiased interaction has on the interaction with the robot and if the participants were able to form any kind of  preference  towards the different algorithms  the experimental presented show that the participants have more of a preference towards the biased algorithm interactions than the robot without the bias   c  2016 elsevier b v  all rights reserved  
 the effects of data density  display organization  and stress on search performance  an eye tracking study of clutter display clutter can cause breakdowns in visual search performance  which can compromise system safety and efficiency  the differential contributions of the two main aspects of clutterdata density and display organization to these breakdowns are not well understood  moreover  it is not clear how these factors interact with stress  another factor that can degrade performance in data rich domains  the aim of this study was to ascertain the effects of data density  display organization  and stress on visual search performance and associated eye movements  obtained via eye tracking   to this end  participants performed visual search tasks in a simulated graphics program  data density  number of icons   organization  grouping of icons   and stress  presence of a time limit  were manipulated  response time  rt   error rate  eye tracking data  and subjective ratings of clutter and stress were collected  results confirmed that high data density and poor organization increase rt and error rate  furthermore  poor organization worsened the effects of high data density on rt  stress did not negatively affect performance but significantly decreased rt  eye tracking metrics provided insight into the effects of the two clutter aspects and stress  in particular  these metrics helped explain how changes in attention allocation resulted in the observed performance effects of long rt and high error rate  the results of this research can be used to inform the design of data rich displays and trigger display adaptations in real time  
 the embodied performance pedagogy of jacques lecoq this article proposes that acting is a valuable area of research for the fields of artificial intelligence and simulated behaviour  this suggestion is supported through applying theories and findings from the field of embodied cognition to the performance pedagogy of french acting teacher jacques lecoq  1921 1999   embodied cognition proposes that thinking and behaviour are properties of the whole human organism  not the brain alone  and that body  brain and cognition are situated   engaged with the surrounding environment  this thesis arises from findings that show that sensorial and motor experiences form the neural foundations for mental concepts and that sensorimotor neural networks are partially re activated by mental and linguistic activity  leading to the concept of embodied simulation  i give examples of the ways in which lecoq s conceptualisation of acting technique is implicitly congruent with the principles of embodied cognition  and often explicitly anticipates its precepts  
 the emergence of inclusive and exclusive virtual communities determined by the preferences of their users consider the decision faced by the user of a social network site  sns  regarding whether or not to accept a friendship request from another user  the user making such a decision is constrained by the limited amount of information available about the requester  therefore  the decision must be based on incomplete information about the main characteristics and preferences describing the requester  we formalize this decision problem by defining the expected utility tradeoffs derived from the request and simulate the resulting acceptance and rejection incentives numerically  these incentives provide the basis on which to build inclusive and exclusive social networks determined by the different expectations and preferences of their users  social networks are generated using a self organizing map to cluster the decision makers  dms  by their friendship acceptance behavior  we analyze the effects on the cluster structure of the resulting social network that follow from modifying the distribution of requesters relative to the preferences of the dms  the disutility derived from accepting the friendship of an unwanted requester  the costs incurred when searching for potential friends to expand the network of connections  and the minimum networking capacities of the friendship requesters demanded by the dms  
 the epistemic value of brain machine systems for the study of the brain bionic systems  connecting biological tissues with computer or robotic devices through brain machine interfaces  can be used in various ways to discover biological mechanisms  in this article i outline and discuss a  stimulation connection  bionics supported methodology for the study of the brain  and compare it with other epistemic uses of bionic systems described in the literature  this methododology differs from the  synthetic   simulative method often followed in theoretically driven artificial intelligence and cognitive  neuro  science  even though it involves machine models of biological systems  i also bring the previous analysis to bear on some claims on the epistemic value of bionic technologies made in the recent philosophical literature  i believe that the methodological reflections proposed here may contribute to the piecewise understanding of the many ways bionic technologies can be deployed not only to restore lost sensory motor functions  but also to discover brain mechanisms  
 the evaluation or trustworthiness to identity health insurance fraud in dentistry objective  according to the investigations of the u s  government accountability office  gao   health insurance fraud has caused an enormous pecuniary loss in the u s  in taiwan  in dentistry the problem is getting worse if dentists  authorized entities  file fraudulent claims  several methods have been developed to solve health insurance fraud  however  these methods are like a rule based mechanism  without exploring the behavior patterns  these methods are time consuming and ineffective  in addition  they are inadequate for managing the fraudulent dentists  methods  based on social network theory  we develop an evaluation approach to solve the problem of cross dentist fraud  the trustworthiness score of a dentist is calculated based upon the amount and type of dental operations performed on the same patient and the same tooth by that dentist and other dentists  results  the simulation provides the following evidence   1  this specific type of fraud can be identified effectively using our evaluation approach   2  a retrospective study for the claims is also performed   3  the proposed method is effective in identifying the fraudulent dentists  conclusions  we provide a new direction for investigating the genuineness of claims data  if the insurer can detect fraudulent dentists using the traditional method and the proposed method simultaneously  the detection will be more transparent and ultimately reduce the losses caused by fraudulent daims   c  2016 elsevier b v  all rights reserved  
 the extended scientific mind according to externalist theories of scientific cognition  scientific theory formation and revision are sometimes achieved by cognitive systems that range beyond the biological boundaries of individual scientists  two kinds of externalist theories deserve to be sharply distinguished  the first is the extended scientific mind  which is an application of the extended mind to scientific contexts  according to this externalist theory  the cognitive systems responsible for a scientist s inferences can extend into her extra cranial environment and incorporate elements with which she is intimately coupled  the second kind of externalist theory is a population level theory of scientific cognition  which characterizes the dynamics of populations and communities of scientific researchers  externalist theories of this type posit cognitive properties of these population level dynamics that are not necessarily instantiated by any individual scientist who is a member of such populations  i argue that population level theories of scientific cognition are more plausible than the extended scientific mind because the latter posits cognitive systems that are explanatorily unwarranted  on the assumption that non scientific cognition and scientific cognition are fundamentally alike  a similar moral holds for externalist accounts of ordinary cognition   c  2016 elsevier b v  all rights reserved  
 the fuzzy cognitive pairwise comparisons for ranking and grade clustering to build a recommender system  an application of smartphone recommendation in a competitive high end product market  many enterprises offer a variety of products to compete the market shares in different segments  due to rich information of plenty of competitive product alternatives  consumers face the challenges to compare and choose the most suitable products  whilst a product comprises different tangible and intangible features  consumers tend to buy the features rather than a product itself  a successful product has most features meeting the consumer needs  perception values of product features from consumers are complex to be measured and predicted  to reduce information overload for searching their preferred products  this paper proposes the fuzzy cognitive pairwise comparison for ranking and grading clustering  fcpc rgc  to build a recommender system  the fuzzy number enables rating flexibility for the users to handle rating uncertainty  the fuzzy cognitive pairwise comparison  fcpc  is used to evaluate consumer preferences for multiple features of a product by pairwise comparison ratings  the fuzzy grade clustering  fgc  is used to group the product alternatives into different consumer preference grades  to verify the validity and applicability of fcpc rgc  a smartphone recommender system using the proposal approach is demonstrated how the system is able to help the consumers to recommend the suitable products according to the customers  individual preference  
 the hasty wisdom of the mob  how market sentiment predicts stock market behavior we explore the ability of sentiment metrics  extracted from micro blogging sites  to predict stock markets  we also address sentiments  predictive time  horizons  the data concern bloggers  feelings about five major stocks  taking independent bullish and bearish sentiment metrics  granular to two minute intervals  we model their ability to forecast stock price direction  volatility  and traded volume  we find evidence of a causal link from sentiments to stock price returns  volatility and volume  the predictive tim  horizon is minutes  rather than hours or days  we argue that diverse and high volume sentiment is more predictive of price volatility and traded volume than near consensus is predictive of price direction  causality is ephemeral  in this sense  the crowd is more a hasty mob than a source of wisdom   c  2017 elsevier ltd  all rights reserved  
 the impact of formats and interactive modes on the effectiveness of mobile advertisements mobile advertising researchers have suggested that the advertising effect is caused by the advertising features  however  research on mobile advertising effectiveness is scant  this article explores advertising formats and interactive modes related to the effectiveness of advertising by adopting the attention  interest  desire and action  aida  model  this utilized an experimental procedure and questionnaire  the analysis of variance  anova  results showed a significant format and interactive mode effect  but no interaction between the format and interactive modes  moreover  rich media showed higher advertising effectiveness than the dynamic banner  the playfulness interactive mode was better than user control and connectedness on advertising effectiveness  in conclusion  this study provided results that promote consumer s willingness to buy  rich media and playfulness interactive advertising modes should be adopted  
 the impact of human robot multimodal communication on mental workload  usability preference  and expectations of robot behavior multimodal communication between humans and autonomous robots is essential to enhance effectiveness of human robot team performance in complex  novel environments  such as in military intelligence  surveillance  and reconnaissance operations in urban settings  it is imperative that a systematic approach be taken to evaluate the factors that each modality contributes to the user s ability to perform successfully and safely  this paper addresses the effects of unidirectional speech and gesture methods of communication on perceived workload  usability preferences  and expectations of robot behavior while commanding a robot teammate to perform a spatial navigation task  each type of communication was performed alone or simultaneously  results reveal that although the speech alone condition elicited the lowest level of perceived workload  the usability preference and expectations of robot behavior after interacting through each communication condition was the same  further  workload ratings between the gesture and speech gesture conditions were similar indicating systems that employ gesture communication could also support speech communication with little to no additional subjectively perceived cognitive burden on the user  findings also reveal that workload alone should not be used as a sole determining factor of communication preference during system and task evaluation and design  additionally  perceived workload did not seem to negatively impact the level of expectations regarding the robot s behavior  recommendations for future human robot communication evaluation are provided  
 the impact of microblogging data for stock market prediction  using twitter to predict returns  volatility  trading volume and survey sentiment indices in this paper  we propose a robust methodology to assess the value of microblogging data to forecast stock market variables  returns  volatility and trading volume of diverse indices and portfolios  the methodology uses sentiment and attention indicators extracted from microblogs  a large twitter dataset is adopted  and survey indices  aaii and ii  usmc and sentix   diverse forms to daily aggregate these indicators  usage of a kalman filter to merge microblog and survey sources  a realistic rolling windows evaluation  several machine learning methods and the diebold mariano test to validate if the sentiment and attention based predictions are valuable when compared with an autoregressive baseline  we found that twitter sentiment and posting volume were relevant for the forecasting of returns of s p 500 index  portfolios of lower market capitalization and some industries  additionally  kf sentiment was informative for the forecasting of returns  moreover  twitter and kf sentiment indicators were useful for the prediction of some survey sentiment indicators  these results confirm the usefulness of microblogging data for financial expert systems  allowing to predict stock market behavior and providing a valuable alternative for existing survey measures with advantages  e g   fast and cheap creation  daily frequency    c  2016 elsevier ltd  all rights reserved  
 the impacts of uncertain factors on decisions of npo and firms the use of potentially hazardous materials has been aroused wide concerns in the world  in order to reduce the hazard of these potentially hazardous materials  some non profit organizations  npos  devote to the substitution of potentially hazardous materials  this paper studies a potentially hazardous material substitution problem in a bi level decision making model with a npo and two competitive firms  the npo  the leader  aims to prompt the two firms to instantly substitute a kind of potentially hazardous material  this purpose is achieved by raising the material s market sensitivity  which is determined by an effort exerted by the npo with an uncertain shock  under the uncertain environment  the two firms  the followers  proceed a static nash game with their strategies of instant substituting and substituting with delay  we analyze the effects of the uncertain shock s volatility on the decisions of the npo and the firms  our results demonstrate that when the marginal cost of instant substituting is very small  the uncertain shock s volatility has no impact on the decisions of the participators  the npo exerts no effort and the firms substitute instantly  through numerical simulations  when the marginal cost of instant substituting is big  the npo exerts a relatively big effort  the impact of the volatility on the npo s effort is contingent on the marginal cost of instant substituting and the difference between the firms  initial market shares   c  2016 elsevier b v  all rights reserved  
 the indian spontaneous expression database for emotion recognition automatic recognition of spontaneous facial expressions is a major challenge in the field of affective computing  head rotation  face pose  illumination variation  occlusion etc  are the attributes that increase the complexity of recognition of spontaneous expressions in practical applications  effective recognition of expressions depends significantly on the quality of the database used  most well known facial expression databases consist of posed expressions  however  currently there is a huge demand for spontaneous expression databases for the pragmatic implementation of the facial expression recognition algorithms  in this paper  we propose and establish a new facial expression database containing spontaneous expressions of both male and female participants of indian origin  the database consists of 428 segmented video clips of the spontaneous facial expressions of 50 participants  in our experiment  emotions were induced among the participants by using emotional videos and simultaneously their self ratings were collected for each experienced emotion  facial expression clips were annotated carefully by four trained decoders  which were further validated by the nature of stimuli used and self report of emotions  an extensive analysis was carried out on the database using several machine learning algorithms and the results are provided for future reference  such a spontaneous database will help in the development and validation of algorithms for recognition of spontaneous expressions  
 the influence of influence  the effect of task repetition on persuaders and persuadees we investigate how the experience of influencing and of being influenced impacts on a subsequent  immediate attempt to influence and be influenced  we conduct an experiment using participant dyads matched in a round robin design which systematically measures the influence one individual has on another in a decision task using a short  anonymous  computer mediated  text based exchange  findings show that being influenced in a round of the task tends to be positively related to being influenced in the subsequent two rounds with the effect weakening each time  wefind no impact on the ability to influence   c  2016 elsevier b v  all rights reserved  
 the influence of the spatio temporal terzi treatment on the kinematics of cursive writing of dysgraphic subjects many different treatment approaches have been applied for handwriting remediation in school aged children  the effects of these treatments are usually evaluated by using ad hoc checklists as well as some standard test protocols like movement abc tests  visual motor integration  v m i   tests  etc  recently  digitizing tablets have allowed the study of kinematic characteristics of handwriting and a series of parameters were estimated on cursive writing of italian children  in this paper  the influence of a new rehabilitation protocol  terzi s method  on kinematics of cursive writing was evaluated  the analysis was carried out on 22 nonproficient handwriting children  attending primary or secondary school  which undertook a series of three exercises  with and without linguistic concerns  a set of kinematic parameters was calculated before and after the rehabilitation process  the results showed that terzi s rehabilitation program is capable of producing a significant velocity increase in the test without linguistic involvement  whereas a significant slowdown in the stroke realization was present in the other tasks  the effectiveness ofterzi s rehabilitation program was proved by significant improvements in the v m i  test  in the movement abc test as well as in writing legibility  evaluated by checklist   
 the internet  cognitive enhancement  and the values of cognition this paper has two distinct but related goals   1  to identify some of the potential consequences of the internet for our cognitive abilities and  2  to suggest an approach to evaluate these consequences  i begin by outlining the google effect  which  allegedly  shows that when we know information is available online  we put less effort into storing that information in the brain  some argue that this strategy is adaptive because it frees up internal resources which can then be used for other cognitive tasks  whereas others argue that this is maladaptive because it makes us less knowledgeable  i argue that the currently available empirical evidence in cognitive psychology does not support strong conclusions about the negative effects of the internet on memory  before we can make value judgements about the cognitive effects of the internet  we need more robust and ecologically valid evidence  having sketched a more nuanced picture of the google effect  i then argue that the value of our cognitive abilities is in part intrinsic and in part instrumental  that is  they are both valuable in themselves and determined by the socio cultural context in which these cognitive abilities are utilised  focussing on instrumental value  i argue that  in an information society such as ours  having the skills to efficiently navigate  evaluate  compare  and synthesize online information are  under most circumstances  more valuable than having a lot of facts stored in biological memory  this is so  partly because using the internet as an external memory system has overall benefits for education  navigation  journalism  and academic scholarship  
 the interplay between free sampling and word of mouth in the online software market free sampling in digital format has become a common business practice in the online market offering consumers first hand experience with products  due to its low marginal cost and extensive online distribution  at the same time  online word of mouth  wom  has also been a prevalent strategy on the internet for increasing product visibility and providing trustworthy product information  those two online marketing strategies are generally considered to stand alone by marketers and prior research  nevertheless  by drawing on integrated information response theory as well as theories for explaining online consumers  review sharing  we argue that free sampling complements wom in the online market by amplifying its sales effect and facilitating its implementation  we provide supportive empirical evidence through a bayesian analysis of software free sampling on cnet download com  cnetd  and sales and wom from amazon com over a 25 week data set  our results show that adoptions of cnetd free sampling positively interact with amazon wom in influencing amazon software sales  in addition  more adoptions of cnetd free sampling lead to a larger volume of amazon wom  and this impact is more significant for less popular products  these findings contribute to our understanding of free sampling in the online market such that  in addition to its direct sales effect  free sampling can also potentially affect sales through influencing online wom  therefore  we suggest that marketers evaluate the free sampling strategy by including its interplay with online wom and apply low cost free sampling to facilitate the relatively more expensive online wom marketing strategy  especially for unpopular products  c  2017 elsevier b v  all rights reserved  
 the its driven affective embodied conversational agent eva  based on a novel conversational behavior generation algorithm as a result of the convergence of different services delivered over the internet protocol  internet protocol television  iptv  may be regarded as the one of the most widespread user interfaces accepted by a highly diverse user domain  every generation  from children to the elderly  can use iptv for recreation  as well as for gaining social contact and stimulating the mind  however  technological advances in digital platforms go hand in hand with the complexity of their user interfaces  and thus induce technological disinterest and technological exclusion  therefore  interactivity and affective content presentations are  from the perspective of advanced user interfaces  two key factors in any application incorporating human computer interaction  hci   furthermore  the perception and understanding of the information  meaning  conveyed is closely interlinked with visual cues and non verbal elements that speakers generate throughout human human dialogues  in this regard  co verbal behavior provides information to the communicative act  it supports the speaker s communicative goal and allows for a variety of other information to be added to his her messages  including  but not limited to  psychological states  attitudes  and personality  in the present paper  we address complexity and technological disinterest through the integration of natural  human like multimodal output that incorporates a novel combined data  and rule driven co verbal behavior generator that is able to extract features from unannotated  general text  the core of the paper discusses the processes that model and synchronize non verbal features with verbal features even when dealing with unknown context and or limited contextual information  in addition  the proposed algorithm incorporates data driven  speech prosody  repository of motor skills  and rule based concepts  grammar  gesticon   the algorithm firstly classifies the communicative intent  then plans the co verbal cues and their form within the gesture unit  generates temporally synchronized co verbal cues  and finally realizes them in the form of human like co verbal movements  in this way  the information can be represented in the form of both meaningfully and temporally synchronized co verbal cues with accompanying synthesized speech  using communication channels to which people are most accustomed  
 the mechanism of additive composition additive composition  foltz et al  in discourse process 15 285 307  1998  landauer and dumais in psychol rev 104 2  211  1997  mitchell and lapata in cognit sci 34 8  1388 1429  2010  is a widely used method for computing meanings of phrases  which takes the average of vector representations of the constituent words  in this article  we prove an upper bound for the bias of additive composition  which is the first theoretical analysis on compositional frameworks from a machine learning point of view  the bound is written in terms of collocation strength  we prove that the more exclusively two successive words tend to occur together  the more accurate one can guarantee their additive composition as an approximation to the natural phrase vector  our proof relies on properties of natural language data that are empirically verified  and can be theoretically derived from an assumption that the data is generated from a hierarchical pitman yor process  the theory endorses additive composition as a reasonable operation for calculating meanings of phrases  and suggests ways to improve additive compositionality  including  transforming entries of distributional word vectors by a function that meets a specific condition  constructing a novel type of vector representations to make additive composition sensitive to word order  and utilizing singular value decomposition to train word vectors  
 the minkowski central partition as a pointer to a suitable distance exponent and consensus partitioning the minkowski weighted k means  mwk means  is a recently developed clustering algorithm capable of computing feature weights  the cluster specific weights in mwk means follow the intuitive idea that a feature with low variance should have a greater weight than a feature with high variance  the final clustering found by this algorithm depends on the selection of the minkowski distance exponent  this paper explores the possibility of using the central minkowski partition in the ensemble of all minkowski partitions for selecting an optimal value of the minkowski exponent  the central minkowski partition appears to be also a good consensus partition  furthermore  we discovered some striking correlation results between the minkowski profile  defined as a mapping of the minkowski exponent values into the average similarity values of the optimal minkowski partitions  and the adjusted rand index vectors resulting from the comparison of the obtained partitions to the ground truth  our findings were confirmed by a series of computational experiments involving synthetic gaussian clusters and real world data   c  2017 elsevier ltd  all rights reserved  
 the multi criteria group decision making methodology using type 2 fuzzy linguistic judgments this paper develops an integrated methodology for multiple criteria and multiple experts decision making problems using type 2 fuzzy sets  t2fs  as the linguistic judgments  the experts or decision maker s opinions are in linguistics forms  the proposed approach is the extension of the analytic hierarchy process  ahp  method for multiple experts  the pairwise or relative comparisons of decision linguistic judgments are characterized by interval type 2 fuzzy sets  it2fs   the integrated methodology consists of the following stages  1  construct it2fs pairwise comparison matrix  pcm   2  elicit it2fs weights and rates  3  normalize the it2fs weights and rates  4  synthesize the it2fs weights and rates  5  calculate the global it2fs rates  6  determine the final decision priority  at the outset of the decision making the it2fs pcm for a set of criteria is created  the quadratic programming models are constructed to elicit the absolute it2fs weights of the criteria in the sense of minimizing the least squares logarithmic regression functions  a normalization procedure is introduced for the it2fs vector to obtain the well defined and normalized it2fs weights  two approaches  the linear programming model and the it2 fuzzy weighted average  fwa   are developed to synthesize and aggregate the local rates and weights  thus  the overall global it2fs rates for the alternatives are obtained  the final decision then can be concluded by the priority of the overall it2fs rates of the alternatives  the proposed approach is applied to new product development  npd  project screening  npd is largely uncertain  because of competition accompanied with modern technology and market changes  it always leads to failure of new product development  the final project screening decision is obtained from the overall global it2fs rates of the projects with respect to the screening criteria   c  2016 elsevier b v  all rights reserved  
 the myerson value for cooperative games on communication structure with fuzzy coalition in the field of cooperative games there is an extensive literature that studies various situations of cooperation  myerson  1977  introduced the communication structure which is an undirected graph describing the bilateral relationships among the players and the myerson value of a game is obtained by taking the shapley value of an auxiliary graph game on communication structure  aubin  1981  proposed fuzzy cooperative games in which players have the possibility to cooperate with different participation levels  in this paper we consider cooperative games on communication structure with fuzzy coalition  the myerson value and its individual rational revision are defined as the shapley value of newly auxiliary graph games and discussed based on choquet integral form and proportional form respectively  they are also characterized in terms of some extended component efficiency and fairness  furthermore  by showing that the myerson value is a fuzzy core allocation  the non emptiness of the fuzzy core is verified for a graph game on communication structure with fuzzy coalition  
 the myosuit  bi articular anti gravity exosuit that reduces hip extensor activity in sitting transfers muscle weakness which can result from neurological injuries  genetic disorders  or typical aging can affect a person s mobility and quality of life  for many people with muscle weakness  assistive devices provide the means to regain mobility and independence  these devices range from well established technology  such as wheelchairs  to newer technologies  such as exoskeletons and exosuits  for assistive devices to be used in everyday life  they must provide assistance across activities of daily living  adls  in an unobtrusive manner  this article introduces the myosuit  a soft  wearable device designed to provide continuous assistance at the hip and knee joint when working with and against gravity in adls  this robotic device combines active and passive elements with a closed loop force controller designed to behave like an external muscle  exomuscle  and deliver gravity compensation to the user  at 4 1 kg  4 6 kg with batteries   the myosuit is one of the lightest untethered devices capable of delivering gravity support to the user s knee and hip joints  this article presents the design and control principles of the myosuit  it describes the textile interface  tendon actuators  and a bi articular  synergy based approach for continuous assistance  the assistive controller  based on bi articular force assistance  was tested with a single subject who performed sitting transfers  one of the most gravity intensive adls  the results show that the control concept can successfully identify changes in the posture and assist hip and knee extension with up to 26  of the natural knee moment and up to 35  of the knee power  we conclude that the myosuit s novel approach to assistance using a bi articular architecture  in combination with the posture based force controller  can effectively assist its users in gravity intensive adls  such as sitting transfers  
 the next generation i m sure you want me to tell you about the next new emerging trend  but i m not going to do that  it is much easier to suggest where trends come from  the next generation   and how to distinguish passing fads  bubbles  from emerging trends  young people are often the early adopters  the first to see what is about to happen  but most people don t see what s coming until well after the fact  those with the most to lose  the establishment  tend to be the most resistant to change  
 the order effect on online review helpfulness  a social influence perspective review helpfulness is receiving increasing attention by academics and practitioners along with the growing problem of information overload in the internet age  prior studies on online review helpfulness mainly focus on the direct influences of review  and reviewer specific characteristics  an implicit underlying assumption of these studies is that reviews for a given product are independent of each other  as such  the existence of social influence among reviewers is much overlooked in the literature on review helpfulness  to fill this gap  this study as a first attempt to explore the order effect on review helpfulness  aims to investigate whether  how  and why the order of a review will affect review helpfulness from a social influence perspective  with an analysis of 70 610 restaurant reviews collected from yelp com  this study finds that the order of a review negatively relates to review helpfulness  moreover  the negative effect of review order on review helpfulness is weakened when the reviewer has more social connectedness or a higher level of expertise  or when the review is more negative in content or more recently posted   c  2016 elsevier b v  all rights reserved  
 the perception of team engagement reduces stress induced situation awareness overconfidence and risk taking much research has examined how stress restricts objective situation awareness  sa   little research  however  has focused on sa overconfidence  the notion that an individual may grasp a situation when in fact they do not  even less sa research has examined the motivational and emotional states of individuals operating in teams in stressful environments  expanding on recent data suggesting that stress creates sa overconfidence  not simply sa loss  the present experiment manipulated stress levels and the perception of team engagement  which is thought to be a positive motivational state of task related well being  teams of soldiers were tested in a virtual combat scenario testing shared risk taking  objective  i e   collaborative ability to answer sa probes   and subjective sa  results indicated that the mere perception of above average team engagement reduced stress induced sa overconfidence and risk taking of teams  these results suggest simple  virtually costless strategies for improving elements of sa that may impact the behavior of teams and potentially improve their decision making   c  2017 elsevier b v  all rights reserved  
 the pictures we like are our image  continuous mapping of favorite pictures into self assessed and attributed personality traits flickr allows its users to tag the pictures they like as  favorite   as a result  many users of the popular photo sharing platform produce galleries of favorite pictures  this article proposes new approaches  based on computational aesthetics  capable to infer the personality traits of flickr users from the galleries above  in particular  the approaches map low level features extracted from the pictures into numerical scores corresponding to the big five traits  both self assessed and attributed  the experiments were performed over 60 000 pictures tagged as favorite by 300 users  the psychoflickr corpus   the results show that it is possible to predict beyond chance both self assessed and attributed traits  in line with the state of the art of personality computing  these latter are predicted with higher effectiveness  correlation up to 0 68 between actual and predicted traits   
 the power of the  like  button  the impact of social media on box office the mainstream research of social factors and box office performance has concentrated on post consumption opinion mining and sentiment analysis  which are difficult to operationalize to the benefits of the industry practitioners whose objective is to maximize box office sales  in this study  we propose the facebook  like  as an effective social marketing tool before the release of movies for several reasons  firstly  people s prerelease  liking  of movies can be influenced by marketing campaigns  secondly  the clicks of  likes  create social impact  as suggested by the social impact theory  on moviegoers  consumption behaviors  and thirdly  facebook  like  provides practitioners with real time visible updates  by studying the impact of prerelease  likes  on box office sales  we not only contribute to the literature by offering a new social metric to evaluate the box office performance  but also provide the industry practitioners with quantitative support for the effectiveness of their social marketing activities  our empirical results indicate that the prerelease  likes  exert a significantly positive impact on box office performance  more specifically  1  increase in the number of  likes  in the one week prior to release is associated with an increase of the opening week box office by about 0 2   as it approaches the release date  the prerelease  like  impact becomes stronger  suggesting that the latest prerelease  likes  are more effective in driving box office performance   c  2016 elsevier b v  all rights reserved  
 the price of anarchy in auctions this survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings  this theory complements traditional economic techniques  which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings  we highlight three user friendly analytical tools  smoothness type inequalities  which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies  extension theorems  which extend such guarantees to randomized strategies  no regret learning outcomes  and incomplete information settings  and composition theorems  which extend such guarantees from simpler to more complex auctions  combining these tools yields tight worst case approximation guarantees for the equilibria of many widely used auction formats  
 the repo market in mexico  empirics and stylized facts this paper is a thorough description of the repo market in mexico  a funding market of significant importance for mexican commercial banks  brokerage houses  and development banks  and which  unlike the repo markets in europe and the u s   is an otc market with no central counterparty or tri partite repos  in this paper  we describe the counterparties which are involved in repo transactions  as well as distribution by collateral type  and provide some further metrics on the trading volume and other informative statistics on haircuts and rates  given that banco de mexico possesses information on individual repo transactions from 1998  we also study the evolution of this market  paying particular attention to the financial crisis that started in 2007  this is one of a few descriptive papers on this market due to the extremely detailed and granular data base on repo transactions in mexico  the main contribution of this paper is to document and describe the structure of a niarket for which there exists a long history of individual repo transactions and which is of great importance from the perspective of the funding structure of banks   c  2017 elsevier b v  all rights reserved  
 the role of autobiographical memory in the development of a robot self this article briefly reviews research in cognitive development concerning the nature of the human self  it then reviews research in developmental robotics that has attempted to retrace parts of the developmental trajectory of the self  this should be of interest to developmental psychologists  and researchers in developmental robotics  as a point of departure  one of the most characteristic aspects of human social interaction is cooperation the process of entering into a joint enterprise to achieve a common goal  fundamental to this ability to cooperate is the underlying ability to enter into  and engage in  a self other relation  this suggests that if we intend for robots to cooperate with humans  then to some extent robots must engage in these self other relations  and hence they must have some aspect of a self  decades of research in human cognitive development indicate that the self is not fully present from the outset  but rather that it is developed in a usage based fashion  that is  through engaging with the world  including the physical world and the social world of animate intentional agents  in an effort to characterize the self  ulric neisser noted that self is not unitary  and he thus proposed five types of self knowledge that correspond to five distinct components of self  ecological  interpersonal  conceptual  temporally extended  and private  he emphasized the ecological nature of each of these levels  how they are developed through the engagement of the developing child with the physical and interpersonal worlds  crucially  development of the self has been shown to rely on the child s autobiographical memory  from the developmental robotics perspective  this suggests that in principal it would be possible to develop certain aspects of self in a robot cognitive system where the robot is engaged in the physical and social world  equipped with an autobiographical memory system  we review a series of developmental robotics studies that make progress in this enterprise  we conclude with a summary of the properties that are required for the development of these different levels of self  and we identify topics for future research  
 the role of the distribution platform in price formation of paid apps in this paper we study the role of the distribution platform as an important determinant of price of paid apps  we also examine how the distribution platform influences the price implications of important developers  app level decisions  to these purposes  we construct a hierarchical model of price formation by using an ad hoc panel dataset consisting of top paid apps from the two major app stores  namely apple s app store and google play  our findings show that prices of paid apps strongly depend on the platform where the apps are marketed  specifically  the app store is associated with lower prices for paid apps than google play  we find evidence that this is because the impact of cross store differences in developer competition prevails over the impact of cross store differences in average consumer willingness to pay  we also find that the price premiums as a return to trialability are more likely to emerge in google play than in the app store  and that developers are more likely to adopt a penetration price policy in google play  thus implying an influence of the distribution platform on the price implications of these app level decisions  finally  our evidence does not confirm the argument that a more marked price reduction for paid apps embedding ads or generating revenues from other interested third parties should be observed in google play   c  2016 elsevier b v  all rights reserved  
 the seaport service rate prediction system  using drayage truck trajectory data to predict seaport service rates for drayage operators the service rate of seaports is crucial for organizing their container pick up delivery operations  this study presents a seaport service rate prediction system that could help drayage operators to improve their predictions of the duration of the pick up delivery operations at a seaport by using the subordinate trucks  trajectory data  the system is constructed based on three components namely  trajectory reconstruction  geo fencing analysis  and gradient boosting modelling  using predictive analytic techniques  the prediction system is trained and validated using more than 15 million data records from over 200 trucks over a period of 19 months  the gradient boosting model based solution provides better predictions compared with the linear model benchmark solution  conclusions and implications are formulated  c  2016 elsevier b v  all rights reserved  
 the state context property formalism  from concept theory to the semantics of music quantum cognition is an emergent area of research that applies quantum structures in cognitive situations where traditional approaches are problematic  independently from quantum cognition  quantum approaches for the semantics of music interpretation and improvisation have been proposed in the context of quantum computational logic and standard quantum mechanics  respectively  these frameworks overcome the non contextual and compositional approaches to the semantics of music  in this paper  we analyze the quantum approach to the semantics of music from the perspective of quantum cognition  we first introduce an operational framework  inspired by the brussels geneva approach to the foundations of quantum theory  named state context property  scop  formalism  that has been applied in quantum cognition to model concepts  next  we show that the quantum approaches to the semantics of music can be operationalized in scop  in particular  we apply the scop definitions of  orthogonal property  and  experiment context  to operationalize the notions of  vague meaning  and  interpretation   and to characterize the structural differences between the semantics of music interpretation and improvisation  
 the structured process modeling method  spmm  what is the best way for me to construct a process model  more and more organizations turn to the construction of process models to support strategical and operational tasks  at the same time  reports indicate quality issues for a considerable part of these models  caused by modeling errors  therefore  the research described in this paper investigates the development of a practical method to determine and train an optimal process modeling strategy that aims to decrease the number of cognitive errors made during modeling  such cognitive errors originate in inadequate cognitive processing caused by the inherent complexity of constructing process models  the method helps modelers to derive their personal cognitive profile and the related optimal cognitive strategy that minimizes these cognitive failures  the contribution of the research consists of the conceptual method and an automated modeling strategy selection and training instrument  these two artefacts are positively evaluated by a laboratory experiment covering multiple modeling sessions and involving a total of 149 master students at ghent university   c  2017 elsevier b v  all rights reserved  
 the technology and economic determinants of cryptocurrency exchange rates  the case of bitcoin cryptocurrencies  such as bitcoin  have ignited intense discussions  despite receiving extensive public attention  theoretical understanding is limited regarding the value of blockchain based cryptocurrendes  as expressed in their exchange rates against traditional currencies  in this paper  we conduct a theory driven empirical study of the bitcoin exchange rate  against usd  determination  taking into consideration both technology and economic factors  to address co integration in a mix of stationary and non stationary time series  we use the autoregressive distributed lag  ardl  model with a bounds test approach in the estimation  meanwhile  to detect potential structural changes  we estimate our empirical model on two periods separated by the closure of mt  gox  one of the largest bitcoin exchange markets   according to our analysis  in the short term  the bitcoin exchange rate adjusts to changes in economic fundamentals and market conditions  the long term bitcoin exchange rate is more sensitive to economic fundamentals and less sensitive to technological factors after mt  gox closed  we also identify a significant impact of mining technology and a decreasing significance of mining difficulty in the bitcoin exchange price determination  c  2016 elsevier b v  all rights reserved  
 the therapist assignment problem in home healthcare structures staff planning in home health care  hhc  context is challenging due to the complexity  such as  unavailability of resources  variation in patient health conditions  and diversity of continuity of care  coc  and patient s priority  pp   this necessitates the implementation of adequately effective models and intelligent systems to improve the robustness of care plans that run with limited input from the support staff  the work proposed an effective  simple  compatible and extensible model for the therapist assignment problem  tap   the model aims at maximizing the assignment rate of demand  subject to the constraints of workload capacity limitation and available time selection clash  it helps the hhc structure managers to make proper decisions through preferred time periods  ptps  selections and weight allocations  the analysis of the ptps claims that the hhc structures applying the tap model should offer a selection of at most five ptps to each patient for the sake of effectiveness and efficiency  following this suggestion  optimal solutions for all instances can be provided within 0 4 s  the weight allocations depend on the various requirements for coc and pp  the analysis of results suggests that the hhc structures can adopt pp in the tap model without hesitation  however  it also advises that they should pay attention on the adoption of coc  because it has a visible effect on the assignment rate of demand with the lower coc levels and the utilization rate of therapists  while slightly affecting the computational time of the tap model and the total number of assigned demands  the work offers the hhc structures a demonstration of the core part of an effective planning system to help them make better decisions that satisfy patient demand  achieve high quality of service  and enhance efficiency   c  2016 elsevier ltd  all rights reserved  
 the time varying nature of social media sentiments in modeling stock returns the broad aim of this paper is to answer the following query  is the relationship between social media sentiments and stock returns time varying  to provide a satisfactory response  a novel methodology a symbiosis of bayesian dynamic linear models and seemingly unrelated regressions is introduced  two sets of dow jones industrial average stock data and corresponding social media data from yahoo  finance stock message boards are used in a comprehensive empirical study  some key findings are   a  affirmative response to the above question   b  models with only social media sentiments and market returns perform at least as well as models that include fama french and momentum factors   c  there are significant correlations between stocks  ranging from 0 8 to 0 6 in both data sets   c  2017 elsevier b v  all rights reserved  
 the truth assignments that differentiate human reasoning from mechanistic reasoning the evidence based argument for lucas  godelian thesis we consider the argument that tarski s classic definitions permit an intelligence whether human or mechanistic to admit finitary evidence based definitions of the satisfaction and truth of the atomic formulas of the first order peano arithmetic pa over the domain n of the natural numbers in two  hitherto unsuspected and essentially different  ways   1  in terms of classical algorithmic verifiabilty  and  2  in terms of finitary algorithmic computability  we then show that the two definitions correspond to two distinctly different assignments of satisfaction and truth to the compound formulas of pa over n i pa n i  sv  and i pa n i  sc   we further show that the pa axioms are true over n  and that the pa rules of inference preserve truth over n  under both n i pa n i  sv  and i pa n i sc   we then show   a  that if we assume the satisfaction and truth of the compound formulas of pa are always non finitarily decidable under n i pa n i  sv   then this assignment corresponds to the classical non finitary standard interpretation i pa n i  s  of pa over the domain n  and  b  that the satisfaction and truth of the compound formulas of pa are always finitarily decidable under the assignment i pa n i  sc   from which we may finitarily conclude that pa is consistent  we further conclude that the appropriate inference to be drawn from godel s 1931 paper on undecidable arithmetical propositions is that we can define pa formulas which under any interpretation over n are algorithmically verifiable as always true over n  but not algorithmically computable as always true over n  we conclude from this that lucas  godelian argument is validated if the assignment n i pa n  sv  can be treated as circumscribing the ambit of human reasoning about  true  arithmetical propositions  and the assignment n i pa n i  sc  as circumscribing the ambit of mechanistic reasoning about  true  arithmetical propositions   c  2016 elsevier b v  all rights reserved  
 the upper and lower bound evaluation based on the quantile efficiency in stochastic data envelopment analysis data envelopment analysis  dea  has been extended to handle random inputs and outputs by using chance constrained programming  in this paper  for dmus with random inputs and outputs  we aim to measure a kind of relative efficiency  and achieve it from the optimistic viewpoint and the pessimistic viewpoint respectively  considering the quantile of the distribution of the weighted output input ratio of each dmu  we develop two stochastic dea models to obtain the upper and lower bounds of the quantile efficiency under a constraint  and then achieve an interval efficiency evaluation  the best quantile efficiency and the worst quantile efficiency achieved by our models are closely similar to the ccr efficiency and belong to relative efficiencies  further  the deterministic equivalents of our models are developed when the input and output vector of each dmu follows a multivariate joint normal distribution  finally  three examples are presented to illustrate the performance of our approach   c  2017 elsevier ltd  all rights reserved  
 the value of vehicle telematics data in insurance risk selection processes the advent of the internet of things enables companies to collect an increasing amount of sensor generated data which creates plenty of new business opportunities  this study investigates how this sensor data can improve the risk selection process in an insurance company  more specifically  several risk assessment models based on three different data mining techniques are augmented with driving behaviour data collected from in vehicle data recorders  this study proves that including standard telematics variables significantly improves the risk assessment of customers  as a result  insurers will be better able to tailor their products to the customers  risk profile  moreover  this research illustrates the importance of including industry knowledge  combined with data expertise  in the variable creation process  especially when a regulator forces the use of easily interpretable data mining techniques  expert based telematics variables are able to improve the risk assessment model in addition to the standard telematics variables  further  the results suggest that if a manager wants to implement usage based insurances  pay as you drive related variables are most valuable to tailor the premium to the risk  finally  the study illustrates that this new type of telematics based insurance product can quickly be implemented since three months of data is already sufficient to obtain the best risk estimations   c  2017 elsevier b v  all rights reserved  
 theoretical accounts to practical models  grounding phenomenon for abstract words in cognitive robots this review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle  from relevant theories to practical models of agents and robots  most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models  to better understand these modelling approaches  it is indispensable to work from the base  theoretical accounts  to the top  computational models   therefore in this paper  succinct definition of abstract words is presented first  and then the symbol grounding issue and accounts of grounded cognition for abstract words are given  the next section discusses the computational modelling approaches for abstract words grounding phenomenon  finally  important cognitive robotics models are reviewed  this paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings   c  2016 elsevier b v  all rights reserved  
 theories  structures and simulations in the research of early mentalizing the objective of this paper is to propose some new approaches that could be used in the research of the development of mindreading in infancy  i argue that the mentalizing system should be approached as a developing hierarchical structure that acquires the ability to orient attention to the most significant features in the environment  the development of this system could be described by a series of implicit theories that share a common structure which provides continuity over theory change  according to this view  the early stages which have a fundamental role for the development of mindreading would be described by low level theories  but as infants become increasingly efficient in predicting behavior of observed agents the higher order entities become prioritized due to their relevance  infants thus gradually develop the ability to predict the behavior of others in situations involving attribution of mental states  i propose a strategy based on the analysis of attention deployment that could help to decide which of the alternative theories provides the most adequate account of the mentalizing system at a certain developmental stage  i also suggest that computational modeling could contribute to the research in this area  simulations of looking behavior based on different ontologies could prove to be useful for choosing the most adequate theory as well as for advancing the understanding of mechanisms involved in the development of theory of mind   c  2016 elsevier b v  all rights reserved  
 theory  practice and performance i focus on the enactivist and extended mind approaches to embodied cognition  ec   and specifically on the concepts of body schema  affectivity  distributed cognition and intersubjectivity to show how ec has relevance to questions about expert performance  and to the theory and practice of performing arts  
 there is no logical negation here  but there are alternatives  modeling conversational negation with distributional semantics logical negation is a challenge for distributional semantics  because predicates and their negations tend to occur in very similar contexts  and consequently their distributional vectors are very similar  indeed  it is not even clear what properties a negated distributional vector should possess  however  when linguistic negation is considered in its actual discourse usage  it often performs a role that is quite different from straightforward logical negation  if someone states  in the middle of a conversation  that this is not a dog  the negation strongly suggests a restricted set of alternative predicates that might hold true of the object being talked about  in particular  other canids and middle sized mammals are plausible alternatives  birds are less likely  skyscrapers and other large buildings virtually impossible  conversational negation acts like a graded similarity function  of the sort that distributional semantics might be good at capturing  in this article  we introduce a large data set of alternative plausibility ratings for conversationally negated nominal predicates  and we show that simple similarity in distributional semantic space provides an excellent fit to subject data  on the one hand  this fills a gap in the literature on conversational negation  proposing distributional semantics as the right tool to make explicit predictions about potential alternatives of negated predicates  on the other hand  the results suggest that negation  when addressed from a broader pragmatic perspective  far from being a nuisance  is an ideal application domain for distributional semantic methods  
 thirty years of the international journal of intelligent systems  a bibliometric review the international journal of intelligent systems was created in 1986  today  the journal is 30 years old  to celebrate this anniversary  this study develops a bibliometric review of all of the papers published in the journal between 1986 and 2015  the results are largely based on the web of science core collection  which classifies leading bibliographic material by using several indicators including total number of publications and citations  the h index  cites per paper  and citing articles  the work also uses the vos viewer software for visualizing the main results through bibliographic coupling and co citation  the results show a general overview of leading trends that have influenced the journal in terms of highly cited papers  authors  journals  universities and countries   c  2016 wiley periodicals  inc  
 three stage performance modeling using dea bpnn for better practice benchmarking this paper proposes an innovative three stage model using data envelopment analysis  dea  and back propagation neural network  bpnn  for supporting  better practice  benchmarking as contrasted with the traditional  best practice  benchmarking  research has shown that dea models have the capability of setting optimal goals  but the drawback of the standard dea approach is its inability to propose actionable targets necessary for incremental improvement  overcoming the shortfalls of dea and its superiority driven practices  the neural network approach accommodates stepwise improvement through adaptive learning and prediction capability  consequently  the proposed three stage model is capable of generating feasible improvement options for managers as an intelligent decision support tool  at its core  the innovative approach provides a sound methodological foundation for shaping a  better practice  paradigm and contributes to the literature through methodological advancement  the effectiveness of the model is empirically tested through the use of data from the healthcare industry  and the results confirm a practical utility of the model   c  2016 elsevier ltd  all rights reserved  
 three way group decision making based on multigranulation fuzzy decision theoretic rough set over two universes decision theoretic rough set provides a new perspective to handle decision making problems under uncertainty and risk  the three way decision theory proposed by yao is based on rough set theory and is a natural extension of the classical two way decision approach  in this paper  we introduce the idea of decision theoretic rough set into multigranulation approximation space and explore the rough approximation of a fuzzy decision object under the framework of two universes  we construct a variable precision multigranulation fuzzy decision theoretic rough set over two universes by using the concept of an arbitrary binary fuzzy relation class between two different universes and the probability measurement of a fuzzy event  several interesting properties of the proposed model are addressed and the decision rules are also deduced using the concept of three way decision making over two universes  moreover  two special types of optimistic and pessimistic models are given by using different precision parameters  we then present a new approach to multiple criteria group decision making problems  based on variable precision multigranulation fuzzy decision theoretic rough set over two universes  meanwhile  we establish a cost based method for sorting among all alternatives of group decision making problems  finally  an example of handling a medical diagnosis problem illustrates this approach   c  2016 elsevier inc  all rights reserved  
 time consistent fuzzy multi period rolling portfolio optimization with adaptive risk aversion factor this study focuses on a time consistent multi period rolling portfolio optimization problem under fuzzy environment  an adaptive risk aversion factor is first defined to incorporate investor s changing psychological risk concerns during the intermediate periods  within the framework of credibility theory  the future returns of risky assets are represented by triangular and trapezoidal fuzzy variables  respectively  which are estimated by utilizing justifiable granularity principle using real financial data from shanghai stock exchange  sse   the return and risk of assets at each investment period are measured by expected value and entropy  respectively  the problem is then formulated by a series of rolling deterministic linear programmings and solved with simplex methods  numerical examples are provided to illustrate the effectiveness of the proposed adaptive risk aversion factor and rolling formulation methodologies  
 time series forecasting with the warimax garch method it is well known that causal forecasting methods that include appropriately chosen exogenous variables  evs  very often present improved forecasting performances over univariate methods  however  in practice  evs are usually difficult to obtain and in many cases are not available at all  in this paper  a new causal forecasting approach  called wavelet auto regressive integrated moving average with exogenous variables and generalized auto regressive conditional heteroscedasticity  warimax garch  method  is proposed to improve predictive performance and accuracy but also to address  at least in part  the problem of unavailable evs  basically  the warimax garch method obtains wavelet  evs   wevs  from auto regressive integrated moving average with exogenous variables and generalized auto regressive conditional heteroscedasticity  arimax garch  models applied to wavelet components  wcs  that are initially determined from the underlying time series  the wevs are  in fact  treated by the warimax garch method as if they were conventional evs  similarly to garch and arima garch models  the warimax garch method is suitable for time series exhibiting non linear characteristics such as conditional variance that depends on past values of observed data  however  unlike those  it can explicitly model frequency domain patterns in the series to help improve predictive performance  an application to a daily time series of dam displacement in brazil shows the warimax garch method to remarkably outperform the arima garch method  as well as the  multi layer perceptron  artificial neural network  ann  and its wavelet version referred to as wavelet artificial neural network  wann  as in  1   on statistical measures for both in sample and out of sample forecasting   c  2016 elsevier b v  all rights reserved  
 time series regression based pairs trading in the korean equities market pairs trading is an instance of statistical arbitrage that relies on heavy quantitative data analysis to profit by capitalising low risk trading opportunities provided by anomalies of related assets  a key element in pairs trading is the rule by which open and close trading triggers are defined  this paper investigates the use of time series regression to define the rule which has previously been identified with fixed threshold based approaches  empirical results indicate that our approach may yield significantly increased excess returns compared to ones obtained by previous approaches on large capitalisation stocks in the korean equities market  
 time informed task planning in multi agent collaboration human robot collaboration requires the two sides to coordinate their actions in order to better accomplish common goals  in such setups  the timing of actions may significantly affect collaborative performance  the present work proposes a new framework for planning multi agent interaction that is based on the representation of tasks sharing a common starting and ending point  as petals in a composite daisy graph  coordination is accomplished through temporal constraints linking the execution of tasks  the planner distributes tasks to the involved parties sequentially  in particular  by considering the properties of the available options at the given moment  the planner accomplishes locally optimal task assignments to agents  optimality is supported by a fuzzy theoretic representation of time intervals which enables fusing temporal information with other quantitative hri aspects  therefore accomplishing a ranking of the available options  the current work aims at a systematic experimental assessment of the proposed framework is pursued  verifying that it can successfully cope with a wide range of hri scenarios   c  2016 the authors  published by elsevier b v  this is an open access article under the cc by license  
 tool body assimilation model considering grasping motion through deep learning we propose a tool body assimilation model that considers grasping during motor babbling for using tools  a robot with tool use skills can be useful in human robot symbiosis because this allows the robot to expand its task performing abilities  past studies that included tool body assimilation approaches were mainly focused on obtaining the functions of the tools  and demonstrated the robot starting its motions with a tool pre attached to the robot  this implies that the robot would not be able to decide whether and where to grasp the tool  in real life environments  robots would need to consider the possibilities of tool grasping positions  and then grasp the tool  to address these issues  the robot performs motor babbling by grasping and nongrasping the tools to learn the robot s body model and tool functions  in addition  the robot grasps various parts of the tools to learn different tool functions from different grasping positions  the motion experiences are learned using deep learning  in model evaluation  the robot manipulates an object task without tools  and with several tools of different shapes  the robot generates motions after being shown the initial state and a target image  by deciding whether and where to grasp the tool  therefore  the robot is capable of generating the correct motion and grasping decision when the initial state and a target image are provided to the robot   c  2017 the authors  published by elsevier b v  
 top corporate brands and the global structure of country brand positioning  an autocm ann approach working on the top 100 interbrand world corporate brands dataset over the 10 years period 2001 10  we analyze the relative positioning of country brands as derived from the structural characteristics of the corresponding portfolios of top corporate brands  we find that the structural complexity of both sector and country variables are not correlated with brand equity  moreover  we apply an innovative ann methodology  autocm  to build the minimum spanning tree  mst  of the multi dimensional similarities among the top corporate brands structures at country level  and carry out a further related analysis in terms of the so called maximum regular graph  mrg   we find that while the usa dominates the ranking of top brands at a global level  it does not have a central positioning in the mst and mrg  whereas germany and other european and far eastern countries do  we show how these results may have significant implications for the strategic intelligence analysis of country and corporate brands  and of their inter relatedness  moreover  we illustrate how autocm qualifies as a new computational approach that usefully expands the toolbox of scholars and analysts in corporate and country branding in a relevant  as yet unexplored direction   c  2016 elsevier ltd  all rights reserved  
 topological invariants can be used to quantify complexity in abstract paintings the abstract art screams of complexity  its visual language purposely creates complex images that are a distorted artist driven vision of the real world  complexity can be recognized from either the composition  form  color  brightness  among other aspects  in this paper we show that it is possible to objectively assess the complexity of abstract paintings by determining the values of the betti numbers associated with the image  these quantities  which are topological invariants  capture the amount of connectivity and spatial distribution of the paint traces  we apply this analysis to a series of abstract paintings  demonstrating that the complexity of jackson pollock paintings produced by his famous dripping technique  is superior compared with many other abstract paintings by different authors  opposed to what was previously discussed considering only fractal properties  the complexity does not simply increase with time  instead  it displays a local maximum at a certain year which coincides with the time when pollock perfected his technique  this tool has been used before to measure complexity in other scientific areas  but not for art assessment   c  2017 elsevier b v  all rights reserved  
 toward a reliable collection of eye tracking data for image quality research  challenges  solutions  and applications image quality assessment potentially benefits from the addition of visual attention  however  incorporating aspects of visual attention in image quality models by means of a perceptually optimized strategy is largely unexplored  fundamental challenges  such as how visual attention is affected by the concurrence of visual signals and their distortions  whether visual attention affected by distortion or that driven by the original scene only should be included in an image quality model  and how to select visual attention models for the image quality application context  remain  to shed light on the above unsolved issues  designing and performing eye tracking experiments are essential  collecting eye tracking data for the purpose of image quality study is so far confronted with a bias due to the involvement of stimulus repetition  in this paper  we propose a new experimental methodology to eliminate such inherent bias  this allows obtaining reliable eye tracking data with a large degree of stimulus variability  in fact  we first conducted 5760 eye movement trials that included 160 human observers freely viewing 288 images of varying quality  we then made use of the resulting eye tracking data to provide insights into the optimal use of visual attention in image quality research  the new eye tracking data are made publicly available to the research community  
 toward use of facial thermal features in dynamic assessment of affect and arousal level automated assessment of affect and arousal level can help psychologists and psychiatrists in clinical diagnoses  and may enable affect aware robot human interaction  this work identifies major difficulties in automating affect and arousal assessment and attempts to overcome some of them  we first analyze thermal infrared images and examine how changes in affect and or arousal level would cause h ae modynamic variations  concentrated along certain facial muscles  these concentrations are used to measure affect arousal induced facial thermal variations  in step 1 of a 2 step pattern recognition schema   between affect  and  between arousal level  variations are used to derive facial thermal features as principal components  pcs  of the facial thermal measurements  the most influential of these pcs are used to cluster the feature space for different affects and subsequently assign a set of thermal features to an affect cluster  in step 2  affect clusters are partitioned into high  medium and mild arousal levels  the distance between a test face vector and the centroids of sub clusters at three arousal levels belonging to a single affective state  identified from step 1  is used to determine the arousal level of the identified affective state  
 towards a catalogue of linguistic graph banks graphs exceeding the formal complexity of rooted trees are of growing relevance to much nlp research  although formally well understood in graph theory  there is substantial variation in the types of linguistic graphs  as well as in the interpretation of various structural properties  to provide a common terminology and transparent statistics across different collections of graphs in nlp  we propose to establish a shared community resource with an open source reference implementation for common statistics  
 towards a product design assessment of visual analytics in decision support applications  a systematic review there is currently an increasing effort to develop visual analytics  va  tools that can support human analytical reasoning and decision making  in the last decade  advances in this field has allowed the application of various kinds of va systems in real world settings  while this represents a promising start from a product design perspective  part of the challenge to the research community is that current va tools have evolved without due consideration of standardized design criteria and processes  accordingly  some questions remain to be addressed on what are the useful  underlying attributes of effective va tools and how their impact can be measured in human product interaction  these considerations indicate a need to identify a specific range of va tools and assess their capabilities through state of the art empirical analysis  to address these issues  we conducted a systematic review of 470 va papers published between 2006 and 2012  we report on the bibliometric techniques  the evaluation attributes and the metrics that were used to sample and analyze the body of literature  the analysis focused mainly on 26 papers that implemented visual analytics decision support tools  the results are presented in the form of six inductively derived design recommendations that  when taken together  uniquely contribute to the fields of product design and visual analytics  
 towards an integrated robotics architecture for social inclusion   the rapp paradigm scientific breakthroughs have led to an increase in life expectancy  to the point where senior citizens comprise an ever increasing percentage of the general population  in this direction  the eu funded rapp project  robotic applications for delivering smart user empowering applications  introduces socially interactive robots that will not only physically assist  but also serve as a companion to senior citizens  the proposed rapp framework has been designed aiming towards a cloud based integrated approach that enables robotic devices to seamlessly deploy robotic applications  relieving the actual robots from computational burdens  the robotic applications  rapps  developed according to the rapp paradigm will empower consumer social robots  allowing them to adapt to versatile situations and materialize complex behaviors and scenarios  the rapp pilot cases involve the development of rapps for the nao humanoid robot and the ang med rollator targeting senior citizens that  a  are technology illiterate   b  have been diagnosed with mild cognitive impairment or  c  are in the process of hip fracture rehabilitation  initial results establish the robustness of rapp in addressing the needs of end users and developers  as well as its contribution in significantly increasing the quality of life of senior citizens   c  2016 elsevier b v  all rights reserved  
 towards circular economy implementation  an agent based simulation approach for business model changes this paper introduces an agent based approach to study customer behavior in terms of their acceptance of new business models in circular economy  ce  context  in a ce customers are perceived as integral part of the business and therefore customer acceptance of new business models becomes crucial as it determines the successful implementation of ce  however  tools or methods are missing to capture customer behavior to assess how customers will react if an organization introduces a new business model such as leasing or functional sales  the purpose of this research is to bring forward a quantitative analysis tool for identifying proper marketing and pricing strategies to obtain best fit demand behavior for the chosen new business model  this tool will support decision makers in determining the impact of introducing new  circular  business models  the model has been developed using an agent based modeling approach which delivers results based on socio demographic factors of a population and customers  relative preferences of product attributes price  environmental friendliness and service orientation  the implementation of the model has been tested using the practical business example of a washing machine  this research presents the first agent based tool that can assess customer behavior and determine whether introduction of new business models will be accepted or not and how customer acceptance can be influenced to accelerate ce implementation  the tool integrates socio demographic factors  product utility functions  social network structures and inter agent communication in order to comprehensively describe behavior on individual customer level  in addition to the tool itself the results of this research indicates the need for systematic marketing strategies which emphasize ce value propositions in order to accelerate customer acceptance and shorten the transition time from linear to circular  agent based models are emphasized as highly capable to fill the gap between diffusion based penetration of information and resulting behavior in the form of purchase decisions  
 towards disappearing user interfaces for ubiquitous computing  human enhancement from sixth sense to super senses the enhancement of human senses electronically is possible when pervasive computers interact unnoticeably with humans in ubiquitous computing  the design of computer user interfaces towards  disappearing  forces the interaction with humans using a content rather than a menu driven approach  thus the emerging requirement for huge number of non technical users interfacing intuitively with billions of computers in the internet of things is met  learning to use particular applications in ubiquitous computing is either too slow or sometimes impossible so the design of user interfaces must be naturally enough to facilitate intuitive human behaviours  although humans from different racial  cultural and ethnic backgrounds own the same physiological sensory system  the perception to the same stimuli outside the human bodies can be different  a novel taxonomy for disappearing user interfaces  duis  to stimulate human senses and to capture human responses is proposed  furthermore  applications of duis are reviewed  duis with sensor and data fusion to simulate the sixth sense is explored  enhancement of human senses through duis and context awareness is discussed as the groundwork enabling smarter wearable devices for interfacing with human emotional memories  
 towards portable natural language interfaces based on case based reasoning natural language interfaces allow non technical people to access information stored in knowledge bases keeping them unaware of the particular structure of the model or the underlying formal query language  early research in the field was devoted to improve the performance of a particular system for a given knowledge base  since adapting the system to new domains usually entailed considerable effort  investigating how to bring portability to nli became a new challenge  in this article  we investigate how case based reasoning could serve to assist the expert in porting the system so as to improve its retrieval performance  our method hits is based on a novel grammar learning algorithm combined with language acquisition techniques that exploit structural analogies  the learner  system  is able to engage the teacher  expert  with clarification dialogues to validate conjectures  hypotheses and deductions  about the language  our method presents the following advantages   i  the customization is naturally defined in the case based cycle   ii  the types of questions the system can deal with are not delimited in advance  and  iii  the system  reasons  about precedent cases to deal with unseen questions  
 towards smart data  improving predictive accuracy in long term football team performance despite recent promising developments with large datasets and machine learning  the idea that automation alone can discover all key relationships between factors of interest remains a challenging task  indeed  in many real world domains  experts can often understand and identify key relationships that data alone may fail to discover  no matter how large the dataset  hence  while pure machine learning provides obvious benefits  these benefits may come at a cost of accuracy  here we focus on what we call smart data  a method which supports data engineering and knowledge engineering approaches that put greater emphasis on applying causal knowledge and real world  facts  to the process of model development  driven by what data are really required for prediction  rather than by what data are available  we demonstrate how we exploited knowledge to develop a model that generates accurate predictions of the evolving performance of football teams based on limited data  the model enables us to predict  before a season starts  the total league points a team is expected to accumulate throughout the season  the results compare favourably against a number of other relevant and different types of models  and are on par with some other models which use far more data  the model results also provide a novel and comprehensive attribution study of the factors most influencing change in team performance  and partly address the cause of the widely accepted favourite longshot bias observed in bookies odds   c  2017 elsevier b v  all rights reserved  
 towards solving the hard problem of consciousness  the varieties of brain resonances and the conscious experiences that they support the hard problem of consciousness is the problem of explaining how we experience qualia or phenomenal experiences  such as seeing  hearing  and feeling  and knowing what they are  to solve this problem  a theory of consciousness needs to link brain to mind by modeling how emergent properties of several brain mechanisms interacting together embody detailed properties of individual conscious psychological experiences  this article summarizes evidence that adaptive resonance theory  or art  accomplishes this goal  art is a cognitive and neural theory of how advanced brains autonomously learn to attend  recognize  and predict objects and events in a changing world  art has predicted that  all conscious states are resonant states   as part of its specification of mechanistic links between processes of consciousness  learning  expectation  attention  resonance  and synchrony  it hereby provides functional and mechanistic explanations of data ranging from individual spikes and their synchronization to the dynamics of conscious perceptual  cognitive  and cognitive emotional experiences  art has reached sufficient maturity to begin classifying the brain resonances that support conscious experiences of seeing  hearing  feeling  and knowing  psychological and neurobiological data in both normal individuals and clinical patients are clarified by this classification  this analysis also explains why not all resonances become conscious  and why not all brain dynamics are resonant  the global organization of the brain into computationally complementary cortical processing streams  complementary computing   and the organization of the cerebral cortex into characteristic layers of cells  laminar computing   figure prominently in these explanations of conscious and unconscious processes  alternative models of consciousness are also discussed   c  2016 the author  published by elsevier ltd  
 towards unravelling the relationship between on body  environmental and emotion data using sensor information fusion approach over the past few years  there has been a noticeable advancement in environmental models and information fusion systems taking advantage of the recent developments in sensor and mobile technologies  however  little attention has been paid so far to quantifying the relationship between environment changes and their impact on our bodies in real life settings  in this paper  we identify a data driven approach based on direct and continuous sensor data to assess the impact of the surrounding environment and physiological changes and emotion  we aim at investigating the potential of fusing on body physiological signals  environmental sensory data and on line self report emotion measures in order to achieve the following objectives   1  model the short term impact of the ambient environment on human body   2  predict emotions based on body sensors and environmental data  to achieve this  we have conducted a real world study  in the wild  with on body and mobile sensors  data was collected from participants walking around nottingham city centre  in order to develop analytical and predictive models  multiple regression  after allowing for possible confounders  showed a noticeable correlation between noise exposure and heart rate  similarly  uv and environmental noise have been shown to have a noticeable effect on changes in electrodermal activity  eda   air pressure demonstrated the greatest contribution towards the detected changes in body temperature and motion  also  significant correlation was found between air pressure and heart rate  finally  decision fusion of the classification results from different modalities is performed  to the best of our knowledge this work presents the first attempt at fusing and modelling data from environmental and physiological sources collected from sensors in a real world setting   c  2017 the authors  published by elsevier b v  
 tracking illicit drug dealing and abuse on instagram using multimodal analysis illicit drug trade via social media sites  especially photo oriented instagram  has become a severe problem in recent years  as a result  tracking drug dealing and abuse on instagram is of interest to law enforcement agencies and public health agencies  however  traditional approaches are based on manual search and browsing by trained domain experts  which suffers from the problem of poor scalability and reproducibility  in this article  we propose a novel approach to detecting drug abuse and dealing automatically by utilizing multimodal data on social media  this approach also enables us to identify drug related posts and analyze the behavior patterns of drug related user accounts  to better utilize multimodal data on social media  multimodal analysis methods including multi task learning and decision level fusion are employed in our framework  we collect three datasets using instagram and web search engine for training and testing our models  experiment results on expertly labeled data have demonstrated the effectiveness of our approach  as well as its scalability and reproducibility over labor intensive conventional approaches  
 traffic safety region estimation based on sfs pca lssvm  an application to highway crash risk evaluation accurate real time crash risk evaluation is essential for making prevention strategy in order to proactively improve traffic safety  quite a number of models have been developed to evaluate traffic crash risk by using real time surveillance data  in this paper  the basic idea of traffic safety region is introduced into highway crash risk evaluation  sequential forward selection  sfs   principal components analysis  pca  and least squares support vector machine  lssvm  are used to estimate the traffic safety region and classify the traffic states  safe condition and unsafe condition   the proposed method works by first extracting state variables from the observed traffic variables  two statistics t 2 and squared prediction error  spe  are calculated by sfs pca and used as the final state variables for traffic state space  next  lssvm is used to estimate the boundary of traffic safety region and identify the traffic states in the traffic state space  to demonstrate the advantage of the proposed method  this study develops two crash risk evaluation models  namely sfs lssvm model and pca lssvm model  based on crash data and non crash data collected on freeway i 880n in alameda  validation results show that the method is of reasonably high accuracy for identifying traffic states  
 training agents with interactive reinforcement learning and contextual affordances in the future  robots will be used more extensively as assistants in home scenarios and must be able to acquire expertise from trainers by learning through crossmodal interaction  one promising approach is interactive reinforcement learning  irl  where an external trainer advises an apprentice on actions to speed up the learning process  in this paper we present an irl approach for the domestic task of cleaning a table and compare three different learning methods using simulated robots  1  reinforcement learning  rl   2  rl with contextual affordances to avoid failed states  and 3  the previously trained robot serving as a trainer to a second apprentice robot  we then demonstrate that the use of irl leads to different performance with various levels of interaction and consistency of feedback  our results show that the simulated robot completes the task with rl  although working slowly and with a low rate of success  with rl and contextual affordances fewer actions are needed and can reach higher rates of success  for good performance with irl it is essential to consider the level of consistency of feedback since inconsistencies can cause considerable delay in the learning process  in general  we demonstrate that interactive feedback provides an advantage for the robot in most of the learning cases  
 transaction security investments in online marketplaces  an analytical examination of financial liabilities with the proliferation of electronic web based and mobile payment systems  the concern about ownership of liabilities for potentially fraudulent transactions has come under the spotlight  this study investigates the consequences of alternative legal regimes on online transaction security  with a specific focus on burden of proof  the two types of online transaction regulations are analyzed by comparing the profits of transacting firms  merchants  and their optimal levels of investment in security  our findings show that in a market where investments in security are highly effective  a legal regime that imposes the burden of proof on the merchant will enable them to achieve higher profits than will a legal regime that places the onus on the customer  in addition  depending on the effectiveness of investments in online transaction security in a given market  even when firms invest less in security  firms will be more profitable under a legal regime that imposes the burden of proof on the merchant  compared with a framework that imposes it on the customer  furthermore  our findings imply that  depending on the effectiveness of investments in online transaction security in a given market  firms could have an incentive to invest more in security and such an action could yield higher social welfare when burden of proof is imposed on firms   c  2016 elsevier b v  all rights reserved  
 transfer learning by prototype generation in continuous spaces in machine learning  learning a task is expensive  many training samples are needed  and it is therefore of general interest to be able to reuse knowledge across tasks  this is the case in aerial robotics applications  where an autonomous aerial robot cannot interact with the environment hazard free  prototype generation is a well known technique commonly used in supervised learning to help reduce the number of samples needed to learn a task  however  little is known about how such techniques can be used in a reinforcement learning task  in this work we propose an algorithm that  in order to learn a new  target  task  first generates new samplesprototypesbased on samples acquired previously in a known  source  task  the proposed approach uses gaussian processes to learn a continuous multidimensional transition function  rendering the method capable of reasoning directly in continuous  states and actions  domains  we base the prototype generation on a careful selection of a subset of samples from the source task  based on known filtering techniques  and transforming such samples using the  little  knowledge acquired in the target task  our experimental evidence gathered in known reinforcement learning benchmark tasks  as well as a challenging quadcopter to helicopter transfer task  suggests that prototype generation is feasible and  furthermore  that the filtering technique used is not as important as a correct transformation model  
 translating research on myoelectric control into clinics are the performance assessment methods adequate  missing an upper limb dramatically impairs daily life activities  efforts in overcoming the issues arising from this disability have been made in both academia and industry  although their clinical outcome is still limited  translation of prosthetic research into clinics has been challenging because of the difficulties in meeting the necessary requirements of the market  in this perspective article  we suggest that one relevant factor determining the relatively small clinical impact of myocontrol algorithms for upper limb prostheses is the limit of commonly used laboratory performance metrics  the laboratory conditions  in which the majority of the solutions are being evaluated  fail to sufficiently replicate real life challenges  we qualitatively support this argument with representative data from seven transradial amputees  their ability to control a myoelectric prosthesis was tested by measuring the accuracy of offline emg signal classification  as a typical laboratory performance metrics  as well as by clinical scores when performing standard tests of daily living  despite all subjects reaching relatively high classification accuracy offline  their clinical scores varied greatly and were not strongly predicted by classification accuracy  we therefore support the suggestion to test myocontrol systems using clinical tests on amputees  fully fitted with sockets and prostheses highly resembling the systems they would use in daily living  as evaluation benchmark  agreement on this level of testing for systems developed in research laboratories would facilitate clinically relevant progresses in this field  
 translating text into pictographs we describe and evaluate a text to pictograph translation system that is used in an online platform for augmentative and alternative communication  which is intended for people who are not able to read and write  but who still want to communicate with the outside world  the system is set up to translate from dutch into sclera and beta  two publicly available pictograph sets consisting of several thousands of pictographs each  we have linked large amounts of these pictographs to synsets or combinations of synsets of cornetto  a lexical semantic database for dutch similar to wordnet  in the translation system  the dutch input text undergoes shallow linguistic analysis and the synsets of the content words are looked up  the system looks for the nearest pictographs in the lexical semantic database and displays the message into pictographs  we evaluated the system and results showed a large improvement over the baseline system which consisted of straightforward string matching between the input text and the filenames of the pictographs  our system provides a clear improvement in the communication possibilities of illiterate people  nevertheless there is room for further improvement  
 translation divergences in chinese english machine translation  an empirical investigation in this article  we conduct an empirical investigation of translation divergences between chinese and english relying on a parallel treebank  to do this  we first devise a hierarchical alignment scheme where chinese and english parse trees are aligned in a way that eliminates conflicts and redundancies between word alignments and syntactic parses to prevent the generation of spurious translation divergences  using this hierarchically aligned chinese english parallel treebank  hacept   we are able to semi automatically identify and categorize the translation divergences between the two languages and quantify each type of translation divergence  our results show that the translation divergences are much broader than described in previous studies that are largely based on anecdotal evidence and linguistic knowledge  the distribution of the translation divergences also shows that some high profile translation divergences that motivate previous research are actually very rare in our data  whereas other translation divergences that have previously received little attention actually exist in large quantities  we also show that hacept allows the extraction of syntax based translation rules  most of which are expressive enough to capture the translation divergences  and point out that the syntactic annotation in existing treebanks is not optimal for extracting such translation rules  we also discuss the implications of our study for attempts to bridge translation divergences by devising shared semantic representations across languages  our quantitative results lend further support to the observation that although it is possible to bridge some translation divergences with semantic representations  other translation divergences are open ended  thus building a semantic representation that captures all possible translation divergences may be impractical  
 transshipment hub selection from a shipper s and freight forwarder s perspective transshipment hub selection becomes increasingly important to the global logistics community  from the perspectives of shippers and freight forwarders  a selection must align with cost control strategy and sustain service reliability across cooperative service providers  this paper assesses the selection with the options of both sea and air transports  and from the influence of country of origin of the company  critical factors of transshipment hub selection  both qualitative and quantitative  are identified through focus group discussions  relative importance of these factors is determined based on collective views of logistics stakeholders  the competitiveness of transshipment hubs is then assessed using an ahp approach  our analysis is based on the historical implementation of direct transportation link policy between mainland china and taiwan  with this empirical work  the finding suggests that even the spawn of other nearby ports with shorter transport distance and closer proximity to cargo sources  there remain overriding factors such as customs regulations   government policies and connectivity that a transshipment hub is preferred   c  2017 elsevier ltd  all rights reserved  
 trends in expert system development  a longitudinal content analysis of over thirty years of expert system case studies research in expert systems  es  has been one of the longest running  and most successful areas of ongoing research within the al field  since the 1980s  many case studies of es applications have been published covering a wide range of functional areas and problem domains  these case studies contain an enormous amount of information about how ess have been developed and how the tools  concepts  and applications have evolved since their inception  this research has painstakingly collected and analysed the content of 311 es case studies dating from 1984 through 2016  a detailed content analysis was performed on this corpus in order to capture as many details as possible from each case  further value was added to the study by using an impact scale to try and gauge the impact or  success  of the resulting application  with such a large sample size  the results are helpful in identifying how es research has evolved and areas for further research   c  2017 elsevier ltd  all rights reserved  
 truth and probability in evolutionary games this paper concerns two composite lewis skyrms signalling games  each consists in a base game that evolves a language descriptive of nature and a metagame that coevolves a language descriptive of the base game and its evolving language  the first composite game shows how a pragmatic notion of truth might coevolve with a simple descriptive language  the second shows how a pragmatic notion of probability might similarly coevolve  each of these pragmatic notions is characterised by the particular game and role that it comes to play in the game  
 twitter data laid almost bare  an insightful exploratory analyser in today s world  social networks and online communities continuously generate tons of data that reflect users  habits  personal interests  opinions and emotions  however  little profit can be gained from such huge raw data collections unless we are able to translate them into useful knowledge  microblogs like twitter have recently attracted a great body of research works to mine useful insights about users interests and preferences in different geographical areas and time periods  indeed  the rather heterogeneous dimensions characterizing twitter data  such as space  time and text content  impose innovative methods in the data mining discovery process  this paper presents tcharm  a data analytics methodology based on cluster analysis and association rule discovery to gain interesting knowledge from large collections of twitter data  tcharm explores tweet collections along the three dimensions characterizing tweets  i e   text content  posting time and place  to support context aware topic trend analysis  to discover groups of tweets with a good cohesion on the three tweet features  tcharm exploits a novel distance measure  taste  which allows driving the clustering task by considering in one step the three tweet features  association rule analysis is then exploited to concisely describe the cluster content with a set of understandable and significant patterns which reveal underlying correlations among frequent topics  tweeting times and places  tcharm can provide useful information to understand the evolution of people s involvement in different topics  across geographical areas and over time  tcharm find applications in various domains by providing a valuable support in decision making to domain experts  the experimental evaluation performed on real datasets demonstrates the effectiveness of the proposed approach in discovering cohesive clusters and actionable knowledge from twitter data   c  2017 elsevier ltd  all rights reserved  
 twitter data models for bank risk contagion a very important and timely area of research in finance is systemic risk modelling  which concerns the estimation of the relationships between different financial institutions  with the aim of establishing which of them are more contagious subject to contagion  the aim of this paper is to develop a systemic risk model which  differently from existing ones  employs not only the information contained in financial market prices  but also big data coming from financial tweets  from a methodological viewpoint  we propose a new framework  based on graphical gaussian models  that can estimate systemic risks with stochastic network models based on two different sources  financial markets and financial tweets  and suggest a way to combine them  using a bayesian approach  from an applied viewpoint  we present the first systemic risk model based on big data  and show that such a model can help predicting the default probability of a bank  conditionally on the others   c  2017 elsevier b v  all rights reserved  
 two evidential data based models for influence maximization in twitter influence maximization is the problem of selecting a set of influential users in the social network  those users could adopt the product and trigger a large cascade of adoptions through the  word of mouth  effect  in this paper  we propose two evidential influence maximization models for twitter social network  the proposed approach uses the theory of belief functions to estimate users influence  furthermore  the proposed influence estimation measure fuses many influence aspects in twitter  like the importance of the user in the network structure and the popularity of user s tweets  messages   in our experiments  we compare the proposed solutions to existing ones and we show the performance of our models   c  2017 elsevier b v  all rights reserved  
 two period supply chain with flexible trade credit contract this paper studies a two period supply chain that consists of a retailer and a supplier  a newsvendorlike retailer is capital constrained and orders products using a supplier s trade credits to satisfy uncertain market demand  most existing studies show that the retailer always postpones payment until the due date  to recall the loans earlier  we present a case in which the supplier  as a stackelberg leader  offers an incentive of a discounted wholesale price in the second order to entice the retailer to choose flexible early payment  the proposed incentive is related to the retailer s early payment time in the first period  in the presence of bankruptcy risks for both the retailer and supplier  we propose a continuous newsvendor model of a two period supply chain to analyze the decisions involved in the flexible trade credit contract  the analytic forms confirm that such an incentive can improve the decentralized supply chain efficiency and decreases the supplier s trade credit risk  the retailer always prefers early payment to payment around the due date to increase revenues  furthermore  the action of paying early might help the retailer adjust cash flow between the two periods  we also find that a revenue sharing contract significantly affects the retailer s payment behavior and supplier s wholesale price  the numerical simulations support our results   c  2016 elsevier ltd  all rights reserved  
 two step benchmarking  setting more realistically achievable targets in dea the models that set the closest targets have made an important contribution to dea as tool for the best practice benchmarking of decision making units  dmus   these models may help defining plans for improvement that require less effort from the dmus  however  in practice we often find cases of poor performance  for which closest targets are still unattainable  for those dmus  we propose a two step benchmarking procedure within the spirit of context dependent dea and that of the models that minimize the distance to the dea efficient frontier  this procedure allows to setting more realistically achievable targets in the short term  in addition  it may offer different alternatives for planning improvements directed towards dea efficient targets  which can be seen as representing improvements in a long term perspective  thus  the sequential approach provides managers with a decision support tool for the design of continuous improvement strategies based on actionable targets  by learning from better practices of others as an expert system  to illustrate  we examine an example which is concerned with the research performance of public spanish universities   c  2017 elsevier ltd  all rights reserved  
 two step classification method based on genetic algorithm for bankruptcy forecasting by present  many models of bankruptcy forecasting have been developed  but this area remains a field of research activity  little is known about the practical application of existing models  in our opinion  this is because the use of existing models is limited by the conditions in which they are developed  another question concerns the factors that can be significant for forecasting  many authors suggest that indicators of the external environment  corporate governance as well as firm size contain important information  on the other hand  the large number of factors does not necessary increase predictive ability of a model  in this paper  we suggest the genetic algorithm based two step classification method  tscm  that allows both selecting the relevant factors and adapting the model itself to application  classifiers of various models are trained at the first step and combined into the voting ensemble at the second step  the combination of random sampling and feature selection techniques were used to ensure the necessary diversity level of classifiers at the first step  the genetic algorithms are applied at the step of features selection and then at the step of weights determination in ensemble  the characteristics of the proposed method have been tested on the balanced set of data  it included 912 observations of russian companies  456 bankrupts and 456 successful  and 55 features  financial ratios and macro micro business environment factors   the proposed method has shown the best accuracy  0 934  value among tested models  it has also shown the most balanced precision recall ratio  it found bankrupts  recall 0 953  and not bankrupts  precision 0 910  rather accurately than other tested models  the ability of method to select the task relevant features has been also tested  excluding the features that are significant for less than 50  of the classifiers in the ensemble improved the all performance metrics  accuracy   0 951  precision 0 932  recall   0 965   so  the proposed method allows to improve the advantages and alleviate the weaknesses inherent in ordinary classifiers  enabling the business decisions support with a higher reliability   c  2017 elsevier ltd  all rights reserved  
 types of  dis  similarities and adaptive mixtures thereof for improved classification learning in this paper  we introduce taxonomies for similarity and dissimilarity measures  respectively  based on their mathematical properties  further  we propose a definition for rank equivalence of  dis similarities regarding given data for prototype based methods  starting with this definition we provide a measure to judge the degree of equivalence  which can be used to compare respective measures as well as to consider the influence of data preprocessing regarding a single  dis similarity measure  in the last part of the paper an adaptive mixture approach of  dis similarity measures for improved classification learning is presented   c  2017 elsevier b v  all rights reserved  
 uncertain expected utility function and its risk premium other than traditional decision theory  this paper employs uncertainty theory to handle indeterminacy  uncertain variables are used to represent uncertain choices  uncertain expected utility function is defined as an increasing function of uncertain choices  several mathematical properties of the uncertain expected utility functions are derived using inverse uncertainty distributions  in order to compare two different choices  the first order dominance and second order dominance via uncertain expected utility functions are introduced  we also investigate risk aversion attitude and risk premium  finally  the relationship between risk premium and risk averse attitude is investigated  
 uncertain mean variance model for project portfolio selection problem with divisibility in this paper  the project portfolio selection problem which considers the divisibility of projects is discussed  in a conventional method  some essential and uncertain variables  such as return  investment and set up cost  are given by estimation of experts due to lack of historical data  to better solve the problem  uncertainty theory is introduced into this study  a mean variance model is proposed to help decision makers find an optimal portfolio and its time schedule  specially  the situation where all the uncertain parameters are normal distribution is discussed  for efficient computation  an equivalent mixed integer linear programming representation is proposed  another mean variance model without considering the divisibility is also proposed to see the contrast effect of net present value  npv   at last  a numerical example under above two scenarios is demonstrated and the effect of risk level to the maximum expected npv is discussed  
 uncertain multi attributes decision making method based on interval number with probability distribution weighted operators and stochastic dominance degree in real world decision making problems  real numbers  random numbers  and interval numbers are often used simultaneously to express the attribute values of alternatives  to solve these uncertain multi attribute decision making problems  we propose a definition of interval number with probability distribution  inpd   this definition gives a uniform form  for real numbers  interval numbers  and random numbers  under certain conditions  an inpd can degrade to one of the three number forms  we then propose three weighted operators that aggregate opinions expressed by inpd  furthermore  we propose a new stochastic dominance degree  sdd  definition based on the idea of almost stochastic dominance to rank two inpd  the new definition overcomes defects in traditional stochastic dominance methods  it takes all stakeholders  preferences into account and can measure both standard and almost sdds  for real numbers and interval numbers  results derived from sdd are consistent with traditional methods  on this basis  a method using inpd weighted operators and sdd is proposed to solve uncertain multi attribute decision making problems  finally  three numerical examples are given to illustrate the applicability and effectiveness of the proposed method   c  2016 elsevier b v  all rights reserved  
 uncertain portfolio selection model considering transaction costs and minimum transaction lots requirement due to the complexity of real security market  sometimes the future security returns can only be valued based on experts  estimations  meanwhile  there are transaction costs and minimum transaction lots requirement in the real transaction process in the trading market  this paper discusses the portfolio selection problem in such a circumstance  security returns are considered as uncertain variables  and a new mean variance model with transaction costs and minimum transaction lots is established  in addition  the impact of minimum transaction lots requirement and transaction costs on optimal portfolio is discussed and a genetic algorithm for solving the optimization model is given  as an illustration  a numerical example is provided  
 uncertain portfolio selection with high order moments in the mean variance skewness kurtosis framework  this paper discusses an uncertain higher order moment portfolio selection problem when security returns are given by experts  evaluations  based on uncertainty theory and the assumption that the security returns are zigzag uncertain variables  an uncertain multi objective portfolio optimization model is proposed by considering the maximizationof both the expected return and skewness of portfolio return while simultaneously minimizing the risk and kurtosis of portfolio return  subsequently  the proposed model is transformed into a single objective programming model by using fuzzy programming approach  in which investor preferences for high moments are incorporated  furthermore  a modified flower pollination algorithm  mfpa  is developed for solution  in which pso in local update strategy  psolus  and dynamic switching probability strategy  dsps  are employed to enhance the local searching and global searching abilities  finally  a numerical example is presented to illustrate the application of the proposed model and solution comparisons are also given to demonstrate the effectiveness of the designed algorithm  
 uncertain project selection model considering sustainability and compatibility project selection is a significant decision making process in most companies  most previous researches focused only on the investment profit and used random variables to determine project parameters  however  in some cases  the distributions of parameters cannot be obtained from similar projects due to the rapid changing environment  so experts  estimations will be adopted  in order to handle inaccurate estimation  an uncertainty theory is applied to solve project selection decision making in this situation  in addition  besides considering financial aspects  the non financial aspects including compatibility  are taken into account  a new project selection model is developed and the deterministic form of the model is given  for the sake of illustration  an example has been presented  this paper aims at proposing a new project appraisal method comprehensively considering financial and non financial aspects  
 uncertain risk aversion this paper discusses the risk aversion within the framework of the uncertainty theory  liu in uncertainty theory  a branch of mathematics for modeling human uncertainty  springer  berlin  2010b   and introduces the notions of uncertain expected utility and uncertain risk premium  in terms of the arrow pratt index  an uncertain version of pratt s theorem is proved  which offers an effective way to make comparisons between different individuals  risk averse attitudes  we suggest that uncertain risk aversion can be used to measure human s risk averse attitudes when uncertainty exists due to lack of the observed data  just as probabilistic risk aversion when sufficient data can be obtained  uncertain risk aversion provides an alternative method to compare the risk aversions between individuals under uncertain situations  
 understanding and overcoming biases in online review systems this study addresses the issues of social influence and selection biases in the context of online review systems  we propose that one way to reduce these biases is to send email invitations to write a review to a random sample of buyers  and not exposing them to existing reviews while they write their reviews  we provide empirical evidence showing how such a simple intervention from the retailer mitigates the biases by analyzing data from four diverse online retailers over multiple years  the data include both self motivated reviews  where the reviewer sees other reviews at the time of writing  and retailer prompted reviews generated by an email invitation to verified buyers  where the reviewer does not see existing reviews  consistent with previous research on the social influence bias  we find that the star ratings of self motivated reviews decrease over time  i e   downward trend   while the star ratings of retailer prompted reviews remain constant  as predicted by theories on motivation  the self motivated reviews are shown to be more negative  lower valence   longer  and more helpful  which suggests that the nature of self motivated and retailer prompted reviews is distinctively different and the influx of retailer prompted reviews would enhance diversity in the overall review system  regarding the selection bias  we found that email invitations can improve the representativeness of reviews by adding a new segment of verified buyers  in sum  implementing appropriate design and policy in online review systems will improve the quality and validity of online reviews and help practitioners provide more credible and representative ratings to their customers   c  2017 elsevier b v  all rights reserved  
 understanding and predicting individual retweeting behavior  receiver perspectives individual retweeting behavior plays a fundamental role in information propagation on social media  while researchers have demonstrated the benefits of individual retweeting behavior to online retailers  a largely uninvestigated issue is the mechanism of individual retweeting behavior  drawing on the social communication theory  we develop an integrated conceptual framework to investigate this issue  an analysis of a panel dataset  which consists of 440 valid users from twitter com across six years  indicates that all the major components of a social communication process have significant impacts on individual retweeting decision   1  information source  e g   source trustworthiness    2  information richness of the stimuli  3  information receiver s topical preference   4  relationship between the source and the receiver  e g   social tie strength   and  5  contextual factors  e g   bandwagon effect   among them  social tie strength and topical relevance with the receiver are the most important factors  followed by information richness of the stimuli and bandwagon effect  however  information source related factors have trivial impacts on individual retweeting decision  we reevaluate the ranking using multiple feature selection methods and confirm the ranking result  prediction performance is carried out between the salient features and the full feature set  finally  we discuss important implications for practice and future research directions   c  2017 elsevier b v  all rights reserved  
 understanding document semantics from summaries  a case study on hindi texts the summary of a document contains words that actually contribute to the semantics of the document  latent semantic analysis  lsa  is a mathematical model that is used to understand document semantics by deriving a semantic structure based on patterns of word correlations in the document  when using lsa to capture semantics from summaries  it is observed that lsa performs quite well despite being completely independent of any external sources of semantics  however  lsa can be remodeled to enhance its capability to analyze correlations within texts  by taking advantage of the model being language independent  this article presents two stages of lsa remodeling to understand document semantics in the indian context  specifically from hindi text summaries  one stage of remodeling is done by providing supplementary information  such as document category and domain information  the second stage of remodeling is done by using a supervised term weighting measure in the process  the remodeled lsa s performance is empirically evaluated in a document classification application by comparing the accuracies of classification to plain lsa  an improvement in the performance of lsa in the range of 4 7  to 6 2  is achieved from the remodel when compared to the plain model  the results suggest that summaries of documents efficiently capture the semantic structure of documents and is an alternative to full length documents for understanding document semantics  
 understanding social causalities behind human action sequences social causality study on human action sequences is useful and important to improve our understandings to human behaviors on online social networks  the redundant indirect causalities and unobserved confounding factors  such as homophily and simultaneity phenomena  contribute to the huge challenges on accurate causal discovery on such human actions  a causal relationship exists between two persons  if the actions of one person are significantly affected by the actions of the other person  while fairly independent of her his own prior actions  in this paper  we design a systematic approach based on conditional independence testing to detect such asymmetric relations  even when there are latent confounders underneath the observational action sequences  technically  a group of asymmetric independence tests are conducted to infer the loose causal directions between action sequence pairs  followed by another group of tests to distinguish different types of relationships  e g   homophily and simultaneity  finally  a causal structure learning method is employed to output pairwise causalities with redundant indirect causalities eliminated  empirical evaluations on simulated data verify the effectiveness and scalability of our proposals  we also present four interesting patterns of causal relations found by our algorithm  on real sina weibo feeds  including two new patterns never reported in previous studies  
 understanding the determinants of online review helpfulness  a meta analytic investigation online consumer reviews can help customers reduce uncertainty and risks faced in online shopping  however  the studies examining the determinants of perceived review helpfulness produce mixed findings  we review extant research about the determinant factors of perceived online review helpfulness  all review related determinants  i e   review depth  review readability  linear review rating  quadratic review rating  review age  and two reviewer related determinants  i e   reviewer information disclosure and reviewer expertise  are found to have inconsistent conclusions on how they affect perceived review helpfulness  we conduct a meta analysis to examine those determinant factors in order to reconcile the contradictory findings about their influence on perceived review helpfulness  the meta analysis results affirm that review depth  review age  reviewer information disclosure  and reviewer expertise have positive influences on review helpfulness  review readability and review rating are found to have no significant influence on review helpfulness  moreover  we find that helpfulness measurement  online review platform  and product type are the three factors that cause mixed findings in extant research  published by elsevier b v  
 understanding the formation of reciprocal hyperlinks betweene e marketplace sellers online sellers in the e marketplace cooperate with each other to increase resources and reduce transaction costs  both of which are crucial to the success of small businesses  a commonly used it enabled strategy is to ally with other online sellers by exchanging hyperlinks  this paper provides theoretical guidance to sellers on how to choose partners to improve reciprocity rates in hyperlink formation  using the resource based view and transaction  cost rationale  we examine the effects of market conditions and seller reputation on reciprocity link formation  using real transaction data from the largest online marketplace in china  the findings indicate that partners are less likely to exchange hyperlinks if the two sellers sharing a link are in highly overlapping markets and are geographically distant from one another  but the two factors weaken each other s negative effects  the study also explores the moderating effect of seller reputation  and finds that the negative effect of market commonality is weakened by seller reputation  the results of this study can be extended to other types of small business cooperation and are also useful to platform operators for designing mechanisms to encourage cooperation among online sellers   c  2017 elsevier b v  all rights reserved  
 uniform random generation and dominance testing for cp nets the generation of preferences represented as cp nets for experiments and empirical testing has typically been done in an ad hoc manner that may have introduced a large statistical bias in previous experimental work  we present novel polynomial time algorithms for generating cp nets with n nodes and maximum in degree c uniformly at random  we extend this result to several statistical cultures commonly used in the social choice and preference reasoning literature  a cp net is composed of both a graph and underlying cp statements  our algorithm is the first to provably generate both the graph structure and cp statements  and hence the underlying preference orders themselves  uniformly at random  we have released this code as a free and open source project  we use the uniform generation algorithm to investigate the maximum and expected flipping lengths  i e   the maximum length over all outcomes o 1  and o 2   of a minimal proof that o 1  is preferred to o 2   using our new statistical evidence  we conjecture that  for cp nets with binary variables and complete conditional preference tables  the expected flipping length is polynomial in the number of preference variables  this has positive implications for the usability of cp nets as compact preference models  
 upper approximation based privacy preserving in online social networks with the advent of the online social network and advancement of technology  people get connected and interact on social network  to better understand the behavior of users on social network  we need to mine the interactions of users and their demographic data  companies with less or no expertise in mining would need to share this data with the companies of expertise for mining purposes  the major challenge in sharing the social network data is maintaining the individual privacy on social network while retaining the implicit knowledge embedded in the social network  thus  there is a need of anonymizing the social network data before sharing it to the third party  the current study proposes to use upper approximation concept of rough sets for developing a solution for privacy preserving social network graph publishing  the proposed algorithm is capable of preserving the privacy of graph structure while simultaneously maintaining the utility or value that can be generated from the graph structure  the proposed algorithm is validated by showing its effectiveness on several graph mining tasks like clustering  classification  and pagerank computation  the set of experiments were conducted on four standard datasets  and the results of the study suggest that the proposed algorithm would maintain the both the privacy of individuals and the accuracy of the graph mining tasks   c  2017 elsevier ltd  all rights reserved  
 usability comparison of conventional direct control versus pattern recognition control of transradial prostheses the goal of this study was to compare the usability of two control schemes for a transradial myoelectric prosthesis  including conventional direct control  dc  and pattern recognition  pr  control  when used by able bodied individuals  three types of response measures were captured to assess the control schemes  including learnability  performance  and cognitive workload  prior research has applied performance and cognitive workload metrics for evaluation of prosthetics  however  workload measures applied in these studies  e g   heart rate  electroencephalography  and respiration rate  have many limitations  this study used eye tracking to compare cognitive load implications of the different control schemes for a two degrees of freedom myoelectric prosthesis  in total  12 participants were assigned to either control condition  six persons each  or perform a clothespin relocation task  results revealed the pr scheme to be more intuitive for users and superior to dc across all response measures  we observed a lower learning percentage  i e   greater learning potential   lower cognitive load  and greater productivity in task performance  this preliminary study illustrates efficacy of using eye tracking based measures of cognitive load and standardize test paradigms for assessment of upper limb prosthetic usability and supports pr prosthetic device control as an intuitive alternative to dc  
 use and communication of probabilistic forecasts probabilistic forecasts are becoming more and more available  how should they be used and communicated  what are the obstacles to their use in practice  we review experience with five problems where probabilistic forecasting played an important role  this leads us to identify five types of potential users  low stakes users  who do not need probabilistic forecasts  general assessors  who need an overall idea of the uncertainty in the forecast  change assessors  who need to know if a change is out of line with expectations  risk avoiders  who wish to limit the risk of an adverse outcome  and decision theorists  who quantify their loss function and perform the decision theoretic calculations  this suggests that it is important to interact with users and consider their goals  cognitive research tells us that calibration is important for trust in probability forecasts and that it is important to match the verbal expression with the task  the cognitive load should be minimized  reducing the probabilistic forecast to a single percentile if appropriate  probabilities of adverse events and percentiles of the predictive distribution of quantities of interest often seem to be the best way to summarize probabilistic forecasts  formal decision theory has an important role but in a limited range of applications   c  2016 wiley periodicals  inc  
 user emotion for modeling retweeting behaviors twitter and other microblogs have rapidly become a significant mean of information propagation in today s web  understanding the main factors that make certain pieces of information spread quickly in these platforms has emerged as a popular topic  therefore  as a simple yet powerful way of disseminating useful information  retweeting has attracted much interest  existing methods for retweets have been conducted for analyzing the social network structure  or understanding the retweeting mechanism  however  little attention is paid to whether users  emotion will affect users  retweeting behavior  in this paper  we study the user emotion problem in a large social network  particularly  we consider users  retweet behaviors and focus on investigating whether users with a certain emotional status will retweet the tweet corresponding with users  current mood from their friends  in order to achieve this goal  we propose a retweeting prediction framework  first  we construct a model of emotion detection via considering two kinds of emotional signals  second  we extract possible retweeted friends and tweets  third  based on the first two steps  we obtain top n retweets using learn to rank method  experiments are performed on two real world datasets  the twitter network and obama mccain debate dataset  with comprehensive measurements  experimental results demonstrate that our retweeting prediction framework has substantial advantages over commonly used retweeting prediction approaches in predicting retweeting behaviors  consider precision in twitter network as an example  for the top n stage  our method can  on average  increase by 15 2  and 11 2  in relation to tweet  sv  and user  ed   respectively  we find that emotion is a vital feature which affects retweetability   c  2017 elsevier ltd  all rights reserved  
 user profiling and behavioral adaptation for hri  a survey to effectively collaborate with people  robots are expected to detect and profile the users they are interacting with  but also to modify and adapt their behavior according to the learned models  the goal of this survey is to focus on the perspective of user profiling and behavioral adaptation  on the one hand  human robot interaction requires a human oriented perception to model and recognize the human actions and capabilities  the intentions and goals behind such actions  and the parameters characterizing the social interaction  on the other hand  the robot behavior should be adapted in its physical movement within the space  in the actions to be selected to achieve collaboration  and by modulating the parameters characterizing the interaction  in this direction  this survey of the current literature introduces a general classification scheme for both the profiling and the behavioral adaptation research topics in terms of physical  cognitive  and social interaction viewpoints   c  2017 elsevier b v  all rights reserved  
 user recommendation in healthcare social media by assessing user similarity in heterogeneous network objective  the rapid growth of online health social websites has captured a vast amount of healthcare information and made the information easy to access for health consumers  e patients often use these social websites for informational and emotional support  however  health consumers could be easily overwhelmed by the overloaded information  healthcare information searching can be very difficult for consumers  not to mention most of them are not skilled information searcher  in this work  we investigate the approaches for measuring user similarity in online health social websites  by recommending similar users to consumers  we can help them to seek informational and emotional support in a more efficient way  methods  we propose to represent the healthcare social media data as a heterogeneous healthcare information network and introduce the local and global structural approaches for measuring user similarity in a heterogeneous network  we compare the proposed structural approaches with the content based approach  results  experiments were conducted on a dataset collected from a popular online health social website  and the results showed that content based approach performed better for inactive users  while structural approaches performed better for active users  moreover  global structural approach outperformed local structural approach for all user groups  in addition  we conducted experiments on local and global structural approaches using different weight schemas for the edges in the network  leverage performed the best for both local and global approaches  finally  we integrated different approaches and demonstrated that hybrid method yielded better performance than the individual approach  conclusion  the results indicate that content based methods can effectively capture the similarity of inactive users who usually have focused interests  while structural methods can achieve better performance when rich structural information is available  local structural approach only considers direct connections between nodes in the network  while global structural approach takes the indirect connections into account  therefore  the global similarity approach can deal with sparse networks and capture the implicit similarity between two users  different approaches may capture different aspects of the similarity relationship between two users  when we combine different methods together  we could achieve a better performance than using each individual method   c  2017 elsevier b v  all rights reserved  
 user segmentation for retention management in online social games this work proposes an innovative model for segmenting online players based on data related to their in game behaviours to support player retention management  this kind of analysis is helpful to explore the potential reasons behind why players leave the game  analyse retention trends  design customised strategies for different player segments  and then boost the overall retention rate  in particular  a new similarity metric which is driven by players  stickiness to the game is developed to cluster players  three feature dimensions  namely engagement features  e g   log in frequency and length of log in time   performance features  e g   level  the number of completed tasks  coins and achievements   and social interactions features  e g   the number of in game friends  whether or not to join a guild  and the guild role   are employed and aggregated to derive the stickiness metric  the applicability and utility of this new segmentation model are illustrated through experiments that are conducted on a realistic mmorpg dataset  the derived results are also discussed and compared against two benchmark models  the results reveal that the new segmentation model not only achieves better clustering performance  but also improves player s lifetime prediction by better distinguishing between loyal customers and churners  the empirical results confirm the effects of social interaction  which is usually underestimated in the current research  on player segmentation  from an operational perspective  the derived results would helpgame developers better understand the different retention behaviour patterns of players  establish effective and customised tactics to retain more players  and boost product revenue   c  2017 published by elsevier b v  
 user vitality ranking and prediction in social networking services  a dynamic network perspective social networking services have been prevalent at many online communities such as twitter com and weibo com  where millions of users keep interacting with each other every day  one interesting and important problem in the social networking services is to rank users based on their vitality in a timely fashion  an accurate ranking list of user vitality could benefit many parties in social network services such as the ads providers and site operators  although it is very promising to obtain a vitality based ranking list of users  there are many technical challenges due to the large scale and dynamics of social networking data  in this paper  we propose a unique perspective to achieve this goal  which is quantifying user vitality by analyzing the dynamic interactions among users on social networks  examples of social network include but are not limited to social networks in microblog sites and academical collaboration networks  intuitively  if a user has many interactions with his friends within a time period and most of his friends do not have many interactions with their friends simultaneously  it is very likely that this user has high vitality  based on this idea  we develop quantitative measurements for user vitality and propose our first algorithm for ranking users based vitality  also  we further consider the mutual influence between users while computing the vitality measurements and propose the second ranking algorithm  which computes user vitality in an iterative way  other than user vitality ranking  we also introduce a vitality prediction problem  which is also of great importance for many applications in social networking services  along this line  we develop a customized prediction model to solve the vitality prediction problem  to evaluate the performance of our algorithms  we collect two dynamic social network data sets  the experimental results with both data sets clearly demonstrate the advantage of our ranking and prediction methods  
 uses and computation of imprecise probabilities from statistical data and expert arguments imprecise probabilities and the theory of coherent previsions offer a rigorous and powerful framework for modelling subjective uncertainty and solving problems of statistical inference  decision making or risk analysis  the paper introduces formulas for computing imprecise probabilities when statistical data and expert arguments are available to a subject  we then show how to use these imprecise probabilities  dialectical probabilities  for comparing the likelihood of events  conditioning on events  comparing decisions  computing optimal decisions or assessing financial risk  we apply the method to stock trading and show in this experiment the added value both of imprecision and of expert arguments derived from technical analysis   c  2016 elsevier inc  all rights reserved  
 using adaptive neuro fuzzy inference system  artificial neural network and response surface method to optimize overall equipment effectiveness for an automotive supplier company total productive maintenance  tpm  is a successful technique that is important in identifying the success and overall effectiveness of the manufacturing process for long term economic viability of business  overall equipment effectiveness  oee  is commonly used and well accepted metric for tpm implementation in many manufacturing industries  as oee is an important performance measure for effectiveness of any equipment  careful analysis is required to know the effect of various components  an attempt has been done in this research to predict the oee by using simulation software  the objective is to identify an optimal oee level to maximize the time between failures and simultaneously minimize the mean repair time  the process of oee is optimized by using response surface methodology  rsm   artificial neural network  ann  and adaptive neuro fuzzy inference system  anfis  to identify optimized zone for maximizing output  finally it is determined the feasible values of inputs using sequential quadratic programming  sqp  algorithm based on trained anfis predictive model  the result from this study can be used the inconvenient impact of the failures on the production process  it is strongly recommended to upgrade the operation management  i e  tpm program  capacity analysis  parts replacement decisions  training programs for technicians operators  spare parts requirement etc  
 using agent transparency to support situation awareness of the autonomous squad member agent transparency has been proposed as a solution to the problem of facilitating operators  situation awareness in human robot teams  sixty participants performed a dual monitoring task  monitoring both an intelligent  autonomous robot teammate and performing threat detection in a virtual environment  the robot displayed four different interfaces  corresponding to information from the situation awareness based agent transparency  sat  model  participants  situation awareness of the robot  confidence in their situation awareness  trust in the robot  workload  cognitive processing  and perceived usability of the robot displays were assessed  results indicate that participants using interfaces corresponding to higher sat level had greater situation awareness  cognitive processing  and trust in the robot than when they viewed lower level sat interfaces  no differences in workload or perceived usability of the display were detected  based on these findings  we observed that transparency has a significant effect on situation awareness  trust  and cognitive processing   c  2017 elsevier b v  all rights reserved  
 using bipartite heterogeneous networks to speed up inductive semi supervised learning and improve automatic text categorization due to the volume of texts available in digital form  the organization  management and knowledge extraction are laborious and frequently impossible to be handled  to automatically cope with these tasks  usually classification models are generated through supervised learning techniques  unfortunately  this type of learning usually demands a huge human effort to label large volume of texts to build accurate classification models  since collecting unlabeled texts is easy and inexpensive in several domains  the generation of classification models through inductive semi supervised learning has been highlighted in recent years  inductive semi supervised learning allows to build a classification model using labeled and unlabeled texts  in this scenario  the goal is to augment the set of labeled documents with unlabeled documents to better discriminate class patterns  hence  fewer texts must be previously labeled  however  semi supervised learning algorithms that consider texts represented in a vector space model usually obtain unsatisfactory classification performances and are surpassed by semi supervised learning algorithms that consider texts represented in a network  nevertheless  despite the classification performances  effective approaches based on networks are generated through the similarities among documents and the classification of a new document are also based on the computation of similarities  this implies to set parameters and compute similarities to both generation the networks and classification of new documents  this approach is not feasible to generate fast responses and consequently to classify a huge volume of texts  in this article  we propose an approach to induce a classification model through semi supervised learning considering text collections represented by bipartite heterogeneous networks  bipartite networks are easily and quickly generated  leading to classification performance equivalent or better than other approaches based on network or vector space model and allows a fast classification of new documents  the results presented in this article demonstrate that the proposed approach is able to  i  speed up semi supervised learning   ii  speed up the classification of new documents and  iii  surpass classification performance of other existing inductive semi supervised learning techniques   c  2017 elsevier b v  all rights reserved  
 using chinese radical parts for sentiment analysis and domain dependent seed set extraction although there has been good progress in english sentiment analysis and resources  studies in english cannot be directly used in chinese owing to the nature of chinese language  previous studies suggested adopting linguistic information  such as grammar and morpheme information  to assist in sentiment analysis for chinese text  however  morpheme based approaches have a problem in identifying seeds  in addition  these methods do not take advantage of radicals in the characters  which contain a great deal of semantic information  a chinese word is composed of one or more characters  each of which has its radical part  we can interpret the partial meaning of a character by analyzing that of the radical in the character  therefore  we not only consider the radical information as the semantic root of a character  but also consider the radical parts between characters in a word as an appropriate linguistic unit for conducting sentiment analysis  in this study  we conducted a series of experiments using radicals as the feature unit in sentiment analysis  using segmented results from part  of speech tools as a meaningful linguistic unit  word  in chinese  we conducted analyses of single feature word  unigram  and frequently seen two words  pointwise mutual information collocated bigrams  through various sentiment analysis measures  it is concluded that radical features could work better than word features and would consume less computing memory and time  an extended study of the extraction of seeds was also conducted  and the results indicated that 50 seed radical features performed well  a cross corpus comparison was also conducted  the results demonstrated that the use of 50 extracted radical features as domain dependent keywords worked better than other sentiment analysis strategies  this study confirmed that radical information could be adopted as a feature unit in sentiment analysis and that domain dependent radicals could be reused in different corpora   c  2017 elsevier ltd  all rights reserved  
 using eye tracking to detect the effects of clutter on visual search in real time display clutter causes decrements in visual search performance and can be a threat to safety and efficiency in complex  data rich domains  addressing the problem requires a means to detect the presence of clutter in real time  predict its effects  and then trigger countermeasures before breakdowns in information search can occur  eye tracking is a promising technique for achieving these goals  however  to date  it has been used almost exclusively offline for display evaluation purposes  the goal of this research was instead to develop and evaluate models that combine eye tracking metrics to detect the effects of clutter early on in the search process  participants were asked to locate targets in a simulated graphics program  three eye tracking metrics scanpath length  mean saccade amplitude  and mean fixation duration were calculated over a 3 second time window  these metrics were then used as input to a set of logistic regression models to predict whether users  response time will be relatively long or short  the accuracy of the models averaged 75  and the true positive rate was above 90   with an ability to predict response time as early as 3 6 s into the visual search task  the results of this study confirm that eye tracking metrics can be used to predict the effects of display clutter in real time  they add to the knowledge base in attention and eye tracking  and they ultimately contribute to the design of adaptive displays that lead to improved operator performance  
 using mixture design and neural networks to build stock selection decision support systems there are three disadvantages of weighted scoring stock selection models  first  they cannot identify the relations between weights of stock picking concepts and performances of portfolios  second  they cannot systematically discover the optimal combination for weights of concepts to optimize the performances  third  they are unable to meet various investors  preferences  this study aimed to more efficiently construct weighted scoring stock selection models to overcome these disadvantages  since the weights of stock picking concepts in a weighted scoring stock selection model can be regarded as components in a mixture  we used the simplex centroid mixture design to obtain the experimental sets of weights  these sets of weights are simulated with us stock market historical data to obtain their performances  performance prediction models were built with the simulated performance data set and artificial neural networks  furthermore  the optimization models to reflect investors  preferences were built up  and the performance prediction models were employed as the kernel of the optimization models  so that the optimal solutions can now be solved with optimization techniques  the empirical values of the performances of the optimal weighting combinations generated by the optimization models showed that they can meet various investors  preferences and outperform those of s p s 500 not only during the training period but also during the testing period  
 using real time cluster configurations of streaming asynchronous features as online state descriptors in financial markets we present a scheme for online  unsupervised state discovery and detection from streaming  multifeatured  asynchronous data in high frequency financial markets  online feature correlations are computed using an unbiased  lossless fourier estimator  a high speed maximum likelihood clustering algorithm is then used to find the feature cluster configuration which best explains the structure in the correlation matrix  we conjecture that this feature configuration is a candidate descriptor for the temporal state of the system  using a simple cluster configuration similarity metric  we are able to enumerate the state space based on prevailing feature configurations  the proposed state representation removes the need for human driven data pre processing for state attribute specification  allowing a learning agent to find structure in streaming data  discern changes in the system  enumerate its perceived state space and learn suitable action selection policies   c  2017 elsevier b v  all rights reserved  
 using self organizing maps to model turnover of sales agents in a call center this paper proposes an approach for modeling employee turnover in a call center using the versatility of supervised self organizing maps  two main distinct problems exist for the modeling employee turnover  first  to predict the employee turnover at a given point in the sales agent s trial period  and second to analyze the turnover behavior under different performance scenarios by using psychometric information about the sales agents  identifying subjects susceptible to not performing well early on  or identifying personality traits in an individual that does not fit with the work style is essential to the call center industry  particularly when this industry suffers from high employee turnover rates  self organizing maps can model non linear relations between different attributes and ultimately find conditions between an individual s performance and personality attributes that make him more predisposed to not remain long in an organization  unlike other models that only consider performance attributes  this work successfully uses psychometric information that describes a sales agent s personality  which enables a better performance in predicting turnover and analyzing potential personality profiles that can identify agents with better prospects of a successful career in a call center  the application of our model is illustrated and real data are analyzed from an outbound call center   c  2017 elsevier b v  all rights reserved  
 using social network analysis to prevent money laundering this research explores the opportunities for the application of network analytic techniques to prevent money laundering  we worked on real world data by analyzing the central database of a factoring company  mainly operating in italy  over a period of 19 months  this database contained the financial operations linked to the factoring business  together with other useful information about the company clients  we propose a new approach to sort and map relational data and present predictive models   based on network metrics   to assess risk profiles of clients involved in the factoring business  we find that risk profiles can be predicted by using social network metrics  in our dataset  the most dangerous social actors deal with bigger or more frequent financial operations  they are more peripheral in the transactions network  they mediate transactions across different economic sectors and operate in riskier countries or italian regions  finally  to spot potential clusters of criminals  we propose a visual analysis of the tacit links existing among different companies who share the same owner or representative  our findings show the importance of using a network based approach when looking for suspicious financial operations and potential criminals   c  2016 elsevier ltd  all rights reserved  
 using thesauruses as a heuristics for mapping values value differences across cultures or social groups are usually framed in terms of different emphases a particular group puts on specific values  for example  western cultures typically prioritize values like autonomy and freedom  whereas east asian cultures put more emphasis on harmony and community  we present an alternative approach for investigating such cultural differences based on thesaurus data bases that reflect the use of value terms in everyday language  we present a methodology that integrates empirical value research with linguistics and novel computer visualization tools to map and visualize value spaces  the maps outline variations in the semantic neighborhood of value terms  based on 460 value terms both for us english and german  we created for each language a map of 78 value classes that were further validated in two surveys  the use of such maps could inform research in three ways  first  by allowing for a controlled variability in the usage of value terms when generating vignettes  second  by indicating potential difficulties when translating value terms that display considerable differences in their semantic neighborhood  and third  as heuristics for better understanding value plurality   c  2016 elsevier b v  all rights reserved  
 using trading mechanisms to investigate large futures data and their implications to market trends market trends have been one of the highly debated phenomena in the financial industries and academia  prior works show the profitability in exploiting transactions via market trend quantification  on the other hand  traders  behaviors and effects on the market trends can be better understood by market trend studies  in general  the trading strategies on the market trend include trend following strategies and contrarian strategies  following the trend  trading strategies exploit the momentum effects  the momentum strategies profit in a long position with the rising market prices  as well as in a short position with the decreasing market prices  on the contrary  the view of contrarian trading strategy is based on the mean reversion property  i e   a long position is taken when the price moves down and a short position is taken when the price moves up  in this paper  we apply the stop loss and stop profit mechanisms to verify the market trends based on two new simple strategies  i e   the buyop  strategy and the buyhi selllo  strategy  we back test these two strategies on the taiwan stock exchange capitalization weighted stock index futures  taiex futures  during the period from may 25  2010 to august 19  2015  we compare the numerical results of its profits and losses through various stop loss thresholds and stop profit thresholds  and verify the existence of the momentum effect via applying these two new trading strategies  besides  we analyze the market trends through the repeated simulations of random trades with the stop loss and stop profit mechanisms  our numerical results reveal that there exist momentum effects in taiex futures  which verifies the market inefficiency and the market profitability in exploiting the market inefficiency  in addition  the techniques of random trades are also applied to the other commodities  such as aapl in nasdaq  ibm  goog in nyse  and  tsmc in tpe  and so on  surprisingly  not all the stocks have the momentum effects  our experimental results show that some stocks or markets are more suitable for the mean reverse strategy  finally  we propose a technique to quantify the momentum effect of a financial market by using jensen shannon divergence  
 utilizing advances in correlation analysis for community structure detection in the era of big data  some data records are interrelated with each other in many areas  such as marketing  management  health care  and education  these interrelated data can be more naturally represented as networks with nodes and edges  inside this type of networks  there is usually a hidden community structure where each community represents a relatively independent functional module  such hidden community structures are useful for many applications  such as word of mouth marketing  promoting decentralized social interactions inside organizations  and searching biological pathways related to various diseases  therefore  how to detect hidden community structures becomes an important task with wide applications  currently  modularity based methods are widely used among many existing community structure detection methods  they detect communities with more internal edges than expected under the null hypothesis of independence  since research in correlation analysis also searches for patterns which occur more than expected under the null hypothesis of independence  this paper proposed a framework of changing the original modularity function according to different existing correlation functions in the correlation analysis research area  such a framework can utilize not only the current but also the future potential research progresses in correlation analysis to advance community detection  in addition  a novel graphical analysis on different modified modularity functions is conducted to analyze their different preferences  which are also validated by our evaluation on both real life and simulated networks  our work to connect modularity based methods with correlation analysis has several significant impacts on the community detection research and its applications to expert and intelligent systems  first  the research progress in correlation analysis can be utilized to define a more effective objective function in community detection for better detection results since different real life applications might need communities with different resolutions  second  any existing research progress for the modularity function  such as the louvain method for speeding up the search and different extensions for overlapping community detection  can be applied in a similar way to the new objective function derived from existing correlation functions  because the new objective function is unified within one framework with the modularity function  third  our framework opens a large unexplored area for the researchers interested in community detection  for example  what is the best heuristic search method for each different objective function  what are the characteristics of each objective function when applied to overlapping community detection  among different extensions to overlapping community detection  which extension is better for each objective function   c  2017 elsevier ltd  all rights reserved  
 utopia in the solution of the bucket order problem this paper deals with group decision making and  in particular  with rank aggregation  which is the problem of aggregating individual preferences  rankings  in order to obtain a consensus ranking  although this consensus ranking is usually a permutation of all the ranked items  in this paper we tackle the situation in which some items can be tied  that is  the consensus shows that there is no preference among them  this problem has arisen recently and is known as the optimal bucket order problem  obop   in this paper we propose two improvements to the standard greedy algorithm usually considered to approach the bucket order problem  the bucket pivot algorithm  bpa   the first improvement is based on the introduction of the utopian matrix  a matrix associated to a pair order matrix that represents the precedences in a collection of rankings  this idealization constitutes a superoptimal solution to the obop  which can be used as an extreme  sometimes feasible  best value  the second improvement is based on the use of several items as pivots to generate the bucket order  in contrast to bpa that only uses a single pivot  the set of items playing the role of decision maker is dynamically created  we analyze separately the contribution of each improvement and also their joint effect  the statistical analysis of the experiments carried out shows that the combined use of both techniques is the best choice  showing a significant improvement in accuracy  17   with respect to the original bpa and providing an important reduction in the variance of the output  moreover  we provide decision rules to help the decision maker to select the right algorithm according to the problem instance   c  2017 elsevier b v  all rights reserved  
 validation of the reasoning of an entry level cyber physical stroke rehabilitation system equipped with engagement enhancing capabilities maintaining and enhancing engagement of patients during stroke rehabilitation exercises are in the focus of current research  in the preceding phase of our research  an entry level cyber physical stroke rehabilitation system  cp srs  has been developed  with the aim of enhancing patients  overall engagement during rehabilitation exercises  as a follow up on the evaluation of the proposed engagement enhancing method and the smart learning mechanism based on the simulated data  this paper presents the validation results of the proposed cp srs system based on real life data  validation included two aspects   i  validation of the effectiveness of the applied stimulation strategies  sss   and  ii  validation of the accuracy of the suggestions of the smart learning mechanism  methodologically  a within subject experiment was designed and completed  eighteen subjects were recruited to participate in the experiments  based on convenience sampling  during the completed game exercises sss were applied individually as well as in combination  the engagement levels of the participants were evaluated and recorded after applying the sss individually and combined  the results were processed by within subject anova in order to test if there was a significant difference between the influences of the different sss and combinations  in addition  training and testing of the smart learning mechanism  slm  was also executed in matlab  the results indicated that several sss significantly increased the engagement of the subjects  and that both neural network based slm and the naive bayes based slm were able to learn and discriminate the effects of the various sss  our conclusion is that they both can be used to assist making decision on effective application of sss  however  applying neural network based slm is more appropriate in the context of increasing engagement   c  2016 elsevier ltd  all rights reserved  
 valuation of convertible bond under uncertain mean reverting stock model different from the classical method of pricing convertible bond under stochastic stock model  we investigate the valuation of convertible bond under uncertain mean reverting stock model which is described by an uncertain differential equation  within the framework of uncertainty theory  we proposed the uncertain pricing method  and present the price formulas of the convertible bond and the callable convertible bond  
 value of the stochastic efficiency in data envelopment analysis this article examines the potential benefits of solving a stochastic dea model over solving a deterministic dea model  it demonstrates that wrong decisions could be made whenever a possible stochastic dea problem is solved when the stochastic information is either unobserved or limited to a measure of central tendency  we propose two linear models  a semi stochastic model where the inputs of the dmu of interest are treated as random while the inputs of the other dmus are frozen at their expected values  and a stochastic model where the inputs of all of the dmus are treated as random  these two models can be used with any empirical distribution in a monte carlo sampling approach  we also define the value of the stochastic efficiency  or semi stochastic efficiency  and the expected value of the efficiency   c  2017 elsevier ltd  all rights reserved  
 value systems for developmental cognitive robotics  a survey this paper surveys value systems for developmental cognitive robotics  a value system permits a biological brain to increase the likelihood of neural responses to selected external phenomena  many machine learning algorithms capture the essence of this learning process  however  computational value systems aim not only to support learning  but also autonomous attention focus to direct learning  this combination of unsupervised attention focus and learning aims to address the grand challenge of autonomous mental development for machines  this survey examines existing value systems for developmental cognitive robotics in this context  we examine the definitions of value used including recent pioneering work in intrinsic motivation as value as well as initialisation strategies for innate values  update strategies for acquired value and the data structures used for storing value  we examine the extent to which existing value systems support attention focus  learning and prediction in an unsupervised setting  the types of robots and applications in which these value systems are used are also examined  as well as the ways that these applications are evaluated  finally  we study the strengths and limitations of current value systems for developmental cognitive robots and conclude with a set of research challenges for this field   c  2016 elsevier b v  all rights reserved  
 variations on the theme of synaptic filtering  a comparison of integrate and express models of synaptic plasticity for memory lifetimes integrate and express models of synaptic plasticity propose that synapses integrate plasticity induction signals before expressing synaptic plasticity  by discerning trends in their induction signals  synapses can control destabilizing fluctuations in synaptic strength  in a feedforward perceptron framework with binary strength synapses for associative memory storage  we have previously shown that such a filter based model outperforms other  nonintegrative  cascade type models of memory storage in most regions of biologically relevant parameter space  here  we consider some natural extensions of our earlier filter model  including one specifically tailored to binary strength synapses and one that demands a fixed  consecutive number of same type induction signals rather than merely an excess before expressing synaptic plasticity  with these extensions  we show that filter based models outperform nonintegrative models in all regions of biologically relevant parameter space except for a small sliver in which all models encode memories only weakly  in this sliver  which model is superior depends on the metric used to gauge memory lifetimes  whether a signal to noise ratio or a mean first passage time   after comparing and contrasting these various filter models  we discuss the multiple mechanisms and timescales that underlie both synaptic plasticity and memory phenomena and suggest that multiple  different filtering mechanisms may operate at single synapses  
 varying spread fuzzy regression for affective quality estimation design of preferred products requires affective quality information which relates to human emotional satisfaction  however  it is expensive and time consuming to conduct a full survey to investigate affective qualities regarding all objective features of a product  therefore  developing a prediction model is essential in order to understand affective qualities on a product  this paper proposes a novel fuzzy regression method in order to predict affective quality and estimate fuzziness in human assessment  when objective features are given  the proposed fuzzy regression also improves on traditional fuzzy regression that simulate only a single characteristic with the resulting limitation that the amount of fuzziness is linear correlated with the independent and dependent variables  the proposed method uses a varying spread to simulate nonlinear and nonsymmetrical fuzziness caused by affective quality assessment  the effectiveness of the proposed method is evaluated by two very different case studies  affective design of an electric iron and image quality assessment  which involve different amounts of data  varying fuzziness  and discrete and continuous data  the results obtained by the proposed method are compared with those obtained by the state of art and the recently developed fuzzy regression methods  the results show that the proposed method can generate better prediction models in terms of three fuzzy criteria  which address both predictions of magnitudes and fuzziness  
 velocity based robotic assistance for refining motor skill training in a complex target hitting task using a bio mimetic trajectory generation model  a pilot study several methods have been proposed for robotic assistance in motor learning training  however  a few major concerns such as the design of the natural motion of the hand of a trainee patient by a robotic device considering the motor ability of the trainee patient and a safe and effective method for teaching a trainee patient  especially in a complex task requiring motor timing  still need to be addressed  this paper proposes velocity based robotic assistance  vra  using a bio mimetic trajectory generation model for motor skill training in a target hitting task considering the concerns mentioned above  in the designed motor task  a trainee has to contact an approaching ball with a racket at the desired time to hit the target on the wall while predicting the behavior of the ball before and after the contact  a set of the racket angle and hand velocity at the contact time is defined as a task related motor skill  in the motor training with vra  the time scale of a primitive reference velocity profile is automatically adapted to individual levels of task related motor skills recorded in the past trials with no robotic assistance  the robotic device then teaches the trainee the customized reference velocity profile to facilitate motor skill training  the effect of vra on motor skill training in a target hitting task was investigated with sixteen healthy volunteers  male university students aged 22 24 years  to verify the concept in this pilot study  the skilled hand movement for the designed motor task was first determined using a set of results measured from four skilled subjects  and the primitive reference velocity profile with multiple peaks was successfully regenerated in a minimum jerk model by utilizing the task related constraints  next  a set of training experiments with and without vra was conducted with twelve subjects who have no experience in the target task  the results for the healthy subjects of this pilot study demonstrated that the proposed vra was efficacious in facilitating the acquisition of task related motor skills  with almost half trials  and in reducing temporal errors of the desired velocity  by approximately 40     c  2017 elsevier b v  all rights reserved  
 visual analysis of pressure in football modern movement tracking technologies enable acquisition of high quality data about movements of the players and the ball in the course of a football match  however  there is a big difference between the raw data and the insights into team behaviors that analysts would like to gain  to enable such insights  it is necessary first to establish relationships between the concepts characterizing behaviors and what can be extracted from data  this task is challenging since the concepts are not strictly defined  we propose a computational approach to detecting and quantifying the relationships of pressure emerging during a game  pressure is exerted by defending players upon the ball and the opponents  pressing behavior of a team consists of multiple instances of pressure exerted by the team members  the extracted pressure relationships can be analyzed in detailed and summarized forms with the use of static and dynamic visualizations and interactive query tools  to support examination of team tactics in different situations  we have designed and implemented a novel interactive visual tool  time mask   it enables selection of multiple disjoint time intervals in which given conditions are fulfilled  thus  it is possible to select game situations according to ball possession  ball distance to the goal  time that has passed since the last ball possession change or remaining time before the next change  density of players  positions  or various other conditions  in response to a query  the analyst receives visual and statistical summaries of the set of selected situations and can thus perform joint analysis of these situations  we give examples of applying the proposed combination of computational  visual  and interactive techniques to real data from games in the german bundesliga  where the teams actively used pressing in their defense tactics  
 visual analytics for supply network management  system design and evaluation we propose a visual analytic system to augment and enhance decision making processes of supply chain managers  several design requirements drive the development of our integrated architecture and lead to three primary capabilities of our system prototype  first  a visual analytic system must integrate various relevant views and perspectives that highlight different structural aspects of a supply network  second  the system must deliver required information on demand and update the visual representation via user initiated interactions  third  the system must provide both descriptive and predictive analytic functions for managers to gain contingency intelligence  based on these capabilities we implement an interactive web based visual analytic system  our system enables managers to interactively apply visual encodings based on different node and edge attributes to facilitate mental map matching between abstract attributes and visual elements  grounded in cognitive fit theory  we demonstrate that an interactive visual system that dynamically adjusts visual representations to the decision environment can significantly enhance decision making processes in a supply network setting  we conduct multi stage evaluation sessions with prototypical users that collectively confirm the value of our system  our results indicate a positive reaction to our system  we conclude with implications and future research opportunities   c  2016 elsevier b v  all rights reserved  
 visual cognitive algorithms for high dimensional data and super intelligence challenges in the long run the cognitive algorithms intend to make super intelligent machines and super intelligent humans  this paper presents a technical process to reach specific aspects of super intelligence that are out of the current human cognitive abilities  these aspects are inabilities to discover patterns in large numeric multidimensional data with a naked eye  this is a long standing problem in data science and modeling in general  the major obstacle is in human inability to see n d data by a naked eye and our needs in visualization means to represent n d data in 2 d losslessly  while these means exist their number and abilities are limited  this paper expands the class of such lossless visual methods  by further developing a new concept of generalized shifted paired coordinates  it shows the advantages of proposed reversible lossless technique by representing real data and by proving mathematical properties   c  2017 elsevier b v  all rights reserved  
 visual decision support for business ecosystem analysis this study comparatively evaluates the effectiveness of three visualization methods  list  matrix  network  and the influence of data complexity  task type  and user characteristics on decision performance in the context of business ecosystem analysis  we pursue this objective using an exploratory study with 14 prototypical users  e g  executives  analysts  investors  and policy makers   the results show that in low complexity contexts  decision performance between visual representations differ but not substantially  in high complexity contexts  however  decision performance suffers significantly if visual representations are not appropriately matched to task types  our study makes several theoretical and practical contributions  theoretically  we extend cognitive fit theory by investigating the impact of business ecosystem task type and complexity  managerially  our study contributes to the relatively underexplored  but emerging area of the design of business ecosystem intelligence tools and presentation of business ecosystem data for the purpose of decision making  we conclude with future research opportunities   c  2016 elsevier ltd  all rights reserved  
 visual knowledge discovery and machine learning for investment strategy knowledge discovery is an important aspect of human cognition  the advantage of the visual approach is in opportunity to substitute some complex cognitive tasks by easier perceptual tasks  however for cognitive tasks such as financial investment decision making this opportunity faces the challenge that financial data are abstract multidimensional and multivariate  i e   outside of traditional visual perception in 2d or 3d world  this paper presents an approach to find an investment strategy based on pattern discovery in multidimensional space of specifically prepared time series  visualization based on the lossless collocated paired coordinates  cpc  plays an important role in this approach for building the criteria in the multidimensional space for finding an efficient investment strategy  criteria generated with the cpc approach allow reducing compressing space using simple directed graphs with beginnings and the ends located in different time points  the dedicated subspaces constructed for time series include characteristics such as bollinger band  difference between moving averages  changes in volume etc  extensive simulation studies have been performed in learning testing context  effective relations were found for one hour eurusd pair for recent and historical data  also the method has been explored for one day eurusd time series n 2d and 3d visualization spaces  the main positive result is finding the effective split of a normalized 3d space on 4 x 4 x 4 cubes in the visualization space that leads to a profitable investment decision   long  short position or nothing   the strategy is ready for implementation in algotrading mode   c  2017 elsevier b v  all rights reserved  
 visual saliency using binary spectrum of walsh hadamard transform and its applications to ship detection in multispectral imagery detection of visual saliency is valuable for applications like robot navigation  adaptive image compression  and object recognition  in this paper  we propose a fast frequency domain visual saliency method by use of the binary spectrum of walsh hadamard transform  wht   the method achieves saliency detection by simply exploiting the wht components of the scene under view  unlike space domain based approaches  our method performs the cortical center surround suppression in frequency domain and thus has implicit biological plausibility  by virtue of simplicity and speed of the wht  the proposed method is very simple and fast in computation  and outperforms existing state of the art saliency detection methods  when evaluated by using the capability of eye fixation prediction  in arduous tasks of ship detection in multispectral imagery  large amount of multispectral data require real time processing and analyzing  as a very fast and effective technique for saliency detection  the proposed method is modified and applied to automatic ship detection in multispectral imagery  the robustness of the method against sea clutters is further proved  
 visualising interactions in bi  and triadditive models for three way tables this paper concerns the visualisation of interaction in three way arrays  it extends some standard ways of visualising biadditive modelling for two way data to the case of three way data  three way interaction is modelled by the parafac method as applied to interaction arrays that have main effects and biadditive terms removed  these interactions are visualised in three and two dimensions  we introduce some ideas to reduce visual overload that can occur when the data array has many entries  details are given on the interpretation of a novel way of representing rank three interactions accurately in two dimensions  the discussion has implications regarding interpreting the concept of interaction in three way arrays  
 visualizing the behavior and some symmetry properties of bayesian confirmation measures bayesian confirmation measures  a special class of interestingness measures  are functions usually adopted in ranking inductive rules generated by data mining methods such as association rule mining  decision trees  rough sets  till now a plethora of measures have been defined in many different ways  identifying and effectively distinguishing among them is a difficult task  in this paper we propose a unified visual approach aimed at comparing and classifying a large subset of bayesian confirmation measures  those satisfying the initial and final probability dependence condition   we first reduce the set of variables in their analytical expression to only two  thus allowing to draw their contour lines on the plane  we observe that two dimensional contour lines plots represent a sort of fingerprints of the confirmation measures and  therefore  this geometric visualization can be used as an effective tool in order to investigate properties and behavior of the measures  we highlight the potential of this approach not only to study known measures but also in order to invent new measures satisfying given required characteristics  we finally define  following the geometry of the plots  a new set of symmetry properties of confirmation measures and describe geometrically four classical symmetries  
 visually grounded meaning representations in this paper we address the problem of grounding distributional representations of lexical meaning  we introduce a new model which uses stacked autoencoders to learn higher level representations from textual and visual input  the visual modality is encoded via vectors of attributes obtained automatically from images  we create a new large scale taxonomy of 600 visual attributes representing more than 500 concepts and 700 k images  we use this dataset to train attribute classifiers and integrate their predictions with text based distributional models of word meaning  we evaluate our model on its ability to simulate word similarity judgments and concept categorization  on both tasks  our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute based input  
 volatility of main metals forecasted by a hybrid ann garch model with regressors in this article  we analyze volatility forecasts associated with the price of gold  silver  and copper  three of the most important metals in the world market  first  a group of garch models are used to forecast volatility  including explanatory variables like the us dollar euro and us dollar yen exchange rates  the oil price  and the chinese  indian  british  and american stock market indexes  subsequently  these model predictions are used as inputs for a neural network in order to analyze the increase in hybrid predictive power  the results obtained show that for these three metals  using the hybrid neural network model increases the forecasting power of out of sample volatility  in order to optimize the results  we conducted a series of sensitizations of the artificial neural network architecture and analyses for different cases  finding that the best models to forecast the price return volatility of these main metals are the ann garch model with regressors  due to the heteroscedasticity in the financial series  the loss function used is heteroskedasticity adjusted mean squared error  hmse   and to test the superiority of the models  the model confidence set is used   c  2017 elsevier ltd  all rights reserved  
 vr based operating modes and metaphors for collaborative ergonomic design of industrial workstations the aim of this paper is to evaluate two new operating design modes and their collaborative metaphors enabling two actors  a design engineer and an end user  to work jointly in a collaborative virtual environment for workstation design  the two operating design modes that correspond to two different design paradigms include  direct design mode  the design engineer manipulates objects while the end user specifies their activity constraints  and supervised design mode  the end user manipulates objects while the design engineer specifies their process constraints   the corresponding collaborative metaphors enable the two actors to manipulate objects while one  the design engineer  can express process constraints and the other  the end user  can express activity constraints  the usability evaluation of the modes and metaphors consisted in placing a workstation design element on a digital workstation mock up where a trade off was to be found between process constraints and activity constraints  the results show that if we consider the completion time as the most important criterion to achieve  the direct design mode must be favoured  otherwise  if the activity constraints are predominant  the supervised design mode must be favoured  in any case  the addition of an ergonomist s point of view is supported and warranted in order to enhance the effectiveness of the system  
 wavelet based gender detection on off line handwritten documents using probabilistic finite state automata detection of gender from handwriting of an individual presents an interesting research problem with applications in forensic document examination  writer identification and psychological studies  this paper presents an effective technique to predict the gender of an individual from off line images of handwriting  the proposed technique relies on a global approach that considers writing images as textures  each handwritten image is converted into a textured image which is decomposed into a series of wavelet sub bands at a number of levels  the wavelet sub bands are then extended into data sequences  each data sequence is quantized to produce a probabilistic finite state automata  pfsa  that generates feature vectors  these features are used to train two classifiers  artificial neural network and support vector machine to discriminate between male and female writings  the performance of the proposed system was evaluated on two databases  quwi and mshd  within a number of challenging experimental scenarios and realized classification rates of up to 80   the experimental results show the superiority of the proposed technique over existing techniques in terms of classification rates   c  2016 elsevier b v  all rights reserved  
 wearable mobile based emotional response monitoring system for drivers negative emotional responses are a growing problem among drivers  particularly in countries with heavy traffic  and may lead to serious accidents on the road  measuring stress and fatigue induced emotional responses by means of a wireless  wearable system would be useful for potentially averting roadway tragedies  the focus of this study was to develop and verify an emotional response monitoring paradigm for drivers  derived from electromyography signals of the upper trapezius muscle  photoplethysmography signals of the earlobe  as well as inertialmotion sensing of the head movement  the relevant sensors were connected to a microcontroller unit equipped with a bluetooth enabled low energy module  which allows the transmission of those sensor readings to a mobile device in real time  a mobile device application was then used to extract the data from the sensors and to determine the driver s current emotion status  via a trained support vector machine  svm   the emotional response paradigm  tested in ten subjects  consisted of 10 min baseline  5 min prestimulus  and 5 min poststimulus measurements  emotional responses were categorized into three classes  relaxed  stressed  and fatigued  the analysis integrated a total of 36 features to train the svm model  and the final stimulus results revealed a high accuracy rate  99 52    the proposed wearable system could be applied to an intelligent driver s safety alert system  to use those emotional responses to prevent accidents affecting themselves and or other innocent victims  
 web design attributes in building user trust  satisfaction  and loyalty for a high uncertainty avoidance culture in this study  we attempt to evaluate the user preferences for web design attributes  i e   typography  color  content quality  interactivity  and navigation  to determine the trust  satisfaction  and loyalty for uncertainty avoidance cultures  content quality and navigation have been observed as strong factors in building user trust with e commerce websites  in contrast  interactivity  color  and typography have been observed as strong determinants of user satisfaction  the most relevant and interesting finding is related to typography  which has been rarely discussed in e commerce literature  a questionnaire was designed to collect data to corroborate the proposed model and hypotheses  furthermore  the partial least squares method was adopted to analyze the collected data from the students who participated in the test  n   558   finally  the results of this study provide strong support to the proposed model and hypotheses  therefore  all the web design attributes were observed as important design features to develop user trust and satisfaction for uncertainty avoidance cultures  although both factors seem to be relevant  the relationship between trust and loyalty was observed to be stronger than between satisfaction and loyalty  thus  trust seems to be a stronger determinant of loyalty for risk high uncertainty avoidance cultures  
 welfare effects of market making in continuous double auctions we investigate the effects of market making on market performance  focusing on allocative efficiency as well as gains from trade accrued by background traders  we employ empirical simulationbased methods to evaluate heuristic strategies for market makers as well as background investors in a variety of complex trading environments  our market model incorporates private and common valuation elements  with dynamic fundamental value and asymmetric information  in this context  we compare the surplus achieved by background traders in strategic equilibrium  with and without a market maker  our findings indicate that the presence of the market maker strongly tends to increase total welfare across various environments  market maker profit may or may not exceed the welfare gain  thus the effect on background investor surplus is ambiguous  we find that market making tends to benefit investors in relatively thin markets  and situations where background traders are impatient  due to limited trading opportunities  the presence of additional market makers increases these benefits  as competition drives the market makers to provide liquidity at lower price spreads  a thorough sensitivity analysis indicates that these results are robust to reasonable changes in model parameters  
 what is life in everyday understanding  a focus group study on lay perspectives on the term life the philosophical and scientific debate about definitions of life as we know it and its value is very diverse  how do non biologists characterize these issues  we held focus groups to shed light on the role of the term life in laypeople s understanding  results show that features of early childhood cognition dominate the understanding of the term life even in adulthood  textbook knowledge and definitions derived from specific knowledge systems and beliefs are of minor importance  for an ethical differentiation between life forms the ability to feel and to suffer is seen as the crucial criterion  we conclude that lay perspectives on the concept of life can shape a normative discourse on existing as well as on new life forms in a crucial way  in addition  these perspectives may also strongly influence the expectations towards the life as it could be that is brought forward by the artificial life community  while some concepts like metabolism exist both in scientific and in everyday reasoning as criteria for life  the normative discussion on life is dominated by such ideas as a hierarchical order of living kinds  which emphasize easy to think concepts of a moral differentiation  these can also form a basis for the moral standing of artificial life  
 what makes classification trees comprehensible  classification trees are attractive for practical applications because of their comprehensibility  however  the literature on the parameters that influence their comprehensibility and usability is scarce  this paper systematically investigates how tree structure parameters  the number of leaves  branching factor  tree depth  and visualisation properties influence the tree comprehensibility  in addition  we analyse the influence of the question depth  the depth of the deepest leaf that is required when answering a question about a classification tree   which turns out to be the most important parameter  even though it is usually overlooked  the analysis is based on empirical data that is obtained using a carefully designed survey with 98 questions answered by 69 respondents  the paper evaluates several tree comprehensibility metrics and proposes two new metrics  the weighted sum of the depths of leaves and the weighted sum of the branching factors on the paths from the root to the leaves  that are supported by the survey results  the main advantage of the new comprehensibility metrics is that they consider the semantics of the tree in addition to the tree structure itself   c  2016 elsevier ltd  all rights reserved  
 when one brain needs to learn from another  the case of observational facilitation of list learning in macaques given a neural system that equips an agent to attempt to carry out some learning task based on its own interaction with the non social world  what extra neural machinery is required to enable learning to be facilitated by  repeated  observation of successful completion of that task by another agent  we provide one answer by exploiting an understanding of data and models on mirror neurons to extend a prior neurocomputational model of list learning by macaques  scp1  which addressed results of the simultaneous chaining paradigm  scp  to yield a new model  scp2  that addresses social facilitation  observational learning  effects based on the scp  scp2 extends scp1 by adding action recognition elements and  vicarious  reward processing elements to facilitate performance following observation of a demonstrator  our simulations suggest prior experience is important for the observed facilitation and serves to bridge the  separately collected  neurophysiological and behavioral data  crucially  the inner workings of scp1  as distinct from its successful performance on the scp dataset  are irrelevant  what is crucial is the wrapping of the do it alone model to support social facilitation  this study provides an example of dyadic brain modeling  simulating brain models of interacting agents  in the case in which the behavior of only one member of the dyad is affected by the behavior of the other  
 who is this   identity and presence in robot mediated communication because a telepresence robot is intended for use in telecommunications  conveying the presence of a remote sender is an important issue  even though certain characteristics of robots such as identity could be effective at generating a sense of presence  this risks yielding a distortion of presence in the remote sender  thus  in order to find effective ways to increase presence  we executed an experiment comparing a telepresence robot with high identity and a telepresence robot with low identity  the 60 participants in this study engaged in a video call with a remote sender using either a telepresence robot with high identity or a telepresence robot with low identity  the results showed that participants felt more remote sender presence when interacting with the telepresence robot with low identity than they did with the one with high identity  on the other hand  when a telepresence robot has high identity  participants felt more presence toward the robot than they did toward the one with low identity  in the second study  n   72   participants experienced two types of telepresence robots  identity level  a telepresence robot with high identity vs  a telepresence robot with low identity  with two types of remote senders  number of remote senders  single remote sender vs  multiple remote senders   the identity level of the robot and the number of remote senders affected the presence of the remote sender  telepresence  and the presence of the robot  we discuss in detail the implications for the design of telepresence robots in terms of increasing presence   c  2016 elsevier b v  all rights reserved  
 whole brain functional connectome based multivariate classification of post stroke aphasia patients with post stroke aphasia  psa  show abnormalities of intrinsic functional connectivity  however  whether the whole brain functional connectome can be used as a feature to distinguish patients with psa from healthy controls is poorly understood  we aim to distinguish psa patients from controls using whole brain functional connectivity based multivariate pattern analysis  these features would be helpful in the understanding of the pathophysiology of psa  in the present study  resting state functional magnetic resonance images  fmri  were acquired in 17 patients with psa and 20 age  and sex matched healthy controls  we used functional connectivity pattern and linear support vector machine to classify two groups  the results showed that the accuracy of classification reached to 86 5   sensitivity reached to 76 5   and specificity reached to 95 0   in addition  consensus connections were mainly located in the fronto parietal  auditory  sensory motor  and visual networks  furthermore  the right rolandic operculum contributed the highest weight  we suggest that whole brain functional connectivity could be used as a potential neuromarker to distinguish psa patients from controls   c  2017 elsevier b v  all rights reserved  
 whose online reviews to trust  understanding reviewer trustworthiness and its impact on business why do top movie reviewers receive invitations to exclusive screenings  even popular technology bloggers get free new gadgets for reviewing  how much do these reviewers really matter for businesses  while the impact of online reviews on sales of products and services has been well established  not much literature is available on impact of reviewers for businesses  source credibility theory expounds how a communication s persuasiveness is affected by the perceived credibility of its source  so  perceived trustworthiness of reviewers should influence acceptance of reviews  and consequently should have an indirect impact on sales  using local business review data from yelp com  this paper successfully tests the premise that reviewer trustworthiness positively moderates the impact of review based online reputation on business patronages  given the importance of reviewer trustworthiness  the next logical question is   how to estimate and predict it  if no direct proxy is available  we propose a theoretical model with several reviewer characteristics  positivity  involvement  experience  reputation  competence  sociability  affecting reviewer trustworthiness  and find all factors to be significant using the robust regression method  further  using these factors  a predictive classification of reviewers into high and low level of potential trustworthiness is done using logistic regression with nearly 83  accuracy  our findings have several implications   firstly  businesses should focus on building a good review based online reputation  secondly  they should encourage top trustworthy reviewers to review their products and services  and thirdly  trustworthy reviewers could be identified and ranked using reviewer characteristics   c  2017 elsevier b v  all rights reserved  
 why post cognitivism does not  necessarily  entail anti computationalism post cognitive approaches to cognitive science  such as enactivism and autopoietic theory  are typically assumed to involve the rejection of computationalism  we will argue that this assumption results from the conflation of computation with the notion of representation  which is ruled out by the post cognitivist rejection of cognitive realism  however  certain theories of computation need not invoke representation  and are not committed to cognitive realism  meaning that post cognitivism need not necessarily imply anti computationalism  finally  we will demonstrate that autopoietic theory shares a mechanistic foundation with these theories of computation  and is therefore well equipped to take advantage of these theories  
 word sense induction with closed frequent termsets the article is devoted to the problem of word sense induction  we propose a method for inducing senses from a raw text corpus  the proposed sense induction algorithm  called sensesearcher  or sns  is based on closed frequent sets  and as a result  it provides a multilevel sense representation  to a large extent  it is a knowledge poor approach  as it does not need any kind of structured knowledge base about senses and there is no deep language knowledge embedded  by discovering a hierarchy of senses  the algorithm enables identifying subsenses  fine grained senses   sns discovers not only frequent  dominating  senses but also infrequent ones  dominated   the method was evaluated in two main areas  lexicography and information retrieval  with the use of the sns algorithm  we provide a tool able to induce from a textual corpus a structure of senses  with a varying number of granularity levels  in the area of information retrieval  sns can be used for clustering search result  according to the discovered senses  the experiments have shown that sns performs better than the methods participating in the semeval2013 wsi task 11 competition  and most of the known search result clustering methods  
 your memory is working against you  how eye tracking and memory explain habituation to security warnings security warnings are critical to the security of end users and their organizations  often representing the final defense against an attack  because warnings require users to make a contextual judgment  it is critical that they pay close attention to warnings  however  research shows that users routinely disregard them  a major factor contributing to the ineffectiveness of warnings is habituation  the decreased response to a repeated warning  although previous research has identified the problem of habituation  the phenomenon has only been observed indirectly through behavioral measures  therefore  it is unclear how habituation develops in the brain in response to security warnings  and how this in turn influences users  perceptions of these warnings  this paper contributes by using eye tracking to measure the eye movement based memory  emm  effect  a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli  we show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions  using guidelines from the warning science literature  we design a polymorphic warning artifact which repeatedly changes its appearance  we demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings  offering an effective solution for practice  finally  our results highlight the value of applying neuroscience to the domain of information security behavior   c  2016 elsevier b v  all rights reserved 
